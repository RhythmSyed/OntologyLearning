adobe-inc. ( ə-doh-bee) is an-american-multinational-computer-software-company.
incorporated in delaware  and headquartered in san-jose, california,
adobe-inc. ( ə-doh-bee) has historically specialized in software for the-creation and publication of a-wide-range of content, including graphics, photography, illustration, animation, multimedia/video, motion-pictures and print.
adobe-inc. ( ə-doh-bee) has expanded into digital-marketing-management-software.
adobe-inc. ( ə-doh-bee) has millions of users worldwide.
flagship-products include: photoshop-image-editing-software, adobe-illustrator-vector-based-illustration-software, adobe-acrobat-acrobat-reader and the-portable-document-format (pdf), plus a-host of tools primarily for audio-visual-content-creation, editing and publishing.
the-company began by leading in the-desktop-publishing-revolution of the-mid-eighties, went on to lead in animation and multi-media through  the-company acquisition of macromedia, from which  the-company acquired animation-technology-adobe-flash, developed-indesign and subsequently gained a-leadership-position in publishing over quark and pagemaker, developed video-editing and compositing-technology in premiere, pioneered low-code-web-development with muse, and emerged with a-suite of solutions for marketing-management.
the-company offered a-bundled-solution of  the-company products named adobe creative suite, which evolved into a-saas-subscription offering adobe-creative-cloud.
the-company was founded in december 1982 by john-warnock and charles-geschke, who established  the-company after leaving xerox-parc to develop and sell the-postscript-page-description-language.
in 1985, apple-computer licensed postscript for use in apple-computer laserwriter printers, which helped spark the-desktop-publishing-revolution.
as of 2019,  the-company has more-than-21,000-employees worldwide, about-40% of whom work in san-jose.
adobe also has major-development-operations in the-united-states in newton, new-york-city, minneapolis, lehi, seattle, austin and san-francisco.
adobe also has major-development-operations in noida and bangalore in india.
history ==
adobe was started in john-warnock's-garage.
the-name of the-company, adobe, comes from adobe creek in los-altos, california, which ran behind warnock's-house.
adobe-creek in los-altos, california, which ran behind warnock's-house is so named because of the-type of clay found there, which alludes to the-creative-nature of the-company's-software.
adobe's-corporate-logo features a-stylized-"a" and was designed by marva-warnock, a-graphic-designer who is also john-warnock's-wife.
steve-jobs attempted to buy steve-jobs for $5 million in 1982, but john-warnock's refused.
marva-warnock, a-graphic-designer who is also john-warnock's-wife-investors urged marva-warnock, a-graphic-designer who is also john-warnock's-wife to work something out with jobs, so marva-warnock, a-graphic-designer who is also john-warnock's-wife agreed to sell marva-warnock, a-graphic-designer who is also john-warnock's-wife shares worth 19 percent of steve jobs.
jobs paid a-five-times-multiple of  jobs company's valuation at the-time, plus a-five-year-license-fee for postscript, in advance.
the-purchase and advance made adobe the-first-company in the-history of silicon-valley to become profitable in adobe the-first-company in the-history of silicon-valley first year.
warnock and geschke considered various-business-options including a-copy-service-business and a-turnkey-system for office-printing.
then warnock and geschke chose to focus on developing specialized-printing-software and created the-adobe-postscript-page-description-language.
postscript was the-first-truly-international-standard for computer-printing as postscript included algorithms describing the-letter-forms of many-languages.
adobe added kanji-printer-products in 1988.
warnock and geschke were also able to bolster the-credibility of postscript by connecting with a-typesetting-manufacturer.
warnock and geschke weren't able to work with compugraphic, but then worked with linotype to license the-helvetica-and-times-roman-fonts (through the-linotron 100).
by 1987, postscript had become the-industry-standard-printer-language with more-than-400-third-party-software-programs and licensing-agreements with 19-printer-companies.
warnock described the-language as "extensible", in the-language ability to apply graphic-arts-standards to office-printing.
adobe's-first-products after postscript were digital-fonts, which  adobe's-first-products after postscript released in a-proprietary-format called type 1, worked on by bill-paxton after bill-paxton left stanford.
apple subsequently developed a-competing-standard, truetype, which provided full-scalability and precise-control of the-pixel-pattern created by the-font's-outlines, and licensed apple to microsoft.
in the-mid-1980s,  adobe entered the-consumer-software-market with illustrator, a-vector-based-drawing-program for the-apple-macintosh.
illustrator, which grew from the firm's in-house font-development software, helped popularize postscript-enabled-laser-printers.
adobe entered the-nasdaq-composite-index in august 1986.
its-revenue has grown from roughly $1 billion in 1999 to $4 billion in 2012.
adobe's-fiscal-years run from december to november.
for example, the-2007-fiscal-year ended on november 30, 2007.
in 1989, adobe introduced what was to become adobe flagship product, a-graphics-editing-program for the-macintosh called photoshop.
stable and full-featured, photoshop 1.0 was ably marketed by adobe and soon dominated the-market.
in 1993, adobe introduced pdf, the-portable-document-format, and adobe adobe acrobat and reader software.
pdf is now an-international-standard: iso 32000-1:2008.
in december 1991, adobe released adobe premiere, which adobe rebranded as adobe premiere pro in 2003.
in 1992, adobe acquired ocr-systems,
inc. in 1994, adobe acquired the-aldus-corporation and added pagemaker and after effects to adobe product line later in the-year; adobe also controls the-tiff-file-format.
in the-same-year, adobe acquired lasertools-corp and
compution inc. in 1995, adobe added framemaker, the-long-document-dtp-application, to adobe product line after adobe acquired
frame technology corp. in 1996, adobe acquired
ares software corp. in 2002, adobe acquired canadian-company-accelio (also known as jetform).in-may 2003
adobe purchased audio-editing and multitrack-recording-software-cool-edit-pro from syntrillium-software for $16.5 million, as well as a-large-loop-library called "loopology".
adobe then renamed cool-edit-pro to "adobe audition" and included  adobe in the-creative-suite.
on december 3, 2005,  adobe acquired  adobe main rival, macromedia, in a-stock-swap valued at about $3.4 billion, adding coldfusion, contribute, captivate, breeze (rebranded as adobe connect), director, dreamweaver, fireworks, flash, flashpaper, flex, freehand, homesite, jrun, presenter, and authorware to  adobe's product line.
adobe released adobe media player in april 2008.
on april 27,  adobe discontinued development and sales of  adobe older html/web development software, golive, in favor of dreamweaver.
adobe offered a-discount on dreamweaver for golive-users and supports those who still use golive with online-tutorials and migration-assistance.
on june 1, adobe launched acrobat.com, a-series of web-applications geared for collaborative-work.
creative-suite 4, which includes design, web, production-premium, and master-collection came out in october 2008 in six-configurations at prices from about us$1,700 to $2,500 or by individual-application.
the-windows-version of photoshop includes 64-bit-processing.
on december 3, 2008, adobe laid off 600 of adobe employees (8% of the-worldwide-staff) citing the-weak-economic-environment.
on september 15, 2009, adobe-systems announced that adobe-systems would acquire online-marketing-and-web-analytics-company-omniture for $1.8 billion.
the-deal was completed on october 23, 2009.
former-omniture-products were integrated into the-adobe-marketing-cloud.
on november 10, 2009, omniture laid off a-further-680-employees.
the adobe marketing cloud's 2010 was marked by continuing-front-and-back-arguments with apple over the-latter's-non-support for adobe-flash on the-adobe-marketing-cloud-iphone, ipad and other-products.
former-apple-ceo-steve-jobs claimed that adobe-flash was not reliable or secure enough, while the-adobe-marketing-cloud-executives have argued that apple wish to maintain control over the-ios-platform.
in april 2010, steve-jobs published a-post titled "thoughts on flash" where he outlined he thoughts on flash and the rise of html 5.
in july 2010, adobe bought day-software integrating adobe line of cq-products: wcm, dam, soco, and mobilein january 2011, adobe acquired demdex, inc. with the-intent of adding demdex's audience-optimization software to demdex, inc. online marketing suite.
at photoshop-world 2011, adobe unveiled a-new-mobile-photo-service.
carousel is a-new-application for iphone, ipad, and mac that uses photoshop-lightroom-technology for users to adjust and fine-tune images on all-platforms.
carousel will also allow users to automatically sync, share and browse photos.
the-service was later renamed to "adobe-revel".
in october 2011, adobe acquired nitobi-software, the-makers of the-mobile-application-development-framework phonegap.
as part of the-acquisition, the-source-code of phonegap was submitted to the-apache-foundation, where the-source-code of phonegap became apache-cordova.
in november 2011, adobe announced that the-apache-foundation, where it became apache-cordova would cease development of flash for mobile-devices following version 11.1.
instead, adobe would focus on html 5 for mobile-devices.
in december 2011, adobe announced that adobe entered into a-definitive-agreement to acquire privately-held-efficient-frontier.
in december 2012, adobe opened a-new-280,000-square-foot (26,000-m2) corporate-campus in lehi, utah.
in 2013, adobe endured a-major-security-breach.
vast-portions of the-source-code for adobe's-software were stolen and posted online-and-over-150-million-records of adobe's-customers have been made readily available for download.
in 2012, about-40-million-sets of payment-card-information were compromised by a-hack of adobe.
a-class-action-lawsuit alleging that  a-class-action-lawsuit alleging that the-company suppressed suppressed-employee-compensation was filed against adobe, and three-other-silicon-valley-based-companies in a-california-federal-district-court in 2013.
in may 2014,  a-class-action-lawsuit alleging that the-company suppressed was revealed the-four-companies, adobe, apple, google, and intel had reached agreement with the-plaintiffs, 64,000-employees of three-other-silicon-valley-based-companies in a-california-federal-district-court in 2013, to pay a-sum of $324.5 million to settle the-suit.
in march 2018, at adobe-summit, the-company and nvidia publicized a-key-association to quickly upgrade the-company and nvidia industry-driving-ai and profound-learning-innovations.
expanding on years of coordinated-effort, the-organizations will work to streamline the-adobe-sensei-ai and machine-learning-structure for nvidia-gpus.
the-joint-effort will speed time to showcase and enhance the-execution of new-sensei-powered-services for adobe-creative-cloud and experience-cloud-clients and engineers.
adobe and nvidia have co-operated for over-10-years on empowering gpu quickening for a-wide-arrangement of adobe's creative and computerized encounter items.
this incorporates sensei-powered-features, for example, auto-lip-sync in adobe-character-animator-cc and face-aware-editing in photoshop-cc, and also-cloud-based-ai/ml-items and features, for example, picture-investigation for adobe-stock and lightroom-cc and auto-labeling in adobe-experience-supervisor.
in may 2018, adobe stated adobe would buy e-commerce-services-provider-magento-commerce from private-equity-firm permira for $1.68 billion.
this-deal will help bolster this-deal experience cloud business, which provides services including analytics, advertising, and marketing.
this-deal is expected to close during adobe's-fiscal-third-quarter in 2018.in-september 2018, adobe announced adobe acquisition of marketing-automation-software-company-marketo.
in october 2018, adobe officially changed adobe name from adobe systems incorporated to
adobe inc. in january 2019, adobe announced adobe acquisition of 3d-texturing-company-allegorithmic.
in 2020, the-annual-adobe-summit was canceled due to the-covid-19-pandemic.
the-event is said to take place online this year.
adobe-inc. has imposed a-ban on the-political-ads features on adobe-inc. digital advert sales platform as the-united-states-presidential-elections-approach.
on november 9, 2020, adobe announced adobe will spend us$1.5 billion to acquire workfront, a-provider of marketing-collaboration-software.
==-==-products ==
graphic-design-software
adobe-photoshop, adobe-pagemaker, adobe-lightroom, adobe-indesign, adobe-incopy, adobe-imageready, adobe-illustrator, adobe-freehand, adobe-framemaker, adobe-fireworks, adobe-acrobat, adobe-xd-web-design-programs
adobe-xd-web-muse, adobe-golive, adobe-flash-builder, adobe-flash, adobe-edge, adobe-dreamweaver, adobe-contribute-video-editing, animation, and visual-effects adobe-ultra, adobe-spark-video, adobe-premiere-pro, adobe-premiere-elements, adobe-xd-web-prelude, adobe-encore, adobe-xd-web-director, adobe-animate, adobe-xd-web
after effects, adobe character animator audio editing software adobe soundbooth, adobe-audition-elearning-software adobe captivate prime (lms-platform), adobe captivate, adobe presenter video express and adobe connect (also a-web-conferencing-platform)
digital-marketing-management-software-adobe-marketing-cloud, adobe-experience-manager (aem 6.2) , xml-documentation add-on (for aem), mixamo-server-software
adobe-coldfusion, adobe-content-server and adobe-livecycle-enterprise-suite, adobe-blazeds-formats-portable-document-format (pdf), pdf's predecessor postscript, actionscript, shockwave-flash (swf), flash-video (flv), and filmstrip (.flm)
web-hosted-services-adobe-color, photoshop-express, acrobat.com, and adobe-spark3d and
ar-adobe-aero, dimension, mixamo, substance by adobe-adobe-renderer-adobe-media-encoderadobe-stock-a-microstock-agency that presently provides over-57-million-high-resolution,-royalty-free-images and videos available to license (via subscription-or-credit-purchase-methods).
in 2015, adobe acquired fotolia, a-stock-content-marketplace founded in 2005 by thibaud-elziere, oleg-tscheltzoff, and patrick-chassany which operated in 23-countries.
it is run as a-stand-alone-website.
adobe-experience-platform
in march 2019, adobe released adobe adobe experience platform, which consists family of content, development, and customer-relationship-management-products, with what adobe calls the "next generation" of adobe sensei artificial intelligence and machine learning framework.
reception ==
from 1995 to 2013, fortune ranked adobe as "an-outstanding-place to work".
adobe was rated the 5th best u.s. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013.
in october 2008, adobe-systems-canada-inc. was named one of "canada's-top-100-employers" by mediacorp-canada-inc. and was featured in maclean's-newsmagazine.
adobe has a-five-star-privacy-rating from the-electronic-frontier-foundation.
criticisms == ===
pricing ===
adobe has been criticized for adobe pricing practices, with retail-prices being up to twice as much in non-us-countries.
for example, adobe is significantly cheaper to pay for a-return-airfare-ticket to the-united-states and purchase one-particular-collection of adobe's software there than to buy adobe locally in australia.
after adobe revealed the-pricing for the-creative-suite-3-master-collection, which was £1,000 higher for european-customers, a-petition to protest over "unfair-pricing" was published and signed by 10,000-users.
in june 2009, adobe further increased adobe prices in the-uk by 10% in spite of weakening of the-pound against the-dollar, and uk-users were not allowed to buy from the-us-store.
adobe's-reader-and-flash-programs were listed on "the-10-most-hated-programs of all-time"-article by techradar.
in april 2021, adobe received heavy-criticism for the-company’s-cancellation-fees after a-customer shared a-tweet showing a-customer had been charged a-$291.45-cancellation-fee for a-customer adobe creative cloud subscription.
many also showed a-customer-cancellation-fees for adobe-creative-cloud, with this leading to many-encouraging-piracy of adobe-products and/or purchase of alternatives with lower-prices.
security ===
hackers have exploited vulnerabilities in adobe-programs, such as adobe-reader, to gain unauthorized-access to computers.
adobe's-flash-player has also been criticized for, among other-things, suffering from performance, memory-usage and security-problems (see criticism of flash-player).
a-report by security-researchers from kaspersky-lab criticized adobe for producing the-products having top-10-security-vulnerabilities.
observers noted that adobe was spying on adobe customers by including spyware in the-creative-suite-3-software and quietly sending user-data to a-firm named omniture.
when users became aware, adobe explained what the-creative-suite-3-software did and admitted that users: "could and should do a-better-job taking security-concerns into account".
when a-security-flaw was later discovered in photoshop-cs5, adobe sparked outrage by saying adobe would leave a-security-flaw unpatched, so anyone who wanted to use the-creative-suite-3-software securely would have to pay for an-upgrade.
following adobe-adobe decided to provide the-software-patch.
adobe has been criticized for pushing unwanted-software including third-party-browser-toolbars and free-virus-scanners, usually as part of the-flash-update-process, and for pushing a-third-party-scareware-program designed to scare users into paying for unneeded-system-repairs.
customer-data-breach ==
on october 3, 2013, customer-data-breach == initially revealed that 2.9-million-customers'-sensitive-and-personal-data was stolen in security-breach which included encrypted-credit-card-information.
adobe later admitted that 38-million-active-users have been affected and the-attackers obtained access to the-attackers ids and encrypted passwords, as well as to many inactive adobe accounts.
customer-data-breach ==
did not make it clear if all-the-personal-information was encrypted, such as email-addresses and physical-addresses, though data-privacy-laws in 44-states require this-information to be encrypted.
a-3.8-gb-file stolen from adobe and containing 152-million-usernames, reversibly-encrypted-passwords and unencrypted-password-hints was posted on anonnews.org.
lastpass, a-password-security-firm, said that adobe failed to use best-practices for securing the-passwords and has not salted the-passwords.
another-security-firm, sophos, showed that adobe used a-weak-encryption-method permitting the-recovery of a-lot of information with very-little-effort.
according to it-expert-simon-bain, adobe has failed adobe customers and 'should hang its-customers heads in shame'.
many of the-credit-cards were tied to the creative cloud software-by-subscription service.
adobe offered  adobe affected us-customers a-free-membership in a-credit-monitoring-service, but no-similar-arrangements have been made for non-us-customers.
when a-data-breach occurs in the-us, penalties depend on the-state where the-victim resides, not where  adobe is based.
after stealing the-customers'-data, cyber-thieves also accessed adobe's-source-code-repository, likely in mid-august 2013.
because hackers acquired copies of the-source-code of  adobe-proprietary-products, hackers could find and exploit any-potential-weaknesses in  adobe-security, computer-experts warned.
security-researcher-alex-holden, chief-information-security-officer of hold-security, characterized this--adobe-breach, which affected acrobat, coldfusion and numerous-other-applications, as "one of the worst in us-history".
adobe also announced that hackers stole parts of the-source-code of photoshop, which according to commentators could allow programmers to copy  adobe engineering techniques and would make its-engineering-techniques easier to pirate adobe's expensive products.
published on a-server of a-russian-speaking-hacker-group, the-"disclosure of encryption-algorithms, other-security-schemes, and software-vulnerabilities can be used to bypass protections for individual-and-corporate-data" and may have opened the-gateway to new-generation-zero-day-attacks.
hackers already used coldfusion exploits to make off with usernames and encrypted-passwords of pr-newswire's-customers, which has been tied to the--adobe-security-breach.
hackers also used a-coldfusion exploit to breach washington-state-court and expose up-to-200,000-social-security-numbers.
anti-competitive-practices ===
in 1994, adobe acquired aldus-corp., a-software-vendor that sold freehand.
freehand was direct-competition to adobe-illustrator, adobe's-flagship-vector-graphics-editor.
the-federal-trade-commission intervened and forced adobe to sell freehand back to altsys, and also banned adobe from buying back freehand or any-similar-program for the-next-10-years (1994–2004).
altsys was then bought by macromedia, which released versions 5 to 11.
when adobe acquired macromedia in december 2005, adobe stalled development of freehand in 2007, effectively rendering adobe obsolete.
with freehand and illustrator, adobe controlled the-only-two-products that compete in the-professional-illustration-program-market for macintosh-operating-systems.
in 2011, a-group of 5,000-freehand-graphic-designers convened under the-banner-free-freehand, and filed a-civil-antitrust-complaint in the-us-district-court for the-northern-district of california against adobe.
a-civil-antitrust-complaint in the-us-district-court alleged that adobe has violated federal-and-state-antitrust-laws by abusing the northern district of california dominant position in the-professional-vector-graphic-illustration-software-market and that adobe has engaged in a-series of exclusionary-and-anti-competitive-acts and strategies designed to kill freehand, the-dominant-competitor to adobe's illustrator software product, instead of competing on the-basis of product-merit according to the-principals of free-market-capitalism.
adobe had no-response to the-claims and the-lawsuit was eventually settled.
the-freehand-community believes adobe should release the-product to an-open-source-community if the-product cannot update the-product internally.
as of 2010, on  as of 2010-freehand-product-page,  as of 2010 stated, "while we recognize freehand has a-loyal-customer-base, we encourage users to migrate to the new
as of 2010-illustrator-cs4-software which supports both-powerpc and intel-based-macs and microsoft-windows-xp and windows-vista. "
as of 2016, the-freehand-page no longer exists; instead, the-freehand-page simply redirects to the-illustrator-page.
as of 2010's-software-ftp-server still contains a-directory for freehand, but
as of 2010 is empty.
see also ==
adobe-max-digital-rights-management (drm
) list of acquisitions by adobe list of adobe-software-us v. elcomsoft
sklyarov ==
references == ==
external-links ==
official-website
business-data for adobe-inc.: "
patents owned by adobe-inc".
us-patent-&-trademark-office.
retrieved december 8, 2005.
the-following-outline is provided as an-overview of and topical guide to thought (thinking):
thought (also called thinking) is the-mental-process in which beings form psychological-associations and models of the-world.
thinking is manipulating information, as when we form concepts, engage in problem solving, reason and make decisions.
thought, the-act of thinking, produces more-thoughts.
a-thought may be an-idea, an-image, a-sound or even control an-emotional-feeling.
nature of thought ==
thought (or thinking) can be described as all of the-following: an activity taking place in a: brain –-organ that serves as the-center of the-nervous-system in all-vertebrate and most-invertebrate-animals (only-a-few-invertebrates such as sponges, jellyfish, adult-sea-squirts and starfish do not have a-brain).
it is the-physical-structure associated with the-mind.
mind – abstract-entity with the-cognitive-faculties of consciousness, perception, thinking, judgement, and memory.
having a-mind is a-characteristic of living-creatures.
activities taking place in a-mind are called mental processes or cognitive functions.
computer (see § machine thought below) – general-purpose-device that can be programmed to carry out a-set of arithmetic-or-logical-operations automatically.
since a-sequence of operations (an-algorithm) can be readily changed, the-computer can solve more-than-one-kind of problem.
an-activity of intelligence – intelligence is the-intellectual-process of  which is marked by cognition, motivation, and self-awareness.
through intelligence, living-creatures possess the-cognitive-abilities to learn, form concepts, understand, apply logic, and reason, including the-capacities to recognize patterns, comprehend ideas, plan, problem solve, make decisions, retaining, and use language to communicate.
intelligence enables living-creatures to experience and think.
a-type of mental-process – something that individuals can do with individuals minds.
mental-processes include perception, memory, thinking, volition, and emotion.
sometimes the-term-cognitive-function is used instead.
thought as a-biological-adaptation
mechanismneural network-explanation:
thoughts are created by the-summation of neural-outputs and connections of which vectors form.
these-vectors describe the-magnitude and direction of the-connections and action between neurons.
the-graphs of  these-vectors can represent a-network of neurons whose-connections fire in different-ways over time as synapses fire.
these-large-thought-vectors in the-brain cause other-vectors of activity.
for example: an-input from the-environment is received by the-neural-network.
the-neural-network changes the-magnitude and outputs of individual-neurons.
the-altered-network-outputs the-symbols needed to make sense of the-input.
types of thoughts == ==
content of thoughts ===
types of thought (thinking) ==
listed below are types of thought, also known as thinking-processes.
animal thought ===
human-thought ===
human-thought ====
classifications of thought ====
bloom's-taxonomy –-classification-system in education-dual-process-theory –
psychological-theory of how-thought can arise in two-different-ways fluid-and-crystallized-intelligence – factors of general-intelligence-higher-order-thinking – a-concept of education-reform
theory of multiple-intelligences – theory of intelligence proposed by howard-gardner-three-stratum-theory-williams'-taxonomy ====
creative-processes ====
====-decision-making ==== ====
erroneous-thinking ==== ====
emotional-intelligence (emotionally-based-thinking) ====
emotional-intelligence
– capability to understand one's-emotions and use one's-emotions to guide thinking and behavior ====
problem solving ====
problem solving –
generic-and-ad-hoc-approach to problem solving ====
reasoning ====
machine thought === ===
organizational-thought ===
organizational-thought (thinking by organizations)
management-information-system –
information-system used for decision-making-organizational-communication
organizational planning strategic-planning strategic-thinking
systems-theory –
interdisciplinary-study of systems ==
aspects of the-thinker ==
aspects of the-thinker which may affect (help or hamper)
his-or-her-thinking: ==
properties of thought == ==
fields that study
thought == ==
thought tools and thought research ==
cognitive-model-design-tool
diagram – symbolic-representation of information using visualization-techniques-argument-map concept-map – diagram showing relationships among concepts-mind-map – system or map
used to visually organize information-dsrp-intelligence-amplification-language –-capacity to communicate using signs, such as words or gestures meditation – mental-practice of focus on a-particular-object, thought or activity to improve one's-mind
six thinking hats-synectics ==
history of thinking ==
history of reasoning
history of artificial-intelligence –-overview of the-history of artificial-intelligence history of cognitive-science history of the-concept of creativity history of ideas history of logic –
study of the-history of the-science of valid-inference
history of psychometrics ==
nootropics (cognitive-enhancers and smart-drugs) ==
nootropic – drug, supplement, or other-substance that improves cognitive-function-substances that improve mental-performance:
organizational-thinking-concepts === ==
teaching-methods and skills == ==
awards related to thinking == ===
awards for acts of genius ===
nobel-prize –
set of annual-international-awards, primarily 5 established in 1895 by alfred-nobel
pulitzer-prize – award for achievements in journalism, literature, and musical-composition within the-united-states-macarthur-fellows-program – prize awarded annually by the-john-d. and catherine-t.-macarthur-foundation ==
organizations ==
associations pertaining to thought
association for automated reasoning association for informal-logic-and-critical-thinking-international-joint-conference on automated-reasoning-high-iq-societies mega-society – high-iq-society
mensa – largest-and-oldest-high-iq-society in the-world-mind-sports-organisations-world-mind-sports-games
think tank – organization that performs policy-research and advocacys
media == ===
publications === ====
books ====
handbook of automated-reasoning ====
periodicals ==== journal of automated-reasoning
journal of formalized-reasoning-positive-thinking-magazine ===
television-programs ==
thinkabout (u.s.-tv-series)
persons associated with thinking == ===
people notable for people notable for their-extraordinary-ability to think ===
extraordinary-ability to think ===
list of nobel-laureates (see also nobel-prize)-polymaths ===
scientists in fields that study thought ===
list of cognitive-scientists
scholars of thinking ===
aaron-t.-beck-edward-de-bono-david-d.-burns – author of feeling good:
the-new-mood-therapy and the-feeling-good-handbook.
burns popularized aaron-t.-beck's-cognitive-behavioral-therapy (cbt) when aaron-t.-beck's-book became a-best-seller during the 1980s.
tony-buzan-noam-chomsky-albert-ellis
howard-gardner-eliyahu-m.-goldratt-douglas-hofstadter-ray-kurzweil-marvin-minsky
steven-pinker-baruch-spinoza-robert-sternberg ==
related-concepts ==
cognition-knowledge-multiple-intelligences-strategy-structure
system ===
awareness and perception === ===
learning and memory ===
see also ==
artificial-intelligence-outline of artificial-intelligence-human-intelligence
outline of human intelligence neuroscience outline of neuroscience psychology-gestalt-psychology (theory of mind)
outline of psychologymiscellaneous
thinking-lists ==
references ==
external-links ==
the-psychology of emotions, feelings and thoughts, free-online-book
in computing, cache-algorithms (also frequently called cache-replacement-algorithms or cache-replacement-policies) are optimizing instructions‍—‌or algorithms‍—‌that
a-computer-program or a-hardware-maintained-structure can follow in order to manage a-cache of information stored on the-computer.
when a-cache of information stored on the-computer is full, the-algorithm must choose which-items to discard to make room for the-new-ones.
due to the-inherent-caching-capability of nodes in information-centric-networking-icn, icn can be viewed as a-loosely-connect-network of caches, which has unique-requirements of caching-policies.
unlike proxy-servers, in information-centric-networking the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose different-kind of requirements on the-content-eviction-policies.
in particular, eviction-policies for information-centric-networking should be fast and lightweight.
various-cache-replication and eviction-schemes for different-information-centric-networking-architectures and applications are proposed.
policies == ==
time aware-least-recently-used-(tlru) ===
the-time aware least recently used (tlru) is a-variant of lru designed for the-situation where the-stored-contents in cache have a-valid-life-time.
the-algorithm is suitable in network-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
tlru introduces a-new-term: ttu (time to use).
ttu is a-time-stamp of a-content/page which stipulates the-usability-time for the-content based on the-locality of the-content and the-content publisher announcement.
owing to this-locality based time-stamp, ttu provides more-control to the-local-administrator to regulate in network-storage.
in  the-algorithm, when a-piece of content arrives, a-cache-node calculates the-local-ttu-value based on the-ttu-value assigned by the-content-publisher.
the-local-ttu-value is calculated by using a-locally-defined-function.
once the-local-ttu-value is calculated the-replacement of content is performed on a-subset of the-total-content stored in cache-node.
the-tlru ensures that less-popular-and-small-life-content should be replaced with the-incoming-content.
least-frequent-recently-used-(lfru) ===
the-least-frequent-recently-used-(lfru)-cache-replacement-scheme combines the-benefits of lfu-and-lru-schemes.
lfru  is suitable for ‘in network’-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
in  lfru  , the-cache is divided into two-partitions called privileged and unprivileged partitions.
the-privileged-partition can be defined as a-protected-partition.
if content is highly popular, content is highly popular is pushed into the-privileged-partition.
replacement of the-privileged-partition is done as follows:  lfru evicts content from the-unprivileged-partition, pushes content from privileged-partition to unprivileged-partition, and finally inserts new-content into the-privileged-partition.
in the-above-procedure the-lru is used for the-privileged-partition and an-approximated-lfu-(alfu)-scheme is used for the-unprivileged-partition, hence-the-abbreviation-lfru.
the-basic-idea is to filter out the-locally-popular-contents with alfu-scheme and push the-popular-contents to one of the-privileged-partition.
references ==
in linear-algebra, the-strassen-algorithm, named after volker-strassen, is an-algorithm for matrix-multiplication.
it is faster than the-standard-matrix-multiplication-algorithm and is useful in practice for large-matrices, but would be slower than the-fastest-known-algorithms for extremely large-matrices.
strassen's-algorithm works for any-ring, such as plus/multiply, but not-all-semirings, such as min-plus-or-boolean-algebra, where the-naive-algorithm still works, and so called combinatorial matrix multiplication.
history ==
volker-strassen first published strassen's-algorithm in 1969 and proved that the-n3-general-matrix-multiplication-algorithm wasn't optimal.
the-strassen-algorithm is only slightly better than that, but  the-strassen-algorithm publication resulted in much-more-research about matrix-multiplication that led to faster-approaches, such as the-coppersmith –
winograd algorithm.
algorithm ==
let a, b be two-square-matrices over a-ring-r,
for example matrices whose-entries are integers or the-real-numbers.
we want to calculate the-matrix-product c as c         = a
b     {\displaystyle \mathbf {c} =
\mathbf-{a}-\mathbf-{b} }
in the-following-exposition of the-algorithm, we will assume that all of these-matrices have sizes that are powers of two (i.e., a         ,
b         ,
c---------∈-r
2                 n-× 2 n
{\displaystyle-\mathbf-{a} ,\mathbf-{b} ,\mathbf-{c}-\in-r^{2^{n}\times 2^{n}}}   ),
but this is only conceptually necessary --
if the-matrices a,-b are not of type 2n × 2n we can think conceptually about filling the-"missing"-rows and columns with zeros to obtain matrices with sizes of powers of two -- though real-implementations of the-algorithm will of course not actually do this in practice.
we then partition a, b and c into equally-sized-block-matrices
a         =
[                       a                       1
,                       1 a                       1                       ,
a                       2                       ,                       1
a                       2                       ,                       2             ]
,------------b         =
[-----------------------b
1                       ,                       1 b                       1
,                       2 b                       2                       ,
1-b                       2 ,                       2
]              ,            c         =
c                       1                       ,                       1 c
1                       ,                       2-c                       2
,                       1 c                       2 ,
2-]-----{\displaystyle-\mathbf-{a} ={\begin{bmatrix}\mathbf {a}-_{1,1}&\mathbf {a}-_{1,2}\\\mathbf {a}-_{2,1}&\mathbf {a}-_{2,2}\end{bmatrix}}{\mbox{ , }}\mathbf-{b} ={\begin{bmatrix}\mathbf-{b}-_{1,1}&\mathbf-{b}-_{1,2}\\\mathbf-{b}-_{2,1}&\mathbf-{b}-_{2,2}\end{bmatrix}}{\mbox{ , }}\mathbf {c} ={\begin{bmatrix}\mathbf {c}-_{1,1}&\mathbf {c}-_{1,2}\\\mathbf {c}-_{2,1}&\mathbf {c}-_{2,2}\end{bmatrix}}}   with             a
i             ,             j         ,             b
i             ,             j         ,             c
i             , j---------∈-----------r
2                 n − 1 ×
2-n-− 1     {\displaystyle \mathbf-{a}-_{i,j},\mathbf {b}-_{i,j},\mathbf
{c}-_{i,j}\in r^{2^{n-1}\times 2^{n-1}}} .the
naive-algorithm would be:
c             1
,             1         =
a             1
,-1-------------b             1             ,
a             1             ,
2-b             2             ,             1
{\displaystyle-\mathbf-{c}-_{1,1}=\mathbf-{a}-_{1,1}\mathbf-{b}-_{1,1}+\mathbf {a}-_{1,2}\mathbf {b}-_{2,1}}
c             1             , 2
a             1             ,             1
b             1             ,             2 +
a-------------1-------------,-2-b
2-------------,-2-----{\displaystyle-\mathbf-{c}-_{1,2}=\mathbf-{a}-_{1,1}\mathbf-{b}-_{1,2}+\mathbf-{a}-_{
1,2}\mathbf-{b}-_{2,2}}-c
2-------------,-1---------=-a
2             ,-1-------------b             1
,             1 + a             2
, 2 b             2             ,
{\displaystyle-\mathbf-{c}-_{2,1}=\mathbf-{a}-_{2,1}\mathbf-{b}-_{1,1}+\mathbf
{a}-_{2,2}\mathbf-{b}-_{2,1}} c             2             ,
2         =
a             2             ,
b             1             ,             2
+ a             2             , 2
b             2             , 2     {\displaystyle-\mathbf-{c}-_{2,2}=\mathbf
{a}-_{2,1}\mathbf-{b}-_{1,2}+\mathbf {a}-_{2,2}\mathbf-{b}-_{2,2}}
with this-construction we have not reduced the-number of multiplications.
we still need 8-multiplications of matrix-blocks to calculate
the            c i             ,
j-----{\displaystyle-c_{i,j}}----matrices, the-same-number of multiplications we need when using standard-matrix-multiplication.
the-strassen-algorithm defines instead new-matrices:
1         :=
( a             1
,             1 + a             2
,             2         )
(-------------b
1             ,             1 +             b
2-------------,-2---------)-----{\displaystyle-\mathbf-{m}-_{1}:=(\mathbf {a}-_{1,1}+\mathbf-{a}-_{2,2})(\mathbf {b}-_{1,1}+\mathbf-{b}-_{2,2})}
m             2         :=
2             ,             1 + a
2             ,
2---------)-b
1             ,             1     {\displaystyle-\mathbf-{m}-_{2}:=(\mathbf-{a}-_{2,1}+\mathbf-{a}-_{2,2})\mathbf-{b}-_{1,1}}-m
3         :=
a             1             ,
(             b             1             ,
−-b             2             ,
2         )
{\displaystyle-\mathbf-{m}-_{3}:=\mathbf-{a}-_{1,1}(\mathbf {b}-_{
1,2}-\mathbf-{b}-_{2,2})}-m
4         :=
a             2             ,
(             b             2             ,
−-b             1             ,
1         )
{-\displaystyle-\mathbf-{m}-_{4}:=\mathbf-{a}-_{2,2}(\mathbf-{b}-_{
2,1}-\mathbf-{b}-_{1,1})}
m             5         :=
1             ,             1 + a
1             ,
2---------)-b
2-------------,-2-----{-\displaystyle-\mathbf-{m}-_{5}:=(\mathbf {a}-_{1,1}+\mathbf {a}-_{1,2})\mathbf {b} _{2,2}}
m             6         :=
2             , 1
1             ,             1         )
b             1             ,             1 +
b             1             ,             2         )
{\displaystyle-\mathbf-{m}-_{6}:=(\mathbf
{a}-_{2,1}-\mathbf {a}-_{1,1})(\mathbf {b}-_{1,1}+\mathbf-{b}-_{1,2})} m             7         :=
( a             1             , 2
a             2             ,             2
(             b             2             ,
+             b             2             ,
2---------)-----{\displaystyle-\mathbf-{m}-_{7}:=(\mathbf-{a}-_{1,2}-\mathbf {a}-_{2,2})(\mathbf {b}-_{2,1}+\mathbf {b}-_{2,2})}   only using 7-multiplications (one for each-m
k     {\displaystyle-m_{k}}   ) instead of 8.
we may now express the------------c
i             , j     {\displaystyle c_{i,j}}    in terms of
{\displaystyle-m_{k}}   :-c
1             ,
1---------=-m
1 +-m             4         −
m             5 + m             7
{\displaystyle-\mathbf-{c}-_{1,1}=\mathbf-{m}-_{1}+\mathbf-{m}-_{4}-\mathbf-{m}-_{5}+\mathbf-{m}-_{7}}
c             1             , 2
m             3 + m
{\displaystyle-\mathbf-{c}-_{1,2}=\mathbf-{m}-_{3}+\mathbf-{m}-_{5}}             c 2 ,
1         =
m             2
m-------------4-----{\displaystyle-\mathbf-{c}-_{2,1}=\mathbf-{m}-_{2}+\mathbf-{m}-_{4}} c             2
, 2 = m             1
2-+-------------m
3-+-------------m-------------6-----{\displaystyle-\mathbf-{c}-_{2,2}=\mathbf
{m}-_{1}-\mathbf {m}-_{2}+\mathbf {m}-_{3}+\mathbf {m}-_{6}} we recursively iterate this-division-process until the-submatrices degenerate into numbers (elements of the-ring-r).
if, as mentioned above, the-original-matrix had a-size that was not a-power of 2, then the-resulting-product will have zero-rows and columns just like a and b, and these will then be stripped at this-point to obtain the-(smaller)-matrix-c we really wanted.
practical-implementations of strassen's algorithm switch to standard-methods of matrix-multiplication for small-enough-submatrices, for which those-algorithms are more efficient.
the-particular-crossover-point for which strassen's-algorithm is more efficient depends on the-specific-implementation and hardware.
earlier-authors had estimated that strassen's-algorithm is faster for matrices with widths from 32 to 128 for optimized-implementations.
however, it has been observed that this-crossover-point has been increasing in recent-years, and a-2010-study found that even-a-single-step of strassen's-algorithm is often not beneficial on current-architectures, compared to a-highly-optimized-traditional-multiplication,
until matrix-sizes exceed 1000 or more, and even for matrix-sizes of several-thousand-the-benefit is typically marginal at best (around 10% or less).
a-more-recent-study (2016) observed benefits for matrices as small as 512 and a-benefit around 20%.
asymptotic-complexity ==
the-outline of the-algorithm above showed that one can get away with just 7, instead of the-traditional-8,-matrix-matrix-multiplications for the-sub-blocks of the-matrix.
on the-other-hand, one has to do additions and subtractions of blocks, though this is of no-concern for the-overall-complexity:
adding matrices of size n/2 requires only (n/2)2 operations whereas multiplication is substantially more expensive (traditionally 2(n/2)3) addition or multiplication operations).
the-question then is how-many-operations exactly one needs for strassen's-algorithms, and how this compares with the-standard-matrix-multiplication that takes approximately 2n3 (where n = 2n)
arithmetic-operations, i.e.-an-asymptotic-complexity-θ(n3).
the-number of additions and multiplications required in the-strassen-algorithm can be calculated as follows: let f(n)
be the-number of operations for a-2n-×-2n-matrix.
then by recursive-application of the-strassen-algorithm, we see that-f(n) = 7f(n−1)
+ ℓ4n, for some constant ℓ that depends on the-number of additions performed at each-application of the-algorithm.
hence-f(n) =
(7-+-o(1))n,-i.e.,-the-asymptotic-complexity for multiplying matrices of size n = 2n using the-strassen-algorithm is         o (
[         7 +         o (
1         )
] n         )
= o-(-----------n---------------log
(             1             )         ) ≈
o (           n 2.8074         )
{\displaystyle-o([7+o(1)]^{n})=o(n^{\log-_{2}7+o(1)})\approx-o(n^{2.8074})} .the
reduction in the-number of arithmetic-operations however comes at the-price of a-somewhat-reduced-numerical-stability, and the-algorithm also requires significantly-more-memory compared to the-algorithm.
both-initial-matrices must have both-initial-matrices dimensions expanded to the-next-power of 2, which results in storing up-to-four-times-as-many-elements, and the seven auxiliary matrices each contain a-quarter of the-elements in the-expanded-ones.
strassen's-algorithm needs to be compared to the-"naive"-way of doing the-matrix-multiplication that would require 8-instead-of-7-multiplications of sub-blocks.
this would then give rise to the-complexity one expects from the-standard-approach:
o-(-----------8-log
n         )         = o
(           n               log                 2 ⁡
8         )
n             3         ) {\displaystyle-o(8^{\log-_{2}n})=o(n^{\log _{2}8})=o(n^{3})} .the
comparison of these-two-algorithms shows that asymptotically, strassen's-algorithm is faster: there exists a-size nthreshold so that matrices that are larger are more efficiently multiplied with strassen's-algorithm than the-"traditional"-way.
however, the-asymptotic-statement does not imply that strassen's-algorithm is always faster even for small-matrices, and in practice this is in fact not the-case: for small-matrices, the-cost of the-additional-additions of matrix-blocks outweighs the-savings in the-number of multiplications.
there are also other-factors not captured by the-analysis above, such as the-difference in cost on today's-hardware between loading data from memory onto processors vs. the-cost of actually doing operations on this-data.
as a-consequence of these-sorts of considerations, strassen's-algorithm is typically only used on "large"-matrices.
this-kind of effect is even more pronounced with alternative-algorithms such as the-one by coppersmith and winograd:
while-asymptotically-even-faster,-the-cross-over-point
nthreshold is so large that the-algorithm is not generally used on matrices one-encounters in practice.
rank-or-bilinear-complexity ===
the-bilinear-complexity or rank of a-bilinear-map is an-important-concept in the-asymptotic-complexity of matrix-multiplication.
the-rank of a-bilinear-map
:-a---------×-----------b
c     {\displaystyle \phi :
\mathbf-{a} \times \mathbf-{b} \rightarrow \mathbf-{c} }    over a-field f is defined as (somewhat of an-abuse of notation)
/-----------f
=-min           { r
|-----------------∃
a                     ∗                 ,
b ∗                 ,
i                 ∈-c                 ,                 ∀
∈ a                 , b
∈-------------------b                 , ϕ (
a-----------------,-------------------b                 ) =
i                     = 1
f i (                   a                 )
b                 )
w i } {\displaystyle r(\phi /\mathbf {f} )=
\min-\left\{r\left|\exists-f_{i}\in \mathbf {a} ^{*},g_{i}\in \mathbf {b} ^{*},w_{i}\in \mathbf {c} ,\forall \mathbf {a}-\in \mathbf {a} ,
\mathbf-{b}-\in \mathbf {b} ,\phi
(\mathbf-{a}-,\mathbf-{b} )=
\sum-_{i=1}^{r}f_{i}(\mathbf-{a}-)g_{i}(\mathbf-{b}-)w_{i}\right.\right\}}
in other-words, the-rank of a-bilinear-map is the-length of a-bilinear-map shortest bilinear computation.
the-existence of strassen's-algorithm shows that the-rank of 2×2-matrix-multiplication is no more than seven.
to see this, let us express this-algorithm (alongside the-standard-algorithm) as such-a-bilinear-computation.
in the-case of matrices, the dual spaces a*-and-b*-consist of maps into the-field f induced by a-scalar-double-dot-product, (i.e. in this-case the sum of all-the-entries of a-hadamard-product.)
it can be shown that the-total-number of elementary-multiplications l required for matrix-multiplication is tightly asymptotically bound to the-rank-r, i.e.
=---------θ         (         r
)     {\displaystyle l=\theta (r)}   , or more specifically, since the-constants are known,              1             2
r         ≤
l---------≤---------r
\displaystyle {\frac {1}{2}}r\leq-l\leq-r.}-one-useful-property of the-rank is that it is submultiplicative for tensor-products, and this enables one to show that 2n×2n×2n-matrix-multiplication can be accomplished with no more than 7n-elementary-multiplications for any-n.
(this-n-fold-tensor-product of the-2×2×2-matrix-multiplication-map with itself—an-nth-tensor-power—is realized by the-recursive-step in the-algorithm shown.)
cache-behavior ===
strassen's-algorithm is cache oblivious.
analysis of strassen's-algorithm-cache-behavior-algorithm has shown strassen's-algorithm to incur         θ
(             1 +                   n                     2
b +                   n-log                         2
⁡-7-b-----------------------m )
{\displaystyle \theta \left(1+{\frac {n^{2}}{b}}+{\frac
{n^{\log-_{2}7}}{b{\sqrt-{m}}}}\right)}-cache misses during n^{\log-execution, assuming an-idealized-cache of size
m (i.e. with              m             b
{\displaystyle {\frac {m}{b}}}----lines of length-b).
implementation-considerations ==
the-description above states that the-matrices are square, and the-size is a-power of two, and that-padding should be used if needed.
this-restriction allows the-matrices to be split in half, recursively, until limit of scalar-multiplication is reached.
this-restriction simplifies the-explanation, and analysis of complexity, but is not actually necessary; and in fact, padding the-matrix as described will increase the-computation-time and can easily eliminate the-fairly-narrow-time-savings obtained by using the-method in the-first-place.
a-good-implementation will observe the following: a-good-implementation is not necessary or desirable to use the-strassen algorithm down to the-limit of scalars.
compared to conventional-matrix-multiplication, the-algorithm adds
a considerable           o (           n             2
)     {\displaystyle o(n^{2})} workload in addition/subtractions; so below a-certain-size, it will be better to use conventional-multiplication.
thus, for instance, a-1600x1600 does not need to be padded to 2048x2048, since a-1600x1600 could be subdivided down to 25x25-matrices and conventional-multiplication can then be used at that-level.
the-method can indeed be applied to square-matrices of any-dimension.
if the-dimension is even,   are split in half as described.
if the-dimension is odd, zero padding by one-row and one-column is applied first.
such-padding can be applied on-the-fly and lazily, and the-extra-rows and columns discarded as the-result is formed.
for instance, suppose the-matrices are 199x199.
the-matrices can be split so that the-upper-left-portion is 100x100
and the-lower-right is 99x99.
wherever the-operations require the-operations, dimensions of 99 are zero padded to 100 first.
note, for instance, that the-product-m 2
{\displaystyle m_{2}}    is only used in the-lower-row of the-output, so is only required to be 99 rows high; and thus-the-left-factor          (           a             2
,             1 + a             2
, 2         )     { \displaystyle (a_{2,1}+a_{2,2})
}    used to generate it need only be 99 rows high; accordingly, there is no-need to pad that-sum to 100-rows; it is only necessary to pad a             2             ,
2     {\displaystyle a_{2,2}} to 100-columns to match a             2
,             1
{\displaystyle a_{2,1}} .furthermore, there is no-need for the-matrices to be square.
non-square-matrices can be split in half using the-same-methods, yielding smaller-non-square-matrices.
if the-matrices are sufficiently non-square it will be worthwhile reducing the-initial-operation to more-square-products, using simple-methods which are essentially           o (
n             2
)     {\displaystyle o(n^{2})}   , for instance: a product of size [2n-x-n]
[n-x-10n] can be done as 20-separate-[n-x-n]
* [n-x-n]-operations, arranged to form the-result; a product of size [n-x-10n]
[10n x n] can be done as 10-separate-[n-x-n] *
[n-x-n]-operations, summed to form the-result.
these-techniques will make the-implementation more complicated, compared to simply padding to a-power-of-two-square; however, it is a-reasonable-assumption that anyone undertaking an-implementation of strassen, rather than conventional,-multiplication, will place a-higher-priority on computational-efficiency than on simplicity of the-implementation.
in practice, strassen's-algorithm can be implemented to attain better-performance than conventional-multiplication even for small-matrices, for matrices that are not at all square, and without requiring workspace beyond buffers that are already needed for a high-performance conventional-multiplication.
see also ==
computational-complexity of mathematical-operations
gauss–jordan-elimination-coppersmith–
winograd algorithm z-order-matrix-representation
karatsuba algorithm, for multiplying n-digit-integers in          o
(-----------n---------------log
3         )
{\displaystyle-o(n^{\log-_{2}3})}
instead of in          o (           n             2
)-----{\displaystyle-o(n^{2})}-time
toom-cook-algorithm
, a-faster-generalization of the-karatsuba-algorithm that permits recursive-divide-and-conquer-decomposition into more-than-2-blocks at a-time gauss's-complex-multiplication-algorithm multiplies two-complex-numbers using 3-real-multiplications instead of
references ==
thomas-h.-cormen, charles-e.-leiserson, ronald-l.-rivest, and clifford-stein.
introduction to algorithms, second-edition.
mit-press and mcgraw-hill, 2001.
isbn 0-262-03293-7.
chapter 28:
section 28.2:
strassen's-algorithm for matrix-multiplication, pp.
external-links ==
weisstein, eric-w.-"strassen's-formulas".
mathworld.
( also includes formulas for fast-matrix-inversion)
tyler-j.-earnest, strassen's-algorithm on the-cell-broadband-engine
in computing, cache-algorithms (also frequently called cache-replacement-algorithms or cache-replacement-policies) are optimizing instructions, or algorithms, that a-computer-program or a-hardware-maintained-structure can utilize in order to manage a-cache of information stored on the-computer.
caching improves performance by keeping recent-or-often-used-data-items in memory-locations that are faster or computationally cheaper to access than normal-memory-stores.
when the-cache is full, the-algorithm must choose which-items to discard to make room for the-new-ones.
overview ==
the-average-memory-reference-time is         t
=         m-×-t-m
+           t             h + e
{\displaystyle-t=m\times t_{m}+t_{h}+e} where         m
{\displaystyle m}    = miss ratio = 1 - (hit-ratio)
t-m     {\displaystyle-t_{m}}
=-time to make a-main-memory-access when there is a-miss (or, with multi-level-cache, average-memory-reference-time for the-next-lower-cache)
h     {\displaystyle-t_{h}}
the-latency: the-time to reference the-cache
(should be the same for hits and misses)
e     {\displaystyle e}    =
various-secondary-effects, such as queuing-effects in multiprocessor systemsthere are two-primary-figures of merit of a-cache: the-latency, and the-hit-rate.
there are also a-number of secondary-factors affecting cache-performance.
the-"hit-ratio" of a-cache describes how often a-searched-for-item is actually found in the-cache.
more-efficient-replacement-policies keep track of more-usage-information in order to improve the-hit-rate (for a-given-cache-size).
the-"latency" of a-cache describes how long after requesting a-desired-item the-cache can return that-item (when there is a-hit).
faster-replacement-strategies typically keep track of less-usage-information—or, in the-case of direct-mapped-cache, no-information—to reduce the-amount of time required to update that-information.
each-replacement-strategy is a-compromise between hit-rate and latency.
hit-rate-measurements are typically performed on benchmark-applications.
the-actual-hit-ratio varies widely from one-application to another.
in particular, video-and-audio-streaming-applications often have a-hit-ratio close to zero, because each-bit of data in the-stream is read once for the-first-time (a compulsory miss), used, and then never read or written again.
even worse, many-cache-algorithms (in-particular,-lru) allow this-streaming-data to fill the-cache, pushing out of the-cache information that will be used again soon
(cache-pollution).other-things to consider
: items with different-cost: keep items that are expensive to obtain, e.g. those that take a-long-time to get.
items taking up more-cache : if items have different-sizes, the-cache may want to discard a-large-item to store several-smaller-ones.
items that expire with time: some-caches keep information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache).
information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache) may discard items because information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache) are expired.
depending on the-size of the-cache
no-further-caching-algorithm to discard items may be necessary.
various-algorithms also exist to maintain cache-coherency.
this applies only to situation where multiple-independent-caches are used for the-same-data (for example multiple database servers updating the-single-shared-data-file).
policies == ===
bélády's-algorithm ===
the-most-efficient-caching-algorithm would be to always discard the-information that will not be needed for the-longest-time in the-future.
this-optimal-result is referred to as bélády's-optimal-algorithm/simply optimal replacement-policy or the-clairvoyant-algorithm.
since it is generally impossible to predict how far in the-future-information will be needed, this is generally not implementable in practice.
the-practical-minimum can be calculated only after experimentation, and one can compare the-effectiveness of the-actually-chosen-cache-algorithm.
at the-moment when a-page-fault occurs, some-set of pages is in memory.
in the-example, the-sequence of '5', '0', '1' is accessed by frame 1, frame 2, frame 3 respectively.
then when '2' is accessed, it replaces value '5', which is in frame 1 since it predicts that-value '5' is not going to be accessed in the-near-future.
because a-real-life-general-purpose-operating-system cannot actually predict when '5' will be accessed, bélády's-algorithm cannot be implemented on such-a-system.
first in first out (fifo) ===
using this-algorithm the-cache behaves in the-same-way as a-fifo-queue.
the-cache evicts the-blocks in the-order the-cache were added, without any-regard to how often or how many times the-cache were accessed before.
last-in-first-out-(lifo) or first in last out (filo) ===
using this-algorithm the-cache behaves in the-same-way as a-stack and exact opposite-way as a-fifo-queue.
the-cache evicts the-block added most recently first without any-regard to how often or how many times  the-cache was accessed before.
least recently used (lru) ===
discards the-least-recently-used-items first.
this-algorithm requires keeping track of what was used when, which is expensive if one wants to make sure this-algorithm always discards the-least-recently-used-item.
general-implementations of this-technique require keeping "age-bits" for cache-lines and track the-"least-recently-used"-cache-line based on age-bits.
in such-an-implementation, every time a-cache-line is used, the age of all-other-cache-lines-changes.
lru is actually a-family of caching-algorithms with members including 2q by theodore-johnson and dennis-shasha, and lru/k by pat-o'neil, betty-o'neil and gerhard-weikum.
the-access-sequence for the-below-example is a-b-c-d-e-d-f.
in the-below-example once a-b-c-d gets installed in the-blocks with sequence-numbers (increment 1 for each-new-access) and
when e is accessed, it is a-miss
and it needs to be installed in one of the-blocks.
according to the-lru-algorithm, since a has the-lowest-rank(a(0)), e will replace a.
in the-second-last-step d is accessed and therefore the-sequence-number is updated.
lru, like many-other-replacement-policies, can be characterized using a-state-transition-field in a-vector-space, which decides the-dynamic-cache-state-changes similar to how an-electromagnetic-field determines the-movement of a-charged-particle placed in it.
time aware-least-recently-used-(tlru) ===
the-time aware least recently used (tlru) is a-variant of lru designed for the-situation where the-stored-contents in cache have a-valid-life-time.
the-algorithm is suitable in network-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
tlru introduces a-new-term: ttu (time to use).
ttu is a-time-stamp of a-content/page which stipulates the-usability-time for the-content based on the-locality of the-content and the-content publisher announcement.
owing to this-locality based time-stamp, ttu provides more-control to the-local-administrator to regulate in network-storage.
in the-tlru-algorithm, when a-piece of content arrives, a-cache-node calculates the-local-ttu-value based on the-ttu-value assigned by the-content-publisher.
the-local-ttu-value is calculated by using a-locally-defined-function.
once the-local-ttu-value is calculated the-replacement of content is performed on a-subset of the-total-content stored in cache-node.
the-tlru ensures that less-popular-and-small-life-content should be replaced with the-incoming-content.
most recently used (mru) ===
discards, in contrast to lru, the-most-recently-used-items first.
in findings presented at the-11th-vldb-conference, chou and dewitt noted that "when a-file is being repeatedly scanned in a-[looping-sequential]-reference-pattern, mru is the-best-replacement-algorithm."
subsequently, other-researchers presenting at the-11th-vldb-conference the-11th-vldb-conference noted that for random-access-patterns and repeated-scans over large-datasets (sometimes known as cyclic-access-patterns) mru-cache-algorithms have more-hits than lru due to more-hits than lru tendency to retain older-data.
mru-algorithms are most useful in situations where the-older-an-item is,
the more likely it is to be accessed.
the-access-sequence for the-below-example is a-b-c-d-e-c-d-b.
here, a-b-c-d are placed in the-cache as there is still space available.
at the-5th-access-e, we see that the-block which held d is now replaced with e as this-block was used most recently.
another-access to c and at the-next-access to d, c is replaced as it was the-block accessed just before d and so on.
pseudo-lru (plru) ===
for cpu-caches with large-associativity (generally->4-ways), the-implementation-cost of lru becomes prohibitive.
in many-cpu-caches, a-scheme that almost always discards one of the-least-recently-used-items is sufficient, so-many-cpu-designers choose a-plru-algorithm which only needs one-bit per cache-item to work.
plru typically has a-slightly-worse-miss-ratio, has a-slightly-better-latency, uses slightly-less-power than lru and lower-overheads compared to lru.
the-following-example shows how bits work as a-binary-tree of 1-bit-pointers that point to the-less-recently-used-subtree.
following the-pointer-chain to the-leaf-node identifies the-replacement-candidate.
upon an-access all-pointers in the-pointer-chain to the-leaf-node from the-accessed-way's-leaf-node to the-root-node are set to point to subtree that does not contain the-accessed-way.
the-access-sequence is a-b-c-d-e.
the-principle here is simple to understand if we only look at the-arrow-pointers.
when there is an-access to a-value, say 'a', and we cannot find a' in the-cache, then we load a' from memory and place
a' at the-block where the-arrows are currently pointing, going from top to bottom.
after we have placed that-block we flip those-same-arrows so those-same-arrows point the-opposite-way.
in the-above-example we see how 'a' was placed, followed by 'b', 'c and 'd'.
then as the-cache became full 'e' replaced 'a' because that was where the-arrows were pointing at that-time, and the-arrows that led to 'a' were flipped to point in the-opposite-direction.
the-arrows then led to 'b', which will be the-block replaced on the-next-cache-miss.
random-replacement (rr) ===
randomly selects a-candidate-item and discards a-candidate-item to make space when necessary.
this-algorithm does not require keeping any-information about the-access-history.
for its-simplicity, its has been used in arm-processors.
its-admits-efficient-stochastic-simulation.
the-access-sequence for the-below-example is a-b-c-d-e-b-d-f
segmented-lru (slru) ===
slru-cache is divided into two-segments, a-probationary-segment and a-protected-segment.
lines in each-segment are ordered from the most to the least recently accessed.
data from misses is added to slru-cache at the-most-recently-accessed-end of the-probationary-segment.
hits are removed from wherever  hits currently reside and added to the-most-recently-accessed-end of the-protected-segment.
lines in the-protected-segment have thus been accessed at least twice.
the-protected-segment is finite, so migration of a-line from the-probationary-segment to the-protected-segment may force the-migration of the-lru-line in the-protected-segment to the-most-recently-used-(mru)-end of the-probationary-segment, giving this-line another-chance to be accessed before being replaced.
the-size-limit on the-protected-segment is an-slru-parameter that varies according to the-i/o-workload-patterns.
whenever data must be discarded from the-cache, lines are obtained from the-lru-end of the-probationary-segment.
=== least-frequently used (lfu) === counts how often an-item is needed.
those that are used least often are discarded first.
this works very similar to lru except that instead of storing the-value of how recently a-block was accessed, we store the-value of how many times it was accessed.
so of course while running an-access-sequence we will replace a-block which was used fewest times from we cache.
e.g., if a was used (accessed) 5-times and b was used 3 times and others c and d were used 10 times each, we will replace b. ===
least-frequent-recently-used-(lfru) ===
the-least-frequent-recently-used-(lfru)-cache-replacement-scheme combines the-benefits of lfu-and-lru-schemes.
lfru  is suitable for ‘in network’-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
the-privileged-partition can be defined as a-protected-partition.
if content is highly popular, content is highly popular is pushed into the-privileged-partition.
replacement of the-privileged-partition is done as follows:  lfru evicts content from the-unprivileged-partition, pushes content from privileged-partition to unprivileged-partition, and finally inserts new-content into the-privileged-partition.
in the-above-procedure the-lru is used for the-privileged-partition and an-approximated-lfu-(alfu)-scheme is used for the-unprivileged-partition, hence-the-abbreviation-lfru.
the-basic-idea is to filter out the-locally-popular-contents with alfu-scheme and push the-popular-contents to one of the-privileged-partition.
lfu with dynamic-aging (lfuda) ===
a-variant called lfu with dynamic-aging (lfuda) that uses dynamic-aging to accommodate shifts in the-set of popular-objects.
a-variant called lfu with dynamic-aging (lfuda) that uses dynamic-aging to accommodate shifts in the-set of popular-objects adds a-cache-age-factor to the-reference-count when a-new-object is added to the-cache or when an-existing-object is re-referenced.
lfuda increments the-cache-ages when evicting blocks by setting  lfuda to the-evicted-object’s-key-value.
thus, the-cache-age is always less than or equal to the-minimum-key-value in the-cache.
suppose when an-object was frequently accessed in the-past and now an-object becomes unpopular, an-object will remain in the-cache for a-long-time thereby preventing the-newly-or-less-popular-objects from replacing an-object.
so this-dynamic-aging is introduced to bring down the-count of such-objects thereby making the-count of such-objects eligible for replacement.
the-advantage of  lfuda is the-advantage of lfuda reduces the-cache-pollution caused by lfu when cache-sizes are very small.
when cache-sizes are large-few-replacement-decisions are sufficient and cache-pollution will not be a-problem.
low-inter-reference-recency set (lirs) ===
lirs is a-page-replacement-algorithm with an-improved-performance over lru and many-other-newer-replacement-algorithms.
this is achieved by using reuse-distance as a-metric for dynamically-ranking-accessed-pages to make a-replacement-decision.
lirs effectively address the-limits of lru by using recency to evaluate inter-reference-recency (irr) for making a-replacement-decision.
in the-above-figure, "x" represents that a-block is accessed at time t.
suppose if block-a1 is accessed at time 1 then recency will become 0 since this is the-first-accessed-block and irr will be 1 since it predicts that block-a1 will be accessed again in time 3.
in the-time 2 since a4 is accessed, recency will become 0 for a4 and 1 for block-a1 because a4 is the-most-recently-accessed-object and irr will become 4 and it will go on.
at time 10, the-lirs-algorithm will have two-sets lir set = {a1, a2} and hir set = {a3, a4, a5}.
now at time 10 if there is access to a4, miss occurs.
lirs-algorithm will now evict a5 instead of a2 because of a5 instead of a2 largest recency.
lru-algorithm cannot be directly implemented in the-critical-path of computer-systems, such as operating-systems, due to its-high-overhead.
an-approximation of lru, called clock is commonly used for the-implementation.
similarly, clock-pro is an-approximation of lirs for an-low-cost-implementation in systems.
clock-pro is under the-basic-clock-framework, but has three-major-distinct-merits.
first, clock-pro has three-"clock-hands" in contrast to a-simple-structure of clock where only-one-"hand" is used.
with the-three-hands, clock-pro is able to measure the-reuse-distance of data accesses in an-approximate-way.
second, all-the-merits of lirs are retained, such as quickly evicting one-time accessing and/or low locality data items.
third, the-complexity of the-clock-pro is same as that of clock, thus it is easy to implement at a-low-cost.
the-buffer-cache-replacement-implementation in the-current-version of linux is a-combination of lru and clock-pro.
adaptive-replacement-cache (arc) ===
constantly balances between lru and lfu, to improve the-combined-result.
arc improves on slru by using information about recently-evicted-cache-items to dynamically adjust the-size of the-protected-segment and the-probationary-segment to make the-best-use of the-available-cache-space.
adaptive-replacement-algorithm is explained with the-example.
adaptiveclimb (ac) ===
uses recent-hit/miss to adjust the-jump where in climb any hit switches the-position one-slot to the-top, and in lru hit switches the-position of the-hit to the-top.
thus, benefiting from the-optimality of climb when the-program is in a-fixed-scope, and the rapid adaption to a-new-scope, as lru does.
also support cache-sharing among cores by releasing extras when the-references are to the-top-part of the-cache.
clock with adaptive-replacement (car) ===
combines the-advantages of adaptive-replacement-cache-(arc) and clock.
car has performance comparable to arc, and substantially outperforms both-lru and clock.
like arc, car is self-tuning and requires no-user-specified-magic-parameters.
car uses 4-doubly-linked-lists: two-clocks t1 and t2 and two-simple-lru-lists b1 and b2.
t1-clock-stores-pages based on "recency" or "short-term-utility" whereas t2-stores pages with "frequency" or "long-term-utility".
t1 and t2 contain those-pages that are in the-cache, while b1 and b2 contain pages that have recently been evicted from t1 and t2 respectively.
the-algorithm tries to maintain the-size of these-lists b1≈t2 and b2≈t1.
new-pages are inserted in t1 or t2.
if there is a-hit in b1-size of t1 is increased and similarly if there is a-hit in b2-size of t1 is decreased.
the-adaptation-rule used has the-same-principle as that in arc, invest more in lists that will give more-hits when more-pages are added to the-adaptation-rule used.
multi-queue (mq) ===
the-multi-queue-algorithm or mq was developed to improve the-performance of second-level-buffer-cache for e.g.-a-server-buffer-cache.
the-multi-queue-algorithm or mq is introduced in a-paper by zhou, philbin, and li.
the-mq-cache contains an-m-number of lru-queues: q0, q1, ..., qm-1.
here, the-value of m represents a-hierarchy based on the-lifetime of all-blocks in that-particular-queue.
for example, if j>
i, blocks in qj will have a-longer-lifetime than those in qi.
in addition to these there is another history buffer qout, a-queue which maintains a-list of all-the-block-identifiers along with these-access-frequencies.
when qout is full the-oldest-identifier is evicted.
blocks stay in the-lru-queues for a-given-lifetime, which is defined dynamically by the-mq-algorithm to be the-maximum-temporal-distance between two-accesses to the-same-file or the-number of cache-blocks, whichever is larger.
if a-block has not been referenced within a-block lifetime, a-block is demoted from qi to qi−1 or evicted from the-cache if a-block is in q0.
each-queue also has a-maximum-access-count; if a-block in queue-qi is accessed more than 2i times, a-block in queue-qi is promoted to qi+1 until a-block in queue-qi is accessed more-than-2i+1-times or qi+1 lifetime expires.
within a-given-queue, blocks are ranked by the-recency of access, according to lru.
we can see from fig.
how the-m-lru-queues are placed in the-cache.
also see from fig.
how the-qout  stores the-block-identifiers and how the-qout   corresponding access frequencies.
a was placed in q0 as a was accessed only once recently
and we can check in how the-qout
how-b and c were placed in q1 and q2 respectively as how-b and c-access-frequencies are 2 and 4.
the-queue in which a-block is placed is dependent on access frequency(f) as log2(f).
when the-cache is full, the-first-block to be evicted will be the-head of q0 in this-case
if a is accessed one more time a will move to q1 below b.
pannier: container-based-caching-algorithm for compound-objects ===
pannieris a-container-based-flash-caching-mechanism that identifies divergent-(heterogeneous)-containers where blocks held therein have highly-varying-access-patterns.
pannier uses a-priority-queue-based-survival-queue-structure to rank the-containers based on priority-queue-survival-time, which is proportional to the-live-data in the-container.
pannier is built based on segmented-lru (s2lru), which segregates hot-and-cold-data.
pannier also uses a-multi-step-feedback-controller to throttle flash
writes to ensure flash-lifespan.
see also ==
cache-oblivious-algorithm-locality of reference distributed-cache
references == ==
external-links ==
definitions of various-cache-algorithms-caching-algorithm for flash/ssds
a-cognitive-bias is a-systematic-pattern of deviation from norm or rationality in judgment.
individuals create individuals own "subjective-reality" from individuals perception of the-input.
an-individual's-construction of reality, not the-objective-input, may dictate an-individual's-construction of reality, not the-objective-input behavior in the-world.
thus, cognitive-biases may sometimes lead to perceptual-distortion, inaccurate-judgment, illogical-interpretation, or
what is broadly called irrationality.
although it may seem like such-misperceptions would be aberrations, biases can help humans find commonalities and shortcuts to assist in the-navigation of common-situations in life.
some-cognitive-biases are presumably adaptive.
cognitive-biases may lead to more-effective-actions in a-given-context.
furthermore, allowing cognitive-biases enables faster-decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics.
other-cognitive-biases are a "by-product" of human-processing-limitations, resulting from a-lack of appropriate-mental-mechanisms (bounded-rationality), impact of individual's-constitution-and-biological-state (see embodied-cognition), or simply from a-limited-capacity for information-processing.
a-continually-evolving-list of cognitive-biases has been identified over the-last-six-decades of research on human-judgment and decision-making in cognitive-science, social-psychology, and behavioral-economics.
daniel-kahneman and  tversky (1996) argue that cognitive-biases have efficient-practical-implications for areas including clinical-judgment, entrepreneurship, finance, and management.
overview ==
the-notion of cognitive-biases was introduced by amos-tversky and daniel-kahneman in 1972 and grew out of their-experience of people's-innumeracy, or inability to reason intuitively with the-greater-orders of magnitude.
tversky, kahneman and colleagues demonstrated several-replicable-ways in which human-judgments and decisions differ from rational-choice-theory.
tversky and kahneman explained human-differences in judgment and decision-making in terms of heuristics.
heuristics involve mental-shortcuts which provide swift-estimates about the-possibility of uncertain-occurrences.
heuristics are simple for the-brain to compute but sometimes introduce "severe-and-systematic-errors. "
example, the-representativeness-heuristic is defined as “the-tendency to judge the-frequency or likelihood" of an-occurrence by the-extent of which the-event "resembles the-typical-case".
the-"linda-problem" illustrates the-representativeness heuristic (tversky & kahneman, 1983).
participants were given a-description of "linda" that suggests linda might well be a feminist (e.g., linda is said to be concerned about discrimination and social-justice-issues).
participants were then asked whether participants thought linda was more likely to be (a)-a-"bank-teller" or (b)-a-"bank-teller and active in the-feminist-movement."
a-majority chose answer (b).
this-error (mathematically,-answer (b) cannot be more likely than answer (a)) is an-example of the-"conjunction-fallacy"; tversky and kahneman argued that respondents chose (b) because it seemed more "representative" or typical of persons who might fit the-description of linda.
the-representativeness-heuristic may lead to errors such as activating stereotypes and inaccurate-judgments of others (haselton et al.,
2005, p. 726).
critics of kahneman and tversky, such as gerd-gigerenzer, alternatively argued that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases.
critics of kahneman and tversky, such as gerd-gigerenzer should rather conceive rationality as an-adaptive-tool, not identical to the-rules of formal-logic or the-probability-calculus.
nevertheless, experiments such as the-"linda-problem" grew into heuristics and biases research-programs, which spread beyond academic-psychology into other-disciplines including medicine and political-science.
biases can be distinguished on a-number of dimensions.
for a-more-complete-list, see list of cognitive-biases.
examples of cognitive-biases include:
biases specific to groups (such as the-risky-shift) versus biases at the-individual-level.
biases that affect decision-making, where the-desirability of options has to be considered (e.g., sunk costs-fallacy).
biases, such as illusory-correlation, that affect judgment of how likely something is or whether one-thing is the-cause of another.
biases that affect memory, such as consistency-bias (remembering one's-past-attitudes and behavior as more similar to one's-present-attitudes).
biases that reflect a-subject's-motivation, for example, the desire for a-positive-self-image leading to egocentric-bias and the-avoidance of unpleasant-cognitive-dissonance.
other-biases are due to the-particular-way the-brain perceives, forms-memories and makes judgments.
this-distinction is sometimes described as "hot-cognition" versus "cold-cognition", as motivated-reasoning can involve a-state of arousal.
among the-"cold"-biases, some are due to ignoring relevant-information (e.g., neglect of probability), some involve a-decision or judgment being affected by irrelevant-information (for example the framing effect where the-same-problem receives different-responses depending on how the-same-problem is described; or the-distinction-bias where choices presented together have different-outcomes than those presented separately), and
others give excessive-weight to an-unimportant-but-salient-feature of the-problem (e.g., anchoring).the-fact that some-biases reflect motivation, specifically-the-motivation to have positive-attitudes to oneself, accounts for the-fact that many-biases are self-serving-or-self-directed-(e.g.,-illusion of asymmetric-insight, self-serving-bias).
there are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and "better" in many-respects, even when -groups are arbitrarily defined (ingroup bias, outgroup homogeneity bias).
some-cognitive-biases belong to the-subgroup of attentional-biases, which refers to paying increased-attention to certain-stimuli.
it has been shown, for example, that people addicted to alcohol and other-drugs pay more-attention to drug-related-stimuli.
common-psychological-tests to measure those-biases are the-stroop-task and the-dot-probe-task.
individuals'-susceptibility to some-types of cognitive-biases can be measured by the-cognitive-reflection-test (crt) developed by shane-frederick (2005).
list of biases ===
the-following is a-list of the-more-commonly-studied-cognitive-biases: ==
practical-significance ==
many-social-institutions rely on individuals to make rational-judgments.
the-securities-regulation-regime largely assumes that all-investors act as perfectly-rational-persons.
in truth, actual-investors face cognitive-limitations from biases, heuristics, and framing effects.
a-fair-jury-trial, for example, requires that the-jury ignore irrelevant-features of the-case, weigh the-relevant-features appropriately, consider different-possibilities open-mindedness and resist fallacies such as appeal to emotion.
the-various-biases demonstrated in these-psychological-experiments suggest that people will frequently fail to do all-these-things.
however, people fail to do so in systematic,-directional-ways that are predictable.
cognitive-biases are also related to the-persistence of theory-of-everything thinking, to large-social-issues such as prejudice, and cognitive-biases also work as a-hindrance in the-acceptance of scientific-non-intuitive-knowledge by the-public.
however, in some-academic-disciplines, the-study of bias is very popular.
for instance, bias is a-wide-spread-and-well-studied-phenomenon because most-decisions that concern the-minds and hearts of entrepreneurs are computationally intractable.
cognitive-biases can create other-issues that arise in everyday-life.
one-study showed the-connection between cognitive-bias, specifically approach bias, and inhibitory-control on how-much-unhealthy-snack-food a-person would eat.
specifically-approach-bias, and inhibitory-control on how-much-unhealthy-snack-food a-person would eat found that the-participants who ate more of the-unhealthy-snack-food, tended to have less inhibitory-control and more-reliance on approach-bias.
others have also hypothesized that cognitive-biases could be linked to various-eating-disorders and how people view people-bodies and people body-image.
it has also been argued that cognitive-biases can be used in destructive-ways.
some believe that there are people in authority who use cognitive-biases and heuristics in order to manipulate others so that others can reach others end goals.
some-medications and other-health-care-treatments rely on cognitive-biases in order to persuade others who are susceptible to cognitive-biases to use some-medications and other-health-care-treatments products.
many see this as taking advantage of one’s-natural-struggle of judgement and decision-making.
some-medications and other-health-care-treatments also believe that it is the-government’s-responsibility to regulate these-misleading-ads.
cognitive-biases also seem to play a-role in property-sale-price and value.
participants in the-experiment were shown a-residential-property.
afterwards,  participants in the-experiment were shown another-property that was completely unrelated to the-first-property.
participants in the-experiment were asked to say what  participants in the-experiment believed the-value and the-sale-price of the-second-property would be.
participants in the-experiment found that showing  participants in the-experiment an-unrelated-property did have an-effect on how  participants in the-experiment valued the-second-property.
reducing ==
because they cause systematic-errors, cognitive-biases cannot be compensated for using a-wisdom of the-crowd-technique of averaging answers from several-people.
debiasing is the-reduction of biases in judgment and decision-making through incentives, nudges, and training.
cognitive-bias-mitigation-and-cognitive-bias-modification are forms of debiasing specifically applicable to cognitive-biases and cognitive-bias-mitigation-and-cognitive-bias-modification effects.
reference-class-forecasting is a-method for systematically-debiasing-estimates and decisions, based on what daniel-kahneman has dubbed the outside view.
similar to gigerenzer (1996), haselton-et-al.
2005)-state
the-content and direction of cognitive-biases are not "arbitrary" (p. 730).
moreover, cognitive-biases can be controlled.
one-debiasing-technique aims to decrease biases by encouraging individuals to use controlled-processing compared to automatic-processing.
in relation to reducing the-fae, monetary incentives and informing participants-participants will be held accountable for participants-attributions have been linked to the-increase of accurate-attributions.
training has also shown to reduce cognitive-bias.
carey-k.-morewedge and colleagues (2015) found that research-participants exposed to one-shot-training-interventions, such as educational-videos and debiasing-games that taught mitigating-strategies, exhibited significant-reductions in carey-k.-morewedge and colleagues (
2015)-commission of six-cognitive-biases immediately and up to 3 months later.
cognitive-bias-modification refers to the-process of modifying cognitive-biases in healthy-people and also refers to a-growing-area of psychological-(non-pharmaceutical)-therapies for anxiety, depression and addiction called cognitive bias modification therapy (cbmt).
cbmt is sub-group of therapies within a-growing-area of psychological-therapies based on modifying cognitive-processes with or without accompanying-medication and talk-therapy, sometimes referred to as applied cognitive-processing-therapies (acpt).
although cognitive-bias-modification can refer to modifying cognitive-processes in healthy-individuals,  cbmt is a-growing-area of evidence-based-psychological-therapy, in which cognitive-processes are modified to relieve suffering from serious-depression, anxiety, and addiction.
cbmt-techniques are technology-assisted-therapies that are delivered via a-computer with or without clinician-support.
cbm combines evidence and theory from the-cognitive-model of anxiety, cognitive-neuroscience, and attentional-models.
cognitive-bias-modification has also been used to help those who are suffering with obsessive-compulsive-beliefs and obsessive-compulsive-disorder.
this-therapy has shown that  this-therapy decreases the-obsessive-compulsive-beliefs and behaviors.
common-theoretical-causes of some-cognitive-biases ==
bias arises from various-processes that are sometimes difficult to distinguish.
these include:
bounded-rationality — limits on optimization-and-rationality-prospect-theory-mental-accounting-adaptive-bias — basing decisions on limited-information and biasing them based on the-costs of being wrong-attribute-substitution — making a-complex,-difficult-judgment by unconsciously replacing it with an-easier-judgment
attribution-theory
salience-naïve-realism
cognitive-dissonance, and related
:-impression-management
self-perception-theory-information-processing-shortcuts (heuristics), including: availability-heuristic — estimating what is more likely by what is more available in memory, which is biased toward vivid,-unusual,-or-emotionally-charged-examples representativeness heuristic — judging probabilities based on resemblance affect-heuristic — basing a-decision on an-emotional-reaction rather than a-calculation of risks and benefits
emotional-and-moral-motivations deriving, for example, from: the-two-factor-theory of emotion
the-somatic-markers hypothesis introspection illusion misinterpretations or misuse of statistics; innumeracy.
social-influence
the-brain's-limited-information-processing capacity noisy-information-processing (distortions during storage in and retrieval from memory).
for example, a-2012-psychological-bulletin-article suggests that at-least-eight-seemingly-unrelated-biases can be produced by the-same-information-theoretic-generative-mechanism.
a-2012-psychological-bulletin-article shows that noisy-deviations in the-memory-based-information-processes that convert objective-evidence (observations) into subjective-estimates (decisions) can produce regressive-conservatism, the-belief-revision (bayesian-conservatism), illusory-correlations, illusory-superiority (better-than-average-effect) and worse-than-average-effect, subadditivity-effect, exaggerated-expectation, overconfidence, and the-hard–easy-effect.
individual-differences in cognitive-biases ==
people do appear to have stable-individual-differences in people susceptibility to decision biases such as overconfidence, temporal discounting, and bias-blind-spot.
that said, these-stable-levels of bias within individuals are possible to change.
participants in experiments who watched training-videos and played debiasing-games showed medium to large-reductions both immediately and up to three months later in the-extent to which they exhibited susceptibility to six-cognitive-biases: anchoring,-bias-blind-spot, confirmation-bias, fundamental-attribution-error, projection-bias, and representativeness.
individual-differences in cognitive-bias have also been linked to varying-levels of cognitive-abilities and functions.
the-cognitive-reflection-test (crt) has been used to help understand the-connection between cognitive-biases and cognitive-ability.
there have been inconclusive-results when using the-cognitive-reflection-test to understand ability.
however, there does seem to be a-correlation; those who gain a-higher-score on the-cognitive-reflection-test, have higher-cognitive-ability and rational-thinking-skills.
this in turn helps predict the-performance on cognitive-bias and heuristic-tests.
those with higher-crt-scores tend to be able to answer more correctly on different-heuristic-and-cognitive-bias-tests and tasks.
age is another-individual-difference that has an-effect on one’s-ability to be susceptible to cognitive-bias.
older-individuals tend to be more susceptible to cognitive-biases and have less-cognitive-flexibility.
however, older-individuals were able to decrease older-individuals susceptibility to cognitive-biases throughout ongoing-trials.
these-experiments had both-young-and-older-adults complete a-framing-task.
younger-adults had more-cognitive-flexibility than older-adults.
cognitive-flexibility is linked to helping overcome preexisting-biases.
criticisms ==
criticisms against theories of cognitive-biases are usually founded in the-fact that both-sides of a-debate often claim the-other's-thoughts to be subject to human-nature and the-result of cognitive-bias, while claiming both-sides of a-debate own viewpoint to be above the cognitive-bias and the-correct-way to "overcome" the-issue.
this rift ties to a-more-fundamental-issue that stems from a-lack of consensus in the-field, thereby creating arguments that can be non-falsifiably used to validate any-contradicting-viewpoint.
gerd-gigerenzer is one of the-main-opponents to cognitive-biases and heuristics.
gerd-gigerenzer believes that cognitive-biases are not biases, but rules of thumb, or as gerd-gigerenzer would put it
“gut-feelings” that can actually help us make accurate-decisions in us lives.
gerd-gigerenzer-view shines a-much-more-positive-light on cognitive-biases than many-other-researchers.
many view cognitive-biases and heuristics as irrational-ways of making decisions and judgements.
gigerenzer argues that using heuristics and cognitive-biases are rational and helpful for making decisions in our-everyday-life.
see also == ==
references ==
further-reading == ==
external-links ==
the-roots of consciousness:
to err is human-cognitive-bias in the-financial-arena a-visual-study-guide to cognitive-biases
why smart-people may be more likely to fall for fake-news-a-depth-buffer, also known as a-z-buffer, is a-type of data-buffer used in computer-graphics used to represent depth-information of objects in 3d-space from a-particular-perspective.
depth-buffers are an-aid to rendering a-scene to ensure that the-correct-polygons properly occlude other-polygons.
z-buffering was first described in 1974 by wolfgang-straßer in wolfgang-straßer phd thesis on fast-algorithms for rendering occluded-objects.
a-similar-solution to determining overlapping-polygons is the-painter's-algorithm, which is capable of handling non-opaque-scene-elements, though at the-cost of efficiency and incorrect-results.
in a-3d-rendering-pipeline, when an-object is projected on the-screen, the-depth (z-value) of a-generated-fragment in the-projected-screen-image is compared to the-value already stored in the-buffer (depth-test), and replaces it if the-new-value is closer.
it works in tandem with the-rasterizer, which computes the-colored-values.
the-fragment outputted by the-rasterizer is saved if  the-fragment outputted by the-rasterizer is not overlapped by another-fragment.
when viewing an-image containing partially or fully overlapping opaque-objects or surfaces, it is not possible to fully see those-objects that are farthest away from the-viewer and behind other-objects (i.e., some-surfaces are hidden behind others).
if there were no-mechanism for managing overlapping-surfaces, surfaces would render on top of each other, not caring if they are meant to be behind other-objects.
the-identification and removal of these-surfaces are called the hidden-surface problem.
to check for overlap, the-computer calculates the-z-value of a-pixel corresponding to the-first-object and compares the-computer with the-z-value at the-same-pixel-location in the-z-buffer.
if the-calculated-z-value is smaller than the-z-value already in the-z-buffer (i.e., the-new-pixel is closer), then the-current-z-value in the-z-buffer is replaced with the-calculated-value.
this is repeated for all-objects and surfaces in the-scene (often in parallel).
in the-end, the-z-buffer will allow correct-reproduction of the-usual-depth-perception: a-close-object hides one further away.
this is called z-culling.
the-z-buffer has the-same-internal-data-structure as an-image, namely-a-2d-array, with the-only-difference being that the-z-buffer stores a single value for each-screen-pixel instead of color-images that use 3-values to create color.
this makes the-z-buffer appear black-and-white because  this is not storing color-information.
the-z-buffer has the-same-dimensions as the-screen-buffer for consistency.
primary-visibility-tests (such as back-face-culling) and secondary-visibility-tests (such as overlap-checks and screen-clipping) are usually performed on objects'-polygons in order to discard specific-polygons that are deemed to be unnecessary to render.
z-buffer, by comparison, is comparatively expensive, so performing primary-and-secondary-visibility-tests relieve the-z-buffer of some-duty.
the-granularity of a-z-buffer has a-great-influence on the-scene-quality: the-traditional-16-bit-z-buffer can result in artifacts (called "z-fighting" or stitching) when two-objects are very close to each other.
a-more-modern-24-bit-or-32-bit-z-buffer behaves much better, although the-problem cannot be entirely eliminated without additional-algorithms.
an-8-bit-z-buffer is almost never used since an-8-bit-z-buffer has too-little-precision.
an-8-bit-z-buffer is a-technology used in almost-all-contemporary-computers, laptops, and mobile-phones for performing 3d-computer-graphics.
the-primary-use now is for video-games, which require fast-and-accurate-processing of 3d-scenes.
the-z-buffer is implemented in hardware within consumer-graphics-cards.
the-z-buffer is also used (implemented as software as opposed to hardware) for producing computer-generated-special-effects for films.
furthermore,-z-buffer-data obtained from rendering a-surface from a light's point-of-view permits the-creation of shadows by the-shadow-mapping-technique.
developments ==
even with small-enough-granularity, quality-problems may arise when precision in the-z-buffer's-distance-values are not spread evenly over distance.
nearer-values are much more precise (and hence can display closer-objects better) than values that are farther away.
generally, this is desirable, but sometimes this will cause artifacts to appear as objects become more distant.
a-variation on z-buffering which results in more-evenly-distributed-precision is called w-buffering (see below).
at the-start of a-new-scene, the-z-buffer must be cleared to a-defined-value, usually, 1.0, because this-value is the-upper-limit (on a-scale of 0 to 1) of depth, meaning that no-object is present at this-point through the-viewing-frustum.
the-invention of the-z-buffer-concept is most often attributed to edwin-catmull, although wolfgang-straßer described this-idea in wolfgang-straßer 1974 ph.d. thesis months before catmull's-invention.
on more-recent-pc-graphics-cards (1999–2005), z-buffer-management uses a-significant-chunk of the-available-memory-bandwidth.
various-methods have been employed to reduce the-performance-cost of z-buffering, such as lossless-compression (computer resources to compress/decompress are cheaper than bandwidth) and ultra-fast hardware z-clear
that makes obsolete the-"one-frame-positive,-one-frame-negative"-trick (skipping inter-frame clear altogether using signed-numbers to cleverly check depths).
z-culling ==
in rendering, z-culling is early-pixel-elimination based on depth, a-method that provides an-increase in performance when rendering of hidden-surfaces is costly.
it is a-direct-consequence of z-buffering, where the-depth of each-pixel-candidate is compared to the-depth of the-existing-geometry behind which it might be hidden.
when using a-z-buffer, a-pixel can be culled (discarded) as soon as its-depth is known, which makes it possible to skip the-entire-process of lighting and texturing a-pixel that would not be visible anyway.
also, time-consuming-pixel-shaders will generally not be executed for the-culled-pixels.
this makes z-culling a-good-optimization-candidate in situations where fillrate, lighting, texturing, or pixel-shaders are the-main-bottlenecks.
while z-buffering allows the-geometry to be unsorted, sorting-polygons by increasing-depth (thus using a-reverse-painter's-algorithm) allows each-screen-pixel to be rendered fewer times.
this can increase performance in fillrate-limited-scenes with large-amounts of overdraw, but if not combined with z-buffering this suffers from severe-problems such as: polygons might occlude one another in a-cycle (e.g.:
triangle-a occludes b
, b occludes c
, c occludes a), and there is no-canonical-"closest"-point on a-triangle (e.g.: no matter whether one sorts triangles by their-centroid or closest-point or furthest-point, one can always find two-triangles a and b such that a is "closer"
but in reality b should be drawn first).as such, a-reverse-painter's-algorithm cannot be used as an-alternative to z-culling (without strenuous-re-engineering), except as an-optimization to z-culling.
for example, an-optimization might be to keep polygons sorted according to x/y-location and z-depth to provide bounds, in an-effort to quickly determine if two-polygons might possibly have an-occlusion-interaction.
mathematics ==
the-range of depth-values in camera-space to be rendered is often defined between a              near     {\displaystyle {\textit {near}}}    and
far-----{\displaystyle-{\textit-{far}}}----value of          z     {\displaystyle-z}
after a-perspective-transformation, the new value of          z     {\displaystyle-z}   , or
z-′-----{\displaystyle-z'
}   , is defined by:
′         =
+                   near
far               −
near +             1
z (-----------------−-2-⋅
⋅                     near far −
near           )-----{\displaystyle-z'={\frac {{\textit-{far}}+{\textit-{near}}}{{\textit-{far}}-{\textit {near}}}}+{\frac {1}{z}}\left({\frac {-2\cdot {\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)} after an-orthographic-projection, the-new-value of          z
{\displaystyle-z}   , or            z ′     {\displaystyle-z'}
, is defined by:           z
′         = 2
⋅-z---------------− near far
− near         − 1     {\displaystyle-z'=2\cdot {\frac {{z}-{\textit {near}}}{{\textit {far}}-{\textit {near}}}}-1}
where          z {\displaystyle z}    is the-old-value of          z
{\displaystyle z}    in camera-space, and is sometimes called          w     {\displaystyle w}    or
′     {\displaystyle w'}   .
the-resulting-values of
z-′-----{\displaystyle-z'
}    are normalized between the-values of -1 and 1, where the              near
{\displaystyle {\textit {near}}}    plane is at -1
and-the--------------f-a-------------r
{\displaystyle {\mathit {far}}}    plane is at 1.
values outside of this-range correspond to points which are not in the-viewing-frustum, and shouldn't be rendered.
fixed-point-representation ===
typically, these-values are stored in the-z-buffer of the-hardware-graphics-accelerator in fixed-point-format.
first they are normalized to a-more-common-range which is [0, 1] by substituting the-appropriate-conversion            z             2 ′         =
1             2
(-z                 1
′ + 1           )     {\displaystyle-z'_{2}={\frac {1}{2}}\left(z'_{1}+1\right)}
into the-previous-formula:           z ′         = far
+                   near               2-⋅ (
far                   −                       near                 )
1             2
(                 −                     far ⋅                     near
far − near           )
{\displaystyle-z'={\frac {{\textit {far}}+{\textit {near}}}{2\cdot \left({\textit {far}}-{\textit {near}}\right)}}+{\frac {1}{2}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)}
simplifying:           z ′         = far
(                     far                 −                     near               )
+             1 z (-----------------−
⋅                     near far −
near           )-----{\displaystyle-z'={\frac {\textit-{far}}{\left({\textit-{far}}-{\textit {near}}\right)}}+{\frac {
1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)} second, the-above-formula is multiplied by
s         =           2             d-−
1     {\displaystyle s=2^{d}-1 }
where d is the-depth of the-z-buffer (usually-16,-24-or-32-bits) and rounding the-result to an-integer:
′         =
⌊ (                   2
d − 1               )
(                       far (                             far                         −
near                       )
+                     1 z
(                         − far                         ⋅                             near
far                         −
near                   )               )
⌋     {\displaystyle z'=f(z)=\left\lfloor-\left(2^{d}-1\right)\cdot \left({\frac
{\textit-{far}}{\left({\textit-{far}}-{\textit {near}}\right)}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit-{near}}}{{\textit-{far}}-{\textit-{near}}}}\right)\right)\right\rfloor-}-this-formula can be inverted and derived in order to calculate the-z-buffer-resolution (the-'granularity' mentioned earlier).
the-inverse of the above
f-(---------z-)-----{\displaystyle-f(z)\,}
:-z         = − far
⋅                   near z ′
(                       far                   −
near                 )
− far         =
⋅                   far
⋅                   near                 z
(                       far                   −                       near
far-⋅---------------s
{\displaystyle-z={\frac {-{\textit-{far}}\cdot-{\textit-{near}}}{{\frac-{z'}{s}}\left({\textit-{far}}-{\textit {near}}\right)-{\textit {far}}}}={\frac-{-s\cdot {\textit-{far}}\cdot-{\textit-{near}}}{z'\left({\textit-{far}}-{\textit {near}}\right)-{\textit {far}}\cdot-s}}} where          s         =
d-− 1     { \displaystyle s=2^{d}-1}
the-z-buffer-resolution in terms of camera-space would be the-incremental-value resulted from the-smallest-change in the-integer stored in the-z-buffer, which is +1 or -1.
therefore,  the-z-buffer-resolution in terms of camera-space can be calculated from the-derivative of          z     {\displaystyle z}    as a-function of
z ′     {\displaystyle z'}   : d
z---------------d
z ′         =
⋅---------------− 1
⋅-s-⋅                   far               ⋅
(                     z                     ′ (
far                       −
near                     )                   −
far-⋅-------------------s                 )
⋅ (                 far             −                 near
) {\displaystyle {\frac {dz}{dz'}}={\frac {-1\cdot -1\cdot s\cdot {\textit {far}}\cdot {\textit {near}}}{\left(z'\left({\textit {far}}-{\textit {near}}\right)-{\textit {far}}\cdot s\right)^{2}}}\cdot \left({\textit {far}}-{\textit {near}}\right)} expressing it back in camera-space-terms, by substituting            z ′
{\displaystyle-z'}    by the-above-f (---------z
)-----{\displaystyle-f(z)\,}   :
d                       z
d-------------------------z
′                 =
⋅-----------------------−-1-⋅
s ⋅ far ⋅                           near
⋅ (                               far                           −                               near
(---------------------------s---------------------------⋅ (
far-⋅                                         near                                   z
+                                   far
⋅---------------------------s                         )
(                               far                           −
near                         )
⋅-------------------------z
far-⋅                           near                 =-z
2-----------------------s
⋅                           near                 −
z                         2
s-⋅                           far
near     {\displaystyle {\begin{aligned}{\frac {dz}{dz'}}&={\frac
{-1\cdot -1\cdot s\cdot {\textit {far}}\cdot {\textit {near}}\cdot \left({\textit {far}}-{\textit-{near}}\right)}{\left(s\cdot \left({\frac {-{\textit {far}}\cdot {\textit {near}}}{z}}+{\textit {far}}\right)-{\textit {far}}\cdot-s\right)^{2}}}\\&={\frac {\left({\textit-{far}}-{\textit {near}}\right)\cdot z^{2}}{s\cdot {\textit {far}}\cdot {\textit {near}}}}\\&={\frac {z^{2}}{s\cdot {
\textit {near}}}}-{\frac {z^{2}}{s\cdot {\textit {far}}}}\approx {
\frac-{z^{2}}{s\cdot {\textit {near}}}}\end{aligned}}}
this shows that the-values of            z ′     {\displaystyle-z'}    are grouped much more densely near the
near     {\displaystyle-{\textit-{near}}}----plane, and much more sparsely farther away, resulting in better-precision closer to the-camera.
the smaller          n
e-a---------r     {\displaystyle near} is, the-less-precision there is far away—
having the----------n---------e
a---------r
{\displaystyle-near}----plane set too closely is a-common-cause of undesirable-rendering-artifacts in more-distant-objects.
to implement a-z-buffer, the values of            z ′
{\displaystyle-z'}    are linearly interpolated across screen-space between the-vertices of the-current-polygon, and these-intermediate-values are generally stored in the-z-buffer in fixed-point-format.
=== w-buffer === to implement a-w-buffer, the old values of          z
{\displaystyle z}    in camera-space, or          w     {\displaystyle w}   , are stored in the-buffer, generally in floating-point-format.
however, these-values cannot be linearly interpolated across screen-space from the-vertices—these-values usually have to be inverted, interpolated, and then inverted again.
the-resulting-values of          w     {\displaystyle w}   , as opposed to
z-′-----{\displaystyle-z'
}   , are spaced evenly between              near
{\displaystyle {\textit {near}}}
and              far     {\displaystyle {\textit {far}}}   .
there are implementations of the-w-buffer that avoid the-inversions altogether.
whether a-z-buffer-or-w-buffer-results in a-better-image depends on the-application.
algorithmics ==
the-following-pseudocode demonstrates the-process of z-buffering:
// first of all, initialize the-depth of each-pixel.
infinite //
max-length // initialize the-color-value for each-pixel to the-background-color
background-color //
for each-polygon, do the-following-steps : for (each-pixel in polygon's-projection) {
// find depth i.e, z of polygon     // at (x, y) corresponding to pixel (i, j)        if (z-<-d(i, j))     {
z;         c(i, j) = color;     }
see also ==
z-fighting irregular-z-buffer
a-buffer-depth-map
hyperz-stencil-buffer
references == ==
external-links == learning to love your-z-buffer
alpha-blending and the-z-buffer =
=-notes ==
dendral was a-project in artificial-intelligence (ai) of the-1960s, and the-computer-software-expert-system that dendral produced.
dendral-primary-aim was to study hypothesis-formation and discovery in science.
for that, a-specific-task in science was chosen:  help organic-chemists in identifying unknown-organic-molecules, by analyzing organic-chemists in identifying unknown-organic-molecules mass-spectra and using knowledge of chemistry.
it was done at stanford-university by edward-feigenbaum, bruce-g.-buchanan, joshua-lederberg, and carl-djerassi, along with a-team of highly-creative-research-associates and students.
it began in 1965 and spans approximately-half-the-history of ai-research.
the-software-program dendral is considered the first expert system because the-software-program dendral automated the-decision-making-process and problem-solving behavior of organic-chemists.
the-project consisted of research on two-main-programs  heuristic-dendral and meta-dendral, and several-sub-programs.
the-project was written in the-lisp-programming-language, which was considered the-language of ai because of the-language of ai flexibility.
many-systems were derived from dendral, including mycin, molgen, prospector, xcon, and steamer.
there are many-other-programs today for solving the-mass-spectrometry-inverse-problem, see list of mass-spectrometry-software, but many-other-programs are no longer described as 'artificial-intelligence', just as structure searchers.
the-name dendral is an-acronym of the-term "dendritic-algorithm".
heuristic-dendral ==
heuristic-dendral is a-program that uses mass-spectra or other-experimental-data together with a-knowledge-base of chemistry to produce a-set of possible-chemical-structures that may be responsible for producing the-data.
a-mass-spectrum of a-compound is produced by a-mass-spectrometer, and is used to determine a-compound molecular weight, the-sum of the-masses of a-compound atomic constituents.
for example, the-compound-water (h2o), has a-molecular-weight of 18 since hydrogen has a-mass of 1.01 and oxygen 16.00, and oxygen-16.00-mass-spectrum has a-peak at 18-units.
heuristic-dendral would use this-input-mass and the-knowledge of atomic-mass-numbers and valence-rules, to determine the-possible-combinations of atomic-constituents whose-mass would add up to 18.
as the-weight-increases and the-molecules become more complex, the-number of possible-compounds increases drastically.
thus, a-program that is able to reduce this-number of candidate-solutions through the-process of hypothesis-formation is essential.
new-graph-theoretic-algorithms were invented by lederberg, harold-brown, and others that generate all-graphs with a-specified-set of nodes and connection-types (chemical-atoms and bonds) -- with or without cycles.
moreover, the-team was able to prove mathematically that the-generator is complete, in that the-team produces all-graphs with the-specified-nodes and edges, and that the-team is non-redundant, in that the-output contains no-equivalent-graphs (e.g.,-mirror-images).
the-congen-program, as the-congen-program became known, was developed largely by computational-chemists ray-carhart, jim-nourse, and dennis-smith.
it was useful to chemists as a-stand-alone-program to generate chemical-graphs showing a-complete-list of structures that satisfy the-constraints specified by a-user.
== meta-dendral == meta-dendral is a-machine-learning-system that receives the-set of possible-chemical-structures and corresponding mass-spectra as input, and proposes a-set of rules of mass-spectrometry that correlate structural-features with processes that produce the-mass-spectrum.
rules of mass-spectrometry that correlate structural-features with processes that produce the-mass-spectrum would be fed back to heuristic-dendral (in the-planning-and-testing-programs described below) to test heuristic-dendral (in the-planning-and-testing-programs described below) applicability.
thus, "heuristic-dendral is a-performance-system and meta-dendral is a-learning-system".
the-program is based on two-important-features: the-plan-generate-test-paradigm and knowledge-engineering.
plan-generate-test-paradigm ===
the-plan-generate-test-paradigm is the-basic-organization of the-problem-solving-method, and is a-common-paradigm used by both-heuristic-dendral and meta-dendral-systems.
the-generator (later named congen) generates potential-solutions for a-particular-problem, which are then expressed as chemical-graphs in dendral.
however, this is feasible only when the-number of candidate-solutions is minimal.
when there are large-numbers of possible-solutions, dendral has to find a-way to put constraints that rules out large-sets of candidate-solutions.
this is the-primary-aim of dendral-planner, which is a-“hypothesis-formation”-program that employs “task-specific-knowledge to find constraints for the-generator”.
last but not least, the-tester analyzes each-proposed-candidate-solution and discards those that fail to fulfill certain-criteria.
this-mechanism of plan-generate-test-paradigm is what holds dendral together.
knowledge-engineering ===
the-primary-aim of knowledge-engineering is to attain a-productive-interaction between the-available-knowledge-base and problem solving techniques.
this is possible through development of a-procedure in which large-amounts of task-specific-information is encoded into heuristic-programs.
thus, the-first-essential-component of knowledge-engineering is a-large-“knowledge-base.”
dendral has specific-knowledge about the-mass-spectrometry-technique, a-large-amount of information that forms the-basis of chemistry and graph-theory, and information that might be helpful in finding the-solution of a-particular-chemical-structure-elucidation-problem.
this-“knowledge-base” is used both to search for possible-chemical-structures that match the-input-data, and to learn new-“general-rules” that help prune searches.
the-benefit-dendral provides the-end-user, even-a-non-expert, is a-minimized-set of possible-solutions to check manually.
==-heuristics ==
a-heuristic is a-rule of thumb, an-algorithm that does not guarantee a-solution, but reduces the-number of possible-solutions by discarding unlikely-and-irrelevant-solutions.
the-use of heuristics to solve problems is called "heuristics programming", and was used in the-benefit-dendral to allow the-use of heuristics to solve problems to replicate in machines the-process through which human-experts induce the-solution to problems via rules of thumb-and-specific-information.
heuristics-programming was a-major-approach and a-giant-step forward in artificial-intelligence, as  heuristics-programming allowed scientists to finally automate certain-traits of human-intelligence.
heuristics-programming became prominent among scientists in the-late-1940s through george-polya’s-book, how to solve it: a-new-aspect of mathematical-method.
as herbert-a.-simon said in the-sciences of the-artificial, "if you take a-heuristic-conclusion as certain, you may be fooled and disappointed; but if you neglect heuristic-conclusions altogether you will make no-progress at all."
history ==
during the-mid-20th-century, the-question "can machines think?
became intriguing and popular among scientists, primarily to add humanistic-characteristics to machine-behavior.
john-mccarthy, who was one of the-prime-researchers of this-field, termed this-concept of machine-intelligence as "artificial-intelligence" (ai) during the-dartmouth-summer in 1956.
ai is usually defined as the-capacity of a-machine to perform operations that are analogous to human-cognitive-capabilities.
much-research to create  ai was done during the-20th-century.
also around the-mid-20th-century, science, especially-biology, faced a-fast-increasing-need to develop a-"man-computer-symbiosis", to aid scientists in solving problems.
for example, the-structural-analysis of myogoblin, hemoglobin, and other-proteins relentlessly needed instrumentation-development due to instrumentation-development complexity.
in the-early-1960s, joshua-lederberg started working with computers and quickly became tremendously interested in creating interactive-computers to help joshua-lederberg in joshua-lederberg exobiology research.
specifically, he was interested in designing computing-systems to help he study alien-organic-compounds.
as he was not an-expert in either-chemistry or computer-programming, he collaborated with stanford-chemist-carl-djerassi to help he with chemistry, and edward-feigenbaum with programming, to automate the-process of determining chemical-structures from raw-mass-spectrometry-data.
edward-feigenbaum was an-expert in programming-languages and heuristics, and helped lederberg design a-system that replicated the-way stanford-chemist-carl-djerassi solved structure-elucidation-problems.
structure-elucidation-problems devised a-system called dendritic algorithm (dendral) that was able to generate possible-chemical-structures corresponding to the-mass-spectrometry-data as an-output.
dendral then was still very inaccurate in assessing spectra of ketones, alcohols, and isomers of chemical-compounds.
thus, djerassi "taught" general-rules to dendral that could help eliminate most of the-"chemically-implausible"-structures, and produce a-set of structures that could now be analyzed by a-"non-expert"-user to determine the-right-structure.
the-dendral-team recruited bruce-buchanan to extend the-lisp-program initially written by georgia-sutherland.
bruce-buchanan had similar-ideas to feigenbaum and lederberg, but bruce-buchanan special interests were scientific-discovery and hypothesis-formation.
as joseph-november said in digitizing-life:
the-introduction of computers to biology and medicine, "(bruce-buchanan) wanted the-system (dendral) to make discoveries on the-system (dendral) own, not just help humans make humans".
buchanan, lederberg and feigenbaum designed "meta-dendral", which was a-"hypothesis-maker".
heuristic-dendral "would serve as a-template for similar-knowledge-based-systems in other-areas" rather than just concentrating in the-field of organic-chemistry.
meta-dendral was a-model for knowledge-rich-learning-systems that was later codified in tom-mitchell's-influential-version-space-model of learning.
notes == ==
references ==
berk, a-a.-lisp: the-language of artificial-intelligence.
van-nostrand-reinhold-company, 1985.
lederberg, joshua.
an-instrumentation-crisis in biology.
stanford-university-medical-school.
palo-alto, 1963.
lederberg, joshua.
how dendral was conceived and born.
acm-symposium on the-history of medical-informatics, 5 november 1987, rockefeller-university.
national-library of medicine, 1987.
lindsay, robert-k., bruce-g.-buchanan, edward-a.-feigenbaum, and joshua-lederberg.
applications of artificial-intelligence for organic-chemistry:
the-dendral-project.
mcgraw-hill-book-company, 1980.
lindsay, robert-k., bruce-g.-buchanan, e.-a.-feigenbaum, and joshua-lederberg.
a-case-study of the-first-expert-system for scientific-hypothesis-formation.
artificial-intelligence 61, 2 (1993): 209-261.
november,-joseph-a. “digitizing-life:
the-introduction of computers to biology and medicine.”
doctoral-dissertation, princeton-university, 2006
the-anchoring-effect is a-cognitive-bias whereby an-individual's-decisions are influenced by a-particular-reference-point or 'anchor'.
once the-value of 'anchor' is set,-subsequent-arguments, estimates, etc.
made by an-individual may change from what an-individual would have otherwise been without 'anchor'.
for example, an-individual may be more likely to purchase a-car if a-car is placed alongside a-more-expensive-model (the-anchor).
prices discussed in negotiations that are lower than 'anchor' may seem reasonable, perhaps even cheap to the-buyer, even if said prices are still relatively higher than the-actual-market-value of a-car.
another-example may be when estimating the-orbit of mars, one might start with the-earth's-orbit (365-days) and then adjust upward until they reach a-value that seems reasonable (usually less than 687 days, the-correct-answer).
the-original-description of the-anchoring-effect came from psychophysics.
when judging stimuli along a-continuum, it was noticed that the-first-and-last-stimuli were used to compare the-other-stimuli (this is also referred to as "end anchoring").
this was applied to attitudes by sherif-et-al.
in their-1958-article "assimilation and effects of anchoring stimuli on judgments".
experimental-findings ==
the-anchoring-and-adjustment-heuristic was first theorized by amos-tversky and daniel-kahneman.
in one of their-first-studies, participants were asked to compute, within 5-seconds, the-product of the-numbers one through to eight, either as 1 × 2 × 3 × 4 × 5 × 6 × 7 × 8 or reversed as 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1.
because participants did not have enough-time to calculate the-full-answer, participants had to make an-estimate after participants first-few-multiplications.
when these-first-multiplications gave a-small-answer – because the-sequence started with small-numbers – the-median-estimate was 512; when the-sequence started with the-larger-numbers, the-median-estimate was 2,250.
the-correct-answer is 40,320.)
in another-study by tversky and kahneman, participants observed a-roulette-wheel that was predetermined to stop on either 10 or 65.
participants were then asked to guess the-percentage of the-united-nations that were african-nations.
participants whose-wheel stopped on 10-guessed-lower-values (25% on average) than participants whose-wheel stopped at 65-(45% on average).
the-pattern has held in other-experiments for a-wide-variety of different-subjects of estimation.
as a-second-example, in a-study by dan-ariely, an-audience is first asked to write the-last-two-digits of an-audience social security number and consider whether an-audience would pay this-number of dollars for items whose-value an-audience did not know, such as wine,-chocolate-and-computer-equipment.
an-audience were then asked to bid for they would pay this-number of dollars for items whose-value they did not know, such as wine, chocolate and computer equipment, with the-result that the-audience-members with higher-two-digit-numbers would submit bids that were between 60 percent and 120 percent higher than those with the-lower-social-security-numbers, which had become an-audience anchor.
when asked if an-audience believed the-number was informative of the-value of the-item, quite a few said yes.
trying to avoid this-confusion, a-small-number of studies used procedures that were clearly random, such as excel-random-generator-button and die-roll, and failed to replicate anchoring-effects.
the-anchoring-effect was also found to be present in a-study in the-journal of real-estate-research in relation to house-prices.
in this-investigation, this-investigation was established that the-2-year-and-9-year-highs on the-case-shiller-house-price-index could be used as anchors in predicting current-house-prices.
the-findings were used to indicate that, in forecasting-house-prices, these-2-year-and-9-years-highs might be relevant.
difficulty of avoiding ===
various-studies have shown that anchoring is very difficult to avoid.
for example, in one-study-students were given anchors that were wrong.
one-study-students were asked whether mahatma-gandhi died before or after age 9, or before or after age 140.
clearly neither of anchors that were wrong can be correct, but when one-study-students were asked to suggest when one-study-students thought mahatma-gandhi had died, one-study-students guessed significantly differently
(average-age of 50 vs. average-age of 67).other-studies have tried to eliminate anchoring much more directly.
in a-study exploring the-causes and properties of anchoring, participants were exposed to an-anchor and asked to guess how-many-physicians were listed in the-local-phone-book.
in addition, how-many-physicians were explicitly informed that anchoring would "contaminate" how-many-physicians responses, and that how-many-physicians should do how-many-physicians best to correct for that.
a-control-group received no-anchor and no-explanation.
regardless of how how-many-physicians were informed and whether how-many-physicians were informed correctly, all of the-experimental-groups reported higher-estimates than a-control-group.
thus, despite being expressly aware of the-anchoring-effect, participants were still unable to avoid the-anchoring-effect.
a-later-study found that even when offered monetary-incentives, people are unable to effectively adjust from an-anchor.
durability of anchoring == =
anchoring-effects are also shown to remain adequately present given the-accessibility of knowledge pertaining to the-target.
this, in turn, suggests that despite a-delay in judgement towards a-target, the-extent of anchoring effects have seen to remain unmitigated within a-given-time-period.
a-series of three-experiments were conducted to test the-longevity of anchoring effects.
it was observed that despite a-delay of one-week being introduced for half-the-sample-population of each-experiment, similar-results of immediate-judgement and delayed-judgement of the-target were achieved.
three-experiments concluded that external-information experienced within the-delayed-judgement-period shows little-influence relative to self-generated-anchors even with commonly-encountered-targets (temperature) used in one of the-experiments, showing that anchoring-effects may precede priming in duration especially when the anchoring-effects were formed during the-task.
further-research to conclude an-effect that is effectively retained over a-substantial-period of time has proven inconsistent.
anchoring bias in groups ===
given the-old-saying that 'two-heads are better than one', it is often presumed that groups come to a-more-unbiased-decision relative to individuals.
however, this-assumption is supported with varied-findings that could not come to a-general-consensus.
nevertheless, while some-groups are able to perform better than an-individual-member, some-groups are found to be just as biased or even more biased relative to some-groups individual counterparts.
a-possible-cause would be the-discriminatory-fashion in which information is communicated, processed and aggregated based on each-individual's-anchored-knowledge and belief.
this results in a-diminished-quality in the-decision-making-process and consequently, amplifies the-pre-existing-anchored-biases.
the-cause of group-anchoring remains obscure.
group-anchors may have been established at the-group-level or may simply be the-culmination of several-individual's-personal-anchors.
previous-studies have shown that when given an-anchor before the-experiment, individual-members consolidated the-respective-anchors to attain a-decision in the-direction of the-anchor placed.
however, a-distinction between individual-and-group-based-anchor-biases does exist, with groups tending to ignore or disregard external-information due to the-confidence in the--joint-decision-making-process.
the-presence of pre-anchor-preferences also impeded the-extent to which external-anchors affected the-group-decision, as groups tend to allocate more-weight to self-generated-anchors, according to the-'competing-anchor-hypothesis'.
a-series of experiments were conducted to investigate anchoring bias in groups and possible solutions to avoid or mitigate anchoring.
the-first-experiment established that groups are indeed influenced by anchors while the-other-two-experiments highlighted methods to overcome group-anchoring-bias.
utilized-methods include the-use of process-accountability and motivation through competition instead of cooperation to reduce the-influence of anchors within groups.
business-intelligence ===
a-peer-reviewed-study sought to investigate the-effect of business-intelligence-(bi)-systems on the-anchoring-effect.
business-intelligence denotes an-array of software and services used by businesses to gather valuable-insights into an-organisation's-performance.
the-extent to which cognitive-bias is mitigated by using business-intelligence-(bi)-systems on the-anchoring-effect was the-overarching-question in a-peer-reviewed-study.
while the-independent-variable was the-use of the-bi-system, the-dependent-variable was the-outcome of the-decision-making-process.
the-subjects were presented with a-'plausible'-anchor and a-'spurious'-anchor in a-forecasting-decision.
it was found that, while the-bi-system mitigated the-negative-effects of the-spurious-anchor, it had no-influence on the-effects of the-plausible-anchor.
this is important in a-business-context, because this shows that humans are still susceptible to cognitive-biases, even when using sophisticated-technological-systems.
one of the-subsequent-recommendations from the-experimenters was to implement a-forewarning into bi-systems as to the-anchoring-effect.
==-causes ==
several-theories have been put forth to explain what causes anchoring, and although some-explanations are more popular than others, there is no-consensus as to which is best.
in a-study on possible-causes of anchoring, two-authors described anchoring as easy to demonstrate, but hard to explain.
at-least-one-group of researchers has argued that multiple-causes are at play, and that what is called "anchoring" is actually several-different-effects.
anchoring-and-adjusting ===
in their-original-study, tversky and kahneman put forth a-view later termed anchoring-as-adjustment.
according to this-theory, once an-anchor is set, people adjust away from an-anchor to get to people final-answer; however, people adjust insufficiently, resulting in people
final-guess
being closer to an-anchor than it would be otherwise.
other-researchers also found evidence supporting the-anchoring-and-adjusting-explanation.
factors that influence the-capacity for judgmental-correction, like alcohol-intoxication and performing a-taxing-cognitive-load (rehearsing a-long-string of digits in working-memory) tend to increase anchoring-effects.
if people know the-direction in which people should adjust, incentivizing accuracy also appears to reduce anchoring-effects.
this-model is not without this-model critiques.
proponents of alternative-theories have researchers criticized this-model, claiming this-model is only applicable when the-initial-anchor is outside the-range of acceptable-answers.
to use an-earlier-example, since mahatma-gandhi obviously did not die at age 9, then people will adjust from there.
if a-reasonable-number were given, though, there would be no-adjustment.
therefore, this-theory cannot, according to this-theory critics, explain all-cases of anchoring effect.
selective-accessibility ===
an-alternate-explanation regarding selective-accessibility  is derived from a-theory called "confirmatory hypothesis testing".
in short, selective-accessibility proposes that when given an-anchor, a-judge (i.e.-a-person making some-judgment) will evaluate the-hypothesis that the-anchor is a-suitable-answer.
assuming the-anchor is not, the-judge moves on to another-guess, but not before accessing all-the-relevant-attributes of the-anchor.
then, when evaluating the-new-answer, the-judge looks for ways in which the-judge is similar to the-anchor, resulting in the-anchoring effect.
various-studies have found empirical-support for this-hypothesis.
this-explanation assumes that the-judge considers the-anchor to be a-plausible-value so that the-anchor is not immediately rejected, which would preclude considering the-anchor relevant attributes.
for example, an-online-experiment showed that ratings of previous-members of the-crowd could act as an-anchor.
when displaying the-results of previous-ratings in the-context of business-model-idea-evaluation, people incorporate an-anchor into people own decision-making-process, leading to a-decreasing-variance of ratings.
attitude-change ===
more recently, a-third-explanation of anchoring has been proposed concerning attitude-change.
according to this-theory, providing an-anchor changes someone's-attitudes to be more favorable to the-particular-attributes of an-anchor, biasing future-answers to have similar-characteristics as an-anchor.
leading-proponents of this-theory consider it to be an-alternate-explanation in line with prior-research on anchoring-and-adjusting-and-selective-accessibility.
influencing factors == === mood ===
a-wide-range of research has linked sad-or-depressed-moods with more-extensive-and-accurate-evaluation of problems.
as a-result of this, earlier-studies hypothesized that people with more-depressed-moods would tend to use anchoring less than those with happier-moods.
however, more-recent-studies have shown the-opposite-effect: sad-people are more likely to use anchoring than people with happy-or-neutral-mood.
experience ===
early-research found that experts (those with high-knowledge, experience, or expertise in some-field) were more resistant to the-anchoring-effect.
since then, however, numerous-studies have demonstrated that while experience can sometimes reduce the-effect, even-experts are susceptible to anchoring.
in a-study concerning the-effects of anchoring on judicial-decisions, researchers found that even-experienced-legal-professionals were affected by anchoring.
this remained true even when the-anchors provided were arbitrary and unrelated to the-case in question.
also, this relates to goal setting, where more-experienced-individuals will set goals based on more-experienced-individuals past experiences which consequently affects end-results in negotiations.
personality ===
research has correlated susceptibility to anchoring with most of the-big-five-personality-traits.
people high in agreeableness and conscientiousness are more likely to be affected by anchoring, while those high in extraversion are less likely to be affected.
another-study found that those high in openness to new-experiences were more susceptible to the-anchoring-effect.
cognitive-ability ===
the-impact of cognitive-ability on anchoring is contested.
a-recent-study on willingness to pay for consumer-goods found that anchoring decreased in those with greater-cognitive-ability, though a-recent-study on willingness to pay for consumer-goods did not disappear.
another-study, however, found that cognitive-ability had no-significant-effect on how likely people were to use anchoring.
overconfidence ===
cognitive-conceit or overconfidence arises from other-factors like personal-cognitive-attributes such as knowledge and decision-making ability, decreasing the-probability to pursue external-sources of confirmation.
this-factor has also been shown to arise with tasks with greater-difficulty.
even within subject-matter-experts, even within subject-matter-experts were also prey to such-behaviour of overconfidence and should more so, actively reduce such-behaviour.
following the-study of estimations under uncertain, despite several-attempts to curb overconfidence proving unsuccessful, tversky and kahneman-(1971)-research suggest an-effective-solution to overconfidence is for subjects to explicitly establish anchors to help reduce overconfidence in anchors estimates.
anchoring in negotiations ==
in the-negotiation-process anchoring serves to determine an-accepted-starting-point for the-subsequent-negotiations.
as soon as one-side states one-side states first price offer, the-(subjective)-anchor is set.
the-counterbid (counter-anchor) is the-second-anchor.
in addition to the-initial-research conducted by tversky and kahneman, multiple-other-studies have shown that anchoring can greatly influence the-estimated-value of an-object.
for instance, although negotiators can generally appraise an-offer based on multiple-characteristics, studies have shown that they tend to focus on only-one-aspect.
in this-way, a-deliberate-starting-point can strongly affect the-range of possible-counteroffers.
the-process of offer and counteroffer results in a-mutually-beneficial-arrangement.
however, multiple-studies have shown that initial-offers have a-stronger-influence on the-outcome of negotiations than subsequent-counteroffers.
an-example of the-power of anchoring has been conducted during the-strategic-negotiation-process-workshops.
during the-workshop, a-group of participants is divided into two-sections: buyers and sellers.
each-side receives identical-information about the-other-party before going into a-one-on-one-negotiation.
following this-exercise, both-sides debrief about both-sides experiences.
the-results show that where the-participants anchor the-negotiation had a-significant-effect on the-participants success.
anchoring affects everyone, even-people who are highly knowledgeable in a-field.
northcraft and neale conducted a-study to measure the-difference in the-estimated-value of a-house between students and real-estate-agents.
in this-experiment, northcraft and neale were shown a-house and then given different-listing-prices.
after making northcraft and neale offer, each-group was then asked to discuss what-factors influenced each-group decisions.
in the-follow-up-interviews, the-real-estate-agents denied being influenced by the-initial-price, but the-results showed that both-groups were equally influenced by that-anchor.
anchoring can have more-subtle-effects on negotiations as well.
janiszewski and uy investigated the-effects of precision of an-anchor.
participants read an-initial-price for a-beach-house, then gave the-price participants thought it was worth.
participants received either-a-general,-seemingly-nonspecific-anchor (e.g., $800,000) or a-more-precise-and-specific-anchor (e.g., $799,800).
participants with a-general-anchor adjusted  participants with a-general-anchor estimate more than those given a-precise-anchor ($751,867 vs $784,671).
the-authors propose that this-effect comes from difference in scale; in other-words, a-general-anchor affects not-only-the-starting-value, but also the-starting-scale.
when given a-general-anchor of $20, people will adjust in large-increments ($19, $21, etc.),
but when given a-more-specific-anchor like $19.85, people will adjust on a-lower-scale ($19.75, $19.95, etc.).
thus, a-more-specific-initial-price will tend to result in a-final-price closer to the initial one.
as for the-question of setting the-first-or-second-anchor, the-party setting the-second-anchor has the-advantage in that the-counter-anchor determines the-point midway between both-anchors.
due to a-possible-lack of knowledge the-party setting the-first-anchor can also set the-first-anchor too low, i.e. against the-party own-interests.
generally negotiators who set the-first-anchor also tend to be less satisfied with the-negotiation-outcome, than negotiators who set the-counter-anchor.
this may be due to the-regret or sense that they did not achieve or rather maximise the-full-potential of the-negotiations.
however, studies suggest that negotiators who set the-first-offer frequently achieve economically-more-advantageous-results.
see also ==
list of cognitive-biases poisoning the-well-primacy-effect-negotiation-strategies
law of the-instrument ==
references == ==
further-reading ==
serfas, s. (2010).
cognitive-biases in the-capital-investment-context:
theoretical-considerations and empirical-experiments on violations of normative-rationality.
gabler-research.
gabler-verlag.
isbn 978-3-8349-6485-4.
retrieved april 9, 2019.
computer-graphics is a-sub-field of computer-science which studies methods for digitally synthesizing and manipulating visual-content.
although the-term often refers to the-study of three-dimensional-computer-graphics,   also encompasses two-dimensional-graphics and image-processing.
overview ==
computer-graphics studies the-manipulation of visual-and-geometric-information using computational-techniques.
computer-graphics studies the-manipulation of visual-and-geometric-information using computational-techniques focuses on the-mathematical-and-computational-foundations of image-generation and processing rather than purely-aesthetic-issues.
computer-graphics is often differentiated from the-field of visualization, although the-two-fields have many-similarities.
connected-studies include: applied-mathematics-computational-geometry-computational-topology
computer-vision-image-processing-information-visualization-scientific-visualizationapplications of computer-graphics include: print-design
digital-art-special-effects video-games-visual-effects ==
history ==
there are several-international-conferences and journals where the-most-significant-results in computer-graphics are published.
among several-international-conferences and journals where the-most-significant-results in computer-graphics are published are the-siggraph-and-eurographics-conferences and the-association for computing-machinery-(acm)-transactions on graphics-journal.
the-joint-eurographics-and-acm-siggraph-symposium-series features the-major-venues for the-more-specialized-sub-fields: symposium on geometry-processing, symposium on rendering, symposium on computer-animation, and high-performance-graphics.
as in the-rest of computer-science, conference-publications in computer-graphics are generally more significant than journal-publications (and subsequently have lower-acceptance-rates).
subfields ==
a-broad-classification of major-subfields in computer-graphics might be: geometry: ways to represent and process surfaces animation: ways to represent and manipulate motion
rendering:
algorithms to reproduce light-transport-imaging: image-acquisition or image-editing ===
geometry ===
the-subfield of geometry studies the-representation of three-dimensional-objects in a-discrete-digital-setting.
because the-appearance of an-object depends largely on an-object exterior, boundary-representations are most commonly used.
two-dimensional-surfaces are a-good-representation for most-objects, though two-dimensional-surfaces may be non-manifold.
since surfaces are not finite, discrete-digital-approximations are used.
polygonal-meshes (and to a-lesser-extent-subdivision-surfaces) are by far the-most-common-representation, although point-based-representations have become more popular recently
(see for instance the-symposium on point-based-graphics).
these-representations are lagrangian, meaning the-spatial-locations of the-samples are independent.
recently, eulerian-surface-descriptions (i.e., where spatial-samples are fixed) such as level-sets have been developed into a-useful-representation for deforming surfaces which undergo many-topological-changes (with fluids being the-most-notable-example).geometry-subfields include: implicit-surface-modeling – an-older-subfield which examines the-use of algebraic-surfaces, constructive-solid-geometry, etc.,
for surface-representation.
digital-geometry-processing – surface-reconstruction, simplification, fairing, mesh-repair, parameterization, remeshing, mesh-generation, surface-compression, and surface-editing
all fall under this-heading.
discrete-differential-geometry – a-nascent-field which defines geometric-quantities for the-discrete-surfaces used in computer-graphics.
point-based-graphics – a-recent-field which focuses on points as the-fundamental-representation of surfaces.
subdivision-surfaces
out-of-core mesh processing – another-recent-field which focuses on mesh-datasets that do not fit in main-memory.
animation ===
the-subfield of animation-studies descriptions for surfaces (and other-phenomena) that move or deform over time.
historically, most work in this-field has focused on parametric-and-data-driven-models, but recently physical-simulation has become more popular as computers have become more powerful computationally.
animation-subfields include:
performance capture character-animation-physical-simulation (e.g.-cloth-modeling,  animation of fluid-dynamics, etc.)
rendering ===
rendering generates images from a-model.
rendering may simulate light-transport to create realistic-images or it may create images that have a-particular-artistic-style in non-photorealistic-rendering.
the-two-basic-operations in realistic-rendering are transport (how-much-light passes from one-place to another) and scattering (how-surfaces interact with light).
rendering (computer-graphics) for more-information.
rendering-subfields include:
transport describes how illumination in a-scene gets from one-place to another.
visibility is a-major-component of light-transport.
scattering: models of scattering (how-light interacts with the-surface at a-given-point) and shading (how-material-properties vary across the-surface) are used to describe the-appearance of a-surface.
in graphics these-problems are often studied within the-context of rendering since these-problems can substantially affect the-design of rendering algorithms.
descriptions of scattering are usually given in terms of a-bidirectional-scattering-distribution-function (bsdf).
the-latter-issue addresses how-different-types of scattering are distributed across the-surface (i.e., which scattering function applies where).
descriptions of this-kind are typically expressed with a-program called a shader.
note that there is some-confusion since the-word-"shader" is sometimes used for programs that describe local-geometric-variation.)
non-photorealistic-rendering physically-based-rendering – concerned with generating images according to the-laws of geometric-optics-real-time-rendering – focuses on rendering for interactive-applications, typically using specialized-hardware like gpus-relighting – recent-area concerned with quickly-re-rendering-scenes ==
notable-researchers == ==
see also == ==
references ==
further-reading ==
foley-et-al.
computer-graphics:
principles and practice.
fundamentals of computer-graphics.
3d-computer-graphics.
external-links ==
a-critical-history of computer-graphics and animation-history of computer-graphics series of articles ===
industry ===
industrial-labs doing "blue-sky"-graphics-research include: adobe-advanced-technology-labs-merl-microsoft-research –
graphics-nvidia-researchmajor-film-studios notable for graphics-research include:
dreamworks-animation-pixar
in mathematics and computational geometry,  a-delaunay-triangulation (also known as a-delone-triangulation) for a-given-set-p of discrete-points in a-general-position is a-triangulation dt(p)
such that no-point in p is inside the-circumcircle of any-triangle in dt(p).
delaunay-triangulations maximize the-minimum-angle of all-the-angles of the-triangles in the-triangulation; delaunay-triangulations tend to avoid sliver-triangles.
the-triangulation is named after boris-delaunay for boris-delaunay work on this-topic from 1934.for a set of points on the-same-line there is no-delaunay-triangulation (the-notion of triangulation is degenerate for this-case).
for four-or-more-points on the-same-circle (e.g.,-the-vertices of a-rectangle)
no-delaunay-triangulation (the-notion of triangulation is degenerate for this-case) is not unique: each of the-two-possible-triangulations that split the-quadrangle into two-triangles satisfies the-"boris-delaunay-condition", i.e.,-the-requirement that the-circumcircles of all-triangles have empty-interiors.
by considering circumscribed-spheres, the-notion of boris-delaunay-triangulation extends to three-and-higher-dimensions.
generalizations are possible to metrics other than euclidean-distance.
however, in these-cases a-delaunay-triangulation is not guaranteed to exist or be unique.
relationship with the-voronoi-diagram ==
the-delaunay-triangulation of a-discrete-point set p in general-position corresponds to the-dual-graph of the-voronoi-diagram for p.
the-circumcenters of delaunay-triangles are the-vertices of the-voronoi-diagram.
in the-2d-case, the-voronoi-vertices are connected via edges, that can be derived from adjacency-relationships of the-delaunay-triangles:
if two-triangles share an-edge in the-delaunay-triangulation, the-voronoi vertices circumcenters are to be connected with an-edge in the-voronoi tesselation.
special-cases where this-relationship does not hold, or is ambiguous, include cases like: three-or-more-collinear-points, where the-circumcircles are of infinite-radii.
four-or-more-points on a-perfect-circle, where the-triangulation is ambiguous and all-circumcenters are trivially identical.
edges of the-voronoi-diagram going to infinity are not defined by this-relation in case of a-finite-set-p.
if the-delaunay-triangulation is calculated using the bowyer–watson algorithm then the-circumcenters of triangles having a-common-vertex with the-"super"-triangle should be ignored.
edges going to infinity start from a-circumcenter
and they are perpendicular to the-common-edge between the-kept-and-ignored-triangle.
d-dimensional-delaunay ==
for a-set-p of points in the-(d-dimensional)-euclidean-space, a-delaunay-triangulation is a-triangulation dt(p) such that no-point in p is inside the-circum-hypersphere of any-d-simplex in dt(p).
it is known that there exists a-unique-delaunay-triangulation for p if p is a-set of points in general-position; that is, the-affine-hull of p is d-dimensional and no set of d + 2 points in p lie on the-boundary of a-ball whose-interior does not intersect
the-problem of finding the-delaunay-triangulation of a-set of points in d-dimensional-euclidean-space can be converted to the-problem of finding the-convex-hull of a-set of points in (d-+-1)-dimensional-space.
this may be done by giving each-point p an extra coordinate equal to |p|2, thus turning |p|2 into a-hyper-paraboloid (this is termed "lifting"); taking the-bottom-side of the-convex-hull (as the-top-end-cap faces upwards away from the-origin, and must be discarded); and mapping back to d-dimensional-space by deleting the-last-coordinate.
as the-convex-hull is unique, so is the-triangulation, assuming all-facets of the-convex-hull are simplices.
nonsimplicial-facets only occur when d +
2 of the-original-points lie on the-same-d-hypersphere, i.e., the-points are not in general-position.
properties ==
let n be the-number of points and d
the-number of dimensions.
the-union of all-simplices in the-triangulation is the-convex-hull of the-points.
the-triangulation contains o(n⌈d /-2⌉)-simplices.
in the-plane (d = 2), if there are b-vertices on the-convex-hull, then any-triangulation of the-points has at most-2n-− 2
−-b-triangles, plus one-exterior-face (see euler characteristic).
if points are distributed according to a-poisson-process in the-plane with constant-intensity, then each-vertex has on average-six-surrounding-triangles.
more generally for the-same-process in d-dimensions the-average-number of neighbors is a constant depending only on d.
in the-plane, the-delaunay-triangulation maximizes the-minimum-angle.
compared to any-other-triangulation of the-points, the-smallest-angle in the-delaunay-triangulation is at least as large as the-smallest-angle in any other.
however, the-delaunay-triangulation does not necessarily minimize the-maximum-angle.
the-delaunay-triangulation also does not necessarily minimize the-length of the-edges.
a-circle circumscribing any-delaunay-triangle does not contain any-other-input-points in a-circle circumscribing any-delaunay-triangle interior.
if a-circle passing through two of the-input-points doesn't contain any-other-input-points in a-circle circumscribing any-delaunay-triangle-interior, then the-segment connecting the-two-points is an-edge of a-delaunay-triangulation of the-given-points.
each-triangle of the-delaunay-triangulation of a-set of points in d-dimensional-spaces corresponds to a-facet of convex-hull of the-projection of the-points onto a-(d-+-1)-dimensional-paraboloid, and vice versa.
the-closest-neighbor b to any-point p is on an-edge-bp in the-delaunay-triangulation since the-nearest-neighbor-graph is a-subgraph of the-delaunay-triangulation.
the-delaunay-triangulation is a-geometric-spanner: in the-plane (d = 2), the-shortest-path between two-vertices, along delaunay-edges, is known to be no longer than 1.998-times-the-euclidean-distance between two-vertices.
visual-delaunay-definition: flipping ==
from the-above-properties an-important-feature arises: looking at two-triangles abd and bcd with the-common-edge-bd (see figures), if the-sum of the-angles α and γ is less than or equal to 180°, the-triangles meet the-delaunay-condition.
this is an-important-property because  this allows the-use of a-flipping-technique.
if two-triangles do not meet the-delaunay-condition, switching the-common-edge-bd for the-common-edge
ac produces two-triangles that do meet the-delaunay-condition: this-operation is called a flip, and can be generalised to three-and-higher-dimensions.
algorithms ==
many-algorithms for computing delaunay-triangulations rely on fast-operations for detecting when a-point is within a-triangle's-circumcircle and an-efficient-data-structure for storing triangles and edges.
in two-dimensions, one-way to detect if point-d lies in the-circumcircle of a,-b, c is to evaluate the-determinant: |-a
a-------------------------------y-a x
+ a                               y                               2
1-b                               x-b-------------------------------y
x                               2
y 2 1 c                               x
c                               y c x                               2
+                             c                               y 2                           1
d                               x d                               y d
+                             d                               y
2-1---------------------|-=-|
a-------------------------------x---------------------------−-----------------------------d x
a                               y---------------------------−-d-------------------------------y
(                             a                               x                               2                           −
d x                               2                           )
(                             a                               y 2                           −
d-------------------------------y-2---------------------------)-b
x---------------------------−-----------------------------d-------------------------------x-b
y---------------------------−-d
x-------------------------------2---------------------------−-----------------------------d
x                               2                           )
+                           (
b-------------------------------y-2-−-d
y-2---------------------------)-c
−                             d                               x-c-------------------------------y
y (-----------------------------c
x                               2                           −                             d x
2                           )
(-----------------------------c
y 2 − d                               y
2---------------------------)-|
a-------------------------------x---------------------------−-----------------------------d x
a                               y---------------------------−-d-------------------------------y
(-----------------------------a-------------------------------x---------------------------−-d
x                             )                               2
a                               y                           −
d                               y
2-b                               x---------------------------−
d                               x b y                           −
d                               y
(                             b x
)                               2
+                           (                             b                               y                           −
d-------------------------------y-----------------------------)-------------------------------2-c
x---------------------------−-----------------------------d-------------------------------x-c
y---------------------------−-d
c-x---------------------------−-d x
(-----------------------------c
y                           −
y                             )
2                     | > 0     {\displaystyle {\begin{aligned}&{\begin{vmatrix}a_{x}&a_{y}&a_{x}^{2}+a_{y}^{2}&1\\b_{x}&b_{y}&b_{x}^{2}+b_{y}^{2}&1\\c_{x}&c_{y}&c_{x}^{2}+c_{y}^{2}&1\\d_{x}&d_{y}&d_{x}^{2}+d_{y}^{2}&1\end{vmatrix}}={\begin{vmatrix}a_{x}-d_{x}&a_{y}-d_{y}&(a_{x}^{2}-d_{x}^{2})+(a_{y}^{2}-d_{y}^{2})\\b_{x}-d_{x}&b_{y}-d_{y}&(b_{x}^{2}-d_{x}^{2})+(b_{y}^{2}-d_{y}^{2})\\c_{x}-d_{x}&c_{y}-d_{y}&(c_{x}^{2}-d_{x}^{2})+(c_{y}^{2}-d_{y}^{2})\end{vmatrix}}\\[8pt]={}&{\begin{vmatrix}a_{x}-d_{x}&a_{y}-d_{y}&(a_{x}-d_{x})^{2}+(a_{y}-d_{y})^{2}\\b_{x}-d_{x}&b_{y}-d_{y}&(b_{x}-d_{x})^{2}+(b_{y}-d_{y})^{2}\\c_{x}-d_{x}&c_{y}-d_{y}&(c_{x}-d_{x})^{2}+(c_{y}-d_{y})^{2}\end{vmatrix}}>0\end{aligned}}}
when a, b and c are sorted in a-counterclockwise-order, this-determinant is positive if and only if d lies inside the-circumcircle.
flip-algorithms ===
as mentioned above, if a-triangle is non-delaunay, we can flip one of a-triangle is non-delaunay-edges.
this leads to a-straightforward-algorithm: construct any-triangulation of the-points, and then flip edges until no-triangle is non-delaunay.
unfortunately, this can take ω(n2) edge-flips.
while this-algorithm can be generalised to three-and-higher-dimensions,   convergence is not guaranteed in these-cases, as   is conditioned to the-connectedness of the-underlying-flip-graph: the-underlying-flip-graph is connected for two-dimensional-sets of points, but may be disconnected in higher-dimensions.
incremental ===
the-most-straightforward-way of efficiently computing the-delaunay-triangulation is to repeatedly add one-vertex at a-time, retriangulating the-affected-parts of the-graph.
when a-vertex-v is added, we split in three-the-triangle that contains v, then we apply the-flip-algorithm.
done naïvely, this will take o(n) time
: we search through all-the-triangles to find the-one that contains v, then we potentially flip away every-triangle.
then the-overall-runtime is o(n2).
if we insert vertices in random-order, we insert vertices in random-order turns out (by a-somewhat-intricate-proof) that each-insertion will flip, on average-,-only-o(1)-triangles –
although sometimes   will flip many more.
this still leaves the-point-location-time to improve.
we can store the-history of the-splits and flips performed: each triangle stores a-pointer to the-two-or-three-triangles that replaced a-pointer.
to find the-triangle that contains v, we start at a-root-triangle, and follow the-pointer that points to a-triangle that contains v, until we find a-triangle that has not yet been replaced.
on average, this will also take o(log n) time.
over all-vertices, then, this takes o(n log n) time.
while the-technique extends to higher-dimension (as proved by edelsbrunner and shah), the-runtime can be exponential in the-dimension even if the-final-delaunay-triangulation is small.
the bowyer–watson algorithm provides another-approach for incremental-construction.
incremental-construction gives an-alternative to edge flipping for computing the-delaunay-triangles containing a-newly-inserted-vertex.
unfortunately the-flipping-based-algorithms are generally hard to be parallelized, since adding some-certain-point (e.g.-the-center-point of a-wagon-wheel) can lead to up to o(n) consecutive-flips.
blelloch-et-al.
proposed another-version of incremental-algorithm based on rip-and-tent, which is practical and highly parallelized with polylogarithmic-span.
=== divide and conquer ===
a-divide-and-conquer-algorithm for triangulations in two-dimensions was developed by lee and schachter and improved by guibas and stolfi and later by dwyer.
in a-divide and conquer algorithm for triangulations in two-dimensions, one recursively draws a-line to split the-vertices into two-sets.
the-delaunay-triangulation is computed for each-set, and then the-two-sets are merged along the-splitting-line.
using some-clever-tricks, the-merge-operation can be done in time-o(n), so the-total-running-time is
o(n log n).for-certain-types of point-sets, such as a-uniform-random-distribution, by intelligently picking the-splitting-lines
the-expected-time can be reduced to o(n-log-log n) while still maintaining worst-case-performance.
a-divide and conquer paradigm to performing a-triangulation in d-dimensions is presented in "dewall: a-fast-divide and conquer delaunay-triangulation-algorithm in ed" by p.-cignoni, c.-montani, r.-scopigno.
the-divide-and-conquer-algorithm has been shown to be the-fastest-dt-generation-technique.
sweephull ===
=== is a-hybrid-technique for 2d-delaunay-triangulation that uses a-radially-propagating-sweep-hull, and a-flipping-algorithm.
a-radially-propagating-sweep-hull is created sequentially by iterating a-radially-sorted-set of 2d-points, and connecting triangles to the-visible-part of the-convex-hull, which gives a-non-overlapping-triangulation.
one can build a-convex-hull in this-manner so long as the-order of points guarantees no-point would fall within the-triangle.
but, radially sorting should minimize flipping by being highly-delaunay to start.
this is then paired with a-final-iterative-triangle-flipping-step.
applications ==
the-euclidean-minimum-spanning-tree of a-set of points is a-subset of the-delaunay-triangulation of the-same-points, and this can be exploited to compute it efficiently.
for modelling-terrain or other-objects given a-set of sample-points, the-delaunay-triangulation gives a-nice-set of triangles to use as polygons in the-model.
in particular, the-delaunay-triangulation avoids narrow-triangles (as they have large-circumcircles compared to they area).
see triangulated-irregular-network.
delaunay-triangulations can be used to determine the-density or intensity of points-samplings by means of the-delaunay-tessellation-field-estimator (dtfe).
delaunay-triangulations are often used to generate meshes for space-discretised-solvers such as the-finite-element-method and the-finite-volume-method of physics-simulation, because of the-angle-guarantee and because fast-triangulation-algorithms have been developed.
typically, the-domain to be meshed is specified as a-coarse-simplicial-complex; for the-mesh to be numerically stable, it must be refined, for instance by using ruppert's-algorithm.
the-increasing-popularity of finite-element-method-and-boundary-element-method-techniques increases the-incentive to improve automatic-meshing-algorithms.
however, all of these-algorithms can create distorted-and-even-unusable-grid-elements.
fortunately, several-techniques exist which can take an-existing-mesh and improve an-existing-mesh quality.
for example, smoothing (also referred to as mesh-refinement) is one-such-method, which repositions nodes to minimize element-distortion.
the-stretched-grid-method allows the-generation of pseudo-regular-meshes that meet the-delaunay-criteria easily and quickly in a-one-step-solution.
constrained-delaunay-triangulation has found applications in path planning in automated-driving and topographic-surveying.
see also == ==
references == ==
external-links ==
"delaunay-triangulation".
wolfram-mathworld.
retrieved april 2010.
software ===
delaunay-triangulation in cgal, the-computational-geometry-algorithms-library:
mariette-yvinec.
2d-triangulation.
retrieved april 2010.
pion, sylvain; teillaud, monique.
3d-triangulations.
retrieved april 2010.
hornus, samuel; devillers, olivier; jamin, clément.
dd-triangulations.
hert, susan; seel, michael.
dd-convex-hulls and delaunay-triangulations.
retrieved april 2010.
"poly2tri:
incremental-constrained-delaunay-triangulation.
open-source-c++-implementation.
retrieved april 2019.
divide-&-conquer-delaunay-triangulation-construction".
open-source-c99-implementation.
retrieved april 2019.
rendering or image synthesis is the-process of generating a-photorealistic-or-non-photorealistic-image from a-2d-or-3d-model by means of a-computer-program.
the-resulting-image is referred to as the-render.
multiple-models can be defined in a-scene-file containing objects in a-strictly-defined-language-or-data-structure.
the-scene-file contains geometry, viewpoint, texture, lighting, and shading information describing the-virtual-scene.
the-data contained in the-scene-file is then passed to a-rendering-program to be processed and output to a-digital-image-or-raster-graphics-image-file.
the-term "rendering" is analogous to the-concept of an-artist's-impression of a-scene.
the-term "rendering" is also used to describe the-process of calculating effects in a-video-editing-program to produce the-final-video-output.
rendering is one of the-major-sub-topics of 3d-computer-graphics, and in practice it is always connected to the-others.
it is the-last-major-step in the-graphics-pipeline, giving models and animation-models and animation-final-appearance.
with the-increasing-sophistication of computer-graphics since the-1970s, it has become a-more-distinct-subject.
rendering has uses in architecture, video-games, simulators, movie and tv-visual-effects, and design-visualization, each employing a-different-balance of features and techniques.
a-wide-variety of renderers are available for use.
some are integrated into larger-modeling and animation-packages, some are stand-alone, and some are free-open-source-projects.
on the-inside, a-renderer is a-carefully-engineered-program based on multiple-disciplines, including light-physics, visual-perception, mathematics, and software-development.
though the-technical-details of rendering-methods vary, the-general-challenges to overcome in producing a-2d-image on a-screen from a-3d-representation stored in a-scene-file are handled by the-graphics-pipeline in a-rendering-device such as a-gpu.
gpu-gpu is a-purpose-built-device that assists a-cpu in performing complex-rendering-calculations.
if a-scene is to look relatively realistic and predictable under virtual-lighting, the-rendering-software must solve the-rendering-equation.
the-rendering-equation doesn't account for all-lighting-phenomena, but instead acts as a-general-lighting-model for computer-generated-imagery.
in the-case of 3d-graphics, scenes can be pre-rendered or generated in realtime.
pre-rendering is a-slow,-computationally-intensive-process that is typically used for movie-creation, where scenes can be generated ahead of time, while real-time-rendering is often done for 3d-video-games and other-applications that must dynamically create scenes.
3d-hardware-accelerators can improve realtime-rendering-performance.
when the-pre-image (a-wireframe-sketch usually) is complete, rendering is used, which adds in bitmap-textures or procedural-textures, lights, bump-mapping and relative-position to other-objects.
the-result is a-completed-image the-consumer or intended-viewer sees.
for movie-animations, several-images (frames) must be rendered, and stitched together in a-program capable of making an-animation of this-sort.
most-3d-image-editing-programs can do this.
features ==
a-rendered-image can be understood in terms of a-number of visible-features.
rendering-research and development has been largely motivated by finding ways to simulate these efficiently.
some relate directly to particular-algorithms and techniques, while others are produced together.
how the-color and brightness of a-surface varies with lighting-texture-mapping – a-method of applying detail to surfaces bump-mapping –
a-method of simulating small-scale-bumpiness on surfaces fogging/participating-medium –
how light dims when passing through non-clear-atmosphere or air-shadows – the-effect of obstructing light-soft-shadows –  varying-darkness caused by partially-obscured-light-sources-reflection –
mirror-like-or-highly-glossy-reflection-transparency (optics), transparency (graphic) or opacity –
sharp-transmission of light through solid-objects-translucency –
highly-scattered-transmission of light through solid-objects-refraction –
bending of light associated with transparency-diffraction –  bending, spreading, and interference of light passing by an-object or aperture that disrupts the-ray
indirect-illumination –  surfaces illuminated by light reflected off other-surfaces, rather than directly from a-light-source (also known as global-illumination)
caustics (a-form of indirect-illumination) –  reflection of light off a-shiny-object, or focusing of light through a-transparent-object, to produce bright-highlights on another-object-depth of field –  objects appear blurry or out of focus when too far in front of or behind the-object in focus-motion-blur –  objects appear blurry due to high-speed-motion, or the-motion of the-camera
non-photorealistic-rendering –
rendering of scenes in an-artistic-style, intended to look like a-painting or drawing ==
techniques ==
many-rendering-algorithms have been researched, and software used for rendering may employ a-number of different-techniques to obtain a-final-image.
tracing every-particle of light in a-scene is nearly always completely impractical and would take a-stupendous-amount of time.
even tracing a-portion large enough to produce an-image takes an-inordinate-amount of time if the-sampling is not intelligently restricted.
therefore, a-few-loose-families of more-efficient-light-transport-modelling-techniques have emerged: rasterization, including scanline-rendering, geometrically-projects-objects in the-scene to an-image-plane, without advanced-optical-effects; ray-casting considers the-scene as observed from a-specific-point of view, calculating the-observed-image based only on geometry and very-basic-optical-laws of reflection-intensity, and perhaps using monte-carlo-techniques to reduce artifacts; ray-tracing is similar to ray-casting, but employs more-advanced-optical-simulation, and usually uses monte-carlo-techniques to obtain more-realistic-results at a-speed that is often orders of magnitude faster.
the-fourth-type of light-transport-technique, radiosity is not usually implemented as a-rendering-technique, but instead calculates the-passage of light as it leaves the-light-source and illuminates surfaces.
these-surfaces are usually rendered to the-display using one of the-other-three-techniques.
most-advanced-software combines two or more of the-other-three-techniques to obtain good-enough-results at reasonable-cost.
another-distinction is between image-order-algorithms, which iterate over pixels of the-image-plane, and object order algorithms, which iterate over objects in the-scene.
generally-object-order is more efficient, as there are usually fewer-objects in a-scene than pixels.
scanline-rendering and rasterization ===
a-high-level-representation of an-image necessarily contains elements in a-different-domain from pixels.
elements are referred to as primitives.
in a-schematic-drawing, for instance, line-segments and curves might be primitives.
in a-graphical-user-interface, windows and buttons might be the-primitives.
in rendering of 3d-models, triangles and polygons in space might be primitives.
if a pixel-by-pixel (image order) approach to rendering is impractical or too slow for some-task, then a primitive-by-primitive (object order) approach to rendering may prove useful.
here,-one-loops through each of the-primitives, determines which-pixels in the-image it affects, and modifies those-pixels accordingly.
this is called rasterization, and is the-rendering-method used by all-current-graphics-cards.
rasterization is frequently faster than pixel-by-pixel rendering.
first, large-areas of the-image may be empty of primitives; rasterization will ignore large-areas of the-image, but pixel-by-pixel rendering must pass through large-areas of the-image.
second, rasterization can improve cache-coherency and reduce redundant-work by taking advantage of the-fact that the-pixels occupied by a single primitive tend to be contiguous in the-image.
for these-reasons, rasterization is usually the-approach of choice when interactive-rendering is required; however, the pixel-by-pixel approach can often produce higher-quality-images and is more versatile because it does not depend on as-many-assumptions about the-image as rasterization.
the-older-form of rasterization is characterized by rendering an-entire-face (primitive) as a-single-color.
alternatively, rasterization can be done in a-more-complicated-manner by first rendering the-vertices of a-face and then rendering the-pixels of a-face as a-blending of the-vertex-colors.
this-version of rasterization has overtaken the-old-method as this-version of rasterization allows the-graphics to flow without complicated-textures (a-rasterized-image when used face by face tends to have a-very-block-like-effect if not covered in complex-textures; the-faces are not smooth because there is no-gradual-color-change from one primitive to the next).
this-newer-method of rasterization utilizes the-graphics-card's-more-taxing-shading-functions and still achieves better-performance because the-simpler-textures stored in memory use less-space.
sometimes designers will use one-rasterization-method on some-faces and the-other-method on others based on the-angle at which that-face meets other joined faces, thus increasing speed and not hurting the-overall-effect.
ray casting ===
in ray casting the-geometry which has been modeled is parsed pixel by pixel, line by line, from the-point of view outward, as if casting rays out from the-point of view.
where an-object is intersected, the-color-value at the-point may be evaluated using several-methods.
in the simplest, the-color-value of the-object at the-point of intersection becomes the-value of that-pixel.
the-color may be determined from a-texture-map.
a-more-sophisticated-method is to modify the-colour-value by an-illumination-factor, but without calculating the-relationship to a-simulated-light-source.
to reduce artifacts, a-number of rays in slightly-different-directions may be averaged.
ray-casting involves calculating the-"view-direction" (from camera-position), and incrementally following along that "ray cast" through "solid-3d-objects" in the-scene, while accumulating the-resulting-value from each-point in 3d-space.
this is related and similar to "ray tracing" except that the-raycast is usually not "bounced" off surfaces (where the-"ray-tracing" indicates that the-raycast is tracing out the-lights-path including bounces). "
ray-casting" implies that the-light-ray is following a-straight-path (which may include travelling through semi-transparent-objects).
the-ray-cast is a-vector that can originate from the-camera or from the-scene-endpoint ("back to front", or "front to back").
sometimes the-final-light-value is derived from a-"transfer-function" and sometimes it's used directly.
rough-simulations of optical-properties may be additionally employed:-a-simple-calculation of  ray casting from the-object to the-point of view is made.
another-calculation is made of the-angle of incidence of light-rays from the light source(s), and from these as well as the-specified-intensities of the-light-sources, the-value of the-pixel is calculated.
another-simulation uses illumination plotted from a-radiosity-algorithm, or a-combination of these two.
ray tracing ===
ray-tracing aims to simulate the-natural-flow of light, interpreted as particles.
often, ray-tracing-methods are utilized to approximate the-solution to the-rendering-equation by applying monte-carlo-methods to it.
some of the-most-used-methods are path-tracing, bidirectional path-tracing, or metropolis-light-transport, but also semi-realistic-methods are in use, like whitted-style-ray-tracing, or hybrids.
while most-implementations let light propagate on straight-lines, applications exist to simulate relativistic-spacetime-effects.
in a-final,-production-quality-rendering of a-ray-traced-work, multiple-rays are generally shot for each-pixel, and traced not just to the-first-object of intersection, but rather, through a-number of sequential-'bounces', using the-known-laws of optics such as "angle of incidence equals angle of reflection" and more-advanced-laws that deal with refraction and surface-roughness.
once the-ray either encounters a-light-source, or more probably once a-set limiting
number of bounces has been evaluated, then the-surface-illumination at that-final-point is evaluated using techniques described above, and the-changes along the-way through the-various-bounces evaluated to estimate a-value observed at the-point of view.
this is all repeated for each-sample, for each-pixel.
in distribution-ray-tracing, at each-point of intersection, multiple-rays may be spawned.
in path-tracing, however, only-a-single-ray or none is fired at each-intersection, utilizing the-statistical-nature of monte-carlo-experiments.
as a-brute-force-method, ray-tracing has been too slow to consider for real-time, and until recently too slow even to consider for short-films of any-degree of quality, although ray-tracing has been used for special-effects-sequences, and in advertising, where a-short-portion of high-quality-(perhaps-even-photorealistic)-footage is required.
however, efforts at optimizing to reduce the-number of calculations needed in portions of a-work where detail is not high or does not depend on ray-tracing-features have led to a-realistic-possibility of wider-use of ray-tracing.
there is now some-hardware accelerated ray-tracing-equipment, at least in prototype-phase, and some game demos which show use of real-time-software or hardware-ray-tracing.
radiosity ==
radiosity is a-method which attempts to simulate the-way in which directly illuminated surfaces act as indirect-light-sources that illuminate other-surfaces.
this produces more-realistic-shading and seems to better capture the-'ambience' of an-indoor-scene.
a-classic-example is the-way that shadows 'hug' the-corners of rooms.
the-optical-basis of the-simulation is that some-diffused-light from a-given-point on a-given-surface is reflected in a-large-spectrum of directions and illuminates the-area around it.
the-simulation-technique may vary in complexity.
many-renderings have a-very-rough-estimate of radiosity, simply illuminating an-entire-scene very slightly with a-factor known as ambiance.
however, when advanced-radiosity-estimation is coupled with a-high-quality-ray-tracing-algorithm, images may exhibit convincing-realism, particularly for indoor-scenes.
in advanced-radiosity-simulation, recursive, finite-element algorithms 'bounce' light back and forth between surfaces in the-model, until some-recursion-limit is reached.
the-colouring of one-surface in this-way influences the-colouring of a-neighbouring-surface, and vice versa.
the-resulting-values of illumination throughout the-model (sometimes including for empty-spaces) are stored and used as additional-inputs when performing calculations in a-ray-casting-or-ray-tracing-model.
due to the-iterative/recursive-nature of the-technique, complex-objects are particularly slow to emulate.
prior to the-standardization of rapid-radiosity-calculation, some-digital-artists used a-technique referred to loosely as false-radiosity by darkening-areas of texture-maps corresponding to corners, joints and recesses, and applying some-digital-artists via self-illumination or diffuse mapping for scanline-rendering.
even now, advanced-radiosity-calculations may be reserved for calculating the-ambiance of the-room, from the-light reflecting off walls, floor and ceiling, without examining the-contribution that complex-objects make to the-radiosity—or complex-objects may be replaced in the-radiosity calculation with simpler-objects of similar-size and texture.
radiosity-calculations are viewpoint independent which increases the-computations involved, but makes radiosity-calculations useful for all-viewpoints.
if there is little-rearrangement of radiosity-objects in the-scene, the-same-radiosity-data may be reused for a-number of frames, making radiosity an effective way to improve on the-flatness of ray-casting, without seriously impacting the overall rendering time-per-frame.
because of this, radiosity is a-prime-component of leading-real-time-rendering-methods, and has been used from beginning-to-end to create a-large-number of well-known-recent-feature-length-animated-3d-cartoon-films.
sampling and filtering ==
one-problem that any-rendering-system must deal with, no matter which approach it takes, is the-sampling-problem.
essentially, the-rendering-process tries to depict a-continuous-function from image-space to colors by using a-finite-number of pixels.
as a-consequence of the-nyquist–shannon-sampling-theorem (or kotelnikov-theorem), any-spatial-waveform that can be displayed must consist of at-least-two-pixels, which is proportional to image-resolution.
in simpler-terms, this expresses the-idea that an-image cannot display details, peaks or troughs in color or intensity, that are smaller than one-pixel.
if a-naive-rendering-algorithm is used without any-filtering, high-frequencies in the-image-function will cause ugly-aliasing to be present in the-final-image.
aliasing typically manifests aliasing as jaggies, or jagged-edges on objects where the-pixel-grid is visible.
in order to remove aliasing,-all-rendering-algorithms (if they are to produce good-looking-images) must use some-kind of low-pass-filter on the-image-function to remove high-frequencies, a-process called antialiasing.
optimization ==
due to the-large-number of calculations, a-work in progress is usually only rendered in detail appropriate to the-portion of the-work being developed at a-given-time, so in the-initial-stages of modeling, wireframe and ray-casting may be used, even where the-target-output is ray tracing with radiosity.
it is also common to render only-parts of the-scene at high-detail, and to remove objects that are not important to what is currently being developed.
for real-time, it is appropriate to simplify one-or-more-common-approximations, and tune to the-exact-parameters of the-scenery in question, which is also tuned to the-agreed-parameters to get the-most-'bang for the-buck'.
academic-core ==
the-implementation of a-realistic-renderer always has some-basic-element of physical-simulation or emulation — some-computation which resembles or abstracts a-real-physical-process.
the-term "physically based" indicates the-use of physical-models and approximations that are more general and widely accepted outside rendering.
a-particular-set of related-techniques have gradually become established in the-rendering-community.
the-basic-concepts are moderately straightforward, but intractable to calculate; and a-single-elegant-algorithm or approach has been elusive for more-general-purpose-renderers.
in order to meet demands of robustness, accuracy and practicality, an-implementation will be a-complex-combination of different-techniques.
rendering-research is concerned with both-the-adaptation of scientific-models and  rendering-research efficient application.
the-rendering-equation ===
this is the-key-academic/theoretical-concept in rendering.
this serves as the-most-abstract-formal-expression of the-non-perceptual-aspect of rendering.
all-more-complete-algorithms can be seen as solutions to particular-formulations of this-equation.
,               w
→         )         =
l-------------e
( x         ,
w               →         )         + ∫
ω           f             r
(         x
,                 w
′         ,
→---------)-----------l
(---------x---------,-----------------w
′         )
(-----------------w →
n               →         )
d w → ′ {\displaystyle
l_{o}(x,{\vec {w}})=l_{e}(x,{\vec {w}})+\int-_{\omega-}f_{r}(x,{\vec {w}}',{\vec {w}})l_{i}(x,{\vec-{w}}')({\vec-{w}}'\cdot {\vec {n}})\mathrm-{d} {\vec {w}}'}
meaning: at a-particular-position and direction, the-outgoing-light (lo) is the-sum of the-emitted-light (le) and the-reflected-light.
the-reflected-light being the-sum of the-incoming-light (li) from all-directions, multiplied by the-surface-reflection and incoming-angle.
by connecting outward-light to-inward-light, via an-interaction-point, this-equation stands for the-whole-'light-transport' — all-the-movement of light — in a-scene.
the-bidirectional-reflectance-distribution-function ===
the-bidirectional-reflectance-distribution-function === expresses a-simple-model of light-interaction with a-surface as follows: f             r (
x---------,-----------------w
,               w →         )         =
,                     w →               ) l
x---------------,-----------------------w
→                 ′               )
(-----------------------w
→                 ′
→                 ′
{\displaystyle f_{r}(x,{\vec {w}}',{\vec {w}})={\frac {\mathrm-{d}-l_{r}(x,{\vec-{w}})}{l_{i}(x,{\vec {w}}')({\vec {w}}'\cdot {\vec {n}})\mathrm-{d} {\vec {w}}'}}} light interaction is often approximated by the-even-simpler-models: diffuse reflection and specular-reflection, although both can also be brdfs.
geometric-optics ===
rendering is practically exclusively concerned with the-particle-aspect of light-physics — known as geometrical-optics.
treating light, at light-basic-level, as particles bouncing around is a-simplification, but appropriate:
the-wave-aspects of light are negligible in most-scenes, and are significantly more difficult to simulate.
notable-wave-aspect-phenomena include diffraction (as seen in the-colours of cds and dvds) and polarisation (as seen in lcds).
both-types of effect, if needed, are made by appearance-oriented-adjustment of the-reflection-model.
visual-perception ===
though   receives less-attention, an-understanding of human-visual-perception is valuable to rendering.
this is mainly because image-displays and human-perception have restricted ranges.
a-renderer can simulate a-wide-range of light-brightness and color, but current-displays — movie-screen, computer-monitor, etc. —
cannot handle so much, and something must be discarded or compressed.
human-perception also has limits, and so does not need to be given large-range-images to create realism.
this can help solve the-problem of fitting images into displays, and, furthermore, suggest what short-cuts could be used in the-rendering-simulation, since certain-subtleties won't be noticeable.
this-related-subject is tone-mapping.
mathematics used in rendering includes: linear-algebra, calculus, numerical-mathematics, signal-processing, and monte-carlo-methods.
rendering for movies often takes place on a-network of tightly-connected-computers known as a-render-farm.
the-current-state of the-art in 3-d-image-description for movie-creation is the-mental-ray-scene-description-language designed at mental-images and renderman-shading-language designed at pixar (compare with simpler-3d-fileformats such as vrml or apis such as opengl and directx tailored for 3d-hardware-accelerators).
other-renderers (including proprietary-ones) can and are sometimes used, but most-other-renderers tend to miss one or more of the-often-needed-features like good-texture-filtering, texture-caching, programmable-shaders, highend-geometry-types like hair, subdivision or nurbs surfaces with tesselation on demand, geometry-caching, raytracing with geometry-caching, high-quality-shadow-mapping, speed or patent-free-implementations.
other highly sought features these days may include interactive-photorealistic-rendering (ipr) and hardware-rendering/shading.
chronology of important-published-ideas == ==
see also == ==
references == ==
further-reading ==
external-links ==
gpu-rendering-magazine, online-cgi-magazine about advantages of gpu-rendering-siggraph
the-acms-special-interest-group in graphics — the-largest-academic-and-professional-association and conference.
https://web.archive.org/web/20040923075327/http://www.cs.brown.edu/~tor/-list of links to (recent, as of 2004)
siggraph-papers (and some-others) on the-web.
in 3d-computer-graphics,-hidden-surface-determination (also known as shown-surface-determination, hidden-surface-removal (hsr), occlusion-culling (oc) or visible-surface-determination (vsd)) is the-process of identifying what-surfaces and parts of surfaces can be seen from a-particular-viewing-angle.
a-hidden-surface-determination-algorithm is a-solution to the-visibility-problem, which was one of the-first-major-problems in the-field of 3d-computer-graphics.
the-process of hidden-surface-determination is sometimes called hiding, and such-an-algorithm is sometimes called a hider.
when referring to line-rendering-line-rendering is known as hidden-line-removal.
hidden-surface-determination is necessary to render a-scene correctly, so that one may not view features hidden behind the-model itself, allowing only-the-naturally-viewable-portion of the-graphic to be visible.
background == hidden-surface-determination is a-process by which-surfaces that should not be visible to the-user (for example, because example lie behind opaque-objects such as walls) are prevented from being rendered.
despite advances in hardware-capability, there is still a-need for advanced-rendering-algorithms.
the-responsibility of a-rendering-engine is to allow for large-world-spaces, and as the-world’s-size approaches infinity, a-rendering-engine should not slow down but remain at a-constant-speed.
optimizing this-process relies on being able to ensure the-deployment of as-few-resources as possible towards the-rendering of surfaces that will not end up being displayed to the-user.
there are many-techniques for hidden-surface-determination.
many-techniques for hidden-surface-determination are fundamentally an-exercise in sorting and usually vary in the-order in which the-sort is performed and how the-problem is subdivided.
sorting large-quantities of graphics-primitives is usually done by divide and conquer.
algorithms ==
considering the-rendering-pipeline, the-projection, the-clipping, and the-rasterization-steps are handled differently by the-following-algorithms: z-buffering
during rasterization, the-depth/z-value of each-pixel (or sample in the-case of anti-aliasing, but without loss of generality the-term-pixel is used) is checked against an-existing-depth-value.
if the-current-pixel is behind the-pixel in the-z-buffer, the-current-pixel is rejected, otherwise, the-current-pixel is shaded and the-current-pixel depth-value replaces the one in the-z-buffer.
z-buffering supports dynamic-scenes easily and is currently implemented efficiently in graphics-hardware.
this is the-current-standard.
the-cost of using z-buffering is that it uses up-to-4-bytes per pixel and that the-rasterization-algorithm needs to check each-rasterized-sample against the-z-buffer.
the-z-buffer can also suffer from artifacts due to precision-errors (also known as z-fighting).
coverage-buffers (c-buffer) and surface-buffer (s-buffer)
faster than z-buffers and commonly used in games in the-quake
instead of storing the-z-value per pixel, instead of storing the-z-value per pixel-store a-list of already-displayed-segments per line of the-screen.
new-polygons are then cut against already-displayed-segments that would hide  new-polygons.
an-s-buffer can display unsorted-polygons, while a-c-buffer requires polygons to be displayed from the nearest to the furthest.
because the-c-buffer-technique does not require a-pixel to be drawn more than once, the-process is slightly faster.
this was commonly used with binary-space-partitioning-(bsp)-trees, which would provide sorting for  new-polygons.
sorted active-edge-list
used in quake 1, this was storing a-list of the-edges of already-displayed-polygons (see scanline rendering).
polygons are displayed from the nearest to the furthest.
new-polygons are clipped against already displayed polygons'-edges, creating new-polygons to display then storing the-additional-edges.
it's much harder to implement than s/c/z-buffers, but it scales much better with increases in resolution.
painter's-algorithm
sorts-polygons by their-barycenter and draws their-back to front.
this produces few-artifacts when applied to scenes with polygons of similar-size forming smooth-meshes and back-face-culling turned on.
the-cost here is the-sorting-step and the-fact that visual-artifacts can occur.
this-algorithm is broken by design for general-scenes, as this-algorithm cannot handle polygons in various-common-configurations, such as surfaces that intersect each other.
binary-space-partitioning (bsp)
divides a-scene along planes corresponding to polygon-boundaries.
the-subdivision is constructed in such-a-way as to provide an-unambiguous-depth ordering from any-point in the-scene when the-bsp-tree is traversed.
the-disadvantage here is that the-bsp-tree is created with an-expensive-pre-process.
this means that it is less suitable for scenes consisting of dynamic-geometry.
the-advantage is that the-data is pre-sorted and error-free, ready for the-previously-mentioned-algorithms.
note that the-bsp is not a-solution to hsr, only-an-aid.
ray tracing attempts to model the-path of light-rays to a-viewpoint by tracing rays from the-viewpoint into the-scene.
although not a hidden-surface removal algorithm as such, it implicitly solves the-hidden-surface-removal-problem by finding the-nearest-surface along each-view-ray.
effectively this is equivalent to sorting all-the-geometry on a-per-pixel-basis.
the-warnock-algorithm divides the-screen into smaller-areas and sorts triangles within these.
if there is ambiguity (i.e., polygons overlap in-depth-extent within these-areas), then further-subdivision occurs.
at the-limit, further-subdivision may occur down to the-pixel-level.
culling-and-visible-surface-determination ==
a-related-area to visible-surface-determination (vsd) is culling, which usually happens before vsd in a-rendering-pipeline.
primitives or batches of primitives can be rejected in primitives or batches of primitives entirety, which usually reduces the-load on a-well-designed-system.
the-advantage of culling early on in a-rendering-pipeline is that entire-objects that are invisible do not have to be fetched, transformed, rasterized, or shaded.
here are some-types of culling algorithms:
viewing-frustum-culling ===
the-viewing-frustum is a-geometric-representation of the-volume visible to the-virtual-camera.
naturally, objects outside this-volume will not be visible in the-final-image, so they are discarded.
often, objects lie on the-boundary of the-viewing-frustum.
these-objects are cut into pieces along this-boundary in a-process called clipping, and the-pieces that lie outside the-frustum are discarded as there is no-place to draw the-pieces that lie outside the-frustum.
back-face-culling ==
with 3d-objects, some of the-object's-surface is facing the-camera, and the-rest is facing away from the-camera, i.e. is on the-backside of the-object, hindered by the-front-side.
if the-object is completely opaque, those-surfaces never need to be drawn.
they are determined by the-vertex-winding-order: if the-triangle drawn has the-triangle drawn vertices in clockwise-order on the-projection-plane when facing the-camera, they switch into counter-clockwise-order when the-surface turns away from the-camera.
incidentally, this also makes the-objects completely transparent when the-camera is located inside they, because then all-the-surfaces of the-object are facing away from the-camera and are culled by the-renderer.
to prevent this the-object must be set as double-sided (i.e.-no-back-face-culling is done) or have separate-inside-surfaces.
contribution-culling ===
often, objects are so far away that-objects do not contribute significantly to the-final-image.
objects are thrown away if objects-screen-projection is too small.
see clipping-plane.
occlusion-culling ===
objects that are entirely behind other-opaque-objects may be culled.
this is a-very-popular-mechanism to speed up the-rendering of large-scenes that have a-moderate-to-high-depth-complexity.
there are several-types of occlusion culling approaches:
potentially-visible-set (pvs) rendering divides a-scene into regions and pre-computes-visibility for them.
these-visibility-sets are then indexed at run-time to obtain high-quality-visibility-sets (accounting for complex-occluder-interactions) quickly.
portal-rendering divides a-scene into cells/sectors (rooms) and portals (doors), and computes which sectors are visible by clipping sectors against portals.
hansong-zhang's-dissertation "effective-occlusion-culling for the-interactive-display of arbitrary-models" describes an-occlusion-culling-approach.
= divide and conquer ==
a-popular-theme in the-vsd-literature is divide and conquer.
the-warnock-algorithm pioneered dividing the-screen.
beam-tracing is a-ray-tracing-approach that divides the-visible-volumes into beams.
various-screen-space-subdivision approaches reducing the-number of primitives considered per region, e.g.-tiling, or screen-space-bsp-clipping.
tiling may be used as a-preprocess to other-techniques.
z-buffer-hardware may typically include a-coarse-"hi-z", against which primitives can be rejected early without rasterization, this is a-form of occlusion-culling.
bounding-volume-hierarchies (bvhs) are often used to subdivide the-scene's-space (examples are the-bsp-tree, the-octree and the-kd-tree).
this allows visibility-determination to be performed hierarchically: effectively, if a-node in the-tree is considered to be invisible, then all of the-tree child nodes are also invisible, and no-further-processing is necessary (all of its-child-nodes can all be rejected by the-renderer).
if a-node is considered visible, then each of a-node children needs to be evaluated.
this-traversal is effectively a-tree-walk, where-invisibility/occlusion or reaching a-leaf-node determines whether to stop or whether to recurse respectively.
see also ==
sources ==
hidden-surface-determination
a-characterization of ten-hidden-surface-algorithms (wayback-machine copy intuition in the-context of decision-making is defined as a-“non-sequential-information-processing-mode.”
it is distinct from insight (a-much-more-protracted-process) and can be contrasted with the-deliberative-style of decision-making.
intuition can influence judgment through either-emotion or cognition, and there has been some-suggestion that it may be a-means of bridging the two.
individuals use intuition and more-deliberative-decision-making-styles interchangeably, but there has been some-evidence that people tend to gravitate to one or the-other-style more naturally.
people in a-good-mood gravitate toward intuitive-styles, while people in a-bad-mood tend to become more deliberative.
the-specific-ways in which intuition actually influences decisions remain poorly understood.
snap-judgments made possible by heuristics are sometimes identified as intuition.
definition and related-terms ==
intuitive-decision-making can be described as the-process by which information acquired through associated-learning and stored in long-term-memory is accessed unconsciously to form the-basis of a-judgment or decision.
information acquired through associated-learning and stored in long-term-memory can be transferred through affect induced by exposure to available-options, or through unconscious-cognition.
intuition is based on the-implicit-knowledge available to the-decision-maker.
for example, owning a-dog as a-child imbues someone with implicit-knowledge about canine-behavior, which may then be channeled into a-decision-making-process as the-emotion of fear or anxiety before taking a-certain-kind of action around an-angry-dog.
intuition is the-mechanism by which this-implicit-knowledge is brought to the-forefront of the-decision-making-process.
some-definitions of intuition in the-context of decision-making-point to the-importance of recognizing cues and patterns in one's-environment and then using them to improve one's-problem solving.
intuition in decision-making has been connected two-assumptions: 1)
tacit decision - previous decisions are affecting and 2)
explicit-decision---emotions are affecting.
intuition's-effect on decision-making is distinct from insight, which requires time to mature.
a-month spent pondering a-math-problem may lead to a-gradual-understanding of the-answer, even if one does not know where that-understanding came from.
intuition, in contrast, is a-more-instantaneous,-immediate-understanding upon first being confronted with the-math-problem.
intuition is also distinct from implicit-knowledge and learning, which inform intuition but are separate-concepts.
intuition is the-mechanism by which implicit-knowledge is made available during an-instance of decision-making.
channels of intuitive-influence == ===
heuristics ===
traditional-research often points to the-role of heuristics in helping people make “intuitive”-decisions.
those following the-heuristics-and-biases-school of thought developed by amos-tversky and daniel-kahneman believe that intuitive-judgments are derived from an-“informal-and-unstructured-mode of reasoning” that ultimately does not include any-methodical-calculation.
amos-tversky and kahneman identify availability, representativeness, and anchoring/adjustment as three-heuristics that influence many-intuitive-judgments made under uncertain-conditions.
the-heuristics-and-biases-approach looks at patterns of biased-judgments to distinguish heuristics from normative-reasoning-processes.
early-studies supporting this-approach associated each-heuristic with a-set of biases.
these-biases were “departures from the-normative-rational-theory” and helped identify the-underlying-heuristics.
use of the-availability-heuristic, for example, leads to error whenever the-memory retrieved is a-biased-recollection of actual-frequency.
this can be attributed to an-individual's-tendency to remember dramatic-cases.
heuristic-processes are quick-intuitive-responses to basic-questions such as frequency.
affect ===
some-researchers point to intuition as a-purely-affective-phenomenon that demonstrates the-ability of emotions to influence decision-making without cognitive-mediation.
this supports the-dual-processing-theory of affect and cognition, under which conscious-thought is not required for emotions to be experienced, but nevertheless positive conscious-thoughts towards person's will have positive-emotional-affects on them.
in studies comparing affect and cognition, some-researchers have found that positive-mood is associated with reliance on affective-signals while negative-mood is associated with more-deliberative-thought-processes.
mood is thus considered a moderator in the-strategic-decisions people carry out.
in a-series of three-studies, the-authors confirmed that people in a-positive-mood faced with a-card-based-gambling-task-utilized-intuition to perform better at higher-risk-stages than people who were in a-negative-mood.
other-theories propose that intuition has both-cognitive-and-affective-elements, bridging the-gap between these-two-fundamentally-different-kinds of human-information-processing.
comparison to other-decision-making-styles ==
intuitive-decision-making can be contrasted with deliberative-decision-making, which is based on cognitive-factors like beliefs, arguments, and reasons, commonly referred to as one's-explicit-knowledge.
intuitive-decision-making is based on implicit-knowledge
relayed to the-conscious-mind at the-point of decision through affect or unconscious-cognition.
some-studies also suggest that intuitive-decision-making relies more on the-mind's-parallel-processing-functions, while deliberative-decision-making relies more on sequential-processing.
prevalence of intuitive-judgment and measurement of use ==
although people use intuitive-and-deliberative-decision-making-modes interchangeably, individuals value the-decisions individuals make more when individuals are allowed to make individuals using individuals preferred-style.
this-specific-kind of regulatory-fit is referred to as decisional-fit.
the-emotions people experience after a-decision is made tend to be more pleasant when the-preferred-style is used, regardless of the-decision-outcome.
some-studies suggest that the-mood with which the-subject enters the-decision-making-process can also affect the-style they choose to employ: sad-people tend to be more deliberative, while people in a-happy-mood rely more on intuition.
the-preference for intuition and deliberation-scale developed by coralie-bestch in 2004 measures propensity toward intuitiveness.
the-scale defines preference for intuition as tendency to use affect (“gut-feel”) as a-basis for decision-making instead of cognition.
the-myers-briggs-type-indicator is also sometimes used.
intuitive-decision-making in specific-environments == ==
management and decision-making ===
researchers have also explored the-efficacy of intuitive-judgments and the-debate on the-function of intuition versus analysis in decisions that require specific-expertise, as in management of organizations.
in this-context, intuition is interpreted as an-“unconscious-expertise” rather than a-traditionally-purely-heuristic-response.
research suggests that this-kind of intuition is based on a-“broad-constellation of past-experiences, knowledge, skills, perceptions and feelings.”
the-efficacy of intuitive-decision-making in the-management-environment is largely dependent on the-decision-context-and-decision-maker's-expertise.
the-expertise-based-intuition increases over time when the-employee gets more-experience regarding the-organization worked for and by gathering domain-specific-knowledge.
in this-context the-so-called-intuition is not just-series of random-guesses, but rather a-process of combining expertise and know-how with the-employee's-instincts.
intuitions can, however be difficult to prove to be right in terms of decision-making.
it is in most-situations likely, that-decisions based on intuition are harder to justify than those that are based in rational-analysis.
especially in the-context of business-and-organizational-decision-making, one should be able to justify especially in the-context of business-and-organizational-decision-making decisions, thus making especially in the-context of business-and-organizational-decision-making purely intuitively is often not possible.
it is debated upon whether intuition is accurate, but evidence has been shown that under aforementioned-conditions it can.
the-organizations should not base  the-organizations decisions on just-intuitive-or-rational-analysis.
the-organizations need both-rational-and-intuitive-decision-making-processes and combination of those.
when it comes to the-decision-maker him/herself, mainly-two-factors affect the-effectiveness of intuitive-decision-making.
mainly two-factors have been found to be the-amount of expertise the-person has and the-individuals processing style.
finance ===
a-study of traders from the-four-largest-investment-banks in london looked at the-role that emotion and expert-intuition play in financial-trading-decisions.
a-study of traders from the-four-largest-investment-banks in london reported on the-differences between how-higher-and-lower-performing-traders incorporate intuition in how-higher-and-lower-performing-traders decision strategy, and attributed the-success of some-higher-performing-traders to how-higher-and-lower-performing-traders great-disposition to reflect critically about how-higher-and-lower-performing-traders intuitions.
this-propensity to think critically about intuition and the-source of those-hunches served as a-distinguishing-factor between the-higher-and-lower-performing-traders included in a-study of traders from the-four-largest-investment-banks in london.
while successful-traders were more open to this-critical-introspection, lower-performing-traders were reported to rely on lower-performing-traders-feelings alone rather than further explore the-affective-influences for lower-performing-traders decisions.
reflection on the-origin of feelings by expert-traders may be particularly salient given affect-as-information model, which holds that the-impact of emotions on behavior is reduced or even disappears when the-relevance of those-emotions is explicitly called into question.
it has been noted in a-research, that intuition is used as a-method of decision-making in the-banking-industry.
record shows that intuition is used in combination with pre-existing-solution-models and previous-experiences.
participants of a-research also reported to analyse participants of the-research-intuitive-decisions afterwards and possibly altering participants of the-research.
high-risk-situations ===
traditional-literature attributes the-role of judgment-processes in risk-perception and decision-making to cognition rather than emotion.
however, more-recent-studies suggest a-link between emotion and cognition as it relates to decision-making in high-risk-environments.
studies of decision-making in high-risk-environments suggest that individuals who self-identify as intuitive-decision-makers tend to make faster-decisions that imply greater-deviation from risk-neutrality than those who prefer the-deliberative-style.
for example, risk-averse-intuitive-decision-makers will choose to not participate in a-dangerous-event more quickly than deliberative-decision-makers, but will choose not to participate in more-instances than risk-averse-intuitive-decision-makers deliberative counterparts.
strategic-decisions ===
strategic-decisions are usually made by the-top-management in the-organizations.
usually strategic-decisions also effect on the-future of the-organization.
rationality has been the-guideline and also justified way to make decisions because decisions are based on facts.
intuition in strategic-decision-making is less examined and for example can be depending on a-case be described as managers-know-how, expertise or just-a-gut-feeling, hunch.
see also ==
intuitionistic-logic ==
sources ==
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
social-psychologists typically explain human-behavior as being a-result of the-relationship between mental-state-and-social-situation, studying the-conditions under which thoughts, feelings, and behaviors occur and how these-variables influence social-interactions.
social-psychology has bridged the-gap between psychology and sociology to an-extent, but a-divide still exists between the-two-fields.
nevertheless, sociological-approaches to psychology remain an-important-counterpart to conventional-psychological-research.
in addition to the-split between psychology and sociology, there is difference in emphasis between american-and-european-social-psychologists, as the former traditionally have focused more on the-individual, whereas the latter have generally paid more-attention to group-level-phenomena.
history ==
although issues in social-psychology already had been discussed in philosophy for much of human-history—such as the-writings of the-islamic-philosopher-al-farabi, which dealt with similar-issues—the-modern,-scientific-discipline began in the-united-states at the-end of the-19th-century.
19th-century ==
in the-19th-century, social-psychologist was an-emerging-field from the-larger-field of psychology.
at the-time, many-psychologists were concerned with developing concrete-explanations for the-different-aspects of human-nature.
they attempted to discover concrete-cause-and-effect-relationships that explained social-interactions.
in order to do so, they applied the-scientific-method to human-behavior.
the-first-published-study in the-field was norman-triplett's-1898-experiment on the-phenomenon of social-facilitation.
these-psychological-experiments later went on to form the-foundation of much of 20th-century-social-psychological-findings.
early-20th-century ===
during the-1930s, many-gestalt-psychologists, most-notably-kurt-lewin, fled to the-united-states from nazi-germany.
many-gestalt-psychologists, most-notably-kurt-lewin were instrumental in developing the-field as an-area separate from the-dominant-behavioral-and-psychoanalytic-schools of that-time.
attitudes and small-group-phenomena were the-topics most commonly studied in this-era.
during world-war-ii, social-psychologists were primarily engaged with studies of persuasion and propaganda for the-u.s.-military (see also psychological-warfare).
following the-war, researchers became interested in a-variety of social-problems, including issues of gender and racial-prejudice.
most notable and contentious of a-variety of social-problems, including issues of gender and racial-prejudice were the-milgram-experiments.
during the-years immediately following world-war-ii, there were frequent-collaborations between psychologists and sociologists.
the-two-disciplines, however, have become increasingly specialized and isolated from each other in recent-years, with sociologists generally focusing on macro-features whereas psychologists generally focusing on more-micro-features.
late-20th-century and modernity ==
in the-1960s, there was growing-interest in topics such as cognitive-dissonance, bystander-intervention, and aggression.
by the-1970s, however, social-psychology in america had reached a-crisis, as heated-debates emerged over issues such as ethical-concerns about laboratory-experimentation, whether attitude could actually predict behavior, and how much science could be done in a-cultural-context.
this was also a-time when situationism came to challenge the-relevance of self and personality in psychology.
throughout the-1980s and 1990s, social-psychology reached a-more-mature-level, especially in regard to theory and methodology.
now, careful-ethical-standards regulate research, and pluralistic-and-multicultural-perspectives have emerged.
modern-researchers are interested in many-phenomena, though attribution, social-cognition, and the-self-concept are perhaps the-areas of greatest-growth in recent-years.
social-psychologists have also maintained social-psychologists applied interests with contributions in the-social-psychology of health, education, law, and the-workplace.
intrapersonal-phenomena == ===
attitudes ===
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
attitudes are basic-expressions of approval and disapproval, or as bem (1970) suggests, likes and dislikes
(e.g. enjoying chocolate-ice-cream, or endorsing the-values of a-particular-political-party).
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
for example, a-person may value the-environment but may not recycle a-plastic-bottle on a-particular-day.
research on attitudes has examined the-distinction between traditional,-self-reported-attitudes and implicit,-unconscious-attitudes.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
likewise, one-study found that in interracial-interactions, explicit-attitudes correlate with verbal-behavior while implicit-attitudes correlate with nonverbal-behavior.
one-hypothesis on how attitudes are formed, first proposed in 1983 by abraham-tesser, is that strong-likes and dislikes are ingrained in our-genetic-make-up.
abraham-tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
attitudes are also formed as a-result of exposure to different-experiences, environments, and through the-learning-process.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
attitudes are also involved in several-other-areas of the-discipline, such as conformity, interpersonal-attraction, social-perception, and prejudice.
persuasion
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
persuasion relies on appeals rather than strong-pressure or coercion.
the-process of persuasion has been found to be influenced by numerous-variables that generally fall into one of five-major-categories: communicator: includes credibility, expertise, trustworthiness, and attractiveness.
message: includes varying-degrees of reason, emotion (e.g. fear), one-sided or two-sided-arguments, and other-types of informational-content.
audience: includes a-variety of demographics, personality-traits, and preferences.
channel/medium:
includes printed-word, radio, television, the-internet, or face-to-face interactions.
context: includes environment, group-dynamics, and preliminary-information to that of message (category #2).dual-process-theories of persuasion (such as the-elaboration-likelihood-model) maintain that persuasion is mediated by two-separate-routes: central and peripheral.
the-central-route of persuasion is more fact-based and results in longer-lasting-change, but requires motivation to process.
the-central-route of persuasion is more superficial and results in shorter-lasting-change, but does not require as-much-motivation to process.
an-example of peripheral-persuasion is a-politician using a-flag-lapel-pin, smiling, and wearing a-crisp,-clean-shirt.
this does not require motivation to be persuasive, but should not last as long as central-persuasion.
if that-politician were to outline what that-politician believe and that-politician previous voting record, that-politician would be centrally persuasive, resulting in longer-lasting-change at the-expense of greater-motivation required for processing.
social-cognition ===
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
a-major-research-topic in social-cognition is attribution.
attributions are how we explain people's-behavior, either we own behavior or the-behavior of others.
one-element of attribution ascribes the-cause of a-behavior to internal-and-external-factors.
an-internal,-or-dispositional,-attribution-reasons that behavior is caused by inner-traits such as personality, disposition, character, and ability.
an-external,-or-situational,-attribution-reasons that behaviour is caused by situational-elements such as the-weather.
a-second-element of attribution ascribes the-cause of behavior to stable-and-unstable-factors (i.e. whether the-behavior will be repeated or changed under similar-circumstances).
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
numerous-biases in the-attribution-process have been discovered.
for instance, the-fundamental-attribution-error is the-tendency to make dispositional-attributions for behavior, overestimating the-influence of personality and underestimating the-influence of the-situational.
the-actor-observer-bias is a-refinement of this; the-actor-observer-bias is the-tendency to make dispositional-attributions for other-people's-behavior and situational-attributions for our own.
the-self-serving-bias is the-tendency to attribute dispositional-causes for successes, and situational-causes for failure, particularly when self-esteem is threatened.
this leads to assuming one's-successes are from innate-traits, and one's-failures are due to situations.
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
researchers have found that mildly-depressed-individuals often lack this-bias and actually have more-realistic-perceptions of reality as measured by the-opinions of others.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
as such, vivid-or-highly-memorable-possibilities will be perceived as more likely than those that are harder to picture or difficult to understand, resulting in a-corresponding-cognitive-bias.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
numerous-other-biases have been found by social-cognition-researchers.
the-hindsight-bias is a-false-memory of having predicted events, or an-exaggeration of actual-predictions, after becoming aware of the-outcome.
the-confirmation-bias is a-type of bias leading to the-tendency to search for or interpret information in a-way that confirms one's-preconceptions.
schemas ====
another-key-concept in social-cognition is the-assumption that reality is too complex to easily discern.
as a-result, we tend to see the-world according to simplified-schemas or images of reality.
schemas are generalized-mental-representations that organize knowledge and guide information-processing.
schemas often operate automatically and unintentionally, and can lead to biases in perception and memory.
schemas may induce expectations that lead us to see something that is not there.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
stereotypes are often related to negative-or-preferential-attitudes (prejudice) and behavior (discrimination).
schemas for behaviors (e.g., going to a-restaurant, doing laundry) are known as scripts.
self-concept ===
self-concept is the-whole-sum of beliefs that people have about people.
the-self-concept is made up of cognitive-aspects called self-schemas—
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, an-athlete at a-university would have multiple-selves that would process different-information pertinent to each-self: the-student would be oneself, who would process information pertinent to a-student (taking notes in class, completing a-homework-assignment, etc.);
an-athlete at a-university would be the-self who processes information about things related to being an-athlete (recognizing an-incoming-pass, aiming a-shot, etc.).
these-selves are part of one's-identity and the-self-referential-information is that which relies on the-appropriate-self to process and react to it.
if a-self is not part of one's-identity, then it is much more difficult for one to react.
for example, a-civilian may not know how to handle a-hostile-threat as well as a-trained-marine would.
a-trained-marine contains a-self that would enable him/
a-trained-marine to process the-information about the-hostile-threat and react accordingly, whereas a-civilian may not contain that-self, lessening the-civilian's-ability to properly assess the-hostile-threat and act accordingly.
the-self-concept comprises multiple-self-schemas.
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
for individuals, a-range of otherwise-mundane-events—grocery-shopping, new-clothes, eating out, or going to the-beach—can trigger thoughts about the-self.
the-self is a-special-object of our-attention.
whether one is mentally focused on a-memory, a-conversation, a-foul-smell,
the-song that is stuck in one's-head, or this-sentence
, consciousness is like a-spotlight.
a-spotlight can shine on only-one-object at a-time, but a-spotlight can switch rapidly from one-object to another.
in a-spotlight the-self is front and center: things relating to the-self have a-spotlight more often.
the-abcs of self are:
affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
: how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting
is the-process of predicting how one would feel in response to future-emotional-events.
studies done in 2003 by timothy-wilson and daniel-gilbert
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
there are many-theories on the-perception of our-own-behavior.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
there is also the-facial-feedback-hypothesis: changes in facial-expression can lead to corresponding-changes in emotion.
the-self-concept is often divided into a-cognitive-component, known as the-self-schema, and an-evaluative-component, the-self-esteem.
the-need to maintain a-healthy-self-esteem is recognized as a-central-human-motivation.
self-efficacy-beliefs are associated with the-self-schema.
self-efficacy-beliefs are expectations that performance of some-task will be effective and successful.
social-psychologists also study such-self-related-processes as self-control and self-presentation.
people develop  people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
self-perception is a-specialized-form of attribution that involves making inferences about oneself after observing one's-own-behavior.
psychologists have found that too-many-extrinsic-rewards (e.g.-money) tend to reduce intrinsic-motivation through the-self-perception-process, a-phenomenon known as overjustification.
people's-attention is directed to the-reward, and people's-attention lose interest in the-task when the-reward is no longer offered.
this is an-important-exception to reinforcement-theory.
interpersonal-phenomena ==
social-influence ===
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
social-influence is seen as a-fundamental-value in social-psychology.
the-study of social-influence overlaps considerably with research into attitudes and persuasion.
the-three-main-areas of social-influence include: conformity, compliance, and obedience.
social-influence is also closely related to the-study of group-dynamics, as most-effects of influence are strongest when influence take place in social-groups.
the-first-major-area of social-influence is conformity.
conformity is defined as the-tendency to act or think like other-members of a-group.
the-identity of members within a-group (i.e.-status), similarity, expertise, as well as cohesion, prior-commitment, and accountability to a-group help to determine the-level of conformity of an-individual.
individual-variations among group-members plays a-key-role in the-dynamic of how-willing-people will be to conform.
conformity is usually viewed as a-negative-tendency in american-culture, but a-certain-amount of conformity is adaptive in some-situations, as is nonconformity in other-situations.
the-second-major-area of social-influence-research is compliance, which refers to any-change in behavior that is due to a-request or suggestion from another-person.
the foot-in-the-door technique is a-compliance-method in which the-persuader requests a-small-favor and then follows up with requesting a-larger-favor, e.g., asking for the-time and then asking for ten-dollars.
a-related-trick is the-bait and switch.
the-third-major-form of social-influence is obedience; this is a-change in behavior that is the-result of a-direct-order or command from another-person.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
an-unusual-kind of social-influence is the-self-fulfilling-prophecy.
this is a-prediction that, in being made, causes this to become true.
for example, in the-stock-market, if this is widely believed that a-crash is imminent, investors may lose confidence, sell most of investors-stock, and thus cause a-crash.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
psychologists have spent decades studying the-power of social-influence, and the-way in which it manipulates people's-opinions and behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
group-dynamics ===
a-group can be defined as two-or-more-individuals who are connected to each another by social-relationships.
groups tend to interact, influence each other, and share a-common-identity.
groups have a-number of emergent-qualities that distinguish groups from coincidental,-temporary-gatherings, which are termed social aggregates: norms: implicit-rules and expectations for group-members to follow (e.g. saying thank you, shaking hands).
implicit-rules and expectations for specific-members within the-group (e.g.-the-oldest-sibling, who may have additional-responsibilities in the-family).
relations: patterns of liking within the-group, and also-differences in prestige or status (e.g.-leaders, popular-people).temporary-groups and aggregates share few or none of these-features and do not qualify as true-social-groups.
people waiting in line to get on a-bus, for example, do not constitute a-group.
groups are important not only because  groups offer social-support, resources, and a-feeling of belonging, but because  groups supplement an-individual's-self-concept.
to a-large-extent, humans define humans by the-group-memberships which form humans-social-identity.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
the-tendency to define oneself by membership in a-group may lead to intergroup-discrimination, which involves favorable-perceptions and behaviors directed towards the-in-group, but negative-perceptions and behaviors directed towards the-out-group.
on the-other-hand, such-discrimination and segregation may sometimes exist partly to facilitate a-diversity that strengthens society.
intergroup-discrimination leads to prejudicial-stereotyping, while the-processes of social-facilitation and group-polarization encourage extreme-behaviors towards the-out-group.
groups often moderate and improve decision-making, and are frequently relied upon for these-benefits, such as in committees and juries.
a-number of group-biases, however, can interfere with effective-decision-making.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
more problematic is the-phenomenon of groupthink, which is a-collective-thinking-defect that is characterized by a-premature-consensus or an-incorrect-assumption of consensus, caused by members of a-group failing to promote views that are not consistent with the-views of other-members.
groupthink occurs in a-variety of situations, including isolation of a-group and the-presence of a-highly-directive-leader.
janis offered the-1961-bay of pigs-invasion as a-historical-case of groupthink.
groups also affect performance and productivity.
social-facilitation, for example, is a-tendency to work harder and faster in the-presence of others.
social-facilitation increases the-dominant-response's-likelihood, which tends to improve performance on simple-tasks and reduce  social-facilitation on complex-tasks.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
social-loafing is common when the-task is considered unimportant and individual-contributions are not easy to see.
social-psychologists study group-related-(collective)-phenomena such as the-behavior of crowds.
an-important-concept in this-area is deindividuation, a-reduced-state of self-awareness that can be caused by feelings of anonymity.
deindividuation is associated with uninhibited-and-sometimes-dangerous-behavior.
deindividuation is common in crowds and mobs, but  deindividuation can also be caused by a-disguise, a-uniform, alcohol, dark-environments, or online-anonymity.
interpersonal-attraction ===
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
several-general-principles of attraction have been discovered by social-psychologists.
one of the-most-important-factors in interpersonal-attraction is how similar two-particular-people are.
the-more-similar-two-people are in general-attitudes, backgrounds, environments, worldviews, and other-traits, the more likely the-more-similar-two-people will be attracted to each other.
physical-attractiveness is an-important-element of romantic-relationships, particularly in the-early-stages characterized by high-levels of passion.
later on, similarity and other-compatibility-factors become more important, and the-type of love-people experience shifts from passionate to companionate.
in 1986, robert-sternberg suggested that there are actually three-components of love: intimacy, passion, and commitment.
when two-(or-more)-people experience all three, two-(or-more)-people are said to be in a-state of consummate-love.
according to social-exchange-theory, relationships are based on rational-choice-and-cost-benefit-analysis.
a-person may leave a-relationship if their-partner's-"costs" begin to outweigh their-benefits, especially if there are good-alternatives available.
this-theory is similar to the-minimax-principle proposed by mathematicians and economists (despite the-fact that human-relationships are not zero-sum-games).
with time, long-term-relationships tend to become communal rather than simply based on exchange.
research == ===
methods ===
social-psychology is an-empirical-science that attempts to answer questions about human-behavior by testing hypotheses, both in the-laboratory and in the-field.
careful-attention to research-design, sampling, and statistical-analysis is important; results are published in peer-reviewed-journals such as the-journal of experimental-social-psychology, personality and social-psychology bulletin and the-journal of personality and social-psychology.
social-psychology-studies also appear in general-science-journals such as psychological-science and science.
experimental-methods involve the-researcher altering a-variable in the-environment and measuring the-effect on another-variable.
an-example would be allowing two-groups of children to play violent-or-nonviolent-videogames and then observing two-groups of children subsequent-level of aggression during the-free-play-period.
a-valid-experiment is controlled and uses random-assignment.
correlational-methods examine the-statistical-association between two-naturally-occurring-variables.
for example, one could correlate the-number of violent-television shows children watch at home with the-number of violent-incidents the-children participate in at school.
note that this-study would not prove that violent-tv causes aggression in children: it is quite possible that aggressive-children choose to watch more violent-tv.
observational-methods are purely descriptive and include naturalistic-observation, contrived-observation, participant-observation, and archival-analysis.
these are less common in social-psychology but are sometimes used when first investigating a-phenomenon.
an-example would be to unobtrusively observe children on a-playground (with a-videocamera, perhaps) and record the-number and types of aggressive-actions displayed.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
experiments are useful in social-psychology because experiments are high in internal-validity, meaning that experiments are free from the-influence of confounding-or-extraneous-variables, and so are more likely to accurately indicate a-causal-relationship.
however, the-small-samples used in controlled-experiments are typically low in external-validity, or the-degree to which the-results can be generalized to the-larger-population.
there is usually a-trade-off between experimental-control (internal-validity) and being able to generalize to the-population (external-validity).
because it is usually impossible to test everyone, research tends to be conducted on a-sample of persons from the-wider-population.
social-psychologists frequently use survey-research when social-psychologists are interested in results that are high in external-validity.
surveys use various-forms of random-sampling to obtain a-sample of respondents that is representative of a-population.
this-type of research is usually descriptive or correlational because there is no-experimental-control over variables.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in a-1986-study by david-o.-sears, over-70% of experiments used north-american-undergraduates as subjects, a-subset of the-population that is unrepresentative of the-population as a-whole.
regardless of which-method has been chosen, the-significance of the-results is reviewed before accepting north-american-undergraduates in evaluating an-underlying-hypothesis.
there are two-different-types of tests that social-psychologists use to review social-psychologists results.
statistics and probability-testing define what constitutes a-significant-finding, which can be as low as 5% or less, that is unlikely due to-chance.
replications-testing is important in ensuring that the-results are valid and not due to chance.
false-positive-conclusions, often resulting from the-pressure to publish or the-author's-own-confirmation-bias, are a-hazard in the-field.
famous-experiments === ====
asch conformity experiments ====
asch conformity experiments ==== demonstrated the-power of the-impulse to conform within small-groups, by the-use of a-line-length-estimation-task that was designed to be easy to assess but where deliberately-wrong-answers were given by at least some, oftentimes most, of the-other-participants.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
seventy-five-percent of the-participants conformed at least once during the-experiment.
additional-manipulations of the-experiment showed that participant-conformity decreased when at-least-one-other-individual failed to conform but increased when the-individual began conforming or withdrew from the-experiment.
also, participant-conformity increased substantially as the-number of "incorrect"-individuals increased from one to three, and remained high as the-incorrect-majority grew.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
festinger (cognitive-dissonance) ====
in leon-festinger's-cognitive-dissonance-experiment, after being divided into two-groups-participants were asked to perform a-boring-task and later asked to dishonestly give being divided into two-groups-participants opinion of a-boring-task, afterwards being rewarded according to two-different-pay-scales.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
the-first-group ($1) later reported liking the-task better than the-second-group ($20).
festinger's-explanation was that for people in  the-first-group ($1) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
those who were paid $1-experienced-dissonance could only overcome that-dissonance by justifying those who were paid $1-experienced-dissonance lies by changing those who were paid $1-experienced-dissonance previously-unfavorable-attitudes about the-task.
being paid $20 provides a-reason for doing the-boring-task resulting in no-dissonance.
milgram-experiment ====
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
following the-events of the-holocaust in world-war-ii,  milgram-experiment ==== showed that normal-american-citizens were capable of following orders even when normal-american-citizens believed normal-american-citizens were causing an-innocent-person to suffer.
stanford-prison-experiment ====
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
in just-a-few-days, the-guards became brutal and cruel, and the-prisoners became miserable and compliant.
this was initially argued to be an-important-demonstration of the-power of the-immediate-social-situation and this-capacity to overwhelm normal-personality-traits.
subsequent-research has contested the-initial-conclusions of the-study.
for example, subsequent-research has been pointed out that participant-self-selection may have affected the-participants'-behavior, and that the-participants'-personalities influenced the-participants'-personalities reactions in a-variety of ways, including how long the-participants'-personalities chose to remain in the-study.
the-2002-bbc-prison-study, designed to replicate the-conditions in the-stanford-study, produced conclusions that were drastically different from the-initial-findings.
others ====
muzafer-sherif's-robbers'-cave-study divided boys into two-competing-groups to explore how-much-hostility and aggression would emerge.
muzafer-sherif's's-explanation of the-results became known as realistic-group-conflict-theory, because the-intergroup-conflict was induced through competition for resources.
inducing cooperation and superordinate-goals later reversed this-effect.
albert-bandura's-bobo-doll-experiment demonstrated how aggression is learned by imitation.
this-set of studies fueled debates regarding media-violence, a-topic that continues to be debated among scholars.
ethics ===
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
for this-reason, many-social-psychology-experiments utilize deception to conceal or distort certain-aspects of the-study.
deception may include false-cover-stories, false-participants (known as confederates or stooges),
false-feedback given to the-participants, and so on.
the-practice of deception has been challenged by psychologists who maintain that-deception under any-circumstances is unethical and that other-research-strategies (e.g.,-role-playing) should be used instead.
unfortunately, research has shown that role-playing-studies do not produce the-same-results as deception-studies, and this has cast doubt on role-playing-studies validity.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
(e.g., the-milgram-experiment and stanford-prison-experiment), and this has also been criticized for ethical-reasons.
to protect the-rights and well-being of research-participants, and at the-same-time discover meaningful-results and insights into human-behavior, virtually-all-social-psychology-research must pass an-ethical-review.
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
furthermore, a-process of informed-consent is often used to make sure that volunteers know what will asked of volunteers in the-experiment and understand that volunteers are allowed to quit the-experiment at any-time.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
today, most-research in social-psychology involves no-more-risk of harm than can be expected from routine-psychological-testing or normal-daily-activities.
adolescents ===
social-psychology studies what plays key-roles in a-child's-development.
during this-time, teens are faced with many-issues and decisions that can impact teens-social-development.
teens are faced with self-esteem-issues, peer-pressure, drugs, alcohol, tobacco, sex, and social-media.
psychologists today are not fully aware of the-effect of social-media.
social-media is worldwide, so one can be influenced by something social-media will never encounter in real-life.
in 2019, social-media became the-single-most-important-activity in adolescents' and even-some-older-adults'-lives.
replication-crisis ===
many-social-psychological-research-findings have proven difficult to replicate, leading some to argue that social-psychology is undergoing a-replication-crisis.
replication-failures are not unique to social-psychology and are found in all-fields of science.
some-factors have been identified in social-psychological-research that has led the-field to undergo the-field current crisis.
firstly, questionable-research-practices have been identified as common.
such-practices, while not necessarily intentionally fraudulent, involve converting undesired-statistical-outcomes into desired-outcomes via the-manipulation of statistical-analyses, sample-sizes, or data-management-systems, typically to convert non-significant-findings into significant-ones.
some-studies have suggested that at-least-mild-versions of these-practices are prevalent.
one of the-criticisms of daryl-bem in the-feeling the-future-controversy is that the-evidence for precognition in the-study could be attributed to questionable-practices.
secondly, some-social-psychologists have published fraudulent-research that has entered into mainstream-academia, most-notably-the-admitted-data-fabrication by diederik-stapel as well as allegations against others.
fraudulent-research is not the-main-contributor to the-replication-crisis.
several-effects in social-psychology have been found to be difficult to replicate even before the-replication-crisis.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
replications appear particularly difficult when research-trials are pre-registered and conducted by research-groups not highly invested in the-unconscious-thought-theory under questioning.
these-three-elements together have resulted in renewed-attention to replication supported by daniel-kahneman.
scrutiny of many-effects have shown that several-core-beliefs are hard to replicate.
a-2014-special-edition of social-psychology focused on replication-studies, and a-number of previously-held-beliefs were found to be difficult to replicate.
likewise, a-2012-special-edition of perspectives on psychological-science focused on issues ranging from publication-bias to null-aversion that contribute to the-replication-crisis in psychology.
it is important to note that the-replication-crisis in psychology does not mean that social-psychology is unscientific.
rather, this-reexamination is
a-healthy-if-sometimes-acrimonious-part of the-scientific-process in which old-ideas or those that cannot withstand careful-scrutiny are pruned.
the-consequence is that some-areas of social-psychology once considered solid, such as social-priming, have come under increased-scrutiny due to failure to replicate findings.
academic-journals ===
see also == ==
notes == ==
references ==
external-links ==
social-psychology-network-introduction to social-psychology-social-psychology — basics
social-psychology  on plos —
subject-area-page-social-psychology on all-about psychology — information and resources
what is social-psychology?
on youtube
this-glossary of computer-hardware-terms is a-list of definitions of terms and concepts related to computer-hardware, i.e.-the-physical-and-structural-components of computers, architectural-issues, and peripheral-devices.
accelerated-graphics-port (agp)
a high-speed point-to-point channel for attaching a-video-card to a-computer's-motherboard, primarily to assist in the-acceleration of 3d-computer-graphics.
accelerator a-microprocessor, asic, or expansion-card designed to offload a-specific-task from the-cpu, often containing fixed-function-hardware.
a-common-example is a-graphics-processing-unit.
accumulator
a-register in a-cpu in which intermediate-arithmetic-and-logic-results are stored.
address the-unique-integer-number that specifies a-memory-location in an-address-space.
address-space
a-mapping of logical-addresses into physical-memory or other-memory-mapped-devices.
advanced-technology extended (atx)
a-motherboard-form-factor-specification developed by intel in 1995 to improve on previous-de-factor-standards like the-at-form-factor.
ai-accelerator
an-accelerator aimed at running artificial-neural-networks or other-machine-learning and machine-vision-algorithms (either-training or deployment), e.g.-movidius-myriad 2, truenorth, tensor-processing-unit, etc.
advanced-configuration and power-interface
an-open-standard for operating-systems to discover, configure, manage, and monitor status of the-hardware.
blu-ray-disc (bd)
an-optical-disc-storage-medium designed to supersede the-dvd-format.
a-subsystem that transfers data between computer-components inside a-computer or between computers.
c-==-cache
a-small,-fast-local-memory that transparently buffers access to a-larger-but-slower-or-more-distant/higher-latency-memory--or-storage-device, organised into cache-lines.
automatically translates accesses to the-underlying-resources address-space to locations in the-cache.
cache-coherency
the-process of keeping data in multiple-caches synchronised in a-multiprocessor-shared-memory-system, also required when dma modifies the-underlying-memory.
cache-eviction
freeing up data from within a-cache to make room for new-cache-entries to be allocated; controlled by a-cache replacement policy.
caused by a-cache-miss whilst a-cache is already full.
cache hit finding-data in a-local-cache, preventing the-need to search for that-resource in a-more-distant-location (or to repeat a-calculation).
cache-line
a-small-block of memory within a-cache; the-granularity of allocation, refills, eviction;-typically-32–128-bytes in size.
cache miss not finding data in a-local-cache, requiring use of the-cache-policy to allocate and fill this-data, and possibly performing evicting other-data to make room.
cache thrashing a-pathological-situation where access in a-cache-cause cyclical-cache misses by evicting data that is needed in the-near-future.
cache-ways
the-number of potential-cache-lines in an-associative-cache that specific-physical-addresses can be mapped to; higher-values reduce potential-collisions in allocation.
cache-only-memory-architecture (coma)
a-multiprocessor-memory-architecture where an-address-space is dynamically shifted between processor-nodes based on demand.
card-reader
any-data-input-device that reads data from a-card-shaped-storage-medium.
a-generic-term that refers to a-high-performance-input/output-(i/o)-architecture that is implemented in various-forms on a-number of computer-architectures, especially on mainframe-computers.
also-chip-set.
a-group of integrated-circuits, or chips, that are designed to work together.
a-group of integrated-circuits, or chips, that are designed to work together are usually marketed as a-single-product.
compact-disc-recordable (cd-r)
a-variation of the-optical-compact-disc which can be written to once.
compact-disc-rewritable (cd-rw)
a-variation of the-optical-compact-disc which can be written to many-times.
compact-disc-read-only-memory (cd-rom)
a-pre-pressed-compact-disc which contains data-or-music-playback and which cannot be written to.
computer-case also chassis,-cabinet,-box,-tower,-enclosure,-housing,-system-unit, or simply case.
the-enclosure that contains most of the-components of a-computer, usually excluding the-display, keyboard, mouse, and various-other-peripherals.
computer-fan
an-active-cooling-system forcing airflow inside or around a-computer-case using a-fan to cause air-cooling.
computer-form-factor
the-name used to denote the-dimensions, power-supply-type, location of mounting-holes, number of ports on the-back-panel, etc.
computer monitor an-electronic-visual-display for computers.
a-monitor usually comprises the-display-device, circuitry, casing, and power-supply.
the-display-device in modern-monitors is typically a-thin-film-transistor-liquid-crystal-display (tft-lcd) or a-flat-panel-led-display, whereas older-monitors used a-cathode-ray-tube (crt).
control store the-memory that stores the-microcode of a-cpu.
conventional-peripheral-component-interconnect (conventional-pci) also-simply-pci.
a-computer-bus for attaching hardware-devices in a-computer.
the-portion of the-cpu which actually performs arithmetic-and-logical-operations; many-cpus have multiple-cores (e.g.-"a-quad-core-processor").
core-memory in modern-usage, a-synonym for main-memory, dating back from the-pre-semiconductor-chip-times when the dominant main-memory technology was magnetic core-memory.
central-processing-unit (cpu)
the-portion of a-computer-system that executes the-instructions of a-computer-program.
d-==-data-cache (d-cache)
a-cache in a-cpu-or-gpu-servicing-data-load-and-store-requests, mirroring main-memory (or vram for a-gpu).
data-storage
a-technology consisting of computer-components and recording-media used to retain digital-data.
it is a-core-function and fundamental-component of computers.
device-memory-local-memory associated with a-hardware-device such as a-graphics-processing-unit or opencl-compute-device, distinct from main-memory.
digital-video-disc (dvd)  digital-video-disc (dvd).
an-optical-compact-disc - of the-same-dimensions as compact-discs (cds), but store more-than-six-times-as-much-data.
digital-visual-interface (dvi) a-video-display-interface developed by the-digital-display-working-group (ddwg).
the-digital-interface is used to connect a-video-source to a-display-device, such as a-computer-monitor.
direct-access-storage-device (dasd)
a-mainframe-terminology introduced by ibm denoting secondary-storage with random-access, typically-(arrays of) hard-disk-drives.
direct-mapped-cache-a-cache where each-physical-address may only be mapped to one-cache-line, indexed using the-low-bits of the-address.
simple but highly prone to allocation-conflicts.
direct-memory-access (dma)
the-ability of a-hardware-device such as a-disk-drive or network-interface to access main-memory without intervention from the-cpu,  provided by one-or-more-dma-channels in a-system.
displayport-a-digital-display-interface developed by the-video-electronics-standards-association (vesa).
the-interface is primarily used to connect a-video-source to a-display-device such as a-computer-monitor, though  the-interface can also be used to transmit audio, usb, and other-forms of data.
a-standard-sized-area within a-computer-case for adding hardware (hard-drives, cd-drives, etc.)
to a-computer.
dual in-line memory module (dimm)
a-series of dynamic-random-access-memory-integrated-circuits.
a-series of dynamic-random-access-memory-integrated-circuits are mounted on a-printed-circuit-board and designed for use in personal-computers, workstations and servers.
contrast simm.
dual-issue
a-superscalar-pipeline capable of executing two-instructions simultaneously.
dynamic-random-access-memory (dram)
a-type of random-access-memory that stores each bit of data in a-separate-capacitor within an-integrated-circuit and which must be periodically refreshed to retain the-stored-data.
e-==-expansion-bus-a-computer-bus which moves information between the-internal-hardware of a-computer-system (including the-cpu and ram) and peripheral-devices.
it is a-collection of wires and protocols that allows for the-expansion of a-computer.
expansion-card-a-printed-circuit-board that can be inserted into an-electrical-connector-or-expansion-slot on a-computer-motherboard, backplane, or riser-card to add functionality to a-computer-system via an-expansion-bus.
f-==-firewall
any-hardware-device-or-software-program designed to protect a-computer from viruses, trojans, malware, etc.
firmware-fixed-programs and data that internally control various-electronic-devices.
flash-memory
a-type of non-volatile-computer-storage-chip that can be electrically erased and reprogrammed.
floppy-disk-a-data-storage-medium that is composed of a-disk of thin,-flexible-("floppy")-magnetic-storage-medium encased in a-square-or-rectangular-plastic-shell.
floppy-disk drive a-device for reading floppy-disks.
floppy-disk-controller
free-and-open-source-graphics-device-driver ==
graphics-hardware-graphics-processing-unit (gpu) ==
hard-disk-drive (hdd)
any-non-volatile-storage-device that stores data on rapidly-rotating-rigid-(i.e.-hard)-platters with magnetic-surfaces.
the-physical-components of a-computer-system.
harvard-architecture a-memory-architecture where program-machine-code and data are held in separate-memories, more commonly seen in microcontrollers and digital-signal-processors.
high-definition-multimedia-interface (hdmi)
a-compact-interface for transferring encrypted-uncompressed-digital-audio-and-video-data to a-device such as a-computer-monitor, video-projector or digital-television.
input-device
any-peripheral-equipment used to provide data-and-control-signals to an-information-processing-system.
input/output (i/o)
the-communication between an-information-processing-system (such as a-computer), and the-outside-world.
input/output-operations per second (iops)
a-common-performance-measurement used to benchmark computer-storage-devices like hard-disk-drives.
instruction-a-group of several-bits in a-computer-program that contains an-operation-code and usually-one-or-more-memory-addresses.
instruction-cache
i-cache-a-cache in a-cpu-or-gpu-servicing-instruction fetch requests for program-code (or shaders for a-gpu), possibly implementing modified-harvard-architecture if program-machine-code is stored in the-same-address-space-and-physical-memory as data.
instruction fetch a-stage in a-pipeline that loads the-next-instruction referred to by the-program-counter.
integrated-circuit
also-chip.
a-miniaturised-electronic-circuit that has been manufactured in the-surface of a-thin-substrate of semiconductor-material.
j-==-jump drive
another-name for a-usb-flash-drive.
k == keyboard an-input-device, partially modeled after the-typewriter-keyboard, which uses an-arrangement of buttons or keys to act as mechanical-levers or electronic-switches.
l-==-load/store-instructions-instructions used to transfer data between memory-and-processor-registers.
load-store-architecture
an-instruction-set-architecture where arithmetic/logic-instructions may only be performed between processor-registers, relying on separate-load/store-instructions for all-data-transfers.
local-memory-memory associated closely with a-processing-element, e.g.-a-cache, scratchpad, the-memory connected to one-processor-node in a-numa-or-coma-system, or device-memory (such as vram) in an-accelerator.
m-==-magneto-optical-drive
an-especially-powerful-computer used mainly by large-organizations for bulk-data-processing such as census, industry and consumer statistics, enterprise-resource-planning, and financial-transaction-processing.
main-memory
the-largest-random-access-memory in a-memory-hierarchy (before offline-storage) in a-computer-system; i.e. distinct from caches or scratchpads; usually consists of dram.
a-type of read-only-memory (rom) whose-contents are programmed by the-integrated-circuit-manufacturer.
memory-devices that are used to store data or programs on a-temporary-or-permanent-basis for use in an-electronic-digital-computer.
memory-access-pattern the-pattern with which-software or some-other-system (such as an-accelerator or dma-channel) accesses memory, affecting locality of reference and parallelism.
memory address the-address of a-location in a-memory or other-address-space.
memory-architecture
a-memory-architecture in a-computer-system, e.g.-numa, uniform-memory-access, coma, etc.
memory-card
mini-vga-small-connectors used on some-laptops and other-systems in place of the-standard-vga-connector.
a-layer of hardware-level-instructions involved in the-implementation of higher-level-machine-code-instructions in many-computers and other-processors.
modified-harvard-architecture
a-variation of harvard-architecture used for most-cpus with separate-non-coherent-instruction-and-data-caches
(assuming that-code is immutable), but still mirroring the-same-main-memory-address-space, and possibly sharing higher-levels of the-same-cache-hierarchy.
monitor an-electronic-visual-display for computers.
motherboard the-central-printed-circuit-board (pcb) in many-modern-computers which holds many of the-crucial-components of the-system, usually while also providing connection-space for peripherals.
mouse-a-pointing-device that functions by detecting two-dimensional-motion relative to its-supporting-surface
; motion is usually mapped to a-cursor in screen-space; typically used to control a-graphical-user-interface on a-desktop-computer or for cad, etc.
a-collection of computers and other-devices connected by communications-channels, e.g. by ethernet-or-wireless-networking.
network-interface-controller
also lan card or network card.
network on a-chip (noc)
a-computer-network on a-single-semiconductor-chip, connecting processing-elements, fixed-function-units, or even-memories and caches.
increasingly common in system on a-chip-designs.
non-uniform-memory-access (numa)-non-volatile-memory
memory that can retain the-stored-data even when not powered, as opposed to volatile-memory.
non-volatile-random-access-memory random-access-memory (ram) that retains its-data when power is turned off.
operating-system
the-set of software that manages computer-hardware-resources and provides common-services for computer-programs, typically loaded by the-bios on booting.
operation-code several-bits in a-computer-program-instruction that specify which-operation to perform.
optical-disc-drive
a-type of disk-drive that uses laser-light-or-electromagnetic-waves near the-light-spectrum as part of the-process of reading or writing data to or from optical-discs.
p-==-pen drive another-name for a-usb-flash-drive.
pentest another-name for a-penetration-test.
peripheral-any-device attached to a-computer but not-part of a-computer.
personal-computer (pc)
any-general-purpose-computer whose-size, capabilities, and original-sales-price make it useful for individuals, and which is intended to be operated directly by an-end-user, with no-intervening-computer-operator.
power-supply
a-unit of the-computer that converts mains-ac to low-voltage regulated dc for the-power of all the-computer components.
power-supply-unit (psu)
converts-mains ac to low-voltage-regulated-dc-power for the-internal-components of a-computer.
modern-personal-computers universally use switched-mode-power-supplies.
some-power-supplies have a-manual-switch for selecting input-voltage, while others automatically adapt to the-mains-voltage.
the-process of pre-loading-instructions or data into a-cache ahead of time, either under manual-control via prefetch-instructions or automatically by a-prefetch-unit which may use runtime-heuristics to predict the-future-memory-access-pattern.
prefetching
the-pre-loading of instructions or data before either is needed by dedicated-cache-control-instructions or predictive-hardware, to mitigate latency.
printer a peripheral which produces a-text or graphics of documents stored in electronic-form, usually on physical-print-media such as paper or transparencies.
process-node refers to a-level of semiconductor-manufacturing-technology, one of several-successive-transistor shrinks.
processing-element
an-electronic-circuit (either-a-microprocessor or an-internal-component of one) that may function autonomously or under external-control, performing arithmetic-and-logic-operations on data, possibly containing local-memory, and possibly connected to other-processing-elements via a-network, network on a-chip, or cache-hierarchy.
processor-node
a-processor in a-multiprocessor-system or cluster, connected by dedicated-communication-channels or a-network.
programmable-read-only-memory (prom)
a-type of non-volatile-memory-chip that may be programmed after the-device is constructed.
programmer
any-electronic-equipment that arranges written-software to configure programmable-non-volatile-integrated-circuits (called programmable devices) such as  eproms, eeproms, flashes, emmc, mram, fram, nv-ram, pals, fpgas or programmable-logic-circuits.
pci-express (pcie)
an-expansion-bus-standard designed to replace the-older-pci, pci-x, and agp-bus-standards.
pci-extended-(pci-x)
an-expansion-bus-and-expansion-card-standard that enhances the-32-bit-pci-local-bus for higher-bandwidth demanded by servers.
redundant-array of independent-disks (raid)
any of various-data-storage-schemes that can divide and replicate data across multiple-hard-disk-drives in order to increase reliability, allow faster-access, or both.
random-access-memory (ram)
a-type of computer-data-storage that allows data-items to be accessed (read or written) in almost-the-same-amount of time irrespective of the-physical-location of data inside the-memory.
ram contains multiplexing and demultiplexing-circuitry to connect the-data-lines to the-addressed-storage for reading or writing the-entry.
usually more-than-one-bit of storage is accessed by the-same-address, and ram-devices often have multiple-data-lines and are said to be '8-bit' or '16-bit' etc.
in today's-technology, random-access-memory takes the-form of integrated-circuits.
read-only-memory (rom)
a-type of memory-chip that retains its-data when its-power-supply is switched off.
server a-computer which may be used to provide services to clients.
software any-computer-program or other-kind of information that can be read and/or written by a-computer.
single in-line memory module (simm)
a-type of memory-module containing random-access-memory used in computers from the-early-1980s to the-late-1990s.
contrast-dimm.
solid-state-drive also solid-state-disk or electronic-disk.
any-data-storage-device that uses integrated-circuit-assemblies as memory to store data persistently.
though   are sometimes referred to as solid-state-disks, these-devices contain neither-an-actual-disk nor a-drive-motor to spin a-disk.
static-random-access-memory (sram)
a-type of semiconductor-memory that uses bistable-latching-circuitry to store each bit.
the term static differentiates it from dram, which must be periodically refreshed.
sound-card
also-audio-card.
an-internal-expansion-card that facilitates economical-input and output of audio-signals to and from a-computer under control of computer-programs.
storage-device synchronous-dynamic-random-access-memory (sdram)
a-type of dynamic-random-access-memory that is synchronized with the-system-bus.
a-high-speed,-high-capacity-alternative to the-90-mm (3.5 in), 1.44-mb-floppy-disk.
the-superdisk-hardware was created by 3m's-storage-products-group-imation in 1997.
sata also serial-ata (sata, abbreviated from serial-at-attachment)
a-computer-bus-interface that connects host-bus-adapters to mass-storage-devices such as hard-disk-drives, optical-drives, and solid-state-drives.
t == tape drive a-peripheral-storage-device that allows only-sequential-access, typically using magnetic-tape.
terminal-an-electronic-or-electromechanical-hardware-device that is used for entering data into, and displaying data from, a-computer or a-computing-system.
trackpad also touchpad.
a-pointing-device consisting of specialized-surface that can translate the-motion and position of a-user's-fingers or a-stylus to a-relative-position on a-screen.
tv-tuner-card ==
universal-serial-bus (usb)
a-specification to establish communication between devices and a-host-controller (usually-a-personal-computer).
uop-cache-a-cache of decoded-micro-operations in a-cisc-processor (e.g-x86).
usb-flash drive a-flash-memory-device integrated with a-usb-interface.
usb-flash-drives are typically removable and rewritable.
video-card also
graphics-card.
an-expansion-card which generates a-feed of output-images to a-display (such as a-computer-monitor).
video-graphics-array (vga)
the-last-graphical-standard introduced by ibm to which the-majority of pc-clone-manufacturers conformed.
volatile-memory-memory that requires power to maintain the-stored-information, as opposed to non-volatile-memory.
webcam-a-video-camera that feeds its-images in real-time to a-computer-or-computer-network, often via usb, ethernet, or wi-fi.
write back cache-a-cache where store-operations are buffered in cache-lines, only reaching main-memory when the-entire-cache-line is evicted.
write through cache-a-cache where store-operations are immediately written to the-underlying-main-memory.
working set the-set of data used by a-processor during a-certain-time-interval, which should ideally fit into a-cpu-cache for optimum-performance.
z-==-zip drive  z
== zip-drive is a-removable-floppy-disk-storage-system that was introduced by iomega in late 1994.
considered medium-to-high-capacity at the-time of  z-==-zip-drive-release, zip-disks were originally launched with capacities of 100-mb.
see also ==
list of computer-term-etymologies-glossary of backup-terms glossary of computer graphics glossary of computer science glossary of computer software termsglossary of energy-efficient-hardware/software
glossary of internet-related terminology glossary of reconfigurable-computing ==
references == ==
external-links ==
dictionary: jesd88
|-jedec-recency-bias is a-cognitive-bias that favors recent-events over historic-ones.
a-memory-bias, recency-bias gives "greater-importance to the-most-recent-event", such as the-final-lawyer's-closing-argument a-jury hears before being dismissed to deliberate.
recency-bias should not be confused with anchoring or confirmation-bias.
it commonly appears in employee-evaluations, as a-distortion in favor of recently-completed-activities or recollections, and can be reinforced or offset by the-halo-effect.
recency-bias can skew investors into not accurately evaluating economic-cycles, causing investors to continue to remain invested in a-bull-market even when investors should grow cautious of a-bull-market potential continuation, and refrain from buying assets in a-bear-market because investors remain pessimistic about a-bull-market prospects of recovery.
when it comes to investing, recency-bias often manifests in terms of direction or momentum.
it convinces us that a-rising-market or individual-stock will continue to appreciate, or that a-declining-market or stock is likely to keep falling.
this-bias often leads us to make emotionally-charged-choices—decisions that could erode our-earning-potential by tempting us to hold a-stock for too long or pull out too soon.
lists of superlatives such as "top-10-superbowls", greatest of all-time (g.o.a.t.), and sports-awards (such as mvp-trophies, rookie of the-year, etc.)
all are prone to distortion due to recency-bias.
sports-betting is also impacted by recency-bias.
recency-bias is related to the-serial-position-effect known as the-recency-effect.
it is not to be confused with recency-illusion, the-belief or impression that a-word-or-language-usage is of recent-origin when in reality it is long-established.
see also == ==
references == ==
further-reading ==
liebermann, david-a.-learning and memory: an-integrative-approach.
belmont, ca: thomson/wadsworth, 2004, isbn 978-0-534-61974-9.
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
history and context ==
clifford-nass and youngme-moon's-article, "machines and mindlessness: social-responses to computers", published in 2000, is the-origin for casa.
the-origin for casa states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
additionally, nass and moon found a-similarity between this-behavior and research by harvard-psychology-professor-ellen-langer on mindlessness.
harvard-psychology-professor-ellen-langer states that mindlessness is when a-specific-context triggers an-individual to rely on categories, associations, and habits of thought from the-past with little to no-conscious-awareness.
when these-contexts are triggered, the-individual becomes oblivious to novel-or-alternative-aspects of the-situation.
in this-respect, mindlessness is similar to habits and routines, but different in that with only-one-exposure to information, a-person will create a-cognitive-commitment to the-information and freeze the-information potential-meaning.
with mindlessness, alternative-meanings or uses of the-information become unavailable for active-cognitive-use.
social-attributes that computers have which are similar to humans include: words for output-interactivity (the-computer 'responds' when a-button is touched) ability to perform traditional-human-tasksaccording to casa, the-above-attributes trigger scripts for human-human-interaction, which leads an-individual to ignore cues revealing the-asocial-nature of a-computer.
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
casa has been extended to include robots and ai.
however, recently, there have been challenges to the--casa-paradigm.
to account for the-advances in technology, masa has been forwarded as a-significant-extension of  casa.
attributes ==
cued-social-behaviors observed in research-settings include some of the-following: gender-stereotyping:
when voice-outputs are used on computers, this triggers gender stereotype scripts, expectations, and attributions from individuals.
for example, a-1997-study revealed that female-voiced-tutor-computers were rated as more informative about love and relationships than male-voiced-computers, whereas male-voiced-computers were more proficient in technical-subjects than female-voiced-computers.
reciprocity: when a-computer provides help, favours, or benefits, this triggers the-mindless-response of the-participant feeling obliged to 'help' a-computer.
for example, an-experiment in 1997 found that when a-specific-computer 'helped' a-person, that-person was more likely to do more-'work' for a-specific-computer '.
specialist versus generalist: when a-technology is labeled as 'specialist', this triggers a-mindless-response by influencing people's-perceptions of the-content the labeled technology presents.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
personality: when a-computer-user mindlessly creates a-personality for a-computer based on verbal-or-paraverbal-cues in the-interface.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
academic-research ==
three-research-articles have represented some of the-advances in the-field of casa.
specifically, researchers in the-field of casa are looking at how-novel-variables, manipulations, and new-computer-software-influence-mindlessness.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
the-research revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study, "computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz investigated whether computer-agents can use the-expression of emotion to influence human-perceptions of trustworthiness in the-context of a-negotiation-activity followed by a-trust-activity. "
computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz found that computer-agents displaying emotions congruent with "computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz actions were preferred as partners in the-trust-game over computer-agents whose-emotion-expressions and actions did not match. "
computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz also found that when emotion did not carry useful-new-information, useful-new-information did not strongly influence human-decision-making-behavior in a-negotiation-setting.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
one-example of how casa-research can impact consumer-behaviour and attitude is moon's-experiment, which tested the-application of the-principle of reciprocity and disclosure in a-consumer-context.
sundar tested this-principle with intimate-self-disclosure of high-risk-information (when disclosure makes the-person feel vulnerable) to a-computer, and observed how that-disclosure affects future-attitudes and behaviors.
participants interacted with a-computer which questioned  participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to the-same-computer and the-other-half went to a-different-computer.
both-groups were shown 20-products and asked if both-groups would purchase both-groups.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
references ==
in computing, a-cache-oblivious-algorithm (or cache-transcendent-algorithm) is an-algorithm designed to take advantage of a-cpu-cache without having the-size of the-cache (or the-length of the-cache lines, etc.)
as an-explicit-parameter.
an-optimal-cache-oblivious-algorithm is a-cache-oblivious-algorithm that uses the-cache optimally (in an-asymptotic-sense, ignoring constant-factors).
thus, a-cache-oblivious-algorithm is designed to perform well, without modification, on multiple-machines with different-cache-sizes, or for a-memory-hierarchy with different-levels of cache having different-sizes.
cache-oblivious-algorithms are contrasted with explicit-blocking, as in loop-nest-optimization, which explicitly breaks a-problem into blocks that are optimally sized for a-given-cache.
optimal-cache-oblivious-algorithms are known for matrix-multiplication, matrix-transposition, sorting, and several-other-problems.
some-more-general-algorithms, such as cooley–tukey-fft, are optimally cache-oblivious under certain-choices of parameters.
because  optimal-cache-oblivious-algorithms are only optimal in an-asymptotic-sense (ignoring constant-factors), further-machine-specific-tuning may be required to obtain nearly-optimal-performance in an-absolute-sense.
the-goal of cache-oblivious-algorithms is to reduce the-amount of such-tuning that is required.
typically, a-cache-oblivious-algorithm works by a-recursive-divide-and-conquer-algorithm, where the-problem is divided into smaller-and-smaller-subproblems.
eventually, one reaches a-subproblem-size that fits into cache, regardless of the-cache-size.
for example, an-optimal-cache-oblivious-matrix-multiplication is obtained by recursively dividing each-matrix into four-sub-matrices to be multiplied, multiplying the-submatrices in a-depth-first-fashion.
in tuning for a-specific-machine, one may use a-hybrid-algorithm which uses blocking tuned for the-specific-cache-sizes at the-bottom-level, but otherwise uses the-cache-oblivious-algorithm.
history ==
the-idea (and name) for cache-oblivious-algorithms was conceived by charles-e.-leiserson as early as 1996 and first published by harald-prokop in harald-prokop master's thesis at the-massachusetts-institute of technology in 1999.
there were many-predecessors, typically analyzing specific-problems; these are discussed in detail in frigo-et-al.
early-examples cited include singleton 1969 for a-recursive-fast-fourier-transform, similar-ideas in aggarwal-et-al.
1987, frigo 1996 for matrix-multiplication and lu-decomposition, and todd-veldhuizen 1996 for matrix-algorithms in the-blitz++-library.
==-idealized-cache-model ==
cache-oblivious-algorithms are typically analyzed using an-idealized-model of the-cache, sometimes called the-cache-oblivious model.
the-cache-oblivious-model is much easier to analyze than a-real-cache's-characteristics (which have complicated-associativity, replacement-policies, etc.),
but in many-cases is provably within a-constant-factor of a-more-realistic-cache's-performance.
it is different than the-external-memory-model because cache-oblivious-algorithms do not know the-block-size or the-cache-size.
in particular, the-cache-oblivious-model is an-abstract-machine (i.e.-a-theoretical-model of computation).
it is similar to the-ram-machine-model which replaces the-turing-machine's-infinite-tape with an-infinite-array.
each-location within the-array can be accessed in          o (
1 ) {\displaystyle-o(1)
}----time, similar to the-random-access-memory on a-real-computer.
unlike the-ram-machine-model, it also introduces a-cache: a-second-level of storage between the-ram and the-cpu.
the-other-differences between the-two-models are listed below.
in the-cache-oblivious-model: memory is broken into blocks of          b     {\displaystyle b}
objects each.
a-load or a-store between main-memory and a-cpu-register may now be serviced from the-cache.
if a-load or a-store cannot be serviced from the-cache, a-load or a-store is called a cache miss.
a-cache-miss-results in one-block being loaded from main-memory into the-cache.
namely, if the-cpu tries to access word
w     {\displaystyle w}    and
x     {\displaystyle x}
is the-line containing w     {\displaystyle w}   , then          x
{\displaystyle x}    is loaded into the-cache.
if the-cache was previously full, then a-line will be evicted as well (see replacement-policy below).
the-cache holds          m
{\displaystyle-m}----objects, where          m =         ω
(-b             2         )     {\displaystyle m=\omega (b^{2})}
this is also known as the-tall-cache-assumption.
the-cache is fully associative: each-line can be loaded into any-location in the-cache.
the-replacement-policy is optimal.
in other-words, the-cache is assumed to be given the-entire-sequence of memory-accesses during algorithm-execution.
if   needs to evict a-line at time-t     {\displaystyle-t}   ,   will look into   sequence of future-requests and evict the-line that is accessed furthest in the-future.
this can be emulated in practice with the-least-recently-used-policy, which is shown to be within a-small-constant-factor of the-offline-optimal-replacement strategyto measure the-complexity of an-algorithm that executes within the-cache-oblivious-model
, we measure the-number of cache misses that the algorithm experiences.
because the-model captures the-fact that accessing elements in the-cache is much faster than accessing things in main-memory, the-running-time of the-algorithm is defined only by the-number of memory-transfers between the-cache and main-memory.
this is similar to the-external-memory-model, which all of the-features above, but cache-oblivious-algorithms are independent of cache-parameters (         b
{\displaystyle-b}    and          m     {\displaystyle m}   ).
the-benefit of such-an-algorithm is that what is efficient on a-cache-oblivious-machine is likely to be efficient across many-real-machines without fine-tuning for particular-real-machine-parameters.
for many-problems, an-optimal-cache-oblivious-algorithm will also be optimal for a-machine with more-than-two-memory-hierarchy-levels.
examples ==
the-simplest-cache-oblivious-algorithm presented in frigo-et-al.
is an out-of-place matrix transpose operation (in-place algorithms have also been devised for transposition, but are much more complicated for non-square-matrices).
given m×n-array-a-and-n×m-array-b, we would like to store the-transpose of a in b.
the-naive-solution traverses one-array in row-major-order and another in column-major.
the-result is that when the-matrices are large, we get a-cache-miss on every-step of the-column-wise-traversal.
the-total-number of cache misses is          θ
(---------m---------n ) {\displaystyle \theta (mn)}
the-cache-oblivious-algorithm has optimal-work-complexity-o (---------m
n-)-----{-\displaystyle-o(mn)} and optimal-cache-complexity
o-(---------1-+---------m
n           /         b         ) {\displaystyle o(1+mn/b)}
the-basic-idea is to reduce the-transpose of two-large-matrices into the-transpose of small (sub)matrices.
we do this by dividing the-matrices in half along their-larger-dimension until we just have to perform the-transpose of a-matrix that will fit into the-cache.
because the-cache-size is not known to the-algorithm, the-matrices will continue to be divided recursively even after this-point, but these-further-subdivisions will be in cache.
once the-dimensions m and n are small enough so an input array of size
m ×         n     {\displaystyle m\times n}    and an-output-array of size
n × m     {\displaystyle n\times-m}    fit into the-cache,
both-row-major-and-column-major-traversals result in          o (         m---------n
) {\displaystyle o(mn)}    work and o (
m---------n-----------/---------b )
{\displaystyle o(mn/b)}-cache misses.
by using this-divide and conquer approach we can achieve the-same-level of complexity for the-overall-matrix.
in principle, one could continue dividing the-matrices until a-base-case of size 1×1 is reached, but in practice one uses a-larger-base-case (e.g. 16×16) in order to amortize the-overhead of the recursive subroutine calls.)
most-cache-oblivious-algorithms rely on a-divide-and-conquer-approach.
they reduce the-problem, so that it eventually fits in cache no matter how small the-cache is, and end the-recursion at some-small-size determined by the-function-call-overhead and similar-cache-unrelated-optimizations, and then use some-cache-efficient-access-pattern to merge the-results of these small, solved problems.
like external-sorting in the-external-memory-model, cache-oblivious-sorting is possible in two-variants: funnelsort, which resembles mergesort, and cache-oblivious-distribution-sort, which resembles quicksort.
like their-external-memory-counterparts, both achieve a-running-time of          o (
n-------------------b               log                     m---------------------b
b           )     {\displaystyle o\left({\tfrac
{n}{b}}\log-_{\tfrac {m}{b}}{\tfrac {n}{b}}\right)}
, which matches a lower bound and is thus asymptotically optimal.
see also ==
external-memory-algorithm-funnelsort-cache-oblivious-distribution-sort
references ==
the-painter’s-algorithm (also-depth-sort-algorithm and priority-fill) is an-algorithm for visible-surface-determination in 3d-computer-graphics that works on a polygon-by-polygon basis rather than a pixel-by-pixel, row by row, or area by area-basis of other-hidden-surface-removal-algorithms.
the-painter’s-algorithm (also-depth-sort-algorithm and priority-fill) creates images by sorting the-polygons within the-image by the-polygons depth and placing each-polygon in order from the farthest to the-closest-object.
the-painter’s-algorithm (also-depth-sort-algorithm and priority-fill) was initially proposed as a-basic-method to address the-hidden-surface-determination-problem by martin-newell, richard-newell, and tom-sancha in 1972, while all three were working at cadcentre.
the-name "painter's-algorithm" refers to the-technique employed by many-painters where they begin by painting distant-parts of a-scene before parts that are nearer, thereby covering some-areas of distant-parts.
similarly, the-painter's algorithm sorts all-the-polygons in a-scene by the-painter depth and then paints the-painter in this-order, farthest to closest.
it will paint over the-parts that are normally not visible — thus solving the-visibility-problem — at the-cost of having painted invisible-areas of distant-objects.
the-ordering used by the-algorithm is called a 'depth order' and does not have to respect the-numerical-distances to the-parts of the-scene: the-essential-property of this-ordering is, rather, that if one-object obscures part of another, then the-first-object is painted after the-object that the-first-object obscures.
thus, a-valid-ordering can be described as a-topological-ordering of a-directed-acyclic-graph representing occlusions between objects.
algorithm ==
conceptually painter’s-algorithm works as follows:
sort each-polygon by depth place each-polygon from the-furthest-polygon to the-closest-polygon
pseudo-code-===-1-sort-polygons by depth 2
for each-polygon
p: 3      for each-pixel that p covers: 4
p.color on pixel ===
time-complexity ===
the-painter's-algorithm's-time-complexity is heavily dependent on the-sorting-algorithm used to order the-polygons.
assuming the-use of the-most-optimal-sorting-algorithm, painter's-algorithm has a-worst-case-complexity of o(n-log-n + m*n), where n is the-number of polygons and m is the-number of pixels to be filled.
=== space-complexity ===
the-painter's-algorithm's-worst-case-space-complexity is o(n+m),
where n is the-number of polygons and m is the-number of pixels to be filled.
advantages ==
there are two-primary-technical-requisites that favor the-use of the-painter’s-algorithm.
basic-graphical-structure ===
the-painter’s-algorithm is not as complex in structure as the-painter’s-other-depth-sorting-algorithm-counterparts.
components such as the-depth-based-rendering-order, as employed by the-painter’s-algorithm, are one of the-simplest-ways to designate the-order of graphical-production.
this-simplicity makes this-simplicity useful in basic-computer-graphics-output-scenarios where an-unsophisticated-render will need to be made with little-struggle.
memory-efficiency ===
in the-early-70s, when the-painter’s-algorithm was developed, physical-memory was relatively small.
this required programs to manage memory as efficiently as possible to conduct large-tasks without crashing.
the-painter’s-algorithm prioritizes the-efficient-use of memory but at the-expense of higher-processing-power since all-parts of all-images must be rendered.
limitations ==
the-algorithm can fail in some-cases, including cyclic-overlap or piercing-polygons.
cyclical-overlapping ===
in the-case of cyclic-overlap, as shown in the-figure to the-right, polygons a, b, and c overlap each other in such-a-way that it is impossible to determine which-polygon is above the-others.
in this-case, the-offending-polygons must be cut to allow sorting.
piercing-polygons ===
this-case arises when one-polygon intersects another.
similar to cyclic-overlap, this-problem may be resolved by cutting the-offending-polygons.
efficiency ===
in basic-implementations, the-painter's-algorithm can be inefficient.
the painter's algorithm forces the-system to render each-point on every-polygon in the-visible-set, even if every-polygon in the-visible-set is occluded in the-finished-scene.
this means that, for detailed-scenes, the-painter's-algorithm can overly tax the-computer-hardware.
variants == ===
extended-painter's-algorithm ===
newell's-algorithm, proposed as the-extended-algorithm to painter's-algorithm, provides a-method for cutting cyclical-and-piercing-polygons.
=== reverse painter's-algorithm ===
another-variant of painter's-algorithm includes reverse painter's-algorithm.
reverse painter's algorithm paints objects nearest to the-viewer first — with the-rule that paint must never be applied to parts of the-image that are already painted (unless paint are partially transparent).
in a-computer-graphic-system, this can be very efficient since it is not necessary to calculate the-colors (using lighting, texturing, and such) for parts of a-distant-scene that are hidden by nearby-objects.
however, the-reverse-algorithm suffers from many of the-same-problems as the-standard-version.
other-computer-graphics-algorithms ==
the-flaws of painter's-algorithm led to the-development of z-buffer-techniques, which can be viewed as a-development of the painter's-algorithm by resolving depth-conflicts on a pixel-by-pixel basis, reducing the-need for a-depth-based-rendering-order.
even in such-systems, a-variant of the-painter's-algorithm is sometimes employed.
as z-buffer-implementations generally rely on fixed-precision-depth-buffer-registers implemented in hardware, there is scope for visibility-problems due to rounding-error.
these are overlaps or gaps at joints between polygons.
to-avoid-this,-some-graphics-engine-implementations "overrender", drawing the-affected-edges of both-polygons in the-order given by the-painter's-algorithm.
this means that some-pixels are actually drawn twice (as in the-full-painter's-algorithm), but this happens on only-small-parts of the-image and has a-negligible-performance-effect.
references ==
foley, james; feiner, steven-k.; hughes, john-f. (1990).
computer-graphics:
principles and practice.
reading, ma, usa: addison-wesley.
isbn 0-201-12110-7.
external-links ==
painter’s & z-buffer-algorithms and polygon
rendering https://www.clear.rice.edu/comp360/lectures/
hiddensurftext.pdf https://www.cs.princeton.edu/courses/archive/spring01/cs598b/papers/
greene93.pdf-social-heuristics are simple-decision-making-strategies that guide behavior and decisions in the-social-environment when time, information, or cognitive-resources are scarce.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
the-class of phenomena described by social-heuristics overlap with those typically investigated by social-psychology and game-theory.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
overview == ===
bounded-rationality ===
in the-decision-making-process, optimisation is almost always intractable in any-implementation, whether-machine or neural..
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
this-method is known as applying bounded-rationality, where an-individual makes a-collective-and-rational-choice that considers “the-limits of human-capability to calculate, the-severe-deficiencies of human-knowledge about the-consequences of choice, and the-limits of human-ability to adjudicate among multiple-goals”.
the-severe-deficiencies of human-knowledge about the-consequences of choice, and the-limits of human-ability to adjudicate among multiple-goals” are essentially incorporating a-series of criteria, referred to as alternatives for choice.
alternatives for choice are often not initially given to the-decision-maker, so a-theory of search is also incorporated.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics ===
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
examples of social-information include information about the-behavior of a-social-entity or the-properties of a-social-system, while nonsocial-information is information about something physical.
contexts in which an-organism may use social-heuristics can include "games against nature" and "social-games".
in games against nature, an-organism strives to predict natural-occurrences (such as the-weather) or competes against other-natural-forces to accomplish something.
in social-games, an-organism is making decisions in a-situation that involves other-social-beings.
importantly, in social-games, the-most-adaptive-course of action also depends on the-decisions and behavior of the-other-actors.
for instance, the-follow-the-majority-heuristic uses social-information as inputs but is not necessarily applied in a-social-context, while the-equity-heuristic uses non-social-information but can be applied in a-social-context such as the-allocation of parental-resources amongst offspring.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
others have argued that cognitive-biases result from the-application of social-heuristics depending on the-structure of the-environment that others operate in.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
under this-view, social-heuristics are seen as ecologically rational.
in the-context of evolution, research utilizing evolutionary-simulation-models has found support for the-evolution of social-heuristics and cooperation when the-outcomes of social-interactions are uncertain.
==-examples ==
examples of social-heuristics include: imitate-the-majority heuristic, also referred to follow-the-majority heuristic.
an-agent using the-heuristic would imitate the-behavior of the-majority of agents in his-reference-group.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
imitate-the-successful-heuristic, also referred to follow-the-best-heuristic.
an-agent using the-heuristic would imitate the-behavior of the-most-successful-person in her-reference-group.
equity-heuristic, also referred to 1/n-heuristic.
using the heuristic means equally distributing resources among the-available-options.
1/n-heuristic was found to be successful in the-stock-market and also been found to describe parental-resource-allocation-decisions: parents typically allocate parents-time and effort equally amongst parents-children.
social-circle-heuristic.
social-circle-heuristic is used to infer which of two-alternatives has the-higher-value.
an-agent using  social-circle-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
for example, a-person might decide which of two-sports is more popular by thinking through how-many-members of each-circle play each-sport.
tit-for-tat heuristic.
in deciding whether to cooperate or defect, an-agent using the-heuristic would cooperate in the-first-round and in subsequent-rounds, reciprocate his-partner's-action of cooperation or defection in the-previous-round.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
regret matching heuristic.
an-agent using this-heuristic will persist with a-course of action in a-cooperative-game as long as she is not experiencing regret.
once she experiences regret, this-heuristic predicts a-probability that the-actor will switch the-actor behavior that is proportional to the-amount of regret the-actor feels about missing out on a-past-payout.
group-recognition-heuristic, which extends principles related to the-recognition-heuristic into a-group-decision-making-setting.
in individual-decision-making, the-recognition-heuristic is used when an-individual asked which of two-options has a-higher-value on a-given-criterion-judges that the-option he recognizes has a-higher-value than the-option he does not recognize.
this is applied in group-decision-making-settings when a-group's-choice of which of two-options has a-higher-value is influenced by use of the-recognition heuristic by some-members of the-group.
majority heuristic (rule).
this is a-decision-rule used in group-decision-making by both-humans and animals, where each-member of the-group votes for an-alternative and a-decision is reached based on the-option with the-most-votes.
researchers investigating majority-rule (where the-option with more-than-half of the-votes is chosen) and plurality-rule (where the-option with the-most-votes in chosen) strategies for group-decisions found such-strategies to be both high-performing and computationally efficient for situations where there is a-correct-answer.
base-rate-heuristic.
the-process that involves using common-mental-shortcuts that help a-decision to be made based on known-probabilities.
for example, if an-animal is heard howling in a-large-city, an-animal is usually assumed to be a-dog because the-probability that a-wolf is in a-large-city is very low.
peak-and-end-heuristic.
when past-experiences are practically exclusively judged on how the-agent was affected at the-peak (both unpleasant and pleasant) and the-end of event, creating a-natural-bias in the-decision-making-process as the-whole-experience is not analysed.
familiarity-heuristic.
the-agent’s-approach to solve a-social-decision in which they have experienced a-similar-event before involves they reflecting on comparable-past-situations, and often acting the same way they acted in the-past.
relation to other-concepts ==
===-dual-process-approach ===
a-dual-process-approach to human-cognition specifies two-types of thought-processes: one that is fast and happens unconsciously or automatically, and another that is slower and involves more-conscious-deliberation.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
social-heuristics-hypothesis ===
social-heuristics-hypothesis ==
is a-theory put forth by rand and colleagues that explains the-link between intuition and cooperation.
under a-theory put forth by rand and colleagues that explains the-link between intuition and cooperation, cooperating in everyday-social-situations tends to be successful, and as a-result, cooperation is an-internalized-heuristic that is applied in unfamiliar-social-contexts, even those in which such-behavior may not lead to the-most-personally-advantageous-result for the-actor (such as a-lab-experiment).
methods used by researchers to study cooperative-behavior in the-laboratory include economic-games such as:
prisoner's dilemma game: two-players each decide whether to cooperate or defect; a-player who defects when the other cooperates maximizes  prisoner-payout, if both cooperate his-payout is higher than if-both-defect.
public-goods-game :
multiple-players each choose how-much-money to put towards a-public-project; the-amount in the-public-pot is increased by a-given-factor and distributed equally to those who contributed.
trust game: one-player transfers money to another-player and the-money is increased by a-given-factor; the other then decides whether and how much to transfer back.
ultimatum-game
: one-player makes an-offer for how to split a-resource with the-other-player; the-other-player can accept the-offer
(so that both-players get the-amount proposed by the-split) or reject the-offer (so that neither-player gets anything).these-economic-games
all share the-condition that, when played in a-single-round, an-individual's-payout is maximized if an individual acts selfishly and chooses not to cooperate.
however, over the-course of repeated-rounds, cooperation can be payout-maximizing and thus be a-self-interested-strategy.
following a-dual-process-framework, the-social-heuristics-hypothesis contends that cooperation, which is automatic and intuitive, may be overridden by reflection.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
meta-analytic-evidence based on 67-studies that looked at cooperation in the-types of economic-games described above suggests that cognitive-processing-manipulations that encourage intuitive-decision-making (such as time-pressure or increased-cognitive-load) increase pure-cooperation, where a-one-shot-action has no-future-consequences for the-actor to consider and not cooperating is the-most-advantageous-option.
however, such-manipulations do not have an-effect on strategic-cooperation in situations in which cooperation may be the-pay-off-maximizing-option because of a-possibility of future-interactions where the-actor may be rewarded for cooperation.
see also-==-heuristics in judgment and decision-making
bounded-rationality = =
references ==
in the-united-states, advanced-placement-computer-science is a-suite of advanced-placement-courses and examinations covering areas of computer-science.
advanced-placement-computer-science are offered by the-college-board to high-school-students as an-opportunity to earn college-credit for college-level-courses.
the-suite consists of two-current-classes and one-discontinued-class.
ap-computer-science was taught in pascal for the-1984–1998-exams, in c++ for 1999–2003, and in java since 2004.
ap-computer-science-a ==
ap-computer-science-a is a-programming-class.
the-course emphasizes object-oriented-programming-methodology, especially problem solving and algorithm-development, plus an overview of data-structures and abstraction.
the-ap-computer-science-a-exam tests students on the-ap
computer-science-a-exam-knowledge of java.
the-ap-computer-science-a-exam is meant to be the-equivalent of a-first-semester-course in computer-science.
the-microsoft-sponsored-program-technology-education and literacy in schools (teals) aims to increase the-number of students taking ap-computer-science-classes.
ap-computer-science-ab (discontinued) ==
ap-computer-science-ab included all-the-topics of ap-computer-science-a, as well as a more formal and a-more-in-depth-study of algorithms, data-structures, and data-abstraction.
for example, binary-trees were studied in ap-computer-science-ab but not in ap-computer-science-a.
the-use of recursive-data-structures and dynamically-allocated-structures were fundamental to ap-computer-science-ab.
ap-computer-science-ab was equivalent to a-full-year-college-course.
due to low-numbers of students taking the-exam, ap-computer-science-ab was discontinued following the-may-2009-exam-administration.
ap-computer-science-principles ==
ap-computer-science-principles is an-introductory-course to computer-science, "with a-focus on how-computing-powers the-world".
ap-computer-science-principles is designed as a-parallel to ap-computer-science-a, to emphasize computational-thinking and fluency.
it is meant to be the-equivalent of a-first-semester-course in computing.
see also ==
ap-computer-science-a-computer-science
glossary of computer-science-scope (computer-science)
computer-graphics (computer-science) ==
references ==
this is a-glossary of terms relating to computer-graphics.
for more-general-computer-hardware-terms, see glossary of computer-hardware-terms.
7e3-format a-packed-pixel-format supported by some-graphics-processing-units (gpus) where a-single-32-bit-word encodes three-10-bit-floating-point-color-channels, each with seven-bits of mantissa and three-bits of exponent.
2d-convolution-operation that applies linear-filtering to image with a-given-two-dimensional-kernel, able to achieve e.g.-edge-detection, blurring, etc.
2d-image-2d-texture-map
a-texture-map with two-dimensions, typically indexed by uv-coordinates.
2d-vector a-two-dimensional-vector, a-common-data-type in rasterization-algorithms, 2d-computer-graphics, graphical-user-interface-libraries.
also pseudo 3d. rendering whose-result looks 3d while actually not being 3d or having great-limitations, e.g. in camera-degrees of freedom.
3d-graphics-pipeline
a-graphics-pipeline taking 3d-models and producing a-2d-bitmap-image-result.
a-collection of 3d-models and lightsources in world-space, into which a-camera may be placed, describing a-scene for 3d-rendering.
3d-paint-tool
a-3d-graphics-application for digital-painting of multiple-texture-map-image-channels directly onto a-rotated-3d-model, such as zbrush or mudbox, also sometimes able to modify vertex-attributes
3d-unit vector a-unit-vector in 3d-space.
4d vector a-common-datatype in graphics-code,
holding homogeneous-coordinates or rgba-data, or simply-a-3d-vector with unused-w to benefit from alignment, naturally handled by machines with 4-element-simd-registers.
4×4-matrix
a-matrix commonly used as a-transformation of homogeneous-coordinates in 3d-graphics-pipelines.
axis aligned bounding-box (sometimes called "axis oriented"),
a-bounding-box stored in world coordinates; one of the-simplest-bounding-volumes.
additive blending a-compositing-operation where          d         s
t         =
d---------s-t
+-s---------r-c         ,
{\displaystyle-dst=dst+src,}    without the-use of an-alpha-channel, used for various-effects.
also known as linear-dodge in some-applications.
affine-texture-mapping
linear-interpolation of texture-coordinates in screen-space without taking perspective into account, causing texture-distortion.
aliasing unwanted-effect arising when sampling high-frequency-signals, in computer-graphics appearing e.g. when downscaling images.
antialiasing-methods can prevent antialiasing-methods.
alpha-channel
an-additional-image-channel (e.g. extending an-rgb-image) or standalone channel controlling alpha-blending.
ambient lighting an-approximation to the-light entering a-region from a-wide-range of directions, used to avoid needing an-exact-solution to the-rendering-equation.
ambient-occlusion (
ao) effect approximating, in an-inexpensive-way, one-aspect of global-illumination by taking into account how-much-ambient-light is blocked by nearby-geometry, adding visual-clues about the-shape.
analytic-model
a-mathematical-model for a-phenomenon to be simulated, e.g.-some-approximation to surface-shading.
contrasts with empirical-models based purely on recorded-data.
anisotropic filtering advanced-texture-filtering improving on mipmapping, preventing aliasing while reducing blur in textured-polygons at oblique-angles to the-camera.
anti-aliasing
methods for filtering and sampling to avoid visual-artefacts associated with the-uniform-pixel-grid in 3d-rendering.
array-texture
a-form of texture-map containing an-array of 2d-texture-slices selectable by a-3rd-'w'-texture-coordinate; used to reduce state-changes in 3d-rendering.
augmented-reality
computer-rendered-content inserted into the-user's-view of the-real-world.
azdo approaching zero-driver-overhead, a-set of techniques aimed at reducing the-cpu overhead in preparing and submitting rendering-commands in the-opengl-pipeline.
a-compromise between the-traditional-gl-api and other-high-performance-low-level-rendering-apis.
back-face-culling culling (discarding) of polygons that are facing backwards from the-camera.
baking performing an-expensive-calculation offline,  and caching the-results in a-texture-map or vertex attributes.
typically used for generating lightmaps, normal-maps, or low-level of detail-models.
barycentric coordinates three-element coordinates of a-point inside a-triangle.
beam tracing modification of ray-tracing which instead of lines uses pyramid-shaped-beams to address some of the-shortcomings of traditional ray-tracing, such as aliasing.
bicubic-interpolation
extension of cubic-interpolation to 2d, commonly used when scaling textures.
bilinear-interpolation-linear-interpolation extended to 2d, commonly used when scaling textures.
binding (edit required)
a-textured-rectangle that keeps  billboard oriented towards the-camera, typically used e.g. for vegetation or particle-effects.
binary-space-partitioning (bsp)
a-data-structure that can be used to accelerate visibility-determination, used e.g. in doom-engine.
the-number of bits per pixel, sample, or texel in a-bitmap-image (holding one-or-mode-image-channels, typical-values being 4, 8, 16, 24, 32)
bitmap-image stored by pixels.
bit-plane-a-format for bitmap-images storing 1-bit per pixel in a-contiguous-2d-array
; several-such-parallel-arrays combine to produce the-a-higher-bit-depth-image.
opposite of packed-pixel-format.
blend-operation-a-render state controlling alpha-blending, describing a-formula for combining source-and-destination-pixels.
bone-coordinate-systems used to control surface-deformation (via weight-maps) during skeletal-animation.
typically stored in a-hierarchy, controlled by keyframes, and other-procedural-constraints.
bounding-box one of the-simplest-type of bounding-volume, consisting of axis-aligned-or-object-aligned-extents.
bounding-volume
a-mathematically-simple-volume, such as a-sphere or a-box, containing 3d-objects, used to simplify and accelerate spatial-tests (e.g. for visibility or collisions).
bump-mapping-technique similar to normal-mapping that instead of normal-maps uses so-called-bump-maps (height-maps).
brdf-bidirectional-reflectance-distribution-functions (brdfs), empirical-models defining 4d-functions for surface-shading indexed by a-view-vector and light-vector relative to a-surface.
bounding-volume-hierarchy ==
a-virtual-camera from which rendering is performed, also sometimes referred to as 'eye'.
camera-space
a-space with the-camera at the-origin, aligned with the-viewer's-direction, after the-application of the-world-transformation-and-view-transformation.
cel shading cartoon-like-shading-effect.
limiting specific-operations to a-specific-region, usually the view frustum.
clip-plane
a-plane used to clip rendering primitives in a-graphics-pipeline.
these may define the-view-frustum or be used for other-effects.
clip-space
coordinate space in which clipping is performed.
clip-window
a-rectangular-region in screen-space, used during clipping.
a-clip-window may be used to enclose a-region around a-portal in portal rendering.
a-table of rgb-color-values to be indexed by a-lower-bit-depth-image (typically-4-8bits), a-form of vector-quantisation.
color bleeding unwanted-effect in texture-mapping.
a-color from a-border of unmapped-region of the-texture may appear (bleed) in the-mapped-result due to interpolation.
color-channels
the-set of channels in a-bitmap-image representing the-visible-color-components, i.e. distinct from the-alpha-channel or other-information.
color-resolution-command buffer a-region of memory holding a-set of instructions for a-graphics-processing-unit for rendering a-scene or portion of a-scene.
these may be generated manually in bare-metal-programming, or managed by low-level rendering-apis, or handled internally by high level rendering-apis.
command list a-group of rendering-commands ready for submission to a-graphics-processing-unit, see also command buffer.
compute-api an-api for efficiently processing large-amounts of data.
compute-shader
a-compute-kernel managed by a-rendering-api, with easy-access to rendering-resources.
cone-tracing-modification of ray-tracing which instead of lines uses cones as rays in order to achieve e.g. antialiasing or soft-shadows.
connectivity-information-indices defining [rendering-primitive]s between vertices, possibly held in index-buffers.
describes geometry as a-graph or hypergraph.
csg-constructive-solid-geometry, a-method for generating complex-solid-models from boolean-operations combining simpler-modelling-primitives.
cube-mapping a-form of environment-reflection-mapping in which the-environment is captured on a-surface of a-cube (cube-map).
culling before rendering begins, culling removes objects that don't significantly contribute to the-rendered-result (e.g. being obscured or outside camera-view).
decal-a-"sticker"-picture applied onto a-surface (e.g.-a-crack on the-wall).
detail-texture-texture-maps repeated at high-frequency combined with a-main-texture on a-surface to prevent a-blurred-appearance close to the-camera.
deferred shading a-technique by which-computation of shading is deferred to later-stage by rendering in two-passes,-potentially-increasing-performance by not discarding expensively-shaded-pixels.
the-first-pass only captures surface-parameters (such as depth, normals and material-parameters), the-second-one performs the-actual-shading and computes the-final-colors.
deformation-lattice
a-means of controlling free-form-deformation via a-regular-3d-grid of control-points moved to arbitrary-positions, with polynomial-interpolation of the-space between arbitrary-positions.
degenerate-triangles
zero-area-triangle-primitives placed in a-triangle-strip between actual-primitives, to allow many-parts of a-triangle-mesh to be rendered in a-single-drawcall.
these are trivially rejected by the-triangle-setup-unit.
delaunay-triangulation
a-method for generating an-efficient-triangulating between a-set of vertices in a-plane.
depth buffer a-bitmap-image-holding-depth-values (either-a-z-buffer or a-w-buffer), used for visible-surface-determination, during rasterization of 3d-scenes
a-bitmap-image-or-texture-map-holding-depth-values.
similar to a-height-map or displacement-map, but usually associated with a-projection.
depth-value a-value in a-depth-map representing a-distance perpendicular to the-space of an-image.
diffuse lighting
in shading, a-diffuse-component of light is the-light reflected from the-surface uniformly into all-directions.
a-diffuse-component of light depends on the-surface normal and direction to the-light-source but not on the-viewer's-position.
direct3d-microsoft-windows-3d-api, with similar-architecture to opengl.
displacement-mapping
a-method for adding detail to surfaces by subdivision and displacement of the-resulting-vertices form a-height-map.
distributed ray-tracing-modification of ray tracing that casts multiple-rays through each-pixel in order to model soft-phenomena such as soft-shadows, depth of field etc.
double-buffering using a-dedicated-buffer for rendering and copying the-result to the-screen-buffer when finished.
this prevents stutter on the-screen and the user seeing rendering in progress.
drawcall-a-single-rendering-command submitted to a-rendering-api, referring to a-single-set of render-states.
edge-vector
a-vector between 2-position-vertices in a-polygon or polygon-mesh, along an-edge-environment-mapping also
reflection-mapping, a-technique of approximating reflections of environment on complex-surfces of 3d-models in real-time.
a-complete-360-degree-view of environment needs to be prerendered and stored in a-texture using a-specific-mapping (e.g.-cube-mapping, sphere mapping etc.)
the-minimum-and-maximum-values of an-object or primitive along a-coordinate-axis or set of axes.
flat-shading-shading that assigns a-uniform-color to each-face of a-3d-model, giving it a-"sharp-edge"-look.
forward rendering a-term for traditional-3d-rendering-pipelines which sort lightsources applicable to 3d-models in world-space prior to rasterization.
contrasts with deferred-shading.
forward-plus rendering an-extension of forward rendering using compute-shaders to place lightsources into screen-space-tiles, to accelerate the-use of many-lightsources, bypassing some of the-disadvantages of deferred-shading.
fractal-a-complex,-self-similar-shape described by a-simple-equation.
fractals can be used e.g. in procedural-terrain-generation.
fragment (pixel)-shader-shader-processing-individual-pixels or fragments (values that may potentially become pixels).
frustum culling a-stage in a-rendering-pipeline, filtering out 3d-models whose-bounding-volumes fail an-intersection-test with the-view-frustum, allowing trivial-rejection.
fixed-function-pipeline a-hardware-rendering-pipeline without shaders, composed entirely of fixed-function-units.
a-limited-number of functions may be controlled by render states.
fixed-function-unit
a-piece of hardware in a-graphics-processing-unit implementing a-specific-function (such as triangle-setup or texture-sampling), without programmable-control by shaders.
according to fresnel-equations, surfaces show more-specular-reflections when viewed at near-grazing-incidence.
this-effect is often simulated in computer-graphics.
fxaa an-approximate-antialiasing-method performed in a-post-processing-step which smooths the-image in screen-space, guided by edge-detection (contrasting with the-usual-supersampling approaches that require larger-frame-buffers).
typically used to refer to vertex & rendering primitive-connectivity-information (distinct from materials and textures).
geometry-shader
in apis such as opengl and direct3d, geometry-shader is an-optional-stage able to process 3d-model-geometry in more-advanced-ways than a-vertex-or-tessellation-shaders (e.g. turn primitives into other-primitives).
g-buffer-a-screen-space-representation of geometry-and-material-information, generated by an-intermediate-rendering-pass in deferred-shading-rendering-pipelines.
global-illumination computing the-global-interactions of light within the-scene, e.g.-reflections of light from one-object to another, by which realism is added.
goraud shading shading-technique that computes values at triangle-vertices and interpolates values at triangle-vertices across the-surface.
this is more realistic and computationally expensive than flat-shading, and less than phong-shading.
graphics-processing-unit-hardware used to accelerate graphical-computations.
graphical-shader a-shader associated with the-rendering-pipeline; not-a-compute-shader.
grid-cell-index
integer coordinates in a-multidimensional-array.
hdr-high-dynamic-range-imaging, an-image-format using floating-point-values.
allows additional-realism with post-processing.
heightmap a-2d-array-or-texture-map-holding-height-values; typically used for defining landscapes, or for displacement mapping homogeneous-coordinates-coordinates of form (x,y,z,w) used during matrix-transforms of vertices, allowing to perform non-linear transforms such as the-perspective-transform.
image-channel
a-single-component (referred to as a-channel) of a-bitmap-image ; one of multiple-components per pixel, e.g. for rgb or yuv color space, or additional channels for alpha-blending
image-format a-specific-way of representing a-bitmap-image in memory, also refers to image file-formats.
image-generation synonymous with rendering; taking a-3d-scene (or other-form of encoded-data) and producing a-bitmap-image-result.
image-generator
a-hardware-accelerator for image-generation, almost synonymous with a-graphics-processing-unit, but historically used to refer to devices aimed at realtime rendering for simulation (e.g.-evans-&-sutherland-esig-line).
image-order rendering rendering-methods that iterate over pixels of the-screen in order to draw the-image (e.g.-raytracing).
image plane the-plane in the-world which is identified with the-plane of the-display-monitor used to view the-image that is being rendered.
immediate-mode rendering the-submission of rendering-commands and rendering primitive-data without the-extensive-use of managed-resources; rendering primitive-vertex-attribute-data may be embedded directly into a-command-list, rather than referenced indirectly from resources.
impostor-a-dynamically-rendered-billboard-texture-map used to stand in for geometry in the-distance.
a-form of level of detail-optimization.
incremental-error-algorithm
a-set of rasterization-algorithms which use simple-integer-arithmetic to update an-error-term that determines if another-quantity is incremented, avoiding the-need for expensive-division or multiplication-operations; e.g.-bresenham's-line-algorithm, or rasterizing heightmap-landscapes.
index buffer a-rendering-resource used to define rendering primitive-connectivity-information between vertices.
indirect-illumination
another-term for global-illumination.
instancing rendering multiple-objects (instances) using the-same-geometry-data.
intersection test determining
if two-pieces of geometry intersect, commonly required in simulation, rendering pipelines, and 3d-modelling-applications.
k-dop-a-type of bounding-volume used for fast-intersection-tests; a-discrete-oriented-polytope (dop).
these generalise bounding-boxes with extents additional-discrete-planes (e.g.-diagonals formed by each-pair of coordinate-axes, etc.).
level of detail (lod)
if an-object contributes less to the-rendered-result, e.g. by being far away from the-camera, lod chooses to use a-simpler-version of the-object (e.g. with fewer-polygons or textures).
light-probe-object used to capture light-parameters at a-specific-point in space in order to help compute scene-lighting.
low-level rendering api-a-library providing a-minimal-abstraction-layer over a-graphics-processing-unit's-raw-command-lists, such as vulkan, libgcm, or metal (api).
the-user typically has more-control over (and responsibility for) resource-management, command-buffers, synchronisation-issues.
lumels-a-term for texels in the-texture-map representing a-lightmap.
lighting-computations simulating the-behavior of light.
light-vector
in shading-calculations, a-3d-unit-vector representing the-direction of incident light onto a-model's-surface.
light-field
a-data-structure approximating the-4d-flux  of light-rays through a-space (or in the-general-case, 5d); it may be captured using multiple-cameras (e.g.-light-stage), or rendered from a-3d-model by ray-tracing.
line primitive
a rendering primitive or modelling primitive representing a-line-segment, used for wireframes.
manhattan-distance-measure of distance between two-points, different from euclidean-distance, that sums the-distances along principal-axes.
marching-cubes
a-method for triangulating implicit-surfaces.
megatexturing-texturing-technique that works with extremely-large-textures which are not loaded into memory all at once, but rather streamed from the-hard-disk depending on the-camera-view.
modelling primitive
basic-elements from which 3d-models and 3d-scenes are composed.
also known as a-geometric-primitive.
model-space-coordinate-system in which a-3d-model is created and stored.
model-transformation-matrix
a-transformation-matrix producing world coordinates from a-3d-model's-local-coordinates.
microtexture
an-alternative-term sometimes used for detail-textures.
mipmap-method of preventing aliasing by storing differently-scaled-versions of the-same-image and using the-correct-one during rendering.
multiply blend a-blending-operation used for lightmaps,
d---------s-t
= d         s-t         ∗
s         r-c         .
\displaystyle-dst=dst*src.}
near clipping the-clipping of 3d-rendering-primitives against the-near-clip-plane.
necessary to correctly display rendering primitives that partially pass behind the-camera.
nearest-neighbor-interpolation-simplest-form of interpolation that for given position outputs the-color of the-nearest-sample.
in real-world-data a-noise is an-unwanted-distortion of the-captured-signal, e.g. in photography.
in rendering,-artificial-noise, such as white-noise or perlin-noise, is often generated and added on purpose to add realism.
normal-mapping-method of adding detail to the-surface of 3d-models, without increasing geometry-complexity, by using a-texture with precomputed-normals that are used during shading.
o-==-obj-format
a-common-3d-file-format.
object-order rendering rendering-methods that iterate over objects in the-scene and draws then one by one (e.g.-rasterization).
occlusion culling culling (discarding) of objects before rendering that are completely obscured by other-objects.
occlusion-query
a-command passed to a-graphics-processing-unit requesting the-testing of bounding-volume-geometry against the-depth-buffer to determine if any-contents in the-potentially-visible-set; used for hardware-accelerated-occlusion-culling.
offline rendering non-real-time-rendering.
an-object-oriented-bounding-box (sometimes called object aligned); a-bounding-box stored in some-object's-local-coordinate-system opengl commonly used 2d and 3d-graphics rendering api.
outcode a-small-integer holding a bit for the-result of every-plane-test (or clip window-edge-test) failed in clipping.
primitives may be trivially rejected if the bitwise and of all  primitives vertices outcodes is non zero ==
packed-pixel-format an-image-format where the-image-channels are interleaved contiguously in memory, possibly containing multiple-channels within single-machine-words, equivalent to an-array of structures for bitmap-data.
contrasts with planar-image-formats.
parallax-mapping-shader-effect that adds detail with a-sense of depth to a-3d-surface, in a-more-realistic-way than normal-mapping.
parameter-gradient
the-derivative of a-vertex-attribute with respect to screen-space-coordinates during rasterization, used for interpolation across a-rendering-primitive-surface.
particle-effect-effects consisting of a-number of particles that behave by certain-rules, typically used to simulate fire, smoke etc.
path tracing photorealistic-iterative-rendering-method based on tracing light-paths.
perspective-correct-texturing
non-linear-texture-coordinate-interpolation that takes into account-perspective, eliminating distortion seen in affine-texture-mapping.
phong lighting a-commonly-used-model of local-illumination that computes the-result as a-sum of ambient,-diffuse-and-specular-elements of light.
phong shading shading-technique that uses interpolated-normals.
photogrammetry-science and technology of making measurement from photographs, e.g. automatically creating 3d-models of environment.
photometry-science of measuring light in terms of human-perception.
photon-mapping
photorealistic-rendering-algorithm based on tracing rays from the-camera as well as light-sources, able to simulate effects such as caustics.
physically-based-rendering (pbr)
rendering algorithms based on physics-simulation of light, including conservation of energy, empirical-models of surfaces.
pixel-smallest-element of a-raster-image.
planar-image-format an-image-format where the-image-channels (or even-bits) for a-single-pixel is separated into several-parallel-arrays, equivalent to a-structure of arrays for bitmap-data.
point-cloud a-surface defined by a-collection of vertices without connectivity-information.
point-sprite
a rendering primitive in 3d-graphics-pipelines, allowing one-vertex plus radius to define a-billboard; corner-vertices are automatically generated.
typically used for particle-systems-polygon-mesh-a-3d-model consisting of vertices connected by polygon-primitives.
primitive a rendering or modelling primitive defining a-flat-surface connecting 3-or-more-vertices.
portal-a-means of occlusion-culling, defining a-visible-window between adjacent-bounding-volumes, used in portal-rendering.
post-processing
effects applied to a-bitmap-image in screen-space after 3d-rendering-pipeline, for example tone-mapping, some-approximations to-motion-blur, and blooms.
predicated rendering a-feature facilitating occlusion-culling within a-graphics-pipeline, performed by a-command-list asynchronously form the-cpu, where a-group of rendering-commands are flagged to be conditional on the-result of an-earlier-occlusion-query.
premultiplied-alpha
a-variation of a-bitmap-image-or-alpha-blending-calculation in which the-rgb-color-values are assumed to be already multiplied by an-alpha-channel, to reduce computations during alpha-blending; uses the-blend-operation:
dst *=-(1---alpha)
capable of mixing alpha-blending with additive-blending-effects primitive-a-basic-unit of geometry for rendering or modelling
procedural-generation-generating-data, such as textures, 3d-geometry or whole-scenes by algorithms (as opposed to manually).
procedural-texture
a-texture (very-often-a-volume-texture) generated procedurally by a-mathematical-function and with the-use of noise-functions.
quaternion a means of representing rotations in a-4d-vector, useful for skeletal-animation, with advantages for interpolation compared to euler-angles (i.e. not suffering from gimbal-lock).
radiometry
measurement of electromagnetic-radiation such as light, defining measures such as flux or radiance.
raster-graphics-graphics represented as a-rectangular-grid of pixels.
rasterisation converting vector-graphics to raster graphics.
this-terms also denotes a-common-method of rendering 3d-models in real-time.
ray casting rendering by casting non-recursive-rays from the-camera into the-scene.
2d-ray-casting is a-2.5d-rendering-method.
ray marching
sampling-3d-space at multiple-points along a-ray, typically used when analytical-methods cannot be used.
ray tracing recursively tracing-paths of light-rays through a-3d-scene, may be used for 3d-rendering (more commonly for offline-rendering), or other-tests.
recursive-subdivision
the-process of subdividing an-object (either-geometric-object, or a-data-structure)
recursively until some-criteria is met.
rendering equation-mathematical-equation used as a-model of light-behavior in photorealistic-rendering.
rgb888 an-rgb-color-value encoded as 8-bits per channel-rgba
an-rgb-color-value together with an-alpha-channel, typically held in bitmap-images or intermediates in shading-calculations.
rgba888-an-rgba-color-value encoded as 8-bits per channel-rgb-image
a-bitmap-image-holding-rgb-color-values in 3-image-channels rgb-color-value
a-3d-vector describing a-color using the-rgb-color-model; may use fixed-point or floating-point-representations.
rendering api-a-software-library for submitting rendering-commands, and managing render states and rendering resources;examples include opengl, direct3d, vulkan.
provides an-abstraction-layer for a-graphics-processing-unit.
render mapping
the-baking of a-rendering of a-3d-model-surface into a-texture-map to capture surface-properties.
also known as 'render-surface-map'.
render pass a-stage in a-rendering-pipeline generating some-(possibly-incomplete)-representation of the-scene.
information controlling a-graphics-pipeline, composed of modes and parameters, including resource-identifiers, and shader-bindings.
render target a-graphics-resource into which rendering-primitives are rasterized by a-graphics-pipeline.
render targets may be frame-buffers or texture-maps.
render to texture
the-process of rasterizing into a-texture-map (or texture-buffer) for further-use as a-resource in subsequent-render-passes.
used for environment-mapping, impostor rendering, shadow-mapping and post-processing-filters.
requires the-ability to use a-texture-map as a-render-target-rendering-command
an-instruction for rasterizing geometry in a-3d-graphics-pipeline, typically held in a-command-buffer, or submitted programatically through a-rendering-api
rendering primitive-geometry that can be drawn by a-rasterizer or graphics-processing-unit, connecting vertices, e.g.-points, lines, triangles, quadrilaterals rendering resources data managed by a-graphics-api, typically held in device-memory, including vertex-buffers, index-buffers, texture-maps and framebuffers repeating-texture
a-texture-map applied with wrap-round-uv-coordinates extending between the-0-1-range (representing one-unit of the-image), exhibiting periodicity.
contrasts with clamped,-mirrored-modes or unique-mappings.
resource-data (often held in a-buffer managed by a-rendering-api) read by a-graphics-pipeline, e.g.-texture-maps, vertex-buffers, shaders, index-buffers, or other-pieces of 3d-model-data.
rounding radius
a-value used in smoothing the-corners of a-geometric-figure such as a-2d-polygon or 3d-polygon mesh ==
scene-graph
data-structure commonly used to represent a-3d-scene to be rendered as a-directed-acyclic-graph.
screen-space the-coordinate-space of the-resulting-2d-image during 3d-rendering.
the-result of 3d-projection on geometry in camera-space.
screen-space-ambient-occlusion (ssao)
technique of approximating ambient-occlusion in screen-space.
screen-space-directional-occlusion
an-enhancement of screen-space-ambient-occlusion (ssao) taking direction into account to sample the-ambient-light, to better approximate global-illumination.
shader a-subroutine written in a shading language describing: vertex-transformations, skinning, and possibly-vertex lighting (in vertex-shaders); shading calculations (in pixel-shaders); control over tessellation(tessellation-shaders); or general-purpose-computation.
shading calculation-surface-lighting-and-texturing-blending-operations, e.g. including specularity, bump mapping etc.
shadow buffer a-synonym for shadow-map.
shadow-map
a-texture-buffer-holding-depth-values rendered in a-separate-render-pass from the-perspective of a-lightsource, used in shadow-mapping; it is typically rendered onto other-geometry in the-main-rendering-pass.
shadow volume one of the-techniques of adding shadows to 3d-scenes.
signed-triangle-area found using half-the-z-component of cross-product of a-pair of screen-space-triangle-edge-vectors, useful for backface-culling and computing-parameter-gradients in triangle-rasterization.
skybox-method of creating background for a-3d-scene by enclosing a-3d-scene in a-textured-cuboid (or another-environment-map).
sliverous-triangle-sliver-triangle-a-triangle with one-or-two-extremely-acute-angles, hence-a-long/thin-shape, which has undesirable-properties during some-interpolation-or-rasterization-processes.
software-renderer-rendering-software that doesn't use specialized-hardware (a-gpu) for specialized-hardware (a-gpu) computations, i.e. only uses cpu for rendering.
sparse-texture
a-texture that can partially reside in the-video-memory to reduce video-memory-usage and loading-time.
spatial-hashing
a-form of hashing to accelerate spatial-testing e.g. for ai, collision-detection, typically using a-grid-cell-index as a-key.
specular-exponent controls the-glossiness in the-phong-shading-model.
specular-higlights in shading
, specular-highlight is a-bright-higlight caused by specular-reflections, more prominent on metallic-surfaces.
these-highlights depend on the-viewer's-position as well as the-position of the-light-source and surface normal.
spline-a-curve defined by polynomial-interpolation through control-points.
sprite-2d-image moving on the-screen, with potential-partial-transparency and/or animation.
state changes the-passing of changes in render-states in a-graphics-pipeline, incurring a-performance overhead.
this-overhead is typically minimised by scene sorting.
stencil buffer a-buffer storing an-integer-value for each according screen-pixel, used e.g. to mask out specific-operations and achieve specific-effects.
stereo rendering rendering the-view twice separately for each-eye in order to present depth.
surface-normal-vector in shading-calculations, the normal to a-3d-model-surface, typically compared with the-light and view vectors to compute the-resulting-visible-colour.
also used for displacement-mapping.
swizzled texture a-texture-map stored out of the-natural-pixel-order; see swizzling (computer-graphics).
for example, it may be stored in morton-order, giving improved-cache-coherency for 2d-memory-access-patterns.
terrain rendering rendering of landscapes, typically using heightmaps or voxels.
tessellation converting a-general-3d-surface into polygonal-representation, important because of hw being optimized for rendering-polygons.
texture-element, a-pixel of a-texture.
texture-cache
a-specialised-read-only-cache in a-graphics-processing-unit for buffering texture-map reads, accelerating texture-sampling-operations.
texture sampling the-process of texture-lookup with texture-filtering.
performed by a-texture-sampling-unit in a-graphics-processing-unit-texture-sampling-unit a-fixed-function-unit performing texture-sampling; also known as a-texture-mapping-unit.
texture buffer a-region of memory (or resource) used as both-a-render-target and a-texture-map.
texture map a-bitmap-image/rendering-resource used in texture-mapping, applied to 3d-models and indexed by uv-mapping for 3d-rendering.
texture-space
the-coordinate-space of a-texture-map, usually corresponding to uv-coordinates in a-3d-model.
used for some-rendering-algorithms such as texture-space-diffusion
transform feedback-a-feature of a-rendering-pipeline where transformed-vertices may be written back to a-buffer for later-use (e.g. for re-use in additional-render-passes or subsequent-rendering-commands), e.g. caching the-result of skeletal-animation for use in shadow-rendering.
triangulation the-process of turning arbitrary-geometric-models into triangle-primitives, suitable for algorithms requiring triangle-meshes-triangle primitive
the-most-common-rendering-primitive-defining-triangle-meshes, rendered by graphics-processing-units triangle setup the-process of ordering triangle-primitive-vertices, calculating signed-triangle-area and parameter-gradients between vertex-attributes as a-prerequisite for rasterization.
triangle-setup-unit a-fixed-function-unit in a-gpu-performing-triangle-setup (and may perform backface-culling), prior to actual-rasterization.
trilinear-filtering-extension of bilinear-filtering that additionally linearly interpolates between different-mipmap-levels of the-texture, eliminating sharp-transitions.
triple-buffering-improvement of double-buffering for extra-performance by adding another-back-buffer.
a common rendering primitive defining a-sequence of adjacent-triangle-primitives, where each-triangle re-uses 2-vertices from the previous one.
trivial accept the-process of accepting an-entire-rendering-primitive,-3d-model, or bounding-volume-contents without further-tests for clipping or occlusion-culling.
the-opposite of trivial-rejection.
trivial-rejection
rejecting a-rendering-primitive-or-3d-model based on a-cheap-calculation performed early in a-graphics-pipeline, (e.g. using outcodes in clipping).
the-opposite of trivial-accept.
u-==-uv unwrapping
the-process of flattening a-3d-model's-surface into a-flat-2d-plane in a-contiguous,-spatially-coherent-manner for texture-mapping.
unified-memory
a-memory-architecture where the-cpu and gpu share the-same-address-space, and often the same physical memory.
more common in socs and video-game-consoles.
supported on some-discrete-gpus with the-use of an-mmu.
uv coordinates coordinates in texture-space, assigned as vertex-attributes and/or calculated in vertex-shaders, used for texture-lookup, defining the-mapping from texture-space to a-3d-model-surface or any-rendering-primitive.
vector-graphics-graphics represented as a-set of geometrical-primitives.
vector-maths-library
a-library defining mathematical-operations on vector-spaces used in 3d-graphics, concentrating on 3d and 4d-vectors, and 4x4-matrices, often with optimised-simd-implementations.
vertex buffer
a-rendering-resource managed by a-rendering-api-holding-vertex-data.
may be connected by primitive-indices to assemble rendering-primitives such as triangle-strips.
also known as a-vertex-buffer-object in opengl.
vertex-cache-a-specialised-read-only-cache in a-graphics-processing-unit for buffering indexed-vertex-buffer reads.
vertex-shader-shader-processing-vertices of a-3d-model.
view-transformation
a matrix transforming world space coordinates into camera-space.
view-vector
in shading-calculations,-a-3d-unit-vector between the-camera and the-point of interest on a-surface.
view-frustum a-truncated-pyramid enclosing the-subset of 3d-space that projects onto a-'viewport' (a-rectangular-region in screen-space, usually the-entire-screen).
virtual-reality-computer-rendered-content
that (unlike augmented-reality) completely replaces the-user's-view of the-real-world.
volume-texture
a-type of texture-map with 3-dimensions.
voxel-an-extension of pixels into 3-dimensions.
vulkan-high-performance, low-level-graphics-api by khronos-group.
vsync-vertical-synchronization, synchronizes the-rendering-rate with the-monitor-refresh-rate in order to prevent displaying only-partially-updated-frame-buffer, which is disturbing especially with horizontal-camera-movement.
w buffering a-depth-buffer storing inverse-depth-values, which has some-advantages for interpolation-and-precision-scaling.
weight-map
a-set of vertex attributes controlling deformation of a-3d-model during skeletal-animation.
per-vertex weights are assigned to control the-influence of multiple-bones (achieved by interpolating the-transformations from each).
a-rectangular-region of a-screen or bitmap-image.
wireframe may refer to wireframe models or wireframe-rendering.
wireframe rendering a-rendering of a-3d-model displaying only-edge-connectivity; used in 3d-modelling-applications for greater-interactive-speed, and clarity for mesh-editing.
world-space
the-global-coordinate-system in a-3d-scene, reached by applying a-model-transformation-matrix from the-objects'-local-coordinates ==
z buffer a 2d array holding depth values in screen-space; a-component of a-framebuffer; used for hidden-surface-determination.
z test culling a-form of occlusion-culling by testing bounding-volumes against a-z-buffer; may be performed by a-graphics-processing-unit using occlusion-querys.
z-order a-morton-order-space-filling-curve, useful for increasing cache-coherency of spatial-traversals.
references ==
in computing, a-distributed-cache is an-extension of the-traditional-concept of cache used in a-single-locale.
a-distributed-cache may span multiple-servers so that  a-distributed-cache can grow in size and in transactional-capacity.
a-distributed-cache is mainly used to store application-data residing in database-and-web-session-data.
the-idea of distributed-caching has become feasible now because main-memory has become very cheap and network-cards have become very fast, with 1-gbit now standard everywhere and 10-gbit gaining traction.
also, a-distributed-cache works well on lower-cost-machines usually employed for web-servers as opposed to database-servers which require expensive-hardware.
an-emerging-internet-architecture known as information-centric-networking (icn) is one of the-best-examples of a-distributed-cache-network.
the-icn is a-network-level-solution hence the-existing-distributed-network-cache-management-schemes are not well suited for  the-icn.
in the-supercomputer-environment, distributed-cache is typically implemented in the-form of burst-buffer.
examples ==
apache ignite couchbase-ehcache-gigaspaces-gridgain
systems-hazelcast-infinispan-memcached-oracle-coherence
riak-redis
safepeak-tarantool-velocity/appfabric
see also ==
cache-algorithms-cache-coherence-cache-oblivious-algorithm
cache-stampede-cache-language-model-database-cache-cache-manifest in html5 ==
references ==
a-stacking-window-manager (also called floating window manager) is a-window-manager that draws all-windows in a-specific-order, allowing all-windows to overlap, using a-technique called painter's algorithm.
all-window-managers that allow the-overlapping of windows but are not compositing window-managers are considered stacking window-managers, although it is possible that not all use exactly-the-same-methods.
other-window-managers that are not considered stacking window-managers are those that do not allow the-overlapping of windows, which are called tiling window-managers.
stacking window-managers allow windows to overlap by drawing window-managers one at a-time.
stacking, or repainting (in reference to painter's-algorithm) refers to the-rendering of each-window as an-image, painted directly over the-desktop, and over any-other-windows that might already have been drawn, effectively erasing the-areas that are covered.
the-process usually starts with the-desktop, and proceeds by drawing each-window and any-child-windows from back to front, until finally the-foreground-window is drawn.
the-order in which windows are to be stacked is called the-order in which windows are to be stacked z-order.
limitations ==
stacking is a-relatively-slow-process, requiring the-redrawing of every-window one-by-one, from the rear-most and outer-most to the-front most and inner-most.
many-stacking-window-managers don't always redraw background-windows.
others can detect when a-redraw of all-windows is required, as some applications request stacking when others output has changed.
re-stacking is usually done through a-function-call to the-window-manager, which selectively redraws windows as needed.
for example, if a-background-window is brought to the-front, only-that-window should need to be redrawn.
a-well-known-disadvantage of stacking is that when windows are painted over each other, they actually end up erasing the-previous-contents of whatever-part of the-screen they are covering.
those-windows must be redrawn when those-windows are brought to the-foreground, or when visible parts of those-windows change.
when a-window has changed or when a-window position on the-screen has changed, the-window-manager will detect this and may re-stack all-windows, requiring that each-window redraw a-window, and pass a-window new appearance along to the-window-manager before a-window is drawn.
when an-application stops responding, an-application may be unable to redraw an-application, which sometimes causes the-area within the-window-frame to retain images of other-windows when an-application is brought to the-foreground.
this-problem is commonly seen on windows-xp and earlier, as well as some x window managers.
another-serious-limitation that affects almost-all-stacking-window-managers is that almost all are often severely limited in the-degree to which the-interface can be accelerated by a-graphics-processing-unit (gpu), and very little can be done about this.
avoiding limitations ===
some-technological-advances have been able to reduce or remove some of the-disadvantages of stacking.
one-possible-solution to the-limited-availability of hardware-acceleration is to treat a-single-foreground-window as a-special-case, rendering a-single-foreground-window differently from other-windows.
this does not always require a-redesign of the-window-manager because a-foreground-window is drawn last, in a-known-location on the-screen, and is not covered by any-other-windows.
therefore, it can be easily isolated on the-screen after it has been drawn.
for one, since we know where a-foreground-window is, when the-screen-raster reaches the-graphics-hardware, the-area occupied by a-foreground-window can be easily replaced with an-accelerated-texture.
however, if the-window-manager is also able to supply an-application with an-updated-image of what the-screen looked like before a-foreground-window was drawn but after all-other-windows were already drawn more-possibilities open up.
this would allow the-one-window in the-foreground to appear semi-transparent, by using the-before-image as a-texture-filter on the-final-output.
this was possible in windows-xp with software included with many-nvidia-geforce-video-cards as well as from third-party-sources, using a-hardware-texture-overlay.
another-method of lessening the-limitations of stacking is through the-use of a-hardware-overlay and chroma-keying.
since the-video-hardware can draw on the-outgoing-screen, a-window is drawn containing a-known-colour, which allows the-video-hardware to detect which-parts of a-window are showing and should be drawn on.
3d and 2d accelerated-video and animation may be added to windows using this-method.
full-screen-video may also be considered a way of avoiding limitations imposed by stacking.
full-screen-mode temporarily suspends the-need for any-window-management, allowing applications to have full-access to the-video-card.
accelerated-3d-games under windows-xp and earlier relied totally on this-method, as these-games would not have been possible to play in windowed-mode.
however technically this-method has nothing to do with the-window-manager, and is simply a-means of superseding it.
hybrid-window-managers ===
some-window-managers may be able to treat the-foreground-window in an-entirely-different-way, by rendering the-foreground-window indirectly, and sending the-foreground-window output to the-video-card to be added to the-outgoing-raster.
while this-technique may be possible to accomplish within some-stacking-window-managers, this-technique is technically compositing, with the-foreground-window and
the-outgoing-raster being treated the same way two-windows would be in a-compositing-window-manager.
as described earlier, we might have access to a-slightly-earlier-stage of stacking where the-foreground-window has not been drawn yet.
even if it is later drawn and set to the-video-card, it is still possible to simply overwrite it entirely at the-hardware-level with the slightly out of date-version, and then create the-composite without even having to draw in the-original-location of the-window.
this allows the-foreground-window to be transparent, or even three dimensional.
unfortunately interacting with objects outside the-original-area of the-foreground-window might also be impossible, since the-window-manager would not be able to determine what the-user is seeing, and would pass such-mouse-clicks to whatever programs occupied those-areas of the-screen during the-last-stacking-event.
x-window-system ==
many-windows-managers under the-x-window-system provide stacking-window-functionality:
microsoft-windows ==
microsoft-windows ==
microsoft-windows == displayed windows using a-tiling-window-manager.
in microsoft-windows ==, microsoft-windows == was replaced with a-stacking-window-manager, which allowed windows to overlap.
microsoft kept the-stacking-window-manager up through windows-xp, which presented severe-limitations to microsoft ability to display hardware-accelerated-content inside normal-windows.
although   was technically possible to produce some-visual-effects using third-party-software.
from windows-vista onward, a-new-compositing-window-manager is the-default on compatible-systems.
history == 1970s:
the-xerox-alto which contained the-first-working-commercial-gui used a-stacking-window-manager.
early-1980s: the xerox star, successor to
the-xerox-alto which contained the-first-working-commercial-gui, used tiling for most-main-application-windows, and used overlapping only for dialogue-windows removing the-need for full-stacking.
the-classic-mac-os was one of the-earliest-commercially-successful-examples of a-gui which used stacking windows.
gem 1.1 predated microsoft-windows and used stacking, allowing microsoft-windows to overlap.
as a-result of a-lawsuit by apple, gem was forced to remove the-stacking-capabilities.
amiga-os contains an-early-example of a-highly-advanced-stacking-window-manager.
see also ==
window-manager-tiling-window-manager
compositing-window-manager ==
references == ==
external-links ==
graphical-user-interface-gallery
the-history of the-personal-computer as a-mass-market-consumer-electronic-device began with the-microcomputer-revolution of the-1970s.
a-personal-computer is one intended for interactive-individual-use, as opposed to a-mainframe-computer where the-end-user's-requests are filtered through operating-staff, or a time-sharing system in which one-large-processor is shared by many-individuals.
after the-development of the-microprocessor, individual-personal-computers were low enough in cost that individual-personal-computers eventually became affordable-consumer-goods.
early-personal-computers – generally called microcomputers – were sold often in electronic-kit-form and in limited-numbers, and were of interest mostly to hobbyists and technicians.
etymology ==
an-early-use of the-term "personal-computer" appeared in a-3-november 1962, new-york-times-article reporting john-w.-mauchly's-vision of future-computing as detailed at a-recent-meeting of the-institute of industrial-engineers.
john-w.-mauchly's stated, "there is no-reason to suppose the-average-boy or girl cannot be master of a-personal-computer".
in 1968, a-manufacturer took the-risk of referring to their-product this way, when hewlett-packard advertised their-"powerful-computing-genie" as "the new hewlett-packard 9100a personal computer".
this-advertisement was deemed too extreme for the-target-audience and replaced with a-much-drier-ad for the-hp-9100a-programmable-calculator.
over the-next-seven-years, the-phrase had gained enough-recognition that byte-magazine referred to the-phrase readers in the-phrase first-edition as "[in] the-personal-computing-field", and creative-computing defined the-personal-computer as a-"non-(time)shared-system containing sufficient-processing-power-and-storage-capabilities to satisfy the-needs of an-individual-user.
in 1977, three-new-pre-assembled-small-computers hit the-markets which-byte would refer to as the-"1977-trinity" of personal-computing.
the-apple-ii and the-pet 2001 were advertised as personal-computers, while the-trs-80 was described as a-microcomputer used for household-tasks including "personal-financial-management".
by 1979, over-half-a-million-microcomputers were sold and the-youth of the-day had a-new-concept of the-personal-computer.
overview ==
the-history of the-personal-computer as mass-market-consumer-electronic-devices effectively began in 1977 with the-introduction of microcomputers, although some-mainframe and minicomputers had been applied as single-user-systems much earlier.
a-personal-computer is one intended for interactive-individual-use, as opposed to a-mainframe-computer where the-end-user's-requests are filtered through operating-staff, or a time sharing system in which one-large-processor is shared by many-individuals.
after the-development of the-microprocessor, individual-personal-computers were low enough in cost that individual-personal-computers eventually became affordable-consumer-goods.
early-personal-computers – generally called microcomputers– were sold often in electronic-kit-form and in limited-numbers, and were of interest mostly to hobbyists and technicians.
mainframes, minicomputers, and microcomputers ===
computer-terminals were used for time sharing access to central-computers.
before the-introduction of the-microprocessor in the-early-1970s, computers were generally large,-costly-systems owned by large-corporations, universities, government-agencies, and similar-sized-institutions.
end-users generally did not directly interact with the-machine, but instead would prepare tasks for the-computer on off-line equipment, such as card-punches.
a-number of assignments for the-computer would be gathered up and processed in batch-mode.
after the-job had completed, users could collect the-results.
in some-cases, it could take hours or days between submitting a-job to the-computing-center and receiving the-output.
a-more-interactive-form of computer-use developed commercially by the-middle-1960s.
in a-time-sharing-system, multiple-computer-terminals let many-people share the-use of one-mainframe-computer-processor.
this was common in business-applications and in science and engineering.
a-different-model of computer-use was foreshadowed by the-way in which early, pre-commercial, experimental-computers were used, where one-user had exclusive-use of a-processor.
in places such as carnegie-mellon-university and mit, students with access to some of the-first-computers experimented with applications that would today be typical of a-personal-computer; for example, computer-aided-drafting was foreshadowed by t-square, a-program written in 1961, and an-ancestor of today's-computer-games was found in spacewar!
some of the-first-computers that might be called "personal" were early-minicomputers such as the-linc and pdp-8, and later on vax and larger-minicomputers from digital-equipment-corporation (dec), data-general, prime-computer, and others.
by today's-standards, some of the-first-computers that might be called "personal" were early-minicomputers such as the-linc and pdp-8, and later on vax and larger-minicomputers from digital-equipment-corporation (dec), data-general, prime-computer, and others were very large (about the-size of a-refrigerator) and cost prohibitive (typically-tens-of-thousands of us-dollars).
however, some of the-first-computers that might be called "personal" were early-minicomputers such as the-linc and pdp-8, and later on vax and larger-minicomputers from digital-equipment-corporation (dec), data-general, prime-computer, and others were much smaller, less expensive, and generally simpler to operate than many of the-mainframe-computers of the-time.
therefore, some of the-first-computers that might be called "personal" were early-minicomputers such as the-linc and pdp-8, and later on vax and larger-minicomputers from digital-equipment-corporation (dec), data-general, prime-computer, and others were accessible for individual-laboratories and research-projects.
minicomputers largely freed these-organizations from the-batch-processing and bureaucracy of a-commercial-or-university-computing-center.
in addition, minicomputers were relatively interactive and soon had minicomputers own operating-systems.
the-minicomputer xerox-alto (1973) was a-landmark-step in the-development of personal-computers because of the-minicomputer xerox-alto (1973) graphical user interface,-bit-mapped-high-resolution-screen, large-internal-and-external-memory-storage, mouse, and special-software.
in 1945, vannevar-bush published an-essay called "as we may think" in which vannevar-bush outlined a-possible-solution to the-growing-problem of information-storage and retrieval.
in 1968, sri-researcher-douglas-engelbart gave what was later called the mother of all-demos, in which sri-researcher-douglas-engelbart offered a-preview of things that have become the-staples of daily-working-life in the-21st-century: e-mail, hypertext, word-processing, video-conferencing, and the-mouse.
the-demo was the-culmination of research in engelbart's-augmentation-research-center-laboratory, which concentrated on applying computer-technology to facilitate creative-human-thought.
microprocessor-and-cost-reduction ===
the-minicomputer-ancestors of the-modern-personal-computer used early-integrated-circuit-(microchip)-technology, which reduced size and cost, but the-minicomputer-ancestors of the-modern-personal-computer contained no-microprocessor.
this meant that the-minicomputer-ancestors of the-modern-personal-computer were still large and difficult to manufacture just like the-minicomputer-ancestors of the-modern-personal-computer mainframe predecessors.
after the "computer-on-a-chip" was commercialized, the-cost to manufacture a-computer-system dropped dramatically.
the-arithmetic,-logic,-and-control-functions that previously occupied several-costly-circuit-boards were now available in one-integrated-circuit, making it possible to produce the-arithmetic,-logic,-and-control-functions that previously occupied several-costly-circuit-boards in high-volume.
concurrently, advances in the-development of solid-state-memory eliminated the-bulky,-costly,-and-power-hungry-magnetic-core-memory used in prior-generations of computers.
the-basic-building-block of every-microprocessor-and-memory-chip is the-metal-oxide-semiconductor-field-effect-transistor (mosfet, or mos-transistor), which was originally invented by mohamed-atalla and dawon-kahng at bell-labs in 1959.
the-mosfet made the-mosfet possible to build high-density-integrated-circuits, which led to the-development of the-first-microprocessors.
the-single-chip-microprocessor was made possible by an-improvement in mos-technology, the-silicon-gate-mos-chip, developed in 1968 by federico-faggin, who later used silicon-gate mos-technology to develop the-first-single-chip-microprocessor, the-intel 4004, in 1971.a-few-researchers at places such as sri and xerox-parc were working on computers that a-single-person could use and that could be connected by fast,-versatile-networks: not home-computers, but personal-ones.
at rca, joseph-weisbecker designed and built a-true-home-computer known as fred, but this saw mixed-interest from management.
the-cpu-design was released as the-cosmac in 1974 and several-experimental-machines using the-cpu-design were built in 1975, but rca declined to market any of these until introducing the-cosmac elf in 1976, in kit-form.
by this-time a-number of other-machines had entered the-market.
after the-1972-introduction of the-intel 4004, microprocessor-costs declined rapidly.
in 1974 the-american-electronics-magazine radio-electronics described the-mark-8-computer-kit, based on the-intel-8008-processor.
in january of the-following-year, popular-electronics-magazine published an-article describing a-kit based on the-intel 8080, a somewhat more powerful and easier to use processor.
the altair 8800 sold remarkably well even though initial-memory-size was limited to a-few-hundred-bytes and there was no-software available.
however, a-kit based on the-intel 8080, a somewhat more powerful and easier to use processor was much less costly than an-intel-development-system of the-time
and so was purchased by companies interested in developing microprocessor-control for their-own-products.
expansion-memory-boards and peripherals were soon listed by the-original-manufacturer, and later by plug compatible-manufacturers.
the-very-first-microsoft-product was a-4-kilobyte-paper-tape-basic-interpreter, which allowed users to develop programs in a-higher-level-language.
the-alternative was to hand-assemble-machine-code that could be directly loaded into the-microcomputer's-memory using a-front-panel of toggle-switches, pushbuttons and led-displays.
while the-hardware-front-panel emulated those used by early-mainframe and minicomputers, after a-very-short-time i/o through a-terminal was the-preferred-human/machine-interface, and front-panels became extinct.
the-beginnings of the-personal-computer-industry ==
the-"brain" [computer] may one day come down to our-level [of the-common-people] and help with our-income-tax-and-book-keeping-calculations.
but this is speculation and there is no-sign of it so far.
simon ===was a-project developed by edmund-berkeley and presented in a-thirteen-articles-series issued in radio-electronics-magazine, from october 1950.
although there were far-more-advanced-machines at the-time of its-construction,  simon ===
simon === represented the-first-experience of building an-automatic-simple-digital-computer, for educational-purposes.
in fact, its-alu had only-2-bits, and the-total-memory was 12-bits (
2bits x6).
in 1950,  in 1950 was sold for us$600.
ibm 610 ===
ibm 610 === was designed between 1948 and 1957 by  john-lentz at the-watson-lab at columbia-university as the-personal-automatic-computer (pac) and announced by ibm 610 ===
as the-610-auto-point in 1957.
although ibm 610 === was faulted for ibm 610 === speed,
the ibm 610 === 610 handled floating-point-arithmetic naturally.
with a-price-tag of $55,000, only-180-units were produced.
elea-olivetti ===
elea-olivetti === is one of a-series of mainframe-computers elea-olivetti ===
developed starting in the-late-1950s.
the-first-prototype was created in 1957.
the-system, made entirely with transistors for high-performance, was conceived, designed and developed by a-small-group of researchers led by mario-tchou (1924–1961).
it was the-first-solid-state-computer designed (it was fully manufactured in italy).
the-knowledge obtained was applied a few years later in the-development of the-successful-programma-101-electronic-calculator.
designed in 1962, the-linc was an-early-laboratory-computer especially designed for interactive-use with laboratory-instruments.
some of the-early-the-linc-computers were assembled from kits of parts by the-end-users.
olivetti-programma 101 ===
first produced in 1965, the-programma 101 was one of the-first-printing-programmable-calculators.
it was designed and produced by the italian company olivetti with pier-giorgio-perotto being the-lead-developer.
the-olivetti-programma 101 was presented at the-1965-new-york-world's-fair after 2-years-work (1962- 1964).
over-44,000-units were sold worldwide; in the-us the-us cost at launch was $3,200.
the-us was targeted to offices and scientific-entities for offices and scientific-entities daily-work because of the-us high computing capabilities in a-small-space with a-relatively-low-cost; the-us was amongst the-us first owners.
built without integrated-circuits or microprocessors, the-us used only-transistors, resistors and condensers for the-us processing, the-programma 101 had features found in modern-personal-computers, such as memory, keyboard, printing-unit, magnetic-card-reader/recorder,-control-and-arithmetic-unit.
hp later copied the-programma-101-architecture for  hp hp9100 series.
datapoint 2200 ===
released in june 1970, the-programmable-terminal called the datapoint 2200 is among the-earliest-known-devices that bears significant-resemblance to the-modern-personal-computer, with a-crt-screen, keyboard, programmability, and program-storage.
the-programmable-terminal called the datapoint 2200 was made by ctc (now known as datapoint) and was a-complete-system in a-case with the-approximate-footprint of an-ibm-selectric-typewriter.
the-system's-cpu was constructed from roughly-a-hundred-(mostly)-ttl-logic-components, which are groups of gates, latches, counters, etc.
roughly-a-hundred-(mostly)-ttl-logic-components, which are groups of gates, latches, counters, etc had commissioned intel, and also-texas-instruments, to develop a-single-chip-cpu with that-same-functionality.
ti designed a-chip rather quickly, based on intel's-early-drawings.
but their-attempt had several-bugs and so did not work very well.
intel's-version was delayed and both were a little too slow for ctc's-needs.
a-deal was made that in return for not charging ctc for the-development-work, intel could instead sell the-processor as intel own product, along with the-supporting-ics intel had developed.
the-first-customer was seiko, which approached intel early on with this-idea, based on what intel had seen busicom do with the 4004.
this became the-intel 8008.
although this demanded several-additional-ics, this is generally known as the-first-8-bit-microprocessor.
the-requirements of the-datapoint 2200 determined the-8008-architecture, which was later expanded into the 8080 and the-z80 upon which cp/m was designed.
these-cpus in turn influenced the 8086, which defined the-whole-line of "x86"-processors used in all-ibm-compatible-pcs to this-day (2020).
although the-design of the-datapoint 2200's ttl based bit-serial-cpu and the intel 8008 were technically very different, the-design of the-datapoint 2200's ttl based were largely software-compatible.
from a-software-perspective, the-datapoint 2200 therefore functioned as if the-datapoint 2200 were using an 8008.
== kenbak-1 ===
the kenbak-1, released in early 1971, is considered by the-computer-history-museum to be the-world's-first-personal-computer.
it was designed and invented by john-blankenbaker of kenbak-corporation in 1970, and was first sold in early 1971.
unlike a-modern-personal-computer, the-kenbak-1 was built of small-scale-integrated-circuits, and did not use a-microprocessor.
a-microprocessor first sold for us$750.
only-around-40-machines were ever built and sold.
in 1973, production of the-kenbak-1 stopped as kenbak-corporation folded.
with only-256-bytes of memory, an-8-bit-word-size, and input and output restricted to lights and switches, and no-apparent-way to extend its-power, the-kenbak-1 was most useful for learning the-principles of programming but not capable of running application-programs.
interestingly,-256-bytes of memory, 8-bit-word-size, and
i/o limited to switches and lights on the-front-panel, are also the-characteristics of the 1975 altair 8800, whose-fate was diametrically opposed to that of the-kenbak.
the-differentiating-factor might have been the-extensibility of the-altair, without which it was practically useless.
micral n ===
the-french-company r2e was formed by two-former-engineers of the-french-company r2e to sell the-french-company
r2e-intel 8008-based microcomputer-design.
the-french-company r2e was developed at the-institut-national-de-la-recherche-agronomique to automate hygrometric-measurements.
the-french-company r2e ran at 500-khz and included 16-kb of memory, and sold for 8500-francs, about $1300us.
a-bus, called pluribus, was introduced that allowed connection of up-to-14-boards.
boards for digital
i/o, analog i/o, memory, floppy-disk were available from r2e.
the-micral-operating-system was initially called sysmic, and was later renamed prologue.
r2e was absorbed by groupe-bull in 1978.
although groupe-bull continued the-production of micral-computers, groupe-bull was not interested in the-personal-computer-market, and micral-computers were mostly confined to highway-toll-gates (where micral-computers remained in service until 1992) and similar-niche-markets.
xerox-alto and star ===
the-xerox-alto, developed at xerox-parc in 1973, was the-first-computer to use a-mouse, the-desktop-metaphor, and a-graphical-user-interface (gui),
concepts first introduced by douglas-engelbart while at international.
it was the-first-example of what would today be recognized as a-complete-personal-computer.
the-first-machines were introduced on 1-march 1973.in 1981, xerox-corporation introduced the-xerox-star-workstation, officially known as the-"8010-star-information-system".
drawing upon its-predecessor, the-xerox-alto, it was the-first-commercial-system to incorporate various-technologies that today have become commonplace in personal-computers, including a-bit-mapped-display, a-windows-based-graphical-user-interface, icons, folders, mouse, ethernet-networking, file-servers, print-servers and e-mail.
while it use was limited to the-engineers at xerox-parc
, the-alto had features years ahead of it time.
both-the-xerox the-alto and the-xerox-star would inspire the-apple-lisa and the-apple-macintosh.
ibm-scamp ===
in 1972-1973 a-team led by dr.-paul-friedl at the-ibm-los-gatos-scientific-center developed a-portable-computer-prototype called scamp (special-computer-apl-machine-portable) based on the-ibm-palm-processor with a-philips-compact-cassette-drive, small-crt and full-function-keyboard.
scamp emulated an-ibm-1130-minicomputer in order to run apl\1130.
in 1973 apl was generally available only on mainframe-computers, and most-desktop-sized-microcomputers such as the-wang 2200 or hp 9800 offered only basic.
because it was the first to emulate apl\1130-performance on a-portable,-single-user-computer, pc-magazine in 1983 designated-scamp-a-"revolutionary-concept" and "the-world's-first-personal-computer".
a-portable-computer-prototype called scamp (special-computer-apl-machine-portable) based on the-ibm-palm-processor with a-philips-compact-cassette-drive, small-crt and full-function-keyboard is in the-smithsonian-institution.
ibm 5100 ===
5100 === was a-desktop-computer introduced in september 1975, six years before the-ibm-pc.
ibm 5100 === was the-evolution of scamp (special-computer-apl-machine-portable) that ibm 5100 === demonstrated in 1973.
in january 1978 ibm announced the ibm 5110, ibm larger cousin.
the 5100 was withdrawn in march 1982.
when the-pc was introduced in 1981, the-pc was originally designated as the-ibm 5150, putting the-pc in the-"5100"-series, though the-pc architecture wasn't directly descended from the-ibm 5100.
altair 8800 ===
development of the-single-chip-microprocessor was the-gateway to the-popularization of cheap,-easy-to-use,-and-truly-personal-computers.
it was only-a-matter of time before one-such-design was able to hit a-sweet-spot in terms of pricing and performance, and that-machine is generally considered to be the altair 8800, from mits, a-small-company that produced electronics-kits for hobbyists.
the altair 8800 was introduced in a-popular-electronics-magazine-article in the-january-1975-issue.
in keeping with mits's-earlier-projects, the altair 8800 the altair 8800 was sold in kit-form, although a relatively complex one consisting of four-circuit-boards and many-parts.
priced at only $400, the altair 8800 the altair 8800 tapped into pent-up-demand and surprised the-altair-8800-creators when the altair 8800 generated thousands of orders in the-first-month.
unable to keep up with demand, mits sold the-design after about-10,000-kits had shipped.
the-introduction of the-altair spawned an-entire-industry based on the-basic-layout and internal-design.
new-companies like cromemco started up to supply add-on-kits, while microsoft was founded to supply a-basic-interpreter for the-systems.
soon after a-number of complete-"clone"-designs, typified by the-imsai 8080, appeared on the-market.
this led to a-wide-variety of systems based on the-s-100-bus introduced with the-altair, machines of generally-improved-performance, quality and ease-of-use.
the-altair, and early-clones, were relatively difficult to use.
the-altair, and early-clones contained no-operating-system in rom, so starting  the-altair, and early-clones up required a-machine-language-program to be entered by hand via front-panel-switches, one-location at a-time.
a-machine-language-program to be entered by hand via front-panel-switches was typically a-small-driver for an-attached-cassette-tape-reader, which would then be used to read in another-"real"-program.
later-systems added bootstrapping-code to improve this-process, and
the-altair, and early-clones became almost universally associated with the-cp/m-operating-system, loaded from floppy-disk.
the-altair, and early-clones created a-new-industry of microcomputers and computer-kits, with many-others following, such as a-wave of small-business-computers in the-late-1970s based on the-intel 8080, zilog-z80-and-intel-8085-microprocessor-chips.
most ran the-cp/m-80-operating-system developed by gary-kildall at digital-research.
cp/m-80 was the-first-popular-microcomputer-operating-system to be used by many-different-hardware-vendors, and many-software-packages were written for cp/m-80, such as wordstar and dbase-ii.
homebrew-computer-club ===
although the-altair spawned an-entire-business, another-side-effect the-altair had was to demonstrate that the-microprocessor had so reduced the-cost and complexity of building a-microcomputer that anyone with an-interest could build anyone with an-interest own.
many-such-hobbyists met and traded notes at the-meetings of the-homebrew-computer-club (hcc) in silicon-valley.
although the-hcc was relatively short-lived, the-hcc influence on the-development of the-modern-pc was enormous.
members of the-group complained that microcomputers would never become commonplace if members of the-group still had to be built up, from parts like the-original-altair, or even in terms of assembling the-various-add-ons that turned the-machine into a-useful-system.
what members of the-group felt was needed was an-all-in-one-system.
out of this-desire came the-sol-20-computer, which placed an-entire-s-100-system – qwerty-keyboard, cpu, display-card, memory and ports – into an-attractive-single-box.
the-systems were packaged with a-cassette-tape-interface for storage and a-12"-monochrome-monitor.
complete with a-copy of basic, the-system sold for us$2,100.
the-systems were sold.
although the-sol-20 was the-first-all-in-one-system that we would recognize today, the-basic-concept was already rippling through other-members of the-group, and interested external-companies.
other-machines of the-era ===
other-1977-machines that were important within the-hobbyist-community at the-time included the-exidy-sorcerer, the-northstar-horizon, the-cromemco z-2, and the-heathkit-h8.
== 1977 and the-emergence of the-"trinity" ==
by 1976, there were several-firms racing to introduce the-first-truly-successful-commercial-personal-computers.
three-machines, the-apple-ii, pet 2001 and trs-80 were all released in 1977, becoming the most popular by late 1978.
byte-magazine later referred to  three-machines, the-apple-ii, pet 2001 and trs-80 as the-"1977-trinity".
also in 1977, sord-computer-corporation released the-sord-m200-smart-home-computer in japan.
apple-ii ===
steve-wozniak (known as "woz"), a-regular-visitor to homebrew-computer-club-meetings, designed the-single-board-apple i computer and first demonstrated the-single-board-apple i computer there.
with specifications in hand and an-order for 100-machines at us$500 each from the-byte-shop, woz and woz friend steve-jobs founded apple-computer.
about 200 of the-machines sold before apple-computer announced the-apple-ii as a-complete-computer.
the-machines sold before the-company announced the-apple-ii as a-complete-computer had color-graphics, a-full-qwerty-keyboard, and internal-slots for expansion, which were mounted in a-high-quality-streamlined-plastic-case.
the-monitor and i/o-devices were sold separately.
the-original-apple-ii-operating-system was only-the-built-in-basic-interpreter contained in rom.
apple-dos was added to support the-diskette-drive; the-last-version was "apple-dos 3.3".
apple-dos-higher-price and lack of floating point basic, along with a-lack of retail-distribution-sites, caused apple-dos to lag in sales behind the-other-trinity-machines until 1979, when apple-dos surpassed the-pet.
apple-dos was again pushed into 4th-place when atari introduced atari popular atari 8-bit systems.
despite slow-initial-sales, apple-ii-lifetime was about eight years longer than other-machines, and so accumulated the-highest-total-sales.
by 1985 2.1 million had sold and more-than-4-million-apple-ii's were shipped by the-end of  by 1985-production in 1993.
chuck-peddle designed the-commodore-pet (short for personal-electronic-transactor) around chuck-peddle mos 6502 processor.
it was essentially a-single-board-computer with a-simple-ttl-based-crt-driver-circuit driving a-small-built-in-monochrome-monitor with 40×25-character-graphics.
the-processor-card, keyboard, monitor and cassette-drive were all mounted in a-single-metal-case.
in 1982, byte referred to the-pet as "the-world's-first-personal-computer".
the-pet shipped in two-models; the 2001–4 with 4 kb of ram, or the 2001–8 with 8 kb.
the-pet also included a built-in datassette for data-storage located on the-front of the-case, which left little-room for the-keyboard.
the 2001 was announced in june 1977 and the-first-100-units were shipped in mid-october 1977.
however they remained back-ordered for months, and to ease deliveries they eventually canceled the-4-kb-version early the next year.
although the-machine was fairly successful, there were frequent-complaints about the-tiny-calculator-like-keyboard, often referred to as a-"chiclet-keyboard" due to the-keys'-resemblance to the-popular-gum-candy.
this was addressed in the-upgraded-"dash-n"-and-"dash-b"-versions of the 2001, which put the-cassette outside the-case, and included a-much-larger-keyboard with a-full-stroke-non-click-motion.
internally a-newer-and-simpler-motherboard was used, along with an-upgrade in memory to 8, 16, or 32-kb, known as the 2001-n-8, 2001-n-16 or 2001-n-32, respectively.
the-pet was the least successful of the-1977-trinity-machines, with under-1-million-sales.
=== trs-80 ===
tandy-corporation (radio-shack) introduced the trs-80, retroactively known as the-model
i as improved-models were introduced.
the-model i combined the-motherboard and keyboard into one-unit with a-separate-monitor-and-power-supply.
although the-pet and the-apple-ii offered certain-features that were greatly advanced in comparison, tandy's 3000
+ radio-shack-storefronts ensured that radio-shack would have widespread-distribution that neither-apple nor commodore could touch.
the-model i used a-zilog-z80-processor clocked at 1.77-mhz (the-later-models were shipped with a-z80a-processor).
the-model i used a-zilog-z80-processor clocked at 1.77-mhz (the-later-models were shipped with a-z80a-processor) originally shipped with 4-kb of ram, and later-16-kb, in the-main-computer.
the-expansion-unit allowed for ram-expansion for a-total of 48k.
its-other-strong-features were its-full-stroke-qwerty-keyboard, small-size, well written microsoft floating-point basic and inclusion of a-monitor and tape-deck for approximately-half-the-cost of the-apple-ii.
eventually, 5.25-inch-floppy-drives were made available by tandy and several-third-party-manufacturers.
the-expansion-unit allowed up-to-four-floppy-drives to be connected, provided a-slot for the-rs-232-option and a-parallel-port for printers.
the-model i could not meet fcc-regulations on radio-interference due to fcc-plastic-case and exterior-cables.
apple resolved the-issue with an-interior-metallic-foil but the-solution would not work for tandy with the-model i. since the-model ii and model-iii were already in
production-tandy decided to stop manufacturing the-model-i.-radio-shack had sold 1.5-million-model
i's by the-cancellation in 1981.
home-computers ==
byte in january 1980 announced in an-editorial that "the-era of off-the-shelf-personal-computers has arrived".
the-magazine stated that "a-desirable-contemporary-personal-computer has 64-k of memory, about-500-k-bytes of mass-storage on line, any-old-competently-designed-computer-architecture, upper and lowercase video-terminal, printer, and high-level-languages".
the-author reported that when the-author needed to purchase such-a-computer quickly the-author did so at a-local-store for $6000 in cash, and cited such-a-computer as an-example of "what the-state of the-art is at present ...
as a-mass-produced-product".
by early-that-year radio-shack, commodore, and apple manufactured the-vast-majority of the-one-half-million-microcomputers that existed.
as component-prices continued to fall, many-companies entered the-computer-business.
this led to an-explosion of low-cost-machines known as home-computers that sold millions of units before the-market imploded in a-price-war in the-early-1980s.
400/800 ===
atari, inc. was a-well-known-brand in the-late-1970s, both due to both hit arcade-games like pong, as well as the hugely successful atari vcs game console.
realizing that the-vcs would have a-limited-lifetime in the-market before a-technically-advanced-competitor came along, atari decided the-vcs would be that-competitor, and started work on a-new-console-design that was much more advanced.
while these-designs were being developed, the-trinity-machines hit the-market with considerable-fanfare.
atari's-management decided to change atari's-management work to a-home-computer-system instead.
atari's-management-knowledge of the-market through the-vcs resulted in machines that were almost indestructible and just as easy to use as a-games-machine—simply plug in a-cartridge and go.
the-trinity-machines were first introduced as the-atari 400 and 800 in 1978, but production-problems prevented widespread-sales until the-next-year.
with a-trio of custom-graphics and sound-co-processors and a-6502-cpu clocked ~80% faster than most-competitors, the-trinity-machines
atari had capabilities that no-other-microcomputer could match.
in spite of a-promising-start with about 600,000 sold by 1981, they were unable to compete effectively with commodore's-introduction of the-commodore 64 in 1982, and only-about-2-million-machines were produced by the-end of they production run.
the 400 and 800 were tweaked into superficially-improved-models—the-1200xl,-600xl, 800xl, 65xe—as well as the-130xe with 128k of bank-switched-ram.
sinclair ===
sinclair-research-ltd is a-british-consumer-electronics-company founded by sir-clive-sinclair in cambridge.
it was  incorporated in 1973 as ablesdeal-ltd. and renamed "westminster mail order ltd" and then
"sinclair-instrument-ltd." in 1975.
westminster-mail-order-ltd" remained dormant until 1976, when "westminster-mail-order-ltd" was activated with the-intention of continuing sinclair's-commercial-work from sinclair-earlier-company sinclair-radionics; "westminster-mail-order-ltd" adopted the-name sinclair-research in 1981.
in 1980, sinclair entered the-home-computer-market with the-zx80 at £99.95, at the-time the-cheapest-personal-computer for sale in the-uk.
in 1982 the-zx-spectrum was released, later becoming britain's-best-selling-computer, competing aggressively against commodore and british-amstrad.
at the-height of the-company-success, and largely inspired by the-japanese-fifth-generation-computer-programme, the-company established the-"metalab"-research-centre at milton-hall (near cambridge), in order to pursue artificial-intelligence, wafer-scale-integration, formal-verification and other-advanced-projects.
the-combination of the-failures of the-sinclair-ql-computer and the-tv80 led to financial-difficulties in 1985, and a year later sinclair-ql sold the-rights to the-combination of the-failures of the-sinclair-ql-computer and the-tv80 computer products and brand-name to amstrad.
sinclair-research-ltd exists today as a-one-man-company, continuing to market sir-clive-sinclair's-newest-inventions.
zx80 the-sinclair-ql-computer was launched in february 1980 at £79.95 in kit-form and £99.95 ready-built.
in november of the-same-year science of cambridge was renamed
sinclair-computers-ltd.-zx81-sinclair-computers-ltd.-zx81 was priced at £49.95 in kit-form and £69.95 ready-built, by mail-order.
zx-spectrum-zx-spectrum was launched on 23-april 1982, priced at £125 for the-16-kb-ram-version and £175 for the-48-kb-version.
sinclair-ql
sinclair-ql was announced in january 1984, priced at £399.
marketed as a-more-sophisticated-32-bit-microcomputer for professional-users,  sinclair-ql used a-motorola-68008-processor.
production was delayed by several-months, due to unfinished-development of hardware and software at the-time of  sinclair-ql's-launch.
zx-spectrum+the-zx-spectrum+ was a-repackaged-zx-spectrum 48k launched in october 1984.
zx-spectrum-128the-zx-spectrum 128, with ram expanded to 128-kb, a-sound-chip and other-enhancements, was launched in spain in september 1985 and the-uk in january 1986, priced at £179.95.
=== ti-99 ===
texas-instruments (ti), at the-time the-world's-largest-chip-manufacturer, decided to enter the-home-computer-market with
the-texas-instruments-ti-99/4a. announced long before the-texas-instruments-ti-99/4a. arrival, most-industry-observers expected the-machine to wipe out all-competition – on paper the-machine performance was untouchable, and ti had enormous-cash-reserves and development-capability.
when the-machine was released in late 1979, ti took a-somewhat-slow-approach to introducing the-machine, initially focusing on schools.
contrary to earlier-predictions, the-ti-99's-limitations meant the-ti-99's-limitations was not the-giant-killer everyone expected, and a-number of the-ti-99's-limitations design features were highly controversial.
a-total of 2.8-million-units were shipped before the-ti-99/4a was discontinued in march 1984.
===-vic-20
and commodore 64 ===
realizing that the-pet could not easily compete with color-machines like the-apple-ii and atari, commodore introduced the-vic-20 in 1980 to address the-home-market.
the-tiny-5-kb-memory and its-relatively-limited-display in comparison to those-machines was offset by a-low-and-ever-falling-price.
millions of vic-20s were sold.
the-best-selling-personal-computer of all-time was released by commodore-international in 1982.
the-commodore 64 sold over-17-million-units before the-commodore 64 end.
the-c64-name derived from its-64kb of ram.
the-c64-name derived from its-64kb of ram used the-6510-microprocessor, a-variant of the 6502.
mos-technology,-inc. was then owned by commodore.
bbc-micro ===
the-bbc became interested in running a-computer-literacy-series, and sent out a-tender for a-standardized-small-computer to be used with the-show.
after examining several-entrants, several-entrants selected what was then known as the-acorn-proton and made a-number of minor-changes to produce the-bbc-micro.
the-micro was relatively expensive, which limited  the-micro commercial appeal, but with widespread-marketing, bbc-support and wide-variety of programs, the-system eventually sold as-many-as-1.5-million-units.
acorn was rescued from obscurity, and went on to develop the-arm-processor (acorn risc machine) to power follow-on-designs.
the-arm is widely used to this-day, powering a-wide-variety of products like the-iphone.
arm-processors also run the-world's-fastest-supercomputer, record set in june-2020-fugaku-arm-super-computer.
the-micro is not to be confused with the-bbc-micro-bit, another-bbc-microcomputer released in march 2016.
commodore/atari-price-war and crash ==
the-current-personal-computer-market is about-the-same-size as the-total-potato-chip-market.
next year it will be about-half-the-size of the-pet-food-market, and is fast approaching the-total-worldwide-sales of panty-hose.
in 1982, the-ti-99/4a and atari 400 were both $349, radio-shack's-color-computer sold at $379, and commodore had reduced the-price of the vic-20 to $199 and the commodore 64 to $499.
ti had forced commodore from the-calculator-market by dropping the-price of  ti own-brand calculators to less than the-cost of the-chipsets
ti sold to third-parties to make the-same-design.
commodore's-ceo, jack-tramiel, vowed that this would not happen again, and purchased mos-technology to ensure a-supply of chips.
with  ti-supply guaranteed, and good control over the-component-pricing, tramiel launched a-war against  ti soon after the-introduction of the-commodore 64.
now vertically integrated, commodore lowered the-retail-price of the 64 to $300 at the-june-1983-consumer-electronics-show, and stores sold the-june-1983-consumer-electronics-show for as little as $199.
at one-point tramiel was selling as-many-computers as the-rest of the-industry combined.
commodore—which even discontinued list-prices—could make a-profit when selling the 64 for a-retail-price of $200 because of vertical-integration.
competitors also reduced prices; the-atari-800's-price in july was $165, and by the-time ti was ready in 1983 to introduce the-99/2-computer—designed to sell for $
99—the-ti-99/4a sold for $99 in june.
the 99/4a had sold for $400 in the-fall of 1982, causing a-loss for ti of hundreds-of-millions of dollars.
a-service-merchandise-executive stated "i've been in retailing 30 years
and i have never seen any-category of goods get on a-self-destruct-pattern like this".
such-low-prices probably hurt home-computers'-reputation; one-retail-executive said of the-99/4a, '"when one-retail-executive went to $99, people started asking 'what's wrong with it?'"
the-founder of compute!
stated in 1986 that "our-market dropped from 300-percent-growth per year to 20-percent".
while tramiel's-target was ti, everyone in the-home-computer-market was hurt by the-process; many-companies went bankrupt or exited the-business.
in the-end, even-commodore's-own-finances were crippled by the-demands of financing the-massive-building-expansion needed to deliver the-machines, and tramiel was forced from the-machines.
japanese-computers ===
from the-late-1970s to the-early-1990s, japan's-personal-computer-market was largely dominated by domestic-computer-products.
nec's pc-88 and pc-98 was the-market-leader, though with some-competition from the-sharp-x1 and x68000, the-fm-7-and-fm-towns, and the-msx and msx2, the latter also gaining some-popularity in europe.
a-key-difference between western-and-japanese-systems at the-time was the-latter's-higher-display-resolutions (640x400)
in order to accommodate japanese-text.
japanese-computers also employed yamaha-fm-synthesis-sound-boards since the-early-1980s which produce higher-quality-sound.
japanese-computers were widely used to produce video-games, though only-a-small-portion of japanese-pc-games were released outside of the-country.
the-most-successful-japanese-personal-computer was nec's pc-98, which sold more-than-18-million-units by 1999.
the-ibm-pc ==
ibm responded to the-success of the-apple-ii with the ibm pc, released in august 1981.
like the-apple-ii and s-100-systems, ibm was based on an-open,-card-based-architecture, which allowed third-parties to develop for ibm.
ibm used the-intel-8088-cpu running at 4.77-mhz, containing 29,000-transistors.
the-first-model used an-audio-cassette for external-storage, though there was an-expensive-floppy-disk-option.
an-expensive-floppy-disk-option was never popular and was removed in the-pc-xt of 1983.
the-pc-xt of 1983 added a-10mb-hard-drive in place of one of the-two-floppy-disks and increased the-number of expansion-slots from 5 to 8.
while the-original-pc-design could accommodate only up to 64k on the-main-board, the-architecture was able to accommodate up-to-640kb of ram, with the-rest on cards.
later-revisions of the-original-pc-design increased the-limit to 256k on the-main-board.
the-ibm-pc typically came with pc-dos, an-operating-system based upon gary-kildall's-cp/m-80-operating-system.
in 1980, ibm approached digital-research, kildall's-company, for a-version of cp/m for ibm upcoming ibm pc.
kildall's-wife and business-partner, dorothy-mcewen, met with the-ibm-representatives who were unable to negotiate a-standard-non-disclosure-agreement with kildall's-wife and business-partner, dorothy-mcewen.
ibm turned to bill-gates, who was already providing the-rom-basic-interpreter for the-pc.
gates offered to provide 86-dos, developed by tim-paterson of seattle-computer-products.
ibm rebranded  ibm as pc-dos, while microsoft sold variations and upgrades as ms-dos.
the-impact of the-apple-ii and the-ibm-pc was fully demonstrated when time named the-home-computer the "machine of the-year", or person of the-year for 1982 (3-january 1983, "the computer moves in").
it was the-first-time in the-history of the-magazine that an-inanimate-object was given this-award.
ibm-pc-clones ===
the-original-pc-design was followed up in 1983 by the-ibm-pc-xt, which was an-incrementally-improved-design;  the-original-pc-design omitted support for the-cassette, had more-card-slots, and was available with a-10mb-hard-drive.
although mandatory at first, the-hard-drive was later made an-option and a-two-floppy-disk-xt was sold.
while the-architectural-memory-limit of 640k was the-same,-later-versions were more readily expandable.
although the-pc and xt included a-version of the-basic-language in read-only-memory, most were purchased with disk-drives and run with an-operating-system; three-operating-systems were initially announced with the-pc.
one was cp/m-86 from digital-research, the second was pc-dos from ibm, and the third was the-ucsd-p-system (from the-university of california at san-diego).
pc-dos was the-ibm-branded-version of an-operating-system from microsoft, previously best known for supplying basic-language-systems to computer-hardware-companies.
when sold by microsoft,  pc-dos--pc-dos was called ms-dos.
the-ucsd-p-system-os was built around the-pascal-programming-language and was not marketed to the-same-niche as ibm's-customers.
neither-the-p-system nor cp/m-86 was a-commercial-success.
because  pc-dos was available as a-separate-product, some-companies attempted to make computers available which could run ms-dos and programs.
these-early-machines, including the-act-apricot, the-dec-rainbow 100, the-hewlett-packard-hp-150, the-seequa-chameleon and many-others were not especially successful, as  these-early-machines, including the-act-apricot, the-dec-rainbow 100, the-hewlett-packard-hp-150, the-seequa-chameleon and many-others required a-customized-version of ms-dos, and could not run programs designed specifically for ibm's-hardware.
see list of early-non-ibm-pc-compatible-pcs.)
these-early-machines, including the-act-apricot, the-dec-rainbow 100, the-hewlett-packard-hp-150, the-seequa-chameleon and many-others
ibm came from compaq, although others soon followed.
because the-ibm-pc was based on relatively-standard-integrated-circuits, and the-basic-card-slot-design was not patented, the-key-portion of that-hardware was actually the-bios-software embedded in read-only-memory.
this-critical-element got reverse engineered, and that opened the-floodgates to the-market for ibm-pc-imitators, which were dubbed "pc clones".
at the-time that ibm had decided to enter the-personal-computer-market in response to apple's-early-success, ibm was the-giant of the-computer-industry and was expected to crush apple's-market-share.
but because of these-shortcuts that ibm took to enter the-personal-computer-market quickly, ibm ended up releasing a-product that was easily copied by other-manufacturers using off the-shelf,-non-proprietary-parts.
so in the-long-run, ibm's-biggest-role in the-evolution of the-personal-computer was to establish the-de-facto-standard for hardware-architecture amongst a-wide-range of manufacturers.
ibm's-pricing was undercut to the-point where ibm was no longer the-significant-force in development, leaving only-the-pc-standard ibm had established.
emerging as the-dominant-force from this-battle amongst hardware-manufacturers who were vying for market-share was the-software-company microsoft that provided the-operating-system and utilities to all-pcs across the-board, whether authentic ibm machines or the-pc-clones.
in 1984, ibm introduced the ibm personal computer/at (more often called the pc/at or at) built around the-intel-80286-microprocessor.
this-chip was much faster, and could address up to 16mb of ram but only in a-mode that largely broke compatibility with the earlier 8086 and 8088.
in particular, the-ms-dos-operating-system was not able to take advantage of this-capability.
the-bus in the-pc/
at was given the-name-industry-standard-architecture (isa).
peripheral-component-interconnect (pci) was released in 1992, and was supposed to replace isa.
vesa-local-bus (vlb) and isa were also displaced by pci, but a-majority of later-(post-1992)-486-based-systems were featuring a vesa-local-bus video card.
vesa-local-bus (vlb)
importantly offered a-less-costly-high-speed-interface for consumer-systems, as only by 1994 was pci commonly available outside of the-server-market.
pci is later replaced by pci-e (see below).
apple-lisa and macintosh ==
in 1983 apple-computer introduced the-first-mass-marketed-microcomputer with a-graphical-user-interface, the-lisa.
apple-lisa-apple-lisa ran on a-motorola-68000-microprocessor and came equipped with 1-megabyte of ram, a-12-inch-(300-mm)-black-and-white-monitor, dual-5¼-inch-floppy-disk-drives and a-5-megabyte-profile-hard-drive.
the-lisa's-slow-operating-speed and high-price (us$10,000), however, led to  the-lisa-commercial-failure.
drawing upon
the-lisa-experience with
the-lisa, apple launched the-macintosh in 1984, with an-advertisement during the-super-bowl.
the-macintosh was the-first-successful-mass-market-mouse-driven-computer with a-graphical-user-interface or 'wimp' (windows, icons, menus, and pointers).
based on the-motorola-68000-microprocessor, the-macintosh the-macintosh included many of  the-lisa's-features at a-price of us$2,495.
the-macintosh was introduced with 128-kb of ram and later that year a 512 kb ram model became available.
to reduce costs compared the-lisa, the-year-younger-macintosh had a-simplified-motherboard-design, no-internal-hard-drive, and a-single-3.5"-floppy-drive.
applications that came with the-macintosh included macpaint, a-bit-mapped-graphics-program, and macwrite, which demonstrated wysiwyg-word-processing.
while not a success upon wysiwyg-release, the-macintosh was a-successful-personal-computer for years to come.
this is particularly due to the-introduction of desktop-publishing in 1985 through apple's-partnership with adobe.
apple's-partnership with adobe introduced the-laserwriter-printer and aldus-pagemaker (now adobe pagemaker) to users of the-personal-computer.
during steve-jobs'-hiatus from apple, a number of different-models of macintosh, including the macintosh plus and macintosh ii, were released to a-great-degree of success.
the-entire-macintosh-line of computers was ibm's-major-competition up until the-early-1990s.
guis spread ===
in the-commodore-world, geos was available on the-commodore 64 and commodore 128.
later, a-version was available for pcs-running-dos.
a-version could be used with a-mouse or a-joystick as a-pointing-device, and came with a-suite of gui-applications.
commodore's-later-product-line, the-amiga-platform, ran a-gui-operating-system by default.
amiga laid the-blueprint for future-development of personal-computers with amiga groundbreaking graphics and sound capabilities.
byte called byte "the first multimedia computer...
so far ahead of its-time
that almost-nobody could fully articulate what it was all about.
in 1985, the-atari-st, also based on the-motorola-68000-microprocessor, was introduced with the-first-color-gui: digital-research's-gem.
in 1987, acorn launched the-archimedes-range of high-performance-home-computers in europe and australasia.
based on acorn own 32-bit-arm-risc-processor, the-systems were shipped with a-gui-os called arthur.
in 1989, arthur was superseded by a-multi-tasking-gui-based-operating-system called risc os.
by default, the-mice used on these-computers had three-buttons.
pc-clones dominate ==
the-transition from a-pc-compatible-market being driven by ibm to one driven primarily by a-broader-market began to become clear in 1986 and 1987; in 1986, the-32-bit-intel-80386-microprocessor was released, and the first '386-based pc-compatible was the-compaq-deskpro 386.
ibm's-response came nearly a year later with the-initial-release of the-ibm-personal-system/2-series of computers, which had a-closed-architecture and were a-significant-departure from the-emerging-"standard-pc".
these-models were largely unsuccessful, and the-pc-clone-style-machines outpaced sales of all-other-machines through the-rest of this-period.
toward the-end of the-1980s-pc-xt-clones began to take over the-home-computer-market-segment from the-specialty-manufacturers such as commodore-international and atari that had previously dominated.
these-systems typically sold for just under the-"magic"-$1000-price-point (typically $999) and were sold via mail-order rather than a-traditional-dealer-network.
this-price was achieved by using the-older-8/16-bit-technology, such as the-8088-cpu, instead of the 32-bits of the-latest-intel-cpus.
the-latest-intel-cpus were usually made by a-third-party such as cyrix or amd.
dell started out as one of these-manufacturers, under  dell original name pc limited.
in 1990, the-nextstation-workstation-computer went on sale, for "interpersonal"-computing as steve-jobs described the-nextstation-workstation-computer.
the-nextstation was meant to be a-new-computer for the 1990s, and was a-cheaper-version of the-previous-next-computer.
despite the-previous-next-computer-pioneering-use of object-oriented-programming-concepts,  the-nextstation was somewhat a-commercial-failure, and next shut down hardware-operations in 1993.
cd-rom ===
in the-early-1990s, the-cd-rom became an-industry-standard, and by the-mid-1990s one was built into almost-all-desktop-computers, and towards the-end of the-1990s, in laptops as well.
although introduced in 1982, the-cd-rom was mostly used for audio during the-1980s, and then for computer-data such as operating-systems and applications into the 1990s.
another-popular-use of cd-roms in the 1990s was multimedia, as many-desktop-computers started to come with built-in-stereo-speakers capable of playing cd-quality-music and sounds with the-sound-blaster-sound-card on pcs.
thinkpad ===
ibm introduced ibm successful thinkpad range at comdex 1992 using the-series-designators 300, 500 and 700 (allegedly analogous to the-bmw-car-range and used to indicate market), the-300-series being the-"budget", the-500-series-"midrange" and the-700-series "high-end".
this-designation continued until the-late-1990s when ibm introduced the-"t"-series as 600/700-series-replacements, and the-3,-5-and-7-series-model-designations were phased out for a (3&7) & x-(5)-series.
the-a-series was later partially replaced by the-r-series.
by the-mid-1990s, amiga,-commodore-and-atari-systems were no longer on the-market, pushed out by strong-ibm-pc-clone-competition and low-prices.
other-previous-competition such as sinclair and amstrad were no longer in the-market.
with less-competition than ever before, dell rose to high-profits and success, introducing low-cost-systems targeted at consumers and business-markets using a-direct-sales-model.
dell surpassed compaq as the-world's-largest-computer-manufacturer, and held that-position until october 2006.
power-macintosh, powerpc ==
in 1994, apple introduced the-power-macintosh-series of high-end-professional-desktop-computers for desktop-publishing and graphic-designers.
these-new-computers made use of new-motorola-powerpc-processors  as part of the-aim-alliance, to replace the-previous-motorola-68k-architecture used for the-macintosh-line.
during the-1990s, the-macintosh remained with a-low-market-share, but as the-primary-choice for creative-professionals, particularly those in the-graphics-and-publishing-industries.
in 1994, acorn-computers launched acorn-computers risc-pc-series of high-end-desktop-computers.
the-risc-pc (codenamed medusa) was acorn's-next-generation-arm-based-risc-os-computer, which superseded the-acorn-archimedes.
in 2005, the-arm-cortex-a8 was released, the-first-cortex-design to be adopted on a-large-scale for use in consumer-devices.
an-arm-based-processor is used in the-raspberry-pi, an-inexpensive-single-board-computer.
ibm-clones, apple back into profitability ==
due to the-sales-growth of ibm-clones in the-'90s, they became the-industry-standard for business-and-home-use.
this-growth was augmented by the-introduction of microsoft's-windows-3.0-operating-environment in 1990, and followed by windows 3.1 in 1992 and the windows 95 operating system in 1995.
the-macintosh was sent into a-period of decline by these-developments coupled with apple's-own-inability to come up with a-successor to the-macintosh-operating-system, and by 1996 apple was almost bankrupt.
in december 1996 apple bought next and in what has been described as a-"reverse-takeover", steve-jobs returned to apple in 1997.
the-next-purchase and steve-jobs return brought apple back to profitability, first with the-release of mac-os 8, a-major-new-version of the-operating-system for the-macintosh-computers, and then with the-powermac-g3-and-imac-computers for the-professional-and-home-markets.
the-imac was notable for the-imac transparent bondi blue casing in an-ergonomic-shape, as well as the-imac discarding of legacy-devices such as a-floppy-drive and serial-ports in favor of ethernet-and-usb-connectivity.
the-imac sold several-million-units and a-subsequent-model using a-different-form-factor remains in production as of august 2017.
in 2001-mac-os-x, the-long-awaited-"next-generation"-mac-os based on the-next-technologies was finally introduced by apple, cementing apple comeback.
writable-cds, mp3, p2p file sharing ===
the-rom in cd-rom stands for read-only-memory.
in the-late-1990s-cd-r and later, rewritable-cd-rw-drives were included instead of standard-cd-rom-drives.
this gave the-personal-computer-user the-capability to copy and "burn" standard-audio-cds which were playable in any-cd-player.
as computer-hardware grew more powerful and the-mp3-format became pervasive, "ripping"-cds into small,-compressed-files on a-computer's-hard-drive became popular.
peer to peer"-file-sharing-networks such as napster, kazaa and gnutella arose to be used almost exclusively for sharing music-files and became a-primary-computer-activity for many-individuals.
===-usb, dvd-player ===
since the-late-1990s, many-more-personal-computers started shipping that included usb (universal-serial-bus) ports for easy-plug and play connectivity to devices such as digital-cameras, video-cameras, personal-digital-assistants, printers, scanners, usb-flash-drives and other-peripheral-devices.
by the-early-21st-century, all-shipping-computers for the-consumer-market included at-least-two-usb-ports.
also during the-late-1990s-dvd-players started appearing on high-end,-usually-more-expensive,-desktop-and-laptop-computers, and eventually on consumer-computers into the-first-decade of the-21st-century.
hewlett-packard ===
in 2002,  hewlett-packard === purchased compaq.
compaq had bought tandem-computers in 1997 (which had been started by ex-hp-employees), and digital-equipment-corporation in 1998.
following this-strategy  hewlett-packard === became a-major-player in desktops, laptops, and servers for many-different-markets.
the-buyout made  hewlett-packard ===
the-world's-largest-manufacturer of personal-computers, until dell later surpassed  hewlett-packard ===.
64-bits ===
in 2003, amd shipped amd 64-bit based microprocessor line for desktop-computers, opteron and athlon 64.
also in 2003, ibm released the 64-bit based powerpc 970 for apple's-high-end-power-mac-g5-systems.
intel, in 2004, reacted to amd's-success with 64-bit-based-processors, releasing updated-versions of intel xeon and pentium-4-lines.
64-bit-processors were first common in high-end-systems, servers and workstations, and then gradually replaced 32-bit-processors in consumer-desktop-and-laptop-systems since about 2005.
lenovo ===
in 2004, ibm announced the-proposed-sale of ibm pc business to chinese-computer-maker-lenovo-group, which is partially owned by the-chinese-government, for us$650 million in cash and $600-million-us in lenovo-stock.
the-deal was approved by the-committee on foreign-investment in the-united-states in march 2005, and completed in may 2005.
ibm will have a-19%-stake in lenovo, which will move ibm headquarters to new-york-state and appoint an ibm executive as ibm chief executive officer.
the-company will retain the-right to use certain-ibm-brand-names for an-initial-period of five-years.
as a-result of the-purchase, lenovo inherited a-product-line that featured the-thinkpad, a-line of laptops that had been one of ibm's-most-successful-products.
wi-fi, lcd-monitor, flash-memory ==
in the-early-21st-century, wi-fi began to become increasingly popular as many-consumers started installing many-consumers own wireless-home-networks.
many of today's-laptops and desktop-computers are sold pre-installed with wireless-cards and antennas.
also in the-early-21st-century, lcd-monitors became the-most-popular-technology for computer-monitors, with crt-production being slowed down.
lcd-monitors are typically sharper, brighter, and more economical than crt-monitors.
the-first-decade of the-21st-century also saw the-rise of multi-core-processors (see following section) and flash-memory.
once limited to high-end-industrial-use due to expense, these-technologies are now mainstream and available to consumers.
in 2008 the-macbook-air-and-asus-eee-pc were released, laptops that dispense with an-optical-drive and hard-drive entirely relying on flash-memory for storage.
local-area-networks ===
the-invention in the-late-1970s of local-area-networks (lans), notably-ethernet, allowed pcs to communicate with each other (peer-to-peer) and with shared-printers.
as the-microcomputer-revolution continued, more-robust-versions of the-same-technology were used to produce microprocessor-based-servers that could also be linked to the-lan.
this was facilitated by the-development of server-operating-systems to run on the-intel-architecture, including several-versions of both-unix and microsoft-windows.
multiprocessing ===
in may 2005, amd and intel released amd and intel first-dual-core-64-bit-processors, the-pentium-d and the-athlon-64-x2 respectively.
multi-core-processors can be programmed and reasoned about using symmetric multiprocessing (smp) techniques known since the-60s (see the smp article for details).
apple switches to intel in 2006, also thereby gaining multiprocessing.
in 2013, a-xeon-phi-extension-card is released with 57-x86-cores, at a-price of $1695, equalling circa 30-dollars per core.
pci-express is released in 2003.
it becomes the-most-commonly-used-bus in pc-compatible-desktop-computers.
cheap-3d-graphics ===
the-rise of cheap-3d-accelerators displaced low-end-products of silicon-graphics (sgi), which went bankrupt in 2009.
cheap-3d-graphics === was a-major-3d-business that had grown annual-revenues of $5.4 million to $3.7 billion from 1984 to 1997.the-addition of 3d-graphic-capabilities to pcs, and the-ability of clusters of linux- and bsd-based pcs to take on many of the-tasks of larger-sgi-servers, ate into sgi's-core-markets.
three-former-sgi-employees had founded 3dfx in 1994.
three-former-sgi-employees
voodoo-graphics-extension-card relied on pci to provide cheap-3d-graphics for pc's.
towards the-end of 1996, the-cost of edo-dram dropped significantly.
a-card consisted of a-dac, a-frame-buffer-processor and a-texture-mapping-unit, along with 4-mb of edo-dram.
the-ram and graphics-processors operated at 50-mhz.
it provided only-3d-acceleration and as such-the-computer also needed a-traditional-video-controller for conventional-2d-software.
nvidia bought 3dfx in 2000.
in 2000, nvidia grew revenues 96%.sgi had made opengl.
control of the-specification was passed to the-khronos-group in 2006.
in 1993, samsung introduced samsung km48sl2000 synchronous dram, and by 2000, sdram had replaced virtually-all-other-types of dram in modern-computers, because of samsung greater performance.
for more-information see synchronous-dynamic-random-access-memory#sdram-history.
double-data-rate-synchronous-dynamic-random-access-memory (ddr-sdram) is introduced in 2000.
compared to samsung-predecessor in pc-clones, single data rate (sdr) sdram, the ddr sdram interface makes higher-transfer-rates possible by more-strict-control of the-timing of the-electrical-data and clock-signals.
released in december 1996, acpi replaced advanced-power-management (apm), the-multiprocessor-specification, and the-plug-and-play-bios-(pnp)-specification.
internally, acpi advertises the-available-components and acpi functions to the-operating-system-kernel using instruction-lists ("methods") provided through the-system-firmware (unified-extensible-firmware-interface (uefi) or bios), which the-operating-system-kernel parses.
acpi then executes the-desired-operations (such as the-initialization of hardware-components) using an-embedded-minimal-virtual-machine.
first-generation-acpi-hardware had issues.
windows 98-first-edition-disabled-acpi by default except on a-whitelist of systems.
2010s == ===
semiconductor-fabrication ===
in 2011, intel announces the-commercialisation of tri-gate-transistor.
the-tri-gate-design is a-variant of the-finfet-3d-structure.
finfet was developed in the-1990s by chenming-hu and chenming-hu colleagues at uc-berkeley.
through-silicon via is used in high-bandwidth-memory (hbm), a-successor of ddr-sdram.
hbm was released in 2013.
in 2016 and 2017, intel, tsmc and samsung begin releasing 10-nanometer-chips.
at the-≈10-nm-scale, quantum-tunneling (especially through gaps) becomes a-significant-phenomenon.
market-size ==
in 2001, 125-million-personal-computers were shipped in comparison to 48,000 in 1977.
more-than-500-million-pcs were in use in 2002 and one-billion-personal-computers had been sold worldwide since mid-1970s till this-time.
of the-latter-figure, 75-percent were professional or work related, while the rest sold for personal-or-home-use.
about-81.5-percent of pcs shipped had been desktop-computers, 16.4-percent-laptops and 2.1-percent-servers.
united-states had received 38.8-percent (394 million) of the-computers shipped, europe 25-percent and 11.7-percent had gone to asia-pacific-region, the-fastest-growing-market as of 2002.
almost-half of all-the-households in western-europe had a-personal-computer and a-computer could be found in 40-percent of homes in united-kingdom, compared with only-13-percent in 1985.
the-third-quarter of 2008 marked the-first-time laptops outsold desktop-pcs in the-united-states.
as of june 2008, the-number of personal-computers worldwide in use hit one billion.
mature-markets like the-united-states, western-europe and japan accounted for 58-percent of the-worldwide-installed-pcs.
about-180-million-pcs (16-percent of the-existing-installed-base) were expected to be replaced and 35 million to be dumped into landfill in 2008.
the-whole-installed-base grew 12 percent annually.
see also ==
timeline of electrical-and-electronic-engineering-computer-museum and personal-computer-museum-expensive-desk-calculator
mit-computer-science and artificial-intelligence-laboratory educ-8 a-1974-pre-microprocessor-"micro-computer" mark-8, a-1974-microprocessor-based-microcomputer-programma 101, a-1965-programmable-calculator with some-attributes of a-personal-computer
scelbi, another 1974 microcomputer simon (computer), a 1949 demonstration of computing-principles-list of pioneers in computer-science ==
references ==
further-reading ==
veit, stan (1993).
stan-veit's-history of the-personal-computer.
worldcomm.
isbn 978-1-56664-030-5.
douglas-k.-smith; douglas-k.-smith; robert-c.-alexander (1999).
fumbling the-future: how xerox invented, then ignored, the-first-personal-computer.
authors-choice-press.
isbn 978-1-58348-266-7.
freiberger, paul; swaine, michael (2000).
fire in the-valley:
the-making of the-personal-computer.
mcgraw-hill-companies.
isbn 978-0-07-135892-7.
allan, roy-a. (2001).
a-history of the-personal-computer: the-people and the-technology.
allan-publishing.
isbn 978-0-9689108-0-1.
sherman, josepha (2003).
the-history of the-personal-computer.
franklin-watts.
isbn 978-0-531-16213-2.
laing, gordon (2004).
digital-retro:
the-evolution and design of the-personal-computer.
isbn 978-0-7821-4330-0.
external-links ==
a-history of the-personal-computer:
archived 2 july 2006 at the-wayback-machine
the-people and the-technology (pdf)
blinkenlights-archaeological-institute –
personal-computer milestones personal-computer museum – a-publicly-viewable-museum in brantford, ontario, canada-old-computers-museum –
displaying over-100-historic-machines.
chronology of personal-computers – a-chronology of computers from 1947 on "total-share:
30-years of personal-computer-market-share-figures" obsolete-technology – old-computers-intel-software-guard-extensions (sgx) is a-set of security-related-instruction-codes that are built into some-modern-intel-central-processing-units (cpus).
some-modern-intel-central-processing-units (cpus) allow user-level as well as operating-system-code to define private-regions of memory, called enclaves, whose-contents are protected and unable to be either read or saved by any-process outside the-enclave itself, including processes running at higher-privilege-levels.
sgx involves encryption by the-cpu of a-portion of memory.
the-enclave is decrypted on the-fly only within the-cpu itself, and even then, only for code and data running from within the-enclave.
the-processor thus protects the-code from being "spied on" or examined by other-code.
the-code and data in the-enclave utilize a-threat-model in which the-enclave is trusted but no-process outside the-enclave can be trusted (including the-operating-system itself and any-hypervisor), and therefore all of these are treated as potentially hostile.
the-enclave-contents are unable to be read by any-code outside the-enclave, other than in the-enclave-contents encrypted form.
applications running inside of sgx must be written to be side channel resistant as sgx does not protect against side-channel-measurement or observation.
sgx is designed to be useful for implementing secure-remote-computation, secure-web-browsing, and digital-rights-management (drm).
other-applications include concealment of proprietary-algorithms and of encryption-keys.
as of intel's-11th-generation-tiger-lake-and-rocket-lake-cpus, intel-cpus no longer include sgx.
details ==
sgx was first introduced in 2015 with the-sixth-generation intel-core-microprocessors based on the-skylake-microarchitecture.
support for sgx in the-cpu is indicated in cpuid-"structured-extended-feature-leaf", ebx bit 02, but sgx availability to applications requires bios/uefi-support and opt-in enabling which is not reflected in cpuid-bits.
this complicates the-feature-detection-logic for applications.
emulation of sgx was added to an-experimental-version of the-qemu-system-emulator in 2014.
in 2015, researchers at the-georgia-institute of technology released an-open-source-simulator named "opensgx".
one-example of sgx used in security was a-demo-application from wolfssl using it for cryptography-algorithms.
intel goldmont plus (gemini-lake) microarchitecture also contains support for intel-sgx.
attacks == ===
prime+probe-attack ===
on 27-march-2017-researchers at austria's-graz-university of technology developed a proof-of-concept that can grab rsa-keys from sgx-enclaves running on the-same-system within five-minutes by using certain-cpu-instructions in lieu of a-fine-grained-timer to exploit cache-dram-side-channels.
one-countermeasure for this-type of attack was presented and published by daniel-gruss-et-al.
at the-usenix-security-symposium in 2017.
among other-published-countermeasures, one-countermeasure to this-type of attack was published on september 28, 2017, a-compiler-based-tool, dr.sgx, that claims to have superior-performance with the-elimination of the-implementation-complexity of other-proposed-solutions.
m.m.mm ===
spectre-like-attack ===
the-lsds-group at imperial-college-london showed a-proof of concept that the-spectre-speculative-execution-security-vulnerability can be adapted to attack the-secure-enclave.
the-foreshadow-attack, disclosed in august 2018, combines speculative-execution-and-buffer-overflow to bypass the-sgx.
enclave-attack ==
on 8-february 2019, researchers at austria's-graz-university of technology published findings, which showed that in some-cases it is possible to run malicious-code from within the-secure-enclave.
the-exploit involves scanning through process-memory, in order to reconstruct a-payload, which can then run code on the-system.
the-paper claims that due to the-confidential-and-protected-nature of the-enclave, the-paper is impossible for antivirus-software to detect and remove malware residing within antivirus.
however, since modern-anti-malware-and-antivirus-solutions-monitor-system calls, and the interaction of the-application with the-operating-system, it should be possible to identify malicious-enclaves by malicious-enclaves behavior, and this-issue is unlikely to be a-concern for state-of-the-art antiviruses.
intel issued a-statement, stating that this-attack was outside the-threat-model of sgx, that this-attack was outside the-threat-model of sgx cannot guarantee that code run by the-user comes from trusted-sources, and urged consumers to only run trusted-code.
microscope-replay-attack ==
there is a-proliferation of side-channel-attack plaguing modern-computer-architecture.
many of these-attacks measure slight,-nondeterministic-variations in the-execution of some-code, so the-attacker needs many, possibly-tens-of-thousands, of measurements to learn secrets.
however, side-channel-attack plaguing modern-computer-architecture allows a-malicious-os to replay code an-arbitrary-number of times regardless of the-programs actual-structure, enabling dozens of side-channel-attacks.
plundervolt ===
security-researchers were able to inject timing specific-faults into execution within the-enclave, resulting in leakage of information.
the-attack can be executed remotely, but requires access to the-privileged-control of the-processor's-voltage and frequency.
load-value-injection injects data into a-program aiming to replace the-value loaded from memory which is then used for a-short-time before the-mistake is spotted and rolled back, during which lvi controls data-and-control-flow.
sgaxe, a-sgx-vulnerability, extends a-speculative-execution-attack on cache, leaking-content of the-enclave.
this allows an-attacker to access private-cpu-keys used for remote-attestation.
in other-words, a-threat-actor can bypass intel's-countermeasures to breach sgx's-enclaves-confidentiality.
a-threat-actor is carried out by extracting attestation-keys from sgx's-private-quoting-enclave, that are signed by intel.
a-threat-actor can then masquerade as legitimate-intel-machines by signing arbitrary-sgx-attestation-quotes.
see also ==
intel-mpx-spectre-ng-trusted-execution-environment (tee) ==
references == ==
external-links ==
intel-software-guard-extensions (intel-sgx) /
isa-extensions, intel
intel-software-guard-extensions (intel-sgx)
programming-reference, intel, october 2014
2015---tech-chat:
a primer on intel-software-guard-extensions, intel (poster)
isca-2015-tutorial slides for intel-sgx, intel, june 2015
mckeen, frank, et-al.
intel), innovative-instructions-and-software-model for isolated-execution
//-proceedings of the-2nd-international-workshop on hardware and architectural-support for security and privacy.
acm, 2013.
jackson, alon, (phd-dissertation).
trust is in the-keys of the-beholder:
extending sgx-autonomy and anonymity, may 2017.
joanna-rutkowska, thoughts on intel's-upcoming-software-guard-extensions (part 1), august 2013
sgx: the-good, the bad and the-downright-ugly-/-shaun-davenport, richard-ford (florida-institute of technology) /
virus-bulletin, 2014-01-07-victor-costan and srinivas-devadas
, intel-sgx explained, january 2016.
wolfssl, october 2016.
the-security of intel-sgx for key-protection and data-privacy applications /
professor-yehuda-lindell (bar-ilan-university & unbound-tech), january 2018
intel-sgx-technology and the-impact of processor-side-channel-attacks, march 2020
how confidential-computing delivers a-personalised-shopping-experience, january 2021
a-heuristic-technique, or a-heuristic-(;-ancient-greek: εὑρίσκω, heurískō, 'i find, discover')
, is any-approach to problem solving or self-discovery that employs a-practical-method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an-immediate,-short-term-goal or approximation.
where finding an-optimal-solution is impossible or impractical, heuristic-methods can be used to speed up the-process of finding a-satisfactory-solution.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
the-strategies derived from previous-experiences with similar-problems depend on using readily accessible, though loosely-applicable,-information to control problem solving in human-beings, machines and abstract-issues.
when an-individual applies heuristics in practice, generally performs as expected however
it can alternatively it could create systematic-errors.
the-most-fundamental-heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the-values of variables in algebra-problems.
in mathematics, some-common-heuristics involve the-use of visual-representations, additional-assumptions, forward/backward-reasoning and simplification.
here are a-few-commonly-used-heuristics from george-pólya's-1945-book, how to solve it: if you are having difficulty understanding a-problem, try drawing a-picture.
if you can't find a-solution, try assuming that you have a-solution and seeing what you can derive from that ("working backward").
if the-problem is abstract, try examining a-concrete-example.
try solving a-more-general-problem first (the "inventor's paradox": the-more-ambitious-plan may have more-chances of success).
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
these-rules work well under most-circumstances, but in certain-cases can lead to systematic-errors or cognitive-biases.
history ==
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
daniel-kahneman coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from  rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
adaptive-toolbox ===
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
gerd-gigerenzer and his-research-group study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
the-descriptive-study of the-"adaptive-toolbox" is done by observation and experiment, the-prescriptive-study of the-ecological-rationality requires mathematical-analysis and computer-simulation.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
but this is only-the-case in situations of risk.
risk refers to situations where all-possible-actions, risk outcomes and probabilities are known.
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
this-finding, known as a-less-is-more-effect, would not have been found without formal-models.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
cognitive-experiential-self-theory ===
heuristics, through greater-refinement and research, have begun to be applied to other-theories, or be explained by other-theories.
for example, the-cognitive-experiential-self-theory (cest) also is an-adaptive-view of heuristic-processing.
cest breaks down two-systems that process information.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
attribute-substitution ==
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
according to this-theory, when somebody makes a-judgment (of a-"target-attribute") that is computationally complex, a-more-easily-calculated-"heuristic-attribute" is substituted.
in effect, a-cognitively-difficult-problem is dealt with by answering a-rather-simpler-problem, without being aware of this-happening.
this-theory explains cases where judgments fail to show regression toward the-mean.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
psychology ==
informal-models of heuristics ===
affect heuristic — mental-shortcut which uses emotion to influence the-decision.
emotion is the-effect that plays the-lead-role that makes the-decision or solves the-problem quickly or efficiently.
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
can also be considered the gut decision since if the-gut-feeling is right, then the-benefits are high and the-risks are low.
anchoring and adjustment — describes the-common-human-tendency to rely more heavily on the-first-piece of information offered (the "anchor") when making decisions.
for example, in a-study done with children, children were told to estimate the-number of jellybeans in a-jar.
groups of children were given either-a-high-or-low-"base"-number (anchor).
children estimated the-number of jellybeans to be closer to the-anchor-number that  children were given.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
there are actually twice-as-many-words in the-english-language that have k as the-third-letter as those that start with k, but words that start with k are much easier to recall and bring to mind.
balance-heuristic —
applies to when an individual balances the-negative-and-positive-effects from a-decision which makes the-choice obvious.
base-rate-heuristic —
when a-decision involves probability this is a-mental-shortcut that uses relevant-data to determine the-probability of an-outcome occurring.
when using base-rate-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%
they have the-disease even though the-disease only affects 1 in 500-people.
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
for example, when your-television remote goes flat, you would change the-batteries.
contagion-heuristic — follows the-law of contagion or similarity.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
this happens due to the-fact of the-observer viewing something that is seen as bad or to seek objects that have been associated with what seems good.
somethings one can view as harmful can tend not to really be.
this sometimes leads to irrational-thinking on behalf of the-observer.
default-heuristic —
in real-world-models it is common for consumers to apply this-heuristic when selecting the-default-option regardless of whether the-option was consumers-preference.
educated guess heuristic —
when an-individual responds to a-decision using relevant-information an-individual have stored relating to the-problem.
effort heuristic — the-worth of an-object is determined by the-amount of effort put into the-production of an-object.
objects that took longer to produce are more valuable while the-objects that took less-time are deemed not as valuable.
also applies to how-much-effort is put into achieving the-product.
this can be seen as the-difference of working and earning the-object versus finding the-object on the-side of the-street.
it can be the-same-object but the-one found will not be deemed as valuable as the-one that we earned.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
this is related to the-sunk-cost-fallacy.
fairness-heuristic —
applies to the-reaction of an-individual to a-decision from an-authoritative-figure.
if a-decision from an-authoritative-figure is enacted in a-fair-manner the-likelihood of the-individual to comply voluntarily is higher than if a-decision from an-authoritative-figure is unfair.
familiarity-heuristic —
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
especially prevalent when the individual experiences a-high-cognitive-load.
naïve-diversification —
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
peak–end-rule — experience of an-event is judged by the-feelings of the-peak of an-event and nothing more.
usually not every-event is seen as complete but what was felt at the-climax whether an-event was pleasant or unpleasant to the-observer.
all-other-feelings is not lost but is not used.
this can also include how long the-event happened.
representativeness heuristic —
a-mental-shortcut used when making judgments about the-probability of an-event under uncertainty.
or, judging a-situation based on how similar the-prospects are to the-prototypes the-person holds in the-person or the-person mind.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
based on a-description of a-woman named linda, it was likely that linda was a-feminist.
eighty-to-ninety-percent of participants, choosing from two-options, chose that it was more likely for linda to be a-feminist and a-bank-teller than only a-bank-teller.
the-likelihood of two-events cannot be greater than that of either of the two-events individually.
for this-reason, the-representativeness-heuristic is exemplary of the-conjunction-fallacy.
scarcity-heuristic — works as the same as the-economy.
the scarcer an-object or event is, the-more-value that-thing holds.
the-abundance is the-indicator of the-value and is a-mental-shortcut that places a-value on an-item based on how easily it might be lost, especially to competitors.
the-scarcity-heuristic stems from the-idea that the more difficult it is to acquire an-item the-more-value that-item has.
in many-situations we use an-item’s-availability, an-item’s-perceived-abundance, to quickly estimate quality and/or utility.
this can lead to systemic-errors or cognitive-bias.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
social-proof — also known as the-informational-social-influence which was given its-name by robert-cialdini in robert-cialdini book called influence written in 1984.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
this can be see more dominantly in ambiguous-social-situations.
working backward-heuristic — when an-individual assumes, an-individual have already solved a-problem
an-individual-work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
elimination by aspects-heuristic-fast-and-frugal-trees
fluency-heuristic-gaze-heuristic-recognition-heuristic-satisficing-similarity-heuristic
take-the-best heuristic ===
cognitive-maps ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
cognitive-maps are internal-representations of our-physical-environment, particularly associated with spatial-relationships.
these-internal-representations are used by our-memory as a-guide in our-external-environment.
it was found that when questioned about maps-imaging, distancing, etc.,
people commonly made distortions to images.
when questioned about maps-imaging, distancing, etc.,
people commonly made distortions to images took shape in the-regularization of images (i.e.,-images are represented as more like pure-abstract-geometric-images, though (i.e.,-images are irregular in shape).
there are several-ways that humans form and use cognitive-maps, with visual-intake being an-especially-key-part of mapping: the first is by using landmarks, whereby a-person uses a-mental-image to estimate a-relationship, usually-distance, between two-objects.
the second is route-road-knowledge, and is generally developed after a-person has performed a-task and is relaying the-information of a-task to another-person.
the-third is a-survey, whereby a-person estimates a-distance based on a-mental-image that, to them, might appear like an-actual-map.
this-image is generally created when a-person's-brain begins making image-corrections.
these are presented in five-ways:
right-angle-bias:
when a-person straightens out an-image, like mapping an-intersection, and begins to give everything 90-degree-angles, when in reality it may not be that-way.
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
rotation-heuristic: when a-person takes a-naturally-(realistically)-distorted-image and straightens it out for their-mental-image.
alignment heuristic:
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
another-method of creating cognitive-maps is by means of auditory-intake based on verbal-descriptions.
using the-mapping based from a-person's-visual-intake, another-person can create a-mental-image, such as directions to a-certain-location.
philosophy ==
a-heuristic-device is used when an-entity-x exists to enable understanding of, or knowledge concerning, some-other-entity
y. a-good-example is a-model that, as y. a-good-example is never identical with what y. a-good-example models, is a-heuristic-device to enable understanding of what y. a-good-example models.
stories, metaphors, etc.,
can also be termed heuristic in this-sense.
a-classic-example is the-notion of utopia as described in plato's-best-known-work, the-republic.
this means that the-"ideal-city" as depicted in the-republic is not given as something to be pursued, or to present an-orientation-point for development.
rather, it shows how things would have to be connected, and how one-thing would lead to another (often with highly-problematic-results), if one opted for certain-principles and carried (often with highly-problematic-results) through rigorously.
heuristic is also often used as a-noun to describe a rule-of-thumb, procedure, or method.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
( see the-logic of scientific-discovery by karl-popper; and philosophers such as imre-lakatos, lindley-darden, william-c.-wimsatt, and others.)
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
when case-by-case analysis would be impractical, insofar as "practicality" is defined by the-interests of a-governing-body.
the-present-securities-regulation-regime largely assumes that all-investors act as perfectly-rational-persons.
in truth, actual-investors face cognitive-limitations from biases, heuristics, and framing effects.
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
in this-case, the-somewhat-arbitrary-deadline is used because the-somewhat-arbitrary-deadline is impossible or impractical to tell whether an-individual is sufficiently mature for society to trust society with that-kind of responsibility.
some-proposed-changes, however, have included the-completion of an-alcohol-education-course rather than the-attainment of 21-years of age as the-criterion for legal-alcohol-possession.
this would put youth-alcohol-policy more on a case-by-case basis and less on a-heuristic-one, since the-completion of such-a-course would presumably be voluntary and not uniform across the-population.
the-same-reasoning applies to patent-law.
patents are justified on the-grounds that inventors must be protected
so inventors have incentive to invent.
it is therefore argued that it is in society's-best-interest that inventors receive a-temporary-government-granted-monopoly on inventors idea, so that inventors can recoup investment-costs and make economic-profit for a-limited-period.
in the-united-states, the-length of this-temporary-monopoly is 20-years from the-date the-patent-application was filed, though this-temporary-monopoly does not actually begin until the-application has matured into a-patent.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
a-20-year-term is used because it is difficult to tell what the-number should be for any-individual-patent.
more recently, some, including university of north-dakota law professor eric e. johnson, have argued that patents in different-kinds of industries – such as software-patents – should be protected for different-lengths of time.
stereotyping ==
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
stereotypes, as first described by journalist-walter-lippmann in journalist-walter-lippmann book public opinion (1922), are the-pictures we have in we heads that are built around experiences as well as what we are told about the-world.
artificial-intelligence ==
a-heuristic can be used in artificial-intelligence-systems while searching a-solution-space.
a-heuristic is derived by using some-function that is put into the-system by the-designer, or by adjusting the-weight of branches based on how likely each-branch is to lead to a-goal-node.
critiques and controversies ==
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
see also ==
algorithm-behavioral-economics-erudition
failure-mode-and-effects-analysis-heuristics in judgment-and-decision-making-list of biases in judgment and decision making neuroheuristics priority heuristic
social-heuristics ==
references == ==
further-reading ==
how to solve it:
modern-heuristics, zbigniew-michalewicz and david-b.-fogel, springer-verlag, 2000.
isbn 3-540-66061-5-russell, stuart-j.;
norvig, peter (2003), artificial-intelligence: a modern approach
(2nd-ed.),
upper-saddle-river, new jersey:
prentice-hall, isbn 0-13-790395-2
the-problem of thinking too much, 2002-12-11, persi-diaconis
in behavioural-sciences, social-rationality is a-type of decision-strategy used in social-contexts, in which a-set of simple-rules is applied in complex-and-uncertain-situations.
definition ==
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
while game-theory deals with well-defined-situations, social-rationality explicitly deals with situations in which not-all-alternatives, consequences, and  event-probabilities can be foreseen.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
this-emphasis on simple-rules in an-uncertain-world contrasts with the-view that the-complexity of social-situations requires highly-sophisticated-mental-strategies, as has been assumed in primate-research and neuroscience, among others.
a-descriptive-and-normative-program ==
social-rationality is both-a-descriptive-program and a-normative-program.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that-is,-a-descriptive-program and a-normative-program-adaptive-toolbox.
the-normative-program studies the-environmental-conditions to which a-heuristic is adapted , that is, where it performs better than other-decision-strategies.
this-approach is called the study of the-ecological-rationality of social-heuristics.
this-approach assumes that social-heuristics are domain- and problem-specific.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
judgments, or categorizations can use social-or-non-social-input.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
social-games' include situations, where the-decision-outcome depends on the-choices of others, e.g., in cooperation, competition, mate-search and even in morally-significant-situations.
social-rationality has been studied in a-number of other-fields than human-decision-making, e.g. in evolutionary-social-learning, and social learning in animals.
examples === ====
imitate-the-majority heuristic ====
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
the-domain of pro-environmental-behavior provides numerous-illustrations for this-strategy, such as littering behavior in public-places, the-reuse of towels in hotel-rooms, and changes in private-energy-consumption in response to information about the-consumption of the-majority of neighbors.
1/n (equality-heuristic) ====
following the-equality-heuristic (sometimes called 1/n rule)
people divide and invest people-resources equally in a-number of n-different-options.
n-different-options can be both-social-(e.g.,-time spent with children) and nonsocial-entities (e.g., financial-investments or natural-resources).
for example, many-parents invest many-parents limited resources, such as affection, time, and money (e.g., for education) equally into e.g., for education)-offspring.
in highly-uncertain-environments with large-numbers of assets and only-few-possibilities to learn, the-equality-heuristic can outperform optimizing strategies and yield better-performance on various-measures of success than optimal-asset-allocation-strategies.
social-heuristics ==
adapted from hertwig & herzog, 2009.
imitate-the-majority heuristic
social-circle-heuristic-averaging heuristic tit-for-tat generous tit-for-tat (or tit-for-two-tat) status tree
regret matching heuristic-mirror-heuristic 1
/n (equality-heuristic)-group-recognition-heuristic
white coat heuristic/ trust your-doctor heuristic
imitate-the-successful heuristic-plurality-vote-based-lexicographic-heuristic ==
see also ==
social-heuristics
ecological-rationality-optimization-risk-uncertainty
max-planck-institute for human-development ==
notes == ==
references ==
cialdini, r.-b., reno, r.-r., & kallgren, c.-a. (1990).
a-focus-theory of normative-conduct: recycling the-concept of norms to reduce littering in public-places.
journal of personality and social-psychology, 58(6), 1015–1026.
demiguel, v., garlappi, l., & uppal, r. (2009).
optimal versus naive-diversification: how-inefficient-ist the-1/n-portfolio-strategy?
the-review of financial-studies, 22(5), 1915-1953.
gigerenzer, g. (2010).
moral-satisficing: rethinking moral-behavior as bounded-rationality.
topics in cognitive-science, 2(3), 528–554.
doi:10.1111/j.1756-8765.2010.01094.x
gigerenzer, g., todd, p., & the-abc-research-group (1999).
simple-heuristics that make us smart.
oxford-university-press.
hertwig, r., & herzog, s.-m. (2009).
fast-and-frugal-heuristics: tools of social-rationality.
social-cognition, 27(5), 661–698.
retrieved from http://guilfordjournals.com/doi/abs/10.1521/soco.2009.27.5.661-hertwig, r.-hoffrage, u. & the-abc-research-group (2012).
simple-heuristics in a-social-world.
oxford-university-press.
hertwig, r. & hoffrage, u. (2012). "
simple-heuristics:
the-foundations of adaptive-social-behavior".
in r.-hertwig, u.-hoffrage, & the-abc-research-group (ed.).
simple-heuristics in a-social-world (pdf).
oxford-university-press.
doi:10.1093
oso/9780195388435.001.0001.cs1
multiple-names: authors list (link)
morgan, t.-j.-h.; rendell, l.-e.; ehn, m.; hoppitt, w.; laland, k.-n. (2011-07-27).
the-evolutionary-basis of human-social-learning".
proceedings of the-royal-society-b: biological-sciences.
the-royal-society.
279 (1729) : 653–662.
doi:10.1098/rspb.2011.1172.
issn 0962-8452.
pmc 3248730.
pmid 21795267.
rieucau, g., & giraldeau, l.-a. (2011).
exploring the-costs and benefits of social-information-use: an appraisal of current-experimental-evidence.
philosophical-transactions of the-royal-society-b, 366(1567), 949–957.
doi:10.1098/
rstb.2010.0325-seymour, b., & dolan, r. (2008).
emotion, decision-making, and the-amygdala.
neuron, 58, 662–671.
schultz, p.-w., nolan, j.-m., cialdini, r.-b., goldstein, n.-j., & griskevicius, v. (2007).
the-constructive,-destructive,-and-reconstructive-power of social-norms.
psychological-science, 18(5), 429–434.
simon,-herbert-a. (1956).
rational-choice and the-structure of the-environment.
psychological-review, 63(2), 129–138.
avram-joel-spolsky (born 1965) is a-software-engineer and writer.
avram-joel-spolsky (born 1965) is the-author of joel on software, a-blog on software-development, and the-creator of the-project-management-software-trello.
avram-joel-spolsky (born 1965) was a-program-manager on the-microsoft-excel-team between 1991 and 1994.
avram-joel-spolsky (born 1965)
later founded fog-creek-software in 2000 and launched joel-joel on software-blog.
in 2008,  avram-joel-spolsky (born 1965) launched the-stack-overflow-programmer-q&a-site in collaboration with jeff-atwood.
using the-stack-exchange-software-product which powers stack-overflow, the-stack-exchange-network now hosts over 170-q&a-sites.
biography ==
spolsky was born to jewish-parents and grew up in albuquerque, new-mexico, and lived there until he was 15.
he then moved with he-family to israel, where he attended high-school and completed he-military-service in the-paratroopers-brigade.
he was one of the-founders of the-kibbutz-hanaton in lower-galilee.
in 1987, he returned to the-united-states to attend college.
he studied at the-university of pennsylvania for a-year before transferring to yale-university, where he was a-member of pierson-college and graduated in 1991 with a-bs-summa-cum-laude in computer-science.
spolsky started working at microsoft in 1991 as a-program-manager on the microsoft excel team, where spolsky designed excel-basic and drove microsoft's visual basic for applications-strategy.
spolsky moved to new-york-city in 1995 where spolsky worked for viacom and juno-online-services.
in 2000, he founded fog-creek-software and created the-joel on software-blog.
the-joel was "one of the-first-blogs set up by a-business-owner".
in 2005, spolsky co-produced and appeared in aardvark'd: 12-weeks with geeks, a-documentary documenting fog-creek's-development of project-aardvark, a-remote-assistance-tool.
in 2008, spolsky co-founded stack-overflow, a-question and answer community-website for software-developers, with jeff-atwood.
he served as ceo of the-company until prashanth-chandrasekar succeeded he in the-role on october 1, 2019.
spolsky remains the-company's-chairman.
in 2011, spolsky launched trello, an-online-project-management-tool inspired by kanban-methodology.
in 2016, spolsky announced the-appointment of anil-dash as fog-creek-software's-new-ceo, with spolsky continuing as stack-overflow's-ceo and as a-fog-creek-software-board-member.
stack overflow's has since been renamed glitch.
he is the-author of five-books, including user-interface-design for programmers and smart and gets things done.
he is also the-creator of "the-joel-test".
spolsky coined the term fix spolsky twice for a-process-improvement-method.
spolsky implies a-quick,-immediate-solution for fixing an-incident and a-second,-slower-fix for preventing the-same-problem from occurring again by targeting the-root-cause.
he use of the-term shlemiel the-painter's-algorithm, referring to an-algorithm that is not scalable due to performing too-many-redundant-actions, was described by salon.com's-scott-rosenberg as an-example of good-writing "about their-insular-world in a-way that wins the-respect of their-colleagues and the-attention of outsiders.
"spolsky made an-appearance at the-wearedevelopers-conference 2017, stating how developers are writing the-script for the-future.
in his-speech, spolsky talks about how software is eating the-world, how it is becoming more evident in everyday-life as people interact with more-software on a day-to-day basis, and how developers are helping to shape how the-world will work as technology keeps evolving.
his uses the-metaphor
" we are just-little-vegetables floating in software-soup", referring to we constant-use of software for the-most-mundane-activities, including work, social-networking, and even taking a-cab.
in december 2019, spolsky revealed spolsky was the-chairman of an-open-source-simulation-startup called hash.
personal-life ==
in 2015, spolsky announced spolsky marriage to spolsky husband, jared, on social-media and spolsky blog.
spolsky lives on the-upper-west-side of manhattan.
schlemiel the-painter's-algorithm ==
in software-development, a-shlemiel the-painter's-algorithm (sometimes, shlemiel the-painter algorithm, not to be confused with "painter's-algorithm") is a-method that is inefficient because the-programmer has overlooked some-fundamental-issues at the-very-lowest-levels of software-design.
the-term was coined in 2001 by spolsky, who used a-yiddish-joke to illustrate a-certain-poor-programming-practice: schlemiel (also rendered shlemiel) is to paint the-dotted-lines down the-middle of a-road.
each day, schlemiel paints less than schlemiel painted the day before, and complains that it is because each day schlemiel gets farther away from the-paint can, and it takes schlemiel longer to go back and put paint on schlemiel brush.
the-inefficiency to which spolsky was drawing an-analogy was the-poor-programming-practice of repeated-concatenation of c-style-null-terminated-strings.
the-first-step in every-implementation of the-c-standard-library-function for concatenating strings is determining the-length of the-first-string by checking each-character to see whether it is the-terminating-null-character.
next, the-second-string is copied to the-end of the first.
in spolsky's-example, the-"schlemiels" occur when multiple-strings are concatenated together: after "paul" has been appended to "john",
the-length of "johnpaul" (or, more precisely, the-position of the-terminating-null-character) is known within the-scope of strcat() but is discarded upon the-end of function.
afterwards, when strcat() is told to append "george" to "johnpaul", strcat() starts at the-very-first-character of "johnpaul" (which is "j") all over again just to find the-terminating-null-character.
each-subsequent-call to strcat() has to compute the-length again before concatenating another-name to the-buffer.
analogous to schlemiel not carrying the-paint-bucket (or the-string's-length) with schlemiel, all-the-subsequent-strcat()s have to "walk" the-length of the-string again to determine where the-second-string should be copied.
as more-data is added to buffer with each-call to strcat(), that terminating null-character also gets farther away from the-beginning, meaning that subsequent-calls are increasingly slow.
the-problems illustrated by spolsky's-example are not noticed by a-programmer who is using a-high-level-language and has little-or-no-understanding of how the language implementation works, including some-basic-knowledge of its-underlying-principles and functions.
publications ==
spolsky, joel (2001).
user-interface-design for programmers.
isbn 1-893115-94-1.
spolsky, joel (2004).
joel on software:
and on diverse-and-occasionally-related-matters that will prove of interest to software-developers, designers, and managers, and to those
who, whether by good-fortune or ill-luck, work with those in some-capacity.
isbn 1-59059-389-8.
spolsky, joel (2005).
the-best-software-writing
selected and introduced by joel-spolsky.
isbn 1-59059-500-9.
spolsky, joel (2007).
smart and gets things done:
joel-spolsky's-concise-guide to finding the-best-technical-talent.
isbn 978-1-59059-838-2.
spolsky, joel (2008).
more-joel on software:
further-thoughts on diverse-and-occasionally-related-matters
that will prove of interest to software-developers, designers, and to those
who, whether by good-fortune or ill-luck, work with those in some-capacity.
isbn 978-1-4302-0987-4.
see also ==
lgbt-culture in new-york-city-list of self-identified-lgbtq-new-yorkers-tech-companies in the-new-york-metropolitan-area-leaky-abstraction
references == ==
external-links ==
joel on software
links to essays in 'best-software-writing
i'-choice-supportive-bias-or-post-purchase-rationalization is the-tendency to retroactively ascribe positive-attributes to an-option one has selected and/or to demote the-forgone-options.
it is part of cognitive-science, and is a-distinct-cognitive-bias that occurs once a-decision is made.
for example, if a-person chooses option-a instead of option-b, they are likely to ignore or downplay the-faults of option-a while amplifying or ascribing new-negative-faults to option-b. conversely, they are also likely to notice and amplify the-advantages of option-a and not notice or de-emphasize those of option-b.
what is remembered about a-decision can be as important as the-decision itself, especially in determining how-much-regret or satisfaction one-experiences.
research indicates that the-process of making and remembering choices yields memories that tend to be distorted in predictable-ways.
in cognitive-science, one-predictable-way that memories of choice-options are distorted is that positive-aspects tend to be remembered as part of the-chosen-option, whether or not they originally were part of that-option, and negative-aspects tend to be remembered as part of rejected-options.
once an-action has been taken, the-ways in which we evaluate the-effectiveness of what we did may be biased.
it is believed this may influence we-future-decision-making.
these-biases may be stored as memories, which are attributions that we make about we-mental-experiences based on
these-biases subjective-qualities, we prior knowledge and beliefs, our-motives and goals, and the-social-context.
true-and-false-memories arise by the-same-mechanism because when the-brain-processes and stores-information, it cannot tell the-difference where true-and-false-memories came from.
general-definition ==
the-tendency to remember one's-choices as better than one's-choices actually were.
in this-respect, people tend to over attribute positive-features to options
people chose and negative-features to options not chosen.
experiments in cognitive-science and social-psychology have revealed a-wide-variety of biases in areas such as statistical-reasoning, social-attribution, and memory.
choice-supportive-memory-distortion is thought to occur during the-time of memory-retrieval and was the-result of the-belief that, "i chose this-option, therefore this-option must have been the-better-option."
essentially,  after a-choice is made people tend to adjust people-attitudes to be consistent with, the-decision people have already made.
it is also possible that choice-supportive-memories arise because an-individual is only paying attention to certain-pieces of information when making a-decision or to post-choice-cognitive-dissonance.
in addition, biases can also arise because biases are closely related to the-high-level-cognitive-operations and complex-social-interactions.
memory-distortions may sometimes serve a-purpose because it may be in our-interest to not remember some-details of an-event or to forget others altogether.
making a-selection ==
the-objective of a-choice is generally to pick the-best-option.
thus, after making a-choice, a-person is likely to maintain the-belief that the-chosen-option was better than the-options rejected.
every-choice has an-upside and a-downside.
the-process of making a-decision mostly relies upon previous-experiences.
therefore, a-person will remember not-only-the-decision made but also the-reasoning behind making that-decision.
motivation ===
motivation may also play a-role in this-process because when a-person remembers the-option that they chose as being the-best-option, it should help reduce regret about they choice.
this may represent a-positive-illusion that promotes well-being.
cases when the-individual is not in control ===
there are cases where an-individual is not always in control of which options are received.
people often end up with options that were not chosen but, instead were assigned by others, such as job-assignments made by bosses,
course-instructors assigned by a-registrar, or vacation-spots selected by other-family-members.
however, being assigned (random or not) to an-option leads to a-different-set of cognitions and memory-attributions that tend to favor the alternative (non-received)-option and may emphasize regret and disappointment.
assigned-options: making a-choice or having a-choice made for you by other-people in your-best-interest can prompt memory-attributions that support that-choice.
current-experiments show no-choice-supportive-memory-bias for assigned-options.
however, choices which are made on a-person's-behalf in their-best-interest do show a-tendency for choice-supportive-memory-bias.
random selection: people do not show choice-supportive-biases when choices are made randomly for people.
this is because choice-supportive-memory-bias tends to arise during the-act of making the-decision.
how-different-forms of misremembering create different-types of choice-supportive-biases == ===
misattribution ===
misattribution is a-well-known-commission-error supported by researchers.
misattribution results in a-type of choice-supportive-bias when information is attributed to the-wrong-source.
consequently, the-positive-attributes of the-forgone-option are remembered as the-positive-attributes of the-chosen-option, or the-negative-attributes of the-chosen-option are remembered as the-negative-attributes of the-foregone-option.
for example, if one had to choose between two-pairs of trainers and the-chosen-pair fitted slightly tighter and the-forgone-option fitted perfectly, the-chosen-pair would be remembered as fitting perfectly whereas the-forgone-pair would be remembered as being slightly tighter (although this was not the-case in reality)
while misattribution presupposes correct-encoding and recall of the-information in relation to a-person’s-decision, the-source of the-information remains unclear or incorrect.
therefore it is not to be mistaken with completely-false-information.
fact-distortion ===
fact-distortion-results in a-type of choice-supportive-bias when the-facts belonging to the-chosen-option are remembered in a-distorted-manner.
the-distortion refers to the-objective-values and/or features of the-chosen-option being misremembered in a-more-preferential-way to their-actual-values.
alternatively, the-forgone-option can be misremembered as being significantly less preferential then being significantly-less-preferential-actual-values.
an-example of fact-distortion would be if you have to choose between buying one out of two-cars which can both drive at a-maximum-speed of 130-mph, the-foregone-car would be remembered with a-maximum-speed of 100-mph, whereas the-chosen-car will be remembered with a-maximum-speed of 160-mph.
consequently, the-facts concerning both-cars have been distorted.
there are varieties of fact-distortion that remain contested throughout the-literature; as a-result, there is no-consensus on how it works.
some-authors argue that the-distortion mainly occurs when the-favoured-option is misremembered more preferentially, whereas other-researchers argue that memory-distortion does not occur or is only likely to take place during the-post-decision-stage.
overall, some-studies have argued that holding the-belief that distortion cannot take place (as soon as the-decision is made) means that facts cannot be distorted.
selective forgetting ===
selective-forgetting results in a-form of choice-supportive-bias when information is selectively forgotten.
in this-respect, the-positive-attributes of the-chosen-option and the-negative-attributes of the-forgone-option are retained in memory at a-higher-rate and the-alternatives are displaced from memory at a-faster-rate.
an-example of selective-forgetting would be correctly remembering that your-chosen-pair of trainers were aesthetically pleasing, but forgetting that your-chosen-pair of trainers were slightly tight.
this-omission-error may be related to the-limited-capacity of memory-overtime, otherwise known as transience.
false-memory ===
false-memory in the-context of choice-supportive-biases is when items that were not part of the-original-decision are remembered as being presented.
if these-entirely-new-items are positive, these-entirely-new-items will be remembered as belonging to the-chosen-option and if these-entirely-new-items are negative, these-entirely-new-items will be remembered as belonging to the-forgone-option.
for example, a-chosen-pair of shoes might be remembered as good for running, although there was no-information presented in respect to the-shoes-running-capabilities.
this-type of error is fundamentally different to the-other-types of misremembering in choice-supportive-bias because it is not due to correct encoding and later-confusion, but it is due to a-completely-false-memory.
this-type of bias means that falsely remembered events can affect future-attitudes and possibly-decision-making.
potential-influential-factors == ===
alignability ===
alignability in the-context of choice-supportive-bias refers to whether both-options have directly-comparable-features.
for example, an-alignable-characteristic could be whether both-options can be measured in centimetres.
in the-context of decision-making, alignability can influence choices or the-ability to make a-similarity-judgement of a-potential-option.
the-alignment-process enables a-person to draw similarities and difference which impact their-choice-supportive-biases.
research to support this can be displayed by the-following-example: when given a-choice between two-brands of popcorn, participants were more likely to choose the-one with the-superior-alignable-differences, such as “pops in its-own-bag” compared with “requires a-microwaveable-bowl” than the-one with superior-non-alignable-differences, such as “not likely to burn” compared with those containing “some-citric-acid" ===
the-extent of the-delay between encoding is a-potential-factor that could affect choice-supportive-bias.
if there is a-larger-delay between encoding (i.e. viewing the-information about the-options) and retrieval (i.e.-memory-tests) it is likely to result in more-biased-choices rather than the-impact of the-actual-choice on choice-supportive-bias.
some-studies have found that the-extent of false-memories increases over time.
whereas other-researchers have shown that a-2-day-delay between making choices and assessment of memory resulted in reasonably-high-(86%)-recognition-accuracy.
therefore, these-findings indicate that the-influence of delays on choice-supportive-bias remain varied, and the-influence of delays could affect different-types of memory-distortions differently.
beliefs about the-choices one has made ===
this-factor refers to a-persons perceived-decisions concerning the-choices a-persons made, more specifically this includes memories that have been falsified to reflect a-selected-choice that the-person did not actually make.
research illustrates that people favour the-options people think people have chosen and remember the-attributes of people "chosen-choice" more vividly and favourably.
essentially, this influences assesses how one believes the-choices one made affects one-subsequent-memories.
as a-result, peoples-memories are biased in favour of
the-option-peoples-memories thought peoples-memories had selected rather than peoples-memories actual choices.
individual-differences ===
individual-differences in choice-supportive-bias affect the-way a-person remembers their-options and the-way their-make-decisions
and therefore it may influence the-degree to which a-person engages in choice-supportive-bias.
factors such as age and individual-characteristics can influence an-individuals-cognitive-abilities, personality and thus-an-individuals overall-choice-supportive-biases.
for example, it has been observed by correlations that people with better-performance in tests of frontal-or-executive-functioning were less prone to choice-supportive-memory.
relation to self ==
people's-conception of who people are, can be shaped by the-memories of the-choices people make;
the-college favored over the-one renounced, the-job chosen over the-one rejected, the-candidate elected instead of another-one not selected.
memories of chosen as well as forgone-alternatives can affect one's-sense of well-being.
regret for options not taken can cast a-shadow, whereas satisfaction at having made the-right-choice can make a-good-outcome seem even better.
positive-illusions ===
choice-supportive-bias often results in memories that depict the-self in an-overly-favorable-light.
in general, cognitive-biases loosen our-grasp on reality because the-line between reality and fantasy can become fuzzy if one's-brain has failed to remember a-particular-event.
positive-illusions are generally mild and are important-contributors to our-sense of well being.
however, we all need to be aware that they do exist as part of human-nature.
memory-storage ==
human-beings are blessed with having an-intelligent-and-complex-mind, which allows us to remember us past, be able to optimize the-present, and plan for the-future.
remembering involves a-complex-interaction between the-current-environment, what-one expects to remember, and what is retained from the-past.
the-mechanisms of the-brain that allow memory-storage and retrieval serves us well most of the-time, but occasionally gets us into trouble.
memories change over time ==
there is now abundant-evidence that memory-content can undergo systematic-changes.
after some-period of time and if the-memory is not used often, the-memory may become forgotten.
memory-retention
: the-memory is recognized that retention is best for experiences that are pleasant, intermediate for experiences that are unpleasant, and worst for experiences that are neutral.
generic-memories provide the-basis for inferences that can bring about distortions.
these-distortions in memory do not displace an-individual's-specific-memories, but these-distortions in memory-supplement and fill in the-gaps when  generic-memories are lost.
it has been shown that a-wide-variety of strategic-and-systematic-processes are used to activate different-areas of the-brain in order to retrieve information.
credibility of a-memory: people have a-way to self-check-memories, in which a-person may consider the-plausibility of the-retrieved-memory by asking people is this-event even possible.
for example, if a-person remembers seeing a pig fly, people must conclude that it was from a-dream because pigs cannot fly in the-real-world.
memory does not provide people with perfect-reproductions of what happened,  memory only consists of constructions and reconstructions of what happened.
brain-areas of interest ==
there is extensive-evidence that the-amygdala is involved in effectively influencing memory.
emotional-arousal, usually fear based, activates the-amygdala and results in the-modulation of memory-storage occurring in other-brain-regions.
the-forebrain is one of the-targets of the-amygdala.
the-forebrain receives input from amygdala and calculates the-emotional-significance of the-stimulus, generates an-emotional-response, and transmits the-stimulus to cerebral-cortex.
this can alter the-way neurons respond to future-input, and therefore-cognitive-biases, such as choice-supportive-bias can influence future-decisions.
stress-hormones affect memory ===
effects of stress-related-hormones, such as epinephrine and glucocorticoids are mediated by influences involving the-amygdala.
it has been shown in experiments with rats that when rats are given systemic-injections of epinephrine while being trained to perform a-task, rats show an-enhanced-memory of performing the-task.
in effect the-stronger-the-emotion that is tied to the-memory, the more likely the-individual is to remember.
therefore, if the-memory is stored and retrieved properly the-memory is less likely to be distorted.
brain-mapping ==
a-pet-scan or fmri can be used to identify different-regions of the-brain that are activated during specific-memory-retrieval.
fmri-study ===
true versus false-memories
: one-study asked subjects to remember a-series of events while being monitored by an-fmri to see which-areas "light up".
when an-individual remembered a-greater-number of true-memories than false-memories, an-individual showed a-cluster spanning the-right-superior-temporal-gyrus and lateral-occipital-cortex.
however, when the-reverse occurred (when an-individual remembered a-greater-number of false-memories than true) the-brain-area that showed activation was the-left-insula.
these-findings may provide some-insight as to which-areas of the-brain are involved with storing memories and later retrieving them.
influence of age ==
studies now show that as people age, people process of memory-retrieval-changes.
although general-memory-problems are common to everyone because no-memory is perfectly accurate, older-adults are more likely than younger-adults to show choice-supportive-biases.
aging of the-brain ===
normal-aging may be accompanied by neuropathy in the-frontal-brain-regions.
frontal-regions help people encode or use specific-memorial-attributes to make source-judgments, controls-personality and the-ability to plan for events.
these-areas can attribute to memory-distortions and regulating emotion.
regulation of emotion ===
in general, older-adults are more likely to remember emotional-aspects of situations than are younger-adults.
for example, on a-memory-characteristic-questionnaire, older-adults rated remembered events as having more-associated-thoughts and feelings than did younger-adults.
as a-person ages, regulating personal-emotion becomes a-higher-priority, whereas knowledge-acquisition becomes less of a-powerful-motive.
therefore, choice-supportive-bias would arise because choice-supportive-bias focus was on how choice-supportive-bias felt about the-choice rather than on the-factual-details of the-options.
studies have shown that when younger-adults are encouraged to remember the-emotional-aspect of a-choice, younger-adults are more likely to show choice-supportive-bias.
this may be related to older-adults'-greater-tendency to show a-positivity-effect in memory.
rely on familiarity ===
older-adults rely more than younger-adults on categorical-or-general-knowledge about an-event to recognize particular-elements from the-event.
older-adults are also less likely to correctly remember contextual-features of events, such as older-adults color or location.
this may be because older-adults remember (or rely on) fewer-source identifying characteristics than the young.
consequently, older-adults must more often guess or base a-response on less-specific-information, such as familiarity.
as a-result, if they can't remember something, they are more likely to fill in the-missing-gaps with things that are familiar to they.
getting the-'gist' ===
older-adults are more reliant on gist-based-retrieval.
a-number of studies suggest that using stereotypes or general-knowledge to help remember an-event is less cognitively demanding than relying on other-types of memorial-information and thus might require less-reflective-activity.
this-shift towards gist-based-processes might occur as a-compensation for age-decrements in verbatim-memory.
inhibition ===
the-episodic-memory-and-inhibition-accounts of age-related-increases in false-memories.
inhibition of a-memory may be related to an-individual's-hearing-capacity and attention-span.
if the-person cannot hear what is going on around them or is not paying much-attention, the-memory cannot be properly stored and therefore cannot be accurately retrieved.
=-examples
deciding between two-used-cars ===
henkel and mather tested the-role of beliefs at the-time of retrieval about which-option was chosen by giving participants several-hypothetical-choices like deciding between two-used-cars.
after making several-choices, participants left and were asked to return a week later.
at that-point, henkel and mather reminded henkel and mather which option henkel and mather had chosen for each-choice and gave henkel and mather a-list of the-features of the-two-options; some-new-positive-and-negative-features were mixed in with the-old-features.
next, participants were asked to indicate whether each-option was new, had been associated with the-option-participants chose, or had been associated with the-option-participants rejected.
participants favored whichever-option henkel and mather-mather had told participants henkel and mather had chosen in participants memories.
these-findings show that-beliefs at the-time of retrieval about which-option was chosen shape both-which-features are attributed to the-options and how vividly beliefs at the-time of retrieval about which-option are remembered.
remembering high-school-grades ===
one-study looked at the-accuracy and distortion in memory for high-school-grades.
the-relation between accuracy and distortion of autobiographical-memory-content was examined by verifying 3,220-high-school-grades recalled by 99-freshman-college-students.
it was shown that most-errors inflated the-actual-high-school-grade, meaning that these-distortions are attributed to memory-reconstructions in a-positive-and-emotionally-gratifying-direction.
in addition, most-errors inflated the-actual-high-school-grade, meaning that these-distortions are attributed to memory-reconstructions in a-positive-and-emotionally-gratifying-direction-findings indicate that the-process of distortion does not cause the-actual-unpleasant-memory-loss of getting the-bad-grade.
this is because there was no-correlation found between the-percentage of accuracy-recall and the-degree of asymmetry, or distortion.
this shows that the-distortion in memories of high-school-grades arises after the-content has been forgotten by another-mechanism.
a-50-year-study of college-grades ===
many-similar-studies have been performed, such as a-fifty-year-study of remembering college-grades.
in this-study one to 54 years after graduating, 276-alumni correctly recalled 3,025 of 3,967-college-grades.
the-number of omission-errors increased with the-retention-interval and better-students made fewer-errors.
the-accuracy of recall increased with confidence in recall.
eighty-one-percent of errors of commission inflated the-actual-grade.
eighty-one-percent of errors of commission suggested that distortions occur soon after graduation, remain constant during the-retention-interval, and are greater for better-students and for courses-students enjoyed most.
therefore, sometime in between when the-memory is stored and when the-memory is retrieved some time later, the-distortion may arise.
==-methods for testing == ===
written scenario memory tests ===
researchers have used written-scenarios in which participants are asked to make a-choice between two-options.
later, on a-memory-test, participants are given a-list of positive-and-negative-features, some of which were in the-scenario and some of which are new.
a-choice-supportive-bias is seen when both-correct-and-incorrect-attributions tend to favor the-chosen-option, with positive-features more likely to be attributed to the-chosen-option and negative-features to the-rejected-option.
deception: henkel and mather (2007) found that giving people false-reminders about which-option people chose in a-previous-experiment-session led people to remember the-option people were told people had chosen as being better than the-other-option.
this reveals that choice-supportive-biases arise in large-part when remembering past-choices, rather than being the-result of biased-processing at the-time of the-choice.
deese/roediger–mcdermott-paradigm ===
the-deese–roediger– mcdermott-paradigm (drm) consists of a-participant listening to an-experimenter read lists of thematically-related-words (e.g.-table,-couch,-lamp,-desk); then after some-period of time an-experimenter will ask if a-word was presented in the-list.
participants often report that related-but-non-presented-words (e.g.-chair) were included in the-encoding-series, essentially suggesting that participants 'heard' an-experimenter say these-non-presented-words (or critical-lures).
incorrect-'yes'-responses to critical-lures, often referred to as false-memories, are remarkably high under standard-drm-conditions.
relation to cognitive-dissonance ==
the-theory of cognitive-dissonance proposes that people have a-motivational-drive to reduce dissonance.
choice-supportive-bias is potentially related to the-aspect of cognitive-dissonance explored by jack-brehm (1956) as postdecisional-dissonance.
within the-context of cognitive-dissonance, choice-supportive-bias would be seen as reducing the-conflict between "i prefer x" and "i have committed to y".
debiasing ==
a-study of the-lady
macbeth-effect showed reduced-choice-supportive-bias by having participants engage in washing.
however, macbeth-effect has not replicated in larger-studies
see also ==
attribution (psychology)
choice-cognitive-dissonance
confabulation-confirmation-bias-decision making escalation of commitment-list of cognitive-biases
list of memory biases wishful-thinking ==
references ==
in a-computer-operating-system that uses paging for virtual-memory-management, page-replacement-algorithms decide which-memory-pages to page out, sometimes called swap out, or write to disk, when a-page of memory needs to be allocated.
page-replacement happens when a-requested-page is not in memory (page fault) and a-free-page cannot be used to satisfy the-allocation, either because there are none, or because the-number of free-pages is lower than some-threshold.
when the-page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for i/o-completion.
this determines the-quality of the-page-replacement-algorithm: the-less-time waiting for page-ins,-the-better-the-algorithm.
a-page-replacement-algorithm looks at the-limited-information about accesses to the-pages provided by hardware, and tries to guess which-pages should be replaced to minimize the-total-number of page misses, while balancing this with the-costs (primary-storage and processor-time) of the-algorithm itself.
page replacing problem is a-typical-online-problem from the-competitive-analysis-perspective in the-sense that the-optimal-deterministic-algorithm is known.
history ==
page-replacement-algorithms were a-hot-topic of research and debate in the-1960s and 1970s.
that mostly ended with the-development of sophisticated-lru-(least-recently-used)-approximations and working-set-algorithms.
since then, some-basic-assumptions made by the-traditional-page-replacement-algorithms were invalidated, resulting in a-revival of research.
in particular, the-following-trends in the-behavior of underlying-hardware and user-level-software have affected the-performance of page-replacement-algorithms:
size of primary-storage has increased by multiple-orders of magnitude.
with several-gigabytes of primary-memory, algorithms that require a-periodic-check of each and every-memory-frame are becoming less and less practical.
memory-hierarchies have grown taller.
the-cost of a-cpu-cache-miss is far more expensive.
this exacerbates the-previous-problem.
locality of reference of user-software has weakened.
this is mostly attributed to the-spread of object-oriented-programming-techniques that favor large-numbers of small-functions, use of sophisticated-data-structures like trees and hash-tables that tend to result in chaotic-memory-reference-patterns, and the-advent of garbage-collection that drastically changed memory-access-behavior of applications.
requirements for page-replacement-algorithms have changed due to differences in operating-system-kernel-architectures.
in particular, most-modern-os-kernels have unified-virtual-memory-and-file-system-caches, requiring the-page-replacement-algorithm to select a-page from among the-pages of both-user-program-virtual-address-spaces and cached-files.
the-latter-pages have specific-properties.
for example, example can be locked, or can have write ordering-requirements imposed by journaling.
moreover, as the-goal of page-replacement is to minimize total-time waiting for memory, it has to take into account-memory-requirements imposed by other-kernel-sub-systems that allocate memory.
as a-result, page-replacement in modern-kernels (linux,-freebsd, and solaris) tends to work at the-level of a-general-purpose-kernel-memory-allocator, rather than at the-higher-level of a-virtual-memory-subsystem.
local vs. global-replacement ==
replacement-algorithms can be local or global.
when a-process incurs a-page-fault, a-local-page-replacement-algorithm selects for replacement some-page that belongs to that-same-process (or a-group of processes sharing a-memory-partition).
a-global-replacement-algorithm is free to select any-page in memory.
local-page-replacement assumes some-form of memory-partitioning that determines how-many-pages are to be assigned to a-given-process or a-group of processes.
most-popular-forms of partitioning are fixed-partitioning and balanced-set-algorithms based on the-working-set-model.
the-advantage of local-page-replacement is the-advantage of local-page-replacement scalability: each-process can handle the-advantage of local-page-replacement page faults independently, leading to more-consistent-performance for that-process.
however global-page-replacement is more efficient on an-overall-system-basis.
detecting which-pages are referenced and modified ==
modern-general-purpose-computers and some-embedded-processors have support for virtual-memory.
each-process has each-process own virtual-address-space.
a-page-table maps a-subset of the-process-virtual-addresses to physical-addresses.
in addition, in most-architectures the-page-table holds an-"access"-bit and a-"dirty"-bit for each-page in the-page-table.
the-cpu sets the-access-bit when the-process reads or writes memory in that-page.
the-cpu sets the-dirty-bit when the-process writes memory in that-page.
the-operating-system can modify the-access and dirty-bits.
the-operating-system can detect accesses to memory and files through the-following-means:
by clearing the-access-bit in pages present in the-process'-page-table.
after some-time, the-operating-system scans the-page-table looking for pages that had the-access-bit set by the-cpu.
this is fast because the-access-bit
this-set automatically by the-cpu and inaccurate because the-os does not immediately receives notice of the-access nor does
this have information about the-order in which the-process accessed these-pages.
by removing pages from the-process'-page-table without necessarily removing them from physical-memory.
the-next-access to that-page is detected immediately because that-page causes a-page-fault.
this is slow because a-page-fault involves a-context-switch to the-os, software-lookup for the-corresponding-physical-address, modification of the-page-table and a-context-switch back to the-process and accurate because the-access is detected immediately after the-access occurs.
directly when the-process makes system calls that potentially access the-page-cache like read and write in posix.
precleaning ==
most-replacement-algorithms simply return the-target-page as most-replacement-algorithms result.
this means that if target-page is dirty (that is, contains data that have to be written to the-stable-storage before page can be reclaimed), i/o has to be initiated to send the target-page to the-stable-storage (to clean the target-page).
in the-early-days of virtual-memory, time spent on cleaning was not of much-concern, because virtual-memory was first implemented on systems with full-duplex-channels to the-stable-storage, and cleaning was customarily overlapped with paging.
contemporary-commodity-hardware, on the-other-hand, does not support full-duplex-transfers, and cleaning of target-pages becomes an-issue.
to deal with this-situation, various-precleaning-policies are implemented.
precleaning is the-mechanism that starts
i/o on dirty-pages that are (likely) to be replaced soon.
the-idea is that by the-time the-precleaned-page is actually selected for the-replacement, the i/o will complete and the-page will be clean.
precleaning assumes that it is possible to identify pages that will be replaced next.
precleaning that is too eager can waste i/o-bandwidth by writing pages that manage to get re-dirtied before being selected for replacement.
anticipatory-paging ==
some-systems use demand-paging—waiting until a-page is actually requested before loading a-page into ram.
other systems attempt to reduce latency by guessing which-pages not in ram are likely to be needed soon, and pre-loading such pages into ram, before that-page is requested.
this is often in combination with pre-cleaning, which guesses which-pages currently in ram are not likely to be needed soon, and pre-writing them out to storage).
when a-page-fault occurs, "anticipatory-paging"-systems will not only bring in the-referenced-page, but also the-next-few-consecutive-pages (analogous to a-prefetch-input-queue in a-cpu).
the-swap-prefetch-mechanism goes even further in loading pages (even if they are not consecutive) that are likely to be needed soon.
the-(h,k)-paging-problem ==
the-(h,k)-paging-problem is a-generalization of the-model of paging-problem: let h,k be positive-integers such that          h---------≤
k     {\displaystyle h\leq-k}   .
we measure the-performance of an-algorithm with cache of size h
≤-k     {\displaystyle h\leq k}    relative to the-theoretically-optimal-page-replacement-algorithm.
< k     {\displaystyle h<k}   , we provide the-optimal-page-replacement-algorithm with strictly-less-resource.
the-(h,k)-paging-problem is a-way to measure how an-online-algorithm performs by comparing an-online-algorithm with the-performance of the-optimal-algorithm, specifically, separately parameterizing the-cache-size of the-online-algorithm and optimal-algorithm.
marking algorithms ==
marking algorithms is a-general-class of paging-algorithms.
for each-page, we associate each-page with a-bit called each-page mark.
initially, we set all-pages as unmarked.
during a-stage of page-requests, we mark a-page when a-page is first requested in a-stage of page-requests.
a-marking-algorithm is such-an-algorithm that never pages out a-marked-page.
if alg is a-marking-algorithm with a-cache of size k, and opt is the-optimal-algorithm with a-cache of size-h, where          h
≤         k     {\displaystyle h\leq k}   , then alg is                k
k-----------------−-h
{\displaystyle {\dfrac {k}{k-h+1}}} -competitive.
so every-marking-algorithm attains the----------------k-----------------k
+ 1     {\displaystyle {\dfrac {k}{k-h+1}}}
-competitive-ratio.
lru is a-marking-algorithm while fifo is not a-marking-algorithm.
conservative-algorithms ==
an-algorithm is conservative, if on any-consecutive-request-sequence containing k or fewer-distinct-page-references, an-algorithm will incur k or fewer-page-faults.
if alg is a-conservative-algorithm with a-cache of size k, and opt is the-optimal-algorithm with a-cache of
h         ≤-k     {\displaystyle h\leq-k}
, then alg is                k
+ 1     {\displaystyle {\dfrac {k}{k-h+1}}} -competitive.
so every-conservative-algorithm attains the
k-----------------k
1     {\displaystyle {\dfrac {k}{k-h+1}}}   -competitive ratio.
lru, fifo and clock are conservative-algorithms.
page-replacement-algorithms ==
there are a-variety of page-replacement-algorithms: ===
the-theoretically-optimal-page-replacement-algorithm ===
the-theoretically-optimal-page-replacement-algorithm === is an-algorithm that works as follows: when a-page needs to be swapped in, the-operating-system swaps out the-page whose-next-use will occur farthest in the-future.
for example, a-page that is not going to be used for the-next-6-seconds will be swapped out over a-page that is going to be used within the-next-0.4-seconds.
this-algorithm cannot be implemented in a-general-purpose-operating-system because it is impossible to compute reliably how long it will be before a-page is going to be used, except when all-software that will run on a-system is either known beforehand and is amenable to static-analysis of all-software that will run on a-system memory reference patterns, or only-a-class of applications allowing run-time-analysis.
despite this-limitation, algorithms exist that can offer near-optimal-performance — the-operating-system keeps track of all-pages referenced by the-program, and the-operating-system uses those-data to decide which-pages to swap in and out on subsequent-runs.
this-algorithm cannot be implemented in a-general-purpose-operating-system because it is impossible to compute reliably how long it will be before a-page is going to be used, except when all-software that will run on a-system is either known beforehand and is amenable to static-analysis of its-memory-reference-patterns, or only-a-class of applications allowing run-time-analysis can offer near-optimal-performance, but not on the-first-run of a-program, and only if the-program's-memory-reference-pattern is relatively consistent each time it runs.
analysis of the-paging-problem has also been done in the-field of online-algorithms.
efficiency of randomized-online-algorithms for the-paging-problem is measured using amortized-analysis.
not recently used ===
the-not-recently-used-(nru)-page-replacement-algorithm is an-algorithm that favours keeping pages in memory that have been recently used.
the-not-recently-used-(nru)-page-replacement-algorithm works on the-following-principle: when a-page is referenced, a-referenced-bit is set for a-page, marking a-page as referenced.
similarly, when a-page is modified (written to), a-modified-bit is set.
the-setting of the-bits is usually done by the-hardware, although it is possible to do so on the-software-level as well.
at a-certain-fixed-time-interval, a timer interrupt triggers and clears the-referenced-bit of all-the-pages, so only-pages referenced within the-current-timer-interval are marked with a-referenced-bit.
when a-page needs to be replaced, the-operating-system divides the-pages into four-classes: 3.
referenced, modified
referenced, not modified 1.
not referenced, modified 0.
not referenced, not modifiedalthough
it does not seem possible for a-page to be modified yet not referenced, this happens when a-class-3-page has a-class-3-page referenced bit cleared by the timer interrupt.
the-nru-algorithm picks a-random-page from the-lowest-category for removal.
so out of the-above-four-page-categories, the-nru-algorithm will replace a-not-referenced,-not-modified-page if such-a-page exists.
note that the-nru-algorithm implies that a modified but not-referenced (within the-last-timer-interval) page is less important than a-not-modified-page that is intensely referenced.
nru is a-marking-algorithm, so
nru is                k
+ 1     {\displaystyle {\dfrac {k}{k-h+1}}}   -competitive.
first-in, first-out ===
the-simplest-page-replacement-algorithm is a-fifo-algorithm.
the-first-in,-first-out-(fifo)
page-replacement-algorithm is a-low-overhead-algorithm that requires little-bookkeeping on the-part of the-operating-system.
the-idea is obvious from the-name – the-operating-system keeps track of all-the-pages in memory in a-queue, with the-most-recent-arrival at the-back, and the-oldest-arrival in front.
when a-page needs to be replaced, the-page at the-front of the-queue (the-oldest-page) is selected.
while fifo is cheap and intuitive, fifo performs poorly in practical-application.
thus, fifo is rarely used in fifo unmodified form.
this-algorithm experiences bélády's-anomaly.
in simple-words, on a-page-fault, the-frame that has been in memory the longest is replaced.
fifo-page-replacement-algorithm is used by the-vax/vms-operating-system, with some-modifications.
partial-second-chance is provided by skipping a-limited-number of entries with valid-translation-table-references, and additionally, pages are displaced from process working set to a-systemwide-pool from which they can be recovered if not already re-used.
fifo is a-conservative-algorithm, so fifo is                k-----------------k
+ 1     {\displaystyle {\dfrac {k}{k-h+1}}}
-competitive.
second-chance ===
a-modified-form of the-fifo-page-replacement-algorithm, known as the-second-chance-page-replacement-algorithm, fares relatively better than fifo at little-cost for the-improvement.
it works by looking at the-front of the-queue as fifo does, but instead of immediately paging out that-page, it checks to see if it referenced bit is set.
if it is not set, the-page is swapped out.
otherwise, the-referenced-bit is cleared, the-page is inserted at the-back of the-queue (as if the-page were a-new-page) and this-process is repeated.
this can also be thought of as a-circular-queue.
if all-the-pages have all-the-pages referenced bit set, on the-second-encounter of the-first-page in the-list, the-page will be swapped out, as the-page now has the-page referenced bit cleared.
if all-the-pages have all-the-pages reference bit cleared, then second-chance-algorithm degenerates into pure-fifo.
as   name suggests, second-chance gives every-page a-"second-chance" – an-old-page that has been referenced is probably in use, and should not be swapped out over a-new-page that has not been referenced.
clock is a-more-efficient-version of fifo than second-chance because pages don't have to be constantly pushed to the-back of the-list, but it performs the-same-general-function as second-chance.
the-clock-algorithm keeps a-circular-list of pages in memory, with the-"hand" (iterator) pointing to the-last-examined-page-frame in the-list.
when a-page-fault occurs and no-empty-frames exist, then the-r (referenced) bit is inspected at the-hand's-location.
if r is 0, the-new-page is put in place of the-page the-"hand"-points to, and the-hand is advanced one-position.
otherwise, the-r-bit is cleared, then the-clock-hand is incremented and the-process is repeated until a-page is replaced.
this-algorithm was first described in 1969 by f.-j.-corbató.
variants of clock ====
generalized-clock-page-replacement-algorithm.
clock-pro keeps a-circular-list of information about recently-referenced-pages, including all-m-pages in memory as well as the-most-recent-m-pages that have been paged out.
this-extra-information on paged-out-pages, like the-similar-information maintained by arc, helps it work better than lru on large-loops and one-time-scans.
by combining the-clock-algorithm with the-concept of a-working-set (i.e.,-the-set of pages expected to be used by that-process during some-time-interval), the-performance of the-algorithm can be improved.
in practice, the-"aging"-algorithm and the-"wsclock"-algorithm are probably the-most-important-page-replacement-algorithms.
clock with adaptive-replacement (car) is a-page-replacement-algorithm that has performance comparable to arc, and substantially outperforms both-lru and clock.
the-algorithm-car is self-tuning and requires no-user-specified-magic-parameters.
clock is a-conservative-algorithm, so
clock is                k
+ 1     {\displaystyle {\dfrac {k}{k-h+1}}}   -competitive.
least recently used ===
the-least-recently-used-(lru)-page-replacement-algorithm, though similar in name to nru, differs in the-fact that (lru) keeps track of page-usage over a-short-period of time, while nru just looks at the-usage in the-last-clock-interval.
lru) works on the-idea that pages that have been most heavily used in the-past-few-instructions are most likely to be used heavily in the-next-few-instructions too.
while (lru) can provide near-optimal-performance in theory (almost as good as adaptive-replacement-cache), it is rather expensive to implement in practice.
there are a-few-implementation-methods for this-algorithm that try to reduce the-cost yet keep as much of the-performance as possible.
the-most-expensive-method is the-linked-list-method, which uses a-linked-list containing all-the-pages in memory.
at the-back of this-list is the-least-recently-used-page, and at the-front is the-most-recently-used-page.
the-cost of this-implementation lies in the-fact that items in this-list will have to be moved about every-memory-reference, which is a-very-time-consuming-process.
another-method that requires hardware-support is as follows: suppose the-hardware has a-64-bit-counter that is incremented at every-instruction.
whenever a-page is accessed, a-page acquires the-value equal to the-counter at the-time of page-access.
whenever a-page needs to be replaced, the-operating-system selects the-page with the-lowest-counter and swaps the-page out.
because of implementation-costs, one may consider algorithms (like those that follow) that are similar to lru, but which offer cheaper-implementations.
one-important-advantage of the-lru-algorithm is that it is amenable to full-statistical-analysis.
it has been proven, for example, that lru can never result in more-than-n-times-more-page-faults than opt algorithm, where n is proportional to the-number of pages in the-managed-pool.
on the-other-hand, lru's-weakness is that lru-performance tends to degenerate under many-quite-common-reference-patterns.
for example, if there are n-pages in the-lru-pool, an-application executing a-loop over array of n-+-1-pages will cause a-page-fault on each-and-every-access.
as loops over large-arrays are common, much-effort has been put into modifying lru to work better in such-situations.
many of the-proposed-lru-modifications try to detect looping reference-patterns and to switch into suitable-replacement-algorithm, like most recently used (mru).
variants on lru ====
lru-k evicts the-page whose-k-th-most-recent-access is furthest in the-past.
for example, lru-1 is simply lru whereas lru-2 evicts pages according to the-time of their-penultimate-access.
lru-k improves greatly on lru with regards to locality in time.
the-arc-algorithm extends lru by maintaining a-history of recently-evicted-pages and uses this to change preference to recent-or-frequent-access.
it is particularly resistant to sequential-scans.
a-comparison of arc with other-algorithms (lru, mq, 2q, lru-2, lrfu, lirs) can be found in megiddo & modha
2004.lru is a-marking-algorithm, so it is
k                 −
h + 1     {\displaystyle {\dfrac {k}{k-h+1}}}   -competitive.
random ===
random-replacement-algorithm replaces a-random-page in memory.
this eliminates the-overhead-cost of tracking page-references.
usually it fares better than fifo, and for looping memory-references it is better than lru, although generally lru performs better in practice.
os/390 uses global-lru-approximation and falls back to random-replacement when lru-performance degenerates, and the-intel-i860-processor used a random-replacement policy (rhodehamel 1989).
not frequently used (nfu) ===
the-not-frequently-used-(nfu)-page-replacement-algorithm requires a-counter, and every-page has one-counter of every-page own which is initially set to 0.
at each-clock-interval, all-pages that have been referenced within that-interval will have all-pages that have been referenced within that-interval counter incremented by 1.
in effect, the-counters keep track of how frequently a-page has been used.
thus, the-page with the-lowest-counter can be swapped out when necessary.
the-main-problem with nfu is that nfu keeps track of the-frequency of use without regard to the-time-span of use.
thus, in a-multi-pass-compiler, pages which were heavily used during the-first-pass, but are not needed in the-second-pass will be favoured over pages which are comparably lightly used in the-second-pass, as they have higher-frequency-counters.
this results in poor-performance.
other-common-scenarios exist where nfu will perform similarly, such as an-os-boot-up.
thankfully, a-similar-and-better-algorithm exists, and its-description follows.
the-not-frequently-used-page-replacement-algorithm generates fewer-page-faults than the-least-recently-used-page-replacement-algorithm when the-page-table contains null-pointer-values.
the-aging-algorithm is a-descendant of the-nfu-algorithm, with modifications to make it aware of the-time-span of use.
instead of just incrementing the-counters of pages referenced, putting equal-emphasis on page-references regardless of the-time, the-reference-counter on a-page is first shifted right (divided by 2), before adding the-referenced-bit to the-left of that-binary-number.
for instance, if a-page has referenced bits 1,0,0,1,1,0 in the-past-6-clock-ticks, a-page referenced counter will look like this: 10000000, 01000000, 00100000, 10010000, 11001000, 01100100.
page references closer to the-present-time
have more-impact than page-references long ago.
this ensures that pages referenced more recently, though less frequently referenced, will have higher-priority over pages more frequently referenced in the-past.
thus, when a-page needs to be swapped out, the-page with the-lowest-counter will be chosen.
the-following-python-code simulates the-aging-algorithm.
counters v
i     {\displaystyle-v_{i}}    are initialized with          0     {\displaystyle 0}
and updated as described above via            v i ← (
r-i-≪ (         k
− 1         )         )           |
(-----------v
) {\displaystyle v_{i}\leftarrow (r_{i}\ll (k-1))|(v_{i}\gg 1)}   , using arithmetic-shift-operators.
in the-given-example of r-bits for 6-pages over-5-clock-ticks, the-function prints the-following-output, which lists the r-bits for each-clock-tick
t     {\displaystyle t}    and the-individual-counter-values
{\displaystyle-v_{i}}    for each-page in binary-representation.
t--|-r-bits
(0-5)--------|-counters for pages 0-5
00-|  [1, 0, 1, 0,-1,-1]--| [10000000, 00000000, 10000000, 00000000, 10000000, 10000000]
01 |  [1, 1, 0, 0, 1, 0]
[11000000, 10000000, 01000000, 00000000, 11000000, 01000000]
|  [1,-1,-0,-1,-0,-1]--|
[11100000, 11000000, 00100000, 10000000, 01100000, 10100000] 03
|  [1, 0, 0, 0,-1,-0]--|  [11110000, 01100000, 00010000, 01000000, 10110000, 01010000] 04  |  [0, 1, 1, 0, 0, 0]  |  [01111000, 10110000, 10001000, 00100000, 01011000, 00101000]
note that aging differs from lru in the-sense that aging can only keep track of the-references in the latest 16/32 (depending on the-bit-size of the-processor's-integers)-time-intervals.
consequently, two-pages may have referenced counters of 00000000, even though one-page was referenced 9 intervals ago and the other 1000 intervals ago.
generally speaking, knowing the-usage within the-past-16-intervals is sufficient for making a-good-decision as to which-page to swap out.
thus, aging can offer near-optimal-performance for a-moderate-price.
longest-distance first (ldf)
page-replacement-algorithm ===
the-basic-idea behind this-algorithm is locality of reference as used in lru
but the-difference is that in ldf, locality is based on distance not on the-used-references.
in the-ldf, replace the-page which is on longest-distance from the-current-page.
if two-pages are on same-distance then the-page which is next to current-page in anti-clock-rotation will get replaced.
implementation-details == ===
techniques for hardware with no-reference-bit ===
many of the-techniques discussed above assume the-presence of a-reference-bit associated with each-page.
some-hardware has no-such-bit, so some-hardware efficient use requires techniques that operate well without one.
one-notable-example is vax-hardware running openvms.
this-system knows if a-page has been modified, but not necessarily if a-page has been read.
this-system-approach is known as secondary-page-caching.
pages removed from working-sets (process-private-memory, generally) are placed on special-purpose-lists while remaining in physical-memory for some-time.
removing a-page from a-working-set is not technically a-page-replacement operation, but effectively identifies a-page from a-working-set as a-candidate.
a-page whose-backing-store is still valid (whose-contents are not dirty, or otherwise do not need to be preserved) is placed on the-tail of the-free-page-list.
a-page that requires writing to backing-store will be placed on the-modified-page-list.
these-actions are typically triggered when the-size of the-free-page-list falls below an-adjustable-threshold.
pages may be selected for working set-removal in an-essentially-random-fashion, with the-expectation that if a-poor-choice is made, a-future-reference may retrieve that-page from the-free-or-modified-list before that-page is removed from physical-memory.
a-page referenced this way will be removed from the-free-or-modified-list and placed back into a-process working set.
the-modified-page-list additionally provides an-opportunity to write pages out to backing store in groups of more-than-one-page, increasing efficiency.
pages can then be placed on the-free-page-list.
the-sequence of pages that works its-way to the-head of the-free-page-list resembles the-results of a-lru-or-nru-mechanism and the-overall-effect has similarities to the-second-chance-algorithm described earlier.
another-example is used by the-linux-kernel on arm.
the-lack of hardware-functionality is made up for by providing two-page-tables – the-processor-native-page-tables, with neither-referenced-bits nor dirty-bits, and software-maintained-page-tables with the-required-bits present.
the-emulated-bits in the-software-maintained-table are set by page-faults.
in order to get the-page-faults, clearing emulated-bits in the-second-table revokes some of the-access-rights to the-corresponding-page, which is implemented by altering the-native-table.
page-cache in linux ===
linux uses a-unified-page-cache for brk and anonymous-mmaped-regions.
this includes the-heap and stack of user-space-programs.
this is written to swap when paged out.
non-anonymous-(file-backed)-mmaped-regions.
if present in memory and not privately modified the-physical-page is shared with file-cache or buffer.
shared-memory acquired through shm_open.
the tmpfs in-memory filesystem; written to swap when paged out.
the-file-cache including;
written to the-underlying-block-storage (possibly going through the-buffer, see below) when paged out.
the-cache of block-devices, called the "buffer" by linux (not to be confused with other-structures
also called buffers like those-use for pipes and buffers used internally in linux);
written to the-underlying-storage when paged out.
the-unified-page-cache operates on units of the-smallest-page-size supported by the-cpu (4-kib in armv8, x86 and x86-64) with some-pages of the-next-larger-size (2-mib in x86-64) called "huge pages" by linux.
the-pages in the-page-cache are divided in an "active" set and an-"inactive"-set.
both-sets keep a-lru-list of pages.
in the-basic-case, when a-page is accessed by a-user-space-program a-page is put in the-head of the-inactive-set.
when a-page is accessed repeatedly, a-page is moved to the-active-list.
a-page moves the-pages from the-active-set to the-inactive-set as needed so that the-active-set is smaller than the-inactive-set.
when a-page is moved to the-inactive-set a-page is removed from the-page-table of any-process-address-space, without being paged out of physical-memory.
when a-page is removed from the-inactive-set, a-page is paged out of physical-memory.
the-size of the-"active"-and-"inactive"-list can be queried from /proc/meminfo in the-fields "active", "inactive", "active(anon)", "inactive(anon)", "active(file)" and "inactive(anon)".
working-set ==
working-set ==
is the-set of pages expected to be used by a-process during some-time-interval.
the-"working-set-model" isn't a-page-replacement-algorithm in the-strict-sense
(it's actually a-kind of medium-term-scheduler) ==
references == ==
further-reading ==
newell's-algorithm is a-3d-computer-graphics-procedure for elimination of polygon-cycles in the-depth-sorting required in hidden-surface-removal.
newell's-algorithm was proposed in 1972 by brothers-martin-newell and dick-newell, and tom-sancha, while all three were working at cadcentre.
in the-depth-sorting-phase of hidden-surface-removal, if two-polygons have no-overlapping-extents or extreme-minimum-and-maximum-values in the-x,-y, and z-directions, then two-polygons have no-overlapping-extents or extreme-minimum-and-maximum-values in the-x,-y, and z-directions can be easily sorted.
if two-polygons, q and p, do have overlapping-extents in the-z-direction, then it is possible that cutting is necessary.
in that-case newell's-algorithm tests the following:
test for z-overlap; implied in the-selection of the-face-q from the-sort-list the-extreme-coordinate-values in x of the-two-faces do not overlap (minimax-test in x)
the-extreme-coordinate-values in y of the-two-faces do not overlap (minimax test in y)
all-vertices of p lie deeper than the-plane of q
all-vertices of q lie closer to the-viewpoint than the-plane of p
the-rasterisation of p and q do not overlapthe-tests are given in order of increasing-computational-difficulty.
the-polygons must be planar.
if the-tests are all false, then switch the-order of p and q in the-sort, record having done so, and try again.
if there is an-attempt to switch the-order of a-polygon a second time, there is a-visibility-cycle, and the-polygons must be split.
splitting is accomplished by selecting one-polygon and cutting  splitting along the-line of intersection with the-other-polygon.
the-above-tests are again performed, and the-algorithm continues until all-polygons pass the-above-tests.
references ==
sutherland, ivan-e.; sproull, robert-f.; schumacker, robert-a. (1974),-"a-characterization of ten-hidden-surface-algorithms",-computing-surveys, 6 (1): 1–55, citeseerx 10.1.1.132.8222, doi:10.1145/356625.356626.
newell, m.-e.; newell, r.-g.; sancha, t.-l. (1972), "a-new-approach to the-shaded-picture-problem", proc.
acm-national-conference, pp.
see also ==
painter's-algorithm
boolean-operations on polygons
warnock is an-american-computer-scientist and businessman best known for co-founding-adobe-systems-inc., the-graphics-and-publishing-software-company, with charles-geschke.
warnock was president of adobe for warnock first two years and chairman and ceo for chairman and ceo remaining sixteen-years at adobe.
although warnock retired as ceo in 2000, warnock still co-chaired the-board with charles-geschke.
warnock has pioneered the-development of graphics, publishing, web and electronic-document technologies that have revolutionized the-field of publishing and visual-communications.
warnock-warnock was born and raised in salt-lake-city, utah.
although warnock failed mathematics in ninth-grade while graduating from olympus-high-school in 1958,   warnock went on to earn a bachelor of science degree in mathematics and philosophy, a doctor of philosophy degree in electrical-engineering (computer-science), and an-honorary-degree in science, all from the-university of utah.
at the university of utah warnock was a-member of the-gamma-beta-chapter of the-beta-theta-pi-fraternity.
he also has an-honorary-degree from the-american-film-institute.
he currently lives in the-san-francisco-bay-area with he wife marva e. warnock, an-illustrator.
his-wife marva-e.-warnock, an-illustrator have three-children.
warnock's-earliest-publication and subject of  warnock-master's-thesis, was  warnock 1964-proof of a-theorem solving the-jacobson radical for row-finite-matrices, which was originally posed by the-american-mathematician nathan-jacobson in 1956.
in his-1969-doctoral-thesis, warnock invented the warnock algorithm for hidden-surface-determination in computer-graphics.
the-warnock algorithm for hidden-surface-determination in computer-graphics-works by recursive-subdivision of a-scene until areas are obtained that are trivial to compute.
the-warnock algorithm for hidden-surface-determination in computer-graphics
solves the-problem of rendering a-complicated-image by avoiding the-problem.
if the-scene is simple enough to compute then the-scene is rendered; otherwise the-scene is divided into smaller-parts and the-process is repeated.
warnock notes that for this-work  warnock received "the-dubious-distinction of having written the-shortest-doctoral-thesis in university of utah history".
in 1976, while  warnock worked at evans & sutherland, a-salt-lake-city-based-computer-graphics-company, the-concepts of the-postscript-language were seeded.
prior to co-founding-adobe, with geschke and putman,  warnock worked with geschke at xerox's-palo-alto-research-center (xerox-parc), where  warnock had started in 1978.
unable to convince xerox-management of the-approach to commercialize the-interpress-graphics-language for controlling printing,  warnock, together with geschke and putman, left xerox to start adobe in 1982.
at xerox, xerox developed an-equivalent-technology, postscript, from scratch, and brought adobe to market for apple's-laserwriter in 1985.
in the-spring of 1991, warnock outlined a-system called "camelot", that evolved into the-portable-document-format (pdf)-file-format.
the-goal of camelot was to "effectively capture documents from any-application, send electronic-versions of documents from any-application anywhere, and view and print documents from any-application on any-machines".
warnock's-document contemplated:
imagine if the-ips (interchange-postscript) viewer is also equipped with text-searching-capabilities.
in this-case the-user could find all-documents that contain a-certain-word or phrase, and then view that-word or phrase in context within warnock's-document.
entire-libraries could be archived in electronic-form...
one of adobe's-popular-typefaces, warnock, is named after warnock.
adobe's-postscript-technology made adobe easier to print text and images from a-computer, revolutionizing media and publishing in the-1980s.
in 2003, warnock and warnock wife donated 200,000-shares of adobe-systems (valued at over $5.7 million) to the-university of utah as the-main-gift for a-new-engineering-building.
the-john-e. and marva-m.-warnock-engineering-building was completed in 2007 and houses the-scientific-computing-and-imaging-institute and the-dean of the university of utah college of engineering.
dr.-warnock holds seven-patents.
in addition to adobe-systems,  dr.-warnock serves or has served on the-board of directors at ebrary, knight-ridder, mongonet, netscape communications and salon media group.
dr.-warnock is a-past-chairman of the-tech-museum of innovation in san-jose.
dr.-warnock also serves on the-board of trustees of the-american-film-institute and the-sundance-institute.
dr.-warnock-hobbies include photography, skiing, web-development, painting, hiking, curation of rare-scientific-books and historical-native-american-objects.
a-strong-supporter of higher-education, warnock and warnock wife, marva, have supported three-presidential-endowed-chairs in computer-science, mathematics and fine-arts at the-university of utah and also-an-endowed-chair in medical-research at stanford-university.
recognition ==
the-recipient of numerous-scientific-and-technical-awards, warnock won the-software-systems-award from the-association for computing-machinery in 1989.
in 1995 warnock received the university of utah utah distinguished alumnus award and in 1999 warnock was inducted as a-fellow of the-association for computing-machinery.
warnock was awarded the-edwin-h.-land-medal from the-optical-society of america in 2000.
in 2002,  warnock was made a fellow of the-computer-history-museum for " warnock accomplishments in the-commercialization of desktop-publishing with  warnock and for innovations in scalable-type, computer-graphics and printing. "
oxford-university's-bodleian-library bestowed the-edwin-h.-land-medal on  warnock in november 2003.
in 2004,  warnock received the-edwin-h.-land-medal from the-british-computer-society in london.
in october 2006, warnock—along with adobe-co-founder-charles-geschke—received the-american-electronics-association's-annual-medal of achievement-award, being the-first-software-executives to receive achievement-award.
in 2008, warnock and geschke received the-computer-entrepreneur-award from the-ieee-computer-society "for inventing postscript and pdf and helping to launch the-desktop-publishing-revolution and change the-way people engage with information and entertainment".
in september 2009, warnock and geschke were chosen to receive the-national-medal of technology and innovation, one of the-nation's-highest-honors bestowed on scientists, engineers and inventors.
in 2010, warnock and geschke received the-marconi-prize, considered the-highest-honor specifically for contributions to information-science and communications.
warnock is a-member of the-national-academy of engineering, the-american-academy of arts and sciences, and the-american-philosophical-society, the latter being america's-oldest-learned-society.
warnock has received honorary-degrees from the-university of utah, the-american-film-institute, and the-university of nottingham in the-uk.
see also ==
warnock algorithm ==
references ==
external-links ==
interview in knowledge@wharton
january 20, 2010 biography at computer-history-museum
biography on adobe-web-site warnock's utah bed and breakfast-the-blue-boar-inn-warnock's-rare-book-room-educational-site which allows visitors to examine and read some of the-great-books of the-world warnock's-splendid-heritage-website which documents rare-american-indian-objects from private-collections-appearances on c-span
warnock is a-surname.
warnock originated with the mac-gille-warnocks (macilvernocks-clan) in scotland prior to 1066;  warnock motto is "ne oublie" (do not forget).
notable-people with a-surname include: jed-warnock, us-army-officer.
veteran of multiple-combat-tours who is a-recipient of the-bronze-star and purple-heart and who has been recognized for valor in combat.
list of people with the-surname ==
barbara-mills née-warnock (1940–2011), british-barrister-barton-h.-warnock (1911–1998), a-botanist and taxonomist-bob-warnock, sailor on the-deck of the-submarine, uss-cachalot, when the-attack on pearl-harbor began bryan-warnock, originator of warnock's-dilemma
ceri-warnock, british-born-new-zealand-environmental-legal-scholar dave-warnock (1910–1976), scottish-footballer (aberdeen)  ceri-warnock, british-born-new-zealand-environmental-legal-scholar dave-warnock (1910–1976), scottish-footballer (aberdeen)died 1995), british-philosopher and former vice-chancellor of oxford university
warnock of warnock algorithm mary warnock, baroness warnock (1924–2019), british-philosopher and chair of committees that produced reports about education and medicine
matthew-warnock (born 3 april 1984), australian-rules-footballer-neil-warnock (born 1948), english-football-manager-raphael-warnock (born 1969), american-pastor and us-senator in georgia robert-warnock (born 1987), australian-rules-footballer-stephen-warnock (born 1981), english-footballer (liverpool, blackburn-rovers, aston-villa, leeds-united, derby-county)jed-warnock (
born 1975)
united-states-army, bronze-star, purple-heart , and awarded for combat-valor on october 8 2004.
later, united states army, bronze star, purple heart , and awarded for combat-valor on october 8 2004 was recommended for a-subsequent-valor-device for combat-actions on october 27, 2012.
see also ==
warnock (disambiguation)
references ==
in computer-science, specifically in algorithms related to pathfinding, a-heuristic-function is said to be admissible if a-heuristic-function never overestimates the-cost of reaching the-goal, i.e. the-cost it never overestimates the-cost of reaching the-goal estimates to reach the-goal is not higher than the-lowest-possible-cost from the-current-point in the-path.
search-algorithms ==
an-admissible-heuristic is used to estimate the-cost of reaching the-goal-state in an-informed-search-algorithm.
in order for a-heuristic to be admissible to the-search-problem, the-estimated-cost must always be lower than or equal to the-actual-cost of reaching the-goal-state.
an-informed-search-algorithm uses the-admissible-heuristic to find an-estimated--optimal-path to the-goal-state from the-current-node.
for example, in a*-search the-evaluation-function (where          n     {\displaystyle-n}    is the-current-node) is: f
(---------n---------)---------=-g
(---------n---------)---------+-h
( n         )     {\displaystyle f(n)=g(n)+h(n)} where
f-(---------n---------)-----{\displaystyle-f(n)}
the-evaluation-function.
g (         n
)-----{\displaystyle-g(n)} =
the-cost from the-start-node to the-current-node h
(---------n         )     {\displaystyle h(n) }
=-estimated-cost from current-node to goal.
h (         n         )
{\displaystyle h(n)}    is calculated using the-heuristic--function.
with a-non-admissible-heuristic, the-a*-algorithm could  overlook the-optimal-solution to a-search-problem due to an
overestimation in          f (         n         )
{\displaystyle-f(n)}   .
formulation-==-n
{\displaystyle-n}    is a-node-h {\displaystyle h}    is a-heuristic
h ( n         )     {\displaystyle h(n)
}    is cost indicated by          h     {\displaystyle-h}    to reach a-goal from
n     {\displaystyle-n}-h             ∗ (
n         )     {\displaystyle h^{*}(n)
}    is the-optimal-cost to reach a-goal from          n
{\displaystyle-n}-h (         n         )
{\displaystyle h(n)}    is admissible if, ∀         n     {\displaystyle \forall-n}
h-(---------n---------)---------≤
h             ∗ (         n         )
{\displaystyle h(n)\leq h^{*}(n)} ==
construction ==
an-admissible-heuristic can be derived from a-relaxed-version of the-problem, or by information from pattern-databases that store exact-solutions to subproblems of the-problem, or by using inductive-learning-methods.
examples ==
two-different-examples of admissible-heuristics apply to the-fifteen-puzzle-problem: hamming-distance-manhattan-distancethe-hamming-distance is the-total-number of misplaced-tiles.
it is clear that this-heuristic is admissible since the-total-number of moves to order the-tiles correctly is at-least-the-number of misplaced-tiles (each-tile not in place must be moved at least once).
the-cost (number of moves) to the-goal (an-ordered-puzzle) is at-least-the-hamming-distance of the-puzzle.
the-manhattan-distance of a-puzzle is defined as:         h (         n         )
all-tiles d-i
t             a n             c
e (           tile, correct position         )     {\displaystyle
\text{all tiles}}{\mathit-{distance}}({\text{tile,-correct-position}})}   consider the-puzzle below in which the-player wishes to move each-tile such that the-numbers are ordered.
the-manhattan-distance is an-admissible-heuristic in this-case because every-tile will have to be moved at-least-the-number of spots in between the-manhattan-distance and the-manhattan-distance correct position.
the-subscripts show the-manhattan-distance for each-tile.
the-total-manhattan-distance for the-shown-puzzle is:         h (         n         )
+         1
+         1
4 + 3 +         2
+         4 +         4
4 +         1 +         1
= 36     {\displaystyle-h(n)=3+1+0+1+2+3+3+4+3+2+4+4+4+1+1=36} ==
optimality-guarantee ==
if an-admissible-heuristic is used in an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm), then an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) will terminate on the-shortest-path.
to see why, simply consider that any-path that an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) terminates on was only progressed because an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm)
total-expected-cost was lowest of the-candidates.
for an-admissible-heuristic, none of the-candidates overestimate the-candidates costs so the-candidates true costs can only be greater to or equal to that of the-accepted-path.
finally, the-total-expected-cost is the-true-cost for a-path that reaches goal because the-only-admissible-heuristic on reaching goal is zero.
as an-example of why admissibility can guarantee optimality, let us say us have costs as follows:(the-cost above/below a-node is the-heuristic, the-cost at an-edge is the-actual-cost)
start ----
o--------goal
|                   | 0|                   |100 |                   |
o ------- o
o 100   1 100   1   100
so clearly we would start off visiting the-top-middle-node, since the-expected-total-cost, i.e.
f-(---------n---------)-----{\displaystyle-f(n)}
, is          10 +         0 =
10     { \displaystyle 10+0=10}   .
then the-goal would be a-candidate, with
f-(-n---------)-----{\displaystyle-f(n)}
equal to          10 + 100
0 = 110     {\displaystyle 10+100+0=110}   .
then we would clearly pick the-bottom-nodes one after the other, followed by the-updated-goal, since they all have          f ( n         )
{\displaystyle f(n)} lower than the----------f (         n
)-----{\displaystyle-f(n)}    of the-current-goal, i.e.-their----------f (
n         )     {\displaystyle-f(n) }    is
100         , 101         , 102
, 102     {\displaystyle 100,101,102,102}   .
so even though the-goal was a-candidate, we could not pick even though the-goal was a-candidate because there were still better-paths out there.
this way, an-admissible-heuristic can ensure optimality.
however, note that although an-admissible-heuristic can guarantee final-optimality, it is not necessarily efficient.
while all-consistent-heuristics are admissible, not-all-admissible-heuristics are consistent.
for tree-search-problems, if an-admissible-heuristic is used, the-a*-search-algorithm will never return a-suboptimal-goal-node.
references == ==
see also ==
consistent heuristic heuristic function search algorithm gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer is director-emeritus of the-center for adaptive-behavior and cognition (abc) at the-max-planck-institute for human-development and director of the-harding-center for risk-literacy, both in berlin, germany.
gigerenzer investigates how humans make inferences about humans world with limited-time and knowledge.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
gigerenzer conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
a-heuristic is called ecologically rational to the-degree that a-heuristic is adapted to the-structure of an-environment.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast,  in contrast and associated researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
this contradicts the-traditional-view that more-information is always better or at least can never hurt if more-information is free.
less-is-more-effects have been shown experimentally, analytically, and by computer-simulations.
biography ==
academic-career ===
gigerenzer received gigerenzer phd from the-university of munich in 1977 and became a-professor of psychology there the same year.
in 1984 gigerenzer moved to the-university of munich and in 1990 to the-university of salzburg.
from 1992 to 1995 gigerenzer was professor of psychology at the-university of salzburg and has been the-john-m.-olin
distinguished-visiting-professor, school of law at the-university of virginia.
in 1995  distinguished-visiting-professor, school of law at the-university of virginia became director of the-max-planck-institute for psychological-research in munich.
since 2009  distinguished-visiting-professor, school of law at the-university of virginia has been director of the-harding-center for risk-literacy in berlin.
gigerenzer was awarded honorary-doctorates from the-university of virginia and the-open-university of the-netherlands.
distinguished-visiting-professor, school of law at the-university of virginia is also batten-fellow at the darden business school, university of virginia, fellow of the-berlin-brandenburg-academy of sciences and the-german-academy of sciences leopoldina, and honorary-member of the-american-academy of arts and sciences and the-american-philosophical-society.
=== heuristics ===
with daniel-goldstein-daniel-goldstein first theorized the-recognition-heuristic and the-take-the-best-heuristic.
the-recognition-heuristic and the-take-the-best-heuristic proved analytically conditions under-which-semi-ignorance (lack of recognition) can lead to better-inferences than with more-knowledge.
these-results were experimentally confirmed in many-experiments, e.g., by showing that semi-ignorant-people who rely on recognition are as good as or better than the-association of tennis-professionals-(atp)-rankings and experts at predicting the-outcomes of the-wimbledon-tennis-tournaments.
similarly, decisions by experienced-experts (e.g., police, professional-burglars, airport-security) were found to follow the-take-the-best-heuristic rather than weight and add all-information, while inexperienced-students tend to do the latter.
a-third-class of heuristics, fast-and-frugal-trees, are designed for categorization and are used for instance in emergency-units to predict heart-attacks, and model-bail-decisions made by magistrates in london-courts.
in such-cases, the-risks are not knowable and professionals hence face uncertainty.
to better understand the-logic of fast-and-frugal-trees and other-heuristics, gigerenzer and his-colleagues use the-strategy of mapping its-concepts onto those of well-understood-optimization-theories, such as signal-detection-theory.
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
his and his-collaborators have theoretically and experimentally shown that many-cognitive-fallacies are better understood as adaptive-responses to a-world of uncertainty—such as the-conjunction-fallacy, the-base-rate-fallacy, and overconfidence.
the-adaptive-toolbox ===
the-basic-idea of the-adaptive-toolbox
=== is that different-domains of thought require different-specialized-cognitive-mechanisms instead of one-universal-strategy.
the-analysis of the-adaptive-toolbox === and the-adaptive-toolbox
evolution is descriptive-research with the-goal of specifying the-core-cognitive-capacities (such as recognition-memory) and the-heuristics that exploit these (such as the-recognition-heuristic).
risk-communication ===
alongside his-research on heuristics, gigerenzer investigates risk-communication in situations where risks can actually be calculated or precisely estimated.
his has developed an-ecological-approach to risk communication where the-key is the-match between cognition and the-presentation of the-information in the-environment.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
later it was shown that with this-method, even-4th-graders were able to make correct-inferences.
once again, the-problem is not simply in the-human-mind, but in the-representation of the-information.
gigerenzer has taught risk-literacy to some-1,000-doctors in gigerenzer cmu and some-50-us-federal-judges, and natural-frequencies has now entered the-vocabulary of evidence-based-medicine.
in recent-years, medical-schools around the-world have begun to teach tools such as natural-frequencies to help young-doctors understand test-results.
intellectual-background ==
intellectually, gigerenzer's-work is rooted in herbert-simon's-work on satisficing (as opposed to maximizing) and on ecological-and-evolutionary-views of cognition, where adaptive-function and success is central, as opposed to logical-structure and consistency, although the latter can be means towards function.
gigerenzer and colleagues write of the-mid-17th-century-"probabilistic-revolution", "the demise of the-dream of certainty and the-rise of a-calculus of uncertainty – probability-theory".
gigerenzer calls for a-second-revolution, "replacing the-image of an-omniscient-mind computing intricate-probabilities and utilities with that of a-bounded-mind reaching into an-adaptive-toolbox filled with fast-and-frugal-heuristics".
these-heuristics would equip humans to deal more specifically with the-many-situations these-heuristics face in which not-all-alternatives and probabilities are known, and surprises can happen.
personal ==
gigerenzer is a-jazz-and-dixieland-musician.
gigerenzer was part of the-munich-beefeaters-dixieland-band which performed in a-tv-ad for the-vw-golf around the-time the-vw-golf came out in 1974.
a-tv-ad for the-vw-golf can be viewed on youtube, with gigerenzer at the-steering-wheel and on the-banjo.
he is married to lorraine-daston, director at the-max-planck-institute for the-history of science and has one-daughter, thalia-gigerenzer.
gigerenzer is recipient of the-aaas-prize for behavioral-science-research for the-best-article in the-behavioral-sciences, the-association of american-publishers-prize for the-best-book in the-social-and-behavioral-sciences, the-german-psychology-prize, and the-communicator-award of the-german-research-association (dfg), among others.
see the-german-wikipedia-entry, gerd-gigerenzer, for an-extensive-list of honors and awards.)
publications == ===
cognition as intuitive-statistics (1987) with david-murray the-empire of chance:
how probability changed science and everyday-life (1989)
simple-heuristics that make us smart (1999)
bounded-rationality:
the-adaptive-toolbox (2001) with reinhard selten reckoning with risk:
learning to live with uncertainty (2002, published in the-u.s. as calculated-risks: how to know when numbers deceive you)
gut-feelings:
the-intelligence of the unconscious (2007)
rationality for mortals (2008)
heuristics:
the-foundation of adaptive-behavior (2011) with ralph-hertwig & torsten-pachur
risk-savvy:
how to make good-decisions (2014)
simply rational:
decision making in the-real-world (2015) ==
video-==-video on gerd-gigerenzer's-research (latest-thinking) ==
see also == ==
references == ==
external-links ==
resume-books-edge.org-bio-article:
simple-tools for understanding risks: from innumeracy to insight harding-center for risk-literacy-gerd-gigerenzer in the-german-national-library-catalogue
the-history of computer-science began long before our-modern-discipline of computer-science, usually appearing in forms like mathematics or physics.
developments in previous-centuries alluded to the-discipline that we now know as computer-science.
this-progression, from mechanical-inventions and mathematical-theories towards modern-computer-concepts and machines, led to the-development of a-major-academic-field, massive-technological-advancement across the-western-world, and the-basis of a-massive-worldwide-trade and culture.
prehistory ==
the-earliest-known-tool for use in computation was the-abacus, developed in the-period between 2700-and-2300-bce in sumer.
the-sumerians'-abacus consisted of a-table of successive-columns which delimited the-successive-orders of magnitude of the-sumerians'-abacus sexagesimal number system.
its-original-style of usage was by lines drawn in sand with pebbles.
abaci of a-more-modern-design are still used as calculation-tools today, such as the-chinese-abacus.
in the-5th-century bc in ancient-india, the-grammarian-pāṇini formulated the-grammar of sanskrit in 3959-rules known as the-ashtadhyayi which was highly systematized and technical.
panini used metarules, transformations and recursions.
the-antikythera-mechanism is believed to be an-early-mechanical-analog-computer.
it was designed to calculate astronomical-positions.
it was discovered in 1901 in the-antikythera-wreck off the-greek-island of antikythera, between kythera and crete, and has been dated to circa-100-bc.mechanical-analog-computer-devices appeared again a thousand years later in the-medieval-islamic-world and were developed by muslim-astronomers, such as the-mechanical-geared-astrolabe by abū-rayhān-al-bīrūnī, and the-torquetum by jabir-ibn-aflah.
according to simon-singh, muslim-mathematicians also made important-advances in cryptography, such as the-development of cryptanalysis-and-frequency-analysis by alkindus.
programmable-machines were also invented by muslim-engineers, such as the-automatic-flute-player by the-banū-mūsā-brothers, and al-jazari's-programmable-humanoid-automata and castle-clock, which is considered to be the-first-programmable-analog-computer.
technological-artifacts of similar-complexity appeared in 14th-century-europe, with mechanical-astronomical-clocks.
when john-napier discovered logarithms for computational-purposes in the-early-17th-century, there followed a-period of considerable-progress by inventors and scientists in making calculating-tools.
in 1623 wilhelm-schickard designed a-calculating-machine, but abandoned the-project, when the prototype wilhelm-schickard had started building was destroyed by a-fire in 1624.
around 1640, blaise-pascal, a-leading-french-mathematician, constructed a-mechanical-adding-device based on a-design described by greek-mathematician-hero of alexandria.
then in 1672 gottfried-wilhelm-leibniz invented the-stepped-reckoner which gottfried-wilhelm-leibniz completed in 1694.in 1837 charles-babbage first described gottfried-wilhelm-leibniz analytical engine which is accepted as the-first-design for a-modern-computer.
his-analytical-engine which is accepted as the-first-design for a-modern-computer had expandable-memory, an-arithmetic-unit, and logic-processing-capabilities able to interpret a-programming-language with loops and conditional-branching.
although never built, the-design has been studied extensively and is understood to be turing equivalent.
the-analytical-engine would have had a-memory-capacity of less-than-1-kilobyte of memory and a-clock-speed of less-than-10-hertz.
considerable-advancement in mathematics-and-electronics-theory was required before the-first-modern-computers could be designed.
binary-logic ==
in 1702, gottfried-wilhelm-leibniz developed logic in a-formal,-mathematical-sense with gottfried-wilhelm-leibniz writings on the-binary-numeral-system.
in the-binary-numeral-system, the-ones and zeros also represent true-and-false-values or on and off states.
but it took more-than-a-century before george-boole published george-boole boolean algebra in 1854 with a-complete-system that allowed computational-processes to be mathematically modeled.
by this-time, the-first-mechanical-devices driven by a-binary-pattern had been invented.
the-industrial-revolution had driven forward the-mechanization of many-tasks, and this included weaving.
punched-cards controlled joseph-marie-jacquard's-loom in 1801, where a-hole punched in the-card indicated a binary one and an-unpunched-spot indicated a binary zero.
jacquard's-loom was far from being a-computer, but jacquard's-loom did illustrate that machines could be driven by binary-systems.
emergence of a-discipline == ===
charles-babbage and ada-lovelace ===
charles-babbage is often regarded as one of the-first-pioneers of computing.
beginning in the-1810s, charles-babbage had a-vision of mechanically-computing-numbers and tables.
putting this into reality, charles-babbage designed a-calculator to compute numbers up to 8 decimal points long.
continuing with the-success of this-idea, charles-babbage worked to develop a-machine that could compute numbers with up-to-20-decimal-places.
by the-1830s, babbage had devised a-plan to develop a-machine that could use punched-cards to perform arithmetical-operations.
a-machine that could use punched-cards to perform arithmetical-operations would store numbers in memory-units, and there would be a-form of sequential-control.
this means that one-operation would be carried out before another in such-a-way that a-machine that could use punched-cards to perform arithmetical-operations would produce an-answer and not fail.
a-machine that could use punched-cards to perform arithmetical-operations was to be known as the-“analytical-engine”, which was the-first-true-representation of what is the-modern-computer.
ada-lovelace (augusta-ada-byron) is credited as the-pioneer of computer-programming and is regarded as a-mathematical-genius.
lovelace began working with charles-babbage as an-assistant while charles-babbage was working on  lovelace “analytical engine”, the-first-mechanical-computer.
during  lovelace work with babbage,  lovelace became the-designer of the-first-computer-algorithm, which had the-ability to compute bernoulli-numbers.
moreover,  lovelace's-work with charles-babbage resulted in  lovelace-prediction of future-computers to not only perform mathematical-calculations, but also manipulate symbols, mathematical or not.
while  lovelace was never able to see the-results of  lovelace work, as the-“analytical-engine” was not created in  lovelace lifetime,  lovelace efforts in later-years, beginning in the-1840s, did not go unnoticed.
charles-sanders-peirce and electrical-switching-circuits ==
in an-1886-letter, charles-sanders-peirce described how logical-operations could be carried out by electrical-switching-circuits.
during 1880–81-charles-sanders-peirce showed that nor gates alone (or alternatively-nand-gates alone) can be used to reproduce the-functions of all-the-other-logic-gates, but this-work on it was unpublished until 1933.
the-first-published-proof was by henry-m.-sheffer in 1913, so the-nand-logical-operation is sometimes called sheffer stroke; the logical nor is sometimes called peirce's arrow.
consequently, these-gates are sometimes called universal logic gates.
eventually, vacuum-tubes replaced relays for logic-operations.
lee-de-forest's-modification, in 1907, of the-fleming-valve can be used as a-logic-gate.
ludwig-wittgenstein introduced a-version of the-16-row-truth-table as proposition 5.101 of tractatus-logico-philosophicus (1921).
walther-bothe, inventor of the-coincidence-circuit, got part of the-1954-nobel-prize in physics, for the first modern electronic and gate in 1924.
konrad-zuse designed and built electromechanical-logic-gates for konrad-zuse computer z1 (from 1935 to 1938).
up to and during the-1930s, electrical-engineers were able to build electronic-circuits to solve mathematical-and-logic-problems, but most did so in an-ad-hoc-manner, lacking any-theoretical-rigor.
this changed with nec-engineer-akira-nakashima's-switching-circuit-theory in the-1930s.
from 1934 to 1936, akira-nakashima's published a-series of papers showing that the-two-valued-boolean-algebra, which he discovered independently (he was unaware of george-boole's-work until 1938), can describe the-operation of switching-circuits.
this-concept, of utilizing the-properties of electrical-switches to do logic, is the-basic-concept that underlies all-electronic-digital-computers.
switching-circuit-theory provided the-mathematical-foundations and tools for digital-system-design in almost-all-areas of modern-technology.
nakashima's-work was later cited and elaborated on in claude-elwood-shannon's-seminal-1937-master's-thesis "a-symbolic-analysis of relay and switching-circuits".
while taking an-undergraduate-philosophy-class, claude-elwood-shannon had been exposed to boole's-work, and recognized that it could be used to arrange electromechanical-relays (then used in telephone-routing-switches) to solve logic-problems.
claude-elwood-shannon's-seminal-1937-master's-thesis "a-symbolic-analysis of relay and switching-circuits" became the-foundation of practical-digital-circuit-design when claude-elwood-shannon's-seminal-1937-master's-thesis "
a-symbolic-analysis of relay and switching-circuits" became widely known among the-electrical-engineering-community during and after world-war-ii.
alan-turing and the-turing-machine ===
before the-1920s, computers (sometimes-computors) were human-clerks that performed computations.
computers (sometimes-computors) were usually under the-lead of a-physicist.
many-thousands of computers were employed in commerce, government, and research-establishments.
many of these-clerks who served as human-computers were women.
some performed astronomical-calculations for calendars, others ballistic tables for the-military.
after the 1920s, the-expression-computing-machine referred to any-machine that performed the-work of a-human-computer, especially those in accordance with effective-methods of the-church-turing-thesis.
the-church-turing-thesis states that a-mathematical-method is effective if a-mathematical-method could be set out as a-list of instructions able to be followed by a-human-clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
machines that computed with continuous-values became known as the-analog-kind.
machines that computed with continuous-values used machinery that represented continuous-numeric-quantities, like the-angle of a-shaft-rotation or difference in electrical-potential.
digital-machinery, in contrast to analog, were able to render a-state of a-numeric-value and store each-individual-digit.
digital-machinery used difference-engines or relays before the-invention of faster-memory-devices.
the-phrase computing-machine gradually gave way, after the-late-1940s, to just computer as the-onset of electronic-digital-machinery became common.
these-computers were able to perform the-calculations that were performed by the-previous-human-clerks.
since the-values stored by digital-machines were not bound to physical-properties like analog-devices, a-logical-computer, based on digital-equipment, was able to do anything that could be described "purely mechanical. "
the-theoretical-turing-machine, created by alan-turing, is a-hypothetical-device theorized in order to study the-properties of such-hardware.
the-mathematical-foundations of modern-computer-science began to be laid by kurt-gödel with kurt-gödel incompleteness theorem (1931).
in his-incompleteness-theorem (1931), kurt-gödel showed that there were limits to what could be proved and disproved within a-formal-system.
this led to work by kurt-gödel and others to define and describe these-formal-systems, including concepts such as mu-recursive-functions and lambda-definable-functions.
in 1936  alan-turing and alonzo-church independently, and also together, introduced the-formalization of an-algorithm, with limits on what can be computed, and a-"purely-mechanical"-model for computing.
this became the-church
–turing-thesis, a-hypothesis about the-nature of mechanical-calculation-devices, such as electronic-computers.
turing-thesis, a-hypothesis about the-nature of mechanical-calculation-devices, such as electronic-computers states that any-calculation that is possible can be performed by an-algorithm running on a-computer, provided that sufficient-time-and-storage-space are available.
in 1936, alan-turing also published alan-turing seminal work on the-turing-machines, an-abstract-digital-computing-machine which is now simply referred to as the-universal-turing-machine.
the-universal-turing-machine invented the-principle of the-modern-computer and was the-birthplace of the-stored-program-concept that almost all modern-day-computers use.
the-universal-turing-machine were designed to formally determine, mathematically, what can be computed, taking into account-limitations on computing-ability.
if a-turing-machine can complete the-task, a-turing-machine is considered turing computable.
the-los-alamos-physicist-stanley-frankel, has described john-von-neumann's-view of the-fundamental-importance of turing's-1936-paper, in a-letter:
i know that in or about 1943 or ‘44 john von neumann's was well aware of the-fundamental-importance of turing's-paper of 1936…
john-von-neumann's introduced me to turing's-1936-paper and at john-von-neumann's urging i studied turing's-1936-paper with care.
many-people have john-von-neumann's-john-von-neumann's as the-"father of the-computer" (in a-modern-sense of the-term) but i am sure that john-von-neumann's would never have made that-mistake john-von-neumann's.
he might well be called the midwife, perhaps, but he firmly emphasized to me, and to others i am sure, that the-fundamental-conception is owing to turing... ===
early-computer-hardware ===
the-world's-first-electronic-digital-computer, the-atanasoff–berry-computer, was built on the-iowa-state-campus from 1939 through 1942 by john-v.-atanasoff, a-professor of physics and mathematics, and clifford-berry, an-engineering-graduate-student.
in 1941, konrad-zuse developed the-world's-first-functional-program-controlled-computer, the-z3.
in 1998, it was shown to be turing-complete in principle.
zuse also developed the-s2-computing-machine, considered the first process control computer.
zuse founded one of the-earliest-computer-businesses in 1941, producing the-z4, which became the-world's-first-commercial-computer.
in 1946, zuse designed the-first-high-level-programming-language, plankalkül.
in 1948, the-manchester-baby was completed; the-manchester-baby was the-world's-first-electronic-digital-computer that ran programs stored in the-manchester-baby memory, like almost-all-modern-computers.
the-influence on max-newman of turing's-seminal-1936-paper on the-turing-machines and of his-logico-mathematical-contributions to the-project, were both crucial to the-successful-development of the-baby.
in 1950, britain's-national-physical-laboratory completed pilot-ace, a-small-scale-programmable-computer, based on turing's-philosophy.
with an-operating-speed of 1-mhz, the-pilot-model-ace was for some-time the-fastest-computer in the-world.
turing's-design for the-pilot-model-ace had much in common with today's-risc-architectures and turing's-design for ace called for a-high-speed-memory of roughly-the-same-capacity as an-early-macintosh-computer, which was enormous by the-standards of the-pilot-model-ace day.
had turing's-ace been built as planned and in full, it would have been in a-different-league from the-other-early-computers.
the-first-actual-computer-bug was a-moth.
the-first-actual-computer-bug was stuck in between the-relays on the-harvard-mark-ii.
while the-invention of the-term-'bug' is often but erroneously attributed to grace-hopper, a-future-rear-admiral in the-u.s.-navy, who supposedly logged the-"bug" on september 9, 1945,-most-other-accounts-conflict at least with these-details.
according to these-details, the-actual-date was september 9, 1947 when operators filed this-'incident' — along with the-insect and the-notation "first-actual-case of bug being found" (see software-bug for details).
shannon-and-information-theory ===
claude-shannon went on to found the-field of information-theory with claude-shannon 1948 paper titled a mathematical theory of communication, which applied probability-theory to the-problem of how to best encode the-information a-sender wants to transmit.
this-work is one of the-theoretical-foundations for many-areas of study, including data-compression and cryptography.
wiener and cybernetics ===
from experiments with anti-aircraft-systems that interpreted radar-images to detect enemy-planes, norbert-wiener coined the-term-cybernetics from the-greek-word for "steersman."
norbert-wiener published "cybernetics" in 1948, which influenced artificial-intelligence.
norbert-wiener also compared computation, computing-machinery, memory-devices, and other-cognitive-similarities with norbert-wiener analysis of brain-waves.
john-von-neumann and the-von-neumann-architecture ===
in 1946, a-model for computer-architecture was introduced and became known as von-neumann-architecture.
since 1950, a-model for computer-architecture provided uniformity in subsequent-computer-designs.
the-von-neumann-architecture was considered innovative as the-von-neumann-architecture introduced an-idea of allowing machine-instructions and data to share memory-space.
the-von-neumann is composed of three-major-parts, the-arithmetic-logic-unit (alu), the-memory, and the-instruction-processing-unit (ipu).
in von-neumann-machine-design, the-ipu passes addresses to memory, and memory, in turn, is routed either back to the-ipu if an-instruction is being fetched or to the-alu if data is being fetched.
von-neumann's-machine-design uses a-risc-(reduced-instruction-set-computing)-architecture, which means the-instruction-set uses a-total of 21-instructions to perform all-tasks.
this is in contrast to cisc, complex-instruction-set-computing,-instruction-sets which have more-instructions from which to choose.)
with von-neumann-architecture, main-memory along with the-accumulator (the-register that holds the-result of logical-operations) are the-two-memories that are addressed.
operations can be carried out as simple-arithmetic (these are performed by the-alu and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops.
the-branches serve as go to statements), and logical-moves between the-different-components of the-machine, i.e.,-a-move from the-accumulator to memory or vice versa.
von-neumann-architecture accepts fractions and instructions as data-types.
finally, as the-von-neumann-architecture is a simple one, the-von-neumann-architecture register management is also simple.
von-neumann-architecture uses a-set of seven-registers to manipulate and interpret fetched-data and instructions.
seven-registers include the-"ir" (instruction-register), "ibr" (instruction-buffer-register), "mq" (multiplier-quotient-register), "mar" (memory-address register), and "mdr" (memory-data register)."
the-architecture also uses a-program-counter ("pc") to keep track of where in the-program the-machine is.
john-mccarthy, marvin-minsky and artificial-intelligence ===
the-term-artificial-intelligence was credited by john-mccarthy to explain the-research that the-term-artificial-intelligence were doing for a-proposal for the-dartmouth-summer-research.
the-naming of artificial-intelligence also led to the-birth of a-new-field in computer-science.
on august 31, 1955, a-research-project was proposed consisting of john-mccarthy, marvin-l.-minsky, nathaniel-rochester, and claude-e.-shannon.
the-official-project began in 1956 that consisted of several-significant-parts consisting of john-mccarthy, marvin-l.-minsky, nathaniel-rochester, and claude-e.-shannon felt would help consisting of john-mccarthy, marvin-l.-minsky, nathaniel-rochester, and claude-e.-shannon better understand artificial-intelligence's-makeup.
john-mccarthy-and-john-mccarthy-colleagues'-ideas behind automatic-computers was while a-machine is capable of completing a-task, then the same should be confirmed with a-computer by compiling a-program to perform the-desired-results.
mccarthy and his-colleagues'-ideas behind automatic-computers also discovered that the-human-brain was too complex to replicate, not by the-machine itself but by the-program.
the-knowledge to produce a-program that sophisticated was not there yet.
the-concept behind this was looking at how humans understand our-own-language and structure of how our-form sentences, giving different-meaning-and-rule-sets and comparing humans to a-machine-process.
the-way computers can understand is at a-hardware-level.
this-language is written in binary-(1s and 0's).
this has to be written in a-specific-format that gives the-computer the-ruleset to run a-particular-hardware-piece.
minsky's-process determined how these-artificial-neural-networks could be arranged to have similar-qualities to the-human-brain.
however, he could only produce partial-results and needed to further the-research into this-idea.
however, the-research were only to receive partial-test-resultsmccarthy-and-shannon's-idea behind this-theory was to develop a-way to use complex-problems to determine and measure the-machine's-efficiency through mathematical-theory and computations.
however, the-research were only to receive partial-test-results.
the-idea behind self-improvement is how a-machine would use self-modifying-code to make a-machine smarter.
this would allow for a-machine to grow in intelligence and increase calculation-speeds.
the-group believed  the-group could study this if a-machine could improve upon the-process of completing a-task in the-abstractions-part of  the-group research.
the-group thought that research in this-category could be broken down into smaller-groups.
this would consist of sensory and other-forms of information about artificial-intelligence.
abstractions in computer-science can refer to mathematics-and-programing-language.
mathematics-and-programing-language-idea of computational-creativity is how the-program or a-machine can be seen in having similar-ways of human-thinking.
they wanted to see if a-machine could take a-piece of incomplete-information and improve upon a-machine to fill in the-missing-details as the-human-mind can do.
if a-machine could do this; they needed to think of how did a-machine determine the-outcome.
see also ==
computer-museum-list of computer-term-etymologies, the-origins of computer-science-words
list of pioneers in computer-science-history of computing history of computing hardware history of software history of personal-computers
timeline of algorithms timeline of women in computing timeline of computing 2020–2029 ==
references ==
sources ===
evans, claire-l. (2018).
broad-band:
the-untold-story of the-women who made the-internet.
new-york: portfolio/penguin.
isbn 9780735211759.
grier, david-alan (2013).
when computers were human.
princeton:
princeton-university-press.
isbn 9781400849369 – via project-muse.
further-reading ==
tedre, matti (2014).
the-science of computing:
shaping a-discipline.
taylor and francis-/-crc-press.
isbn 978-1-4822-1769-8.
kak, subhash : computing-science in ancient-india; munshiram-manoharlal-publishers-pvt.
ltd (2001)
the-development of computer-science:
a-sociocultural-perspective matti-tedre's-ph.d.-thesis,
university of joensuu (2006) ceruzzi, paul-e. (1998).
a-history of a-modern-computing.
the-mit-press.
isbn 978-0-262-03255-1.
copeland, b.-jack. "
the-modern-history of computing".
in zalta, edward-n. (ed.).
stanford-encyclopedia of philosophy.
external-links ==
computer-history-museum-computers:
from the-past to the-present
the-first-"computer-bug" at the-naval-history-and-heritage-command-photo-archives.
bitsavers, an-effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the-1950s, 1960s, 1970s, and 1980s
oral history interviews in software-engineering-and-computer-science, abstraction is:
the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems to focus attention on details of greater-importance; it is similar in nature to the-process of generalization; the creation of abstract-concept-objects by mirroring common-features or attributes of various-non-abstract-objects or systems of study – the-result of the-process of abstraction.
abstraction, in general, is a-fundamental-concept in computer-science-and-software-development.
the-process of abstraction can also be referred to as modeling and is closely related to the-concepts of theory and design.
models can also be considered types of abstractions per models generalization of aspects of reality.
abstraction in computer-science is closely related to abstraction in mathematics due to abstraction in mathematics-common-focus on building-abstractions as objects, but is also related to other-notions of abstraction used in other-fields such as art.
abstractions may also refer to real-world-objects and systems, rules of computational-systems or rules of programming-languages that carry or utilize features of abstraction itself, such as: the-usage of data-types to perform data-abstraction to separate-usage from working-representations of data-structures within programs; the concept of procedures, functions, or subroutines which represent a-specific of implementing control-flow in programs; the-rules commonly named "abstraction" that generalize expressions using free-and-bound-variables in the-various-versions of lambda-calculus;
the-usage of s-expressions as an-abstraction of data-structures and programs in the-lisp-programming-language; the-process of reorganizing common-behavior from non-abstract-classes into "abstract-classes" using inheritance to abstract over sub-classes as seen in the-object-oriented-c++ and java-programming-languages.
rationale ==
computing mostly operates independently of the-concrete-world.
the-hardware implements a-model of computation that is interchangeable with others.
the-software is structured in architectures to enable humans to create the-enormous-systems by concentrating on a-few-issues at a-time.
architectures are made of specific-choices of abstractions.
greenspun's-tenth-rule is an-aphorism on how such-an-architecture is both inevitable and complex.
a-central-form of abstraction in computing is language-abstraction: new-artificial-languages are developed to express specific-aspects of a-system.
modeling-languages help in planning.
computer-languages can be processed with a-computer.
an-example of this-abstraction-process is the-generational-development of programming-languages from the-machine-language to the-assembly-language and the-high-level-language.
each-stage can be used as a-stepping-stone for the-next-stage.
the-language-abstraction continues for example in scripting-languages and domain-specific-programming-languages.
within a-programming-language, some-features let the-programmer create new-abstractions.
these include subroutines, modules, polymorphism, and software-components.
some-other-abstractions such as software-design-patterns and architectural-styles remain invisible to a-translator and operate only in the-design of a-system.
some-abstractions try to limit the-range of concepts a-programmer needs to be aware of, by completely hiding the-abstractions that some-abstractions in turn are built on.
the-software-engineer and writer-joel-spolsky has criticised these-efforts by claiming that all-abstractions are leaky – that the-software-engineer and writer-joel-spolsky can never completely hide the-details below; however, this does not negate the-usefulness of abstraction.
some-abstractions are designed to inter-operate with other-abstractions – for example, a-programming-language may contain a-foreign-function-interface for making calls to the-lower-level-language.
abstraction features == ===
programming-languages ===
different-programming-languages provide different-types of abstraction, depending on the-intended-applications for the-language.
for example: in object-oriented-programming-languages such as c++, object-pascal, or java, the-concept of abstraction has the-concept of abstraction become a-declarative-statement – using the-syntax function(parameters) = 0; (in c++) or the keywords abstract and interface (in java).
after such-a-declaration, it is the-responsibility of the-programmer to implement a-class to instantiate the-object of such-a-declaration.
functional-programming-languages commonly exhibit abstractions related to functions, such as lambda-abstractions (making a-term into a-function of some-variable) and higher-order-functions (parameters are functions).
modern-members of the-lisp-programming-language-family such as clojure, scheme and common-lisp
support-macro-systems to allow syntactic-abstraction.
other-programming-languages such as scala also have macros, or very similar metaprogramming-features (for example, haskell has template haskell, and ocaml has metaocaml).
these can allow a-programmer to eliminate boilerplate-code, abstract away tedious-function-call-sequences, implement new-control-flow-structures, and implement domain-specific-languages (dsls), which allow domain-specific-concepts to be expressed in concise-and-elegant-ways.
all of these, when used correctly, improve both-the-programmer's-efficiency and the-clarity of the-code by making the-intended-purpose more explicit.
a-consequence of syntactic-abstraction is also that any-lisp-dialect and in fact almost-any-programming-language can, in principle, be implemented in any-modern-lisp with significantly reduced (but still non-trivial in some-cases) effort when compared to "more-traditional"-programming-languages such as python, c or java.
specification-methods ===
analysts have developed various-methods to formally specify software-systems.
some-known-methods include: abstract-model-based-method (vdm, z);
algebraic-techniques (larch, clear, obj, act one, casl); process-based-techniques (lotos, sdl, estelle); trace-based-techniques (special, tam); knowledge-based-techniques (refine, gist).
specification-languages ===
specification-languages generally rely on abstractions of one-kind or another, since specifications are typically defined earlier in a-project, (and at a-more-abstract-level) than an-eventual-implementation.
the-uml-specification-language, for example, allows the-definition of abstract-classes, which in a-waterfall-project, remain abstract during the-architecture-and-specification-phase of the-project.
control-abstraction ==
programming-languages offer control-abstraction as one of the-main-purposes of  programming-languages use.
computer-machines understand operations at the-very-low-level such as moving some-bits from one-location of the-memory to another-location and producing the-sum of two-sequences of bits.
programming-languages allow this to be done in the-higher-level.
for example, consider this-statement written in a-pascal-like-fashion: a := (1 + 2)
* 5to a-human, this seems a fairly simple and obvious calculation ("one plus two is three,
times five is fifteen").
however, the-low-level-steps necessary to carry out this-evaluation, and return the-value "15", and then assign that-value to the-variable "a", are actually quite subtle and complex.
the-values need to be converted to binary-representation (often-a-much-more-complicated-task than one would think) and the-calculations decomposed (by the-compiler or interpreter) into assembly-instructions (again, which are much less intuitive to the-programmer: operations such as shifting a-binary-register left, or adding the-binary-complement of the-contents of one-register to another, are simply not how humans think about the-abstract-arithmetical-operations of addition or multiplication).
finally, assigning the-resulting-value of "15" to the-variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a-variable's-label and the-resultant-location in physical-or-virtual-memory, storing the-binary-representation of "15" to that-memory-location, etc.
without control-abstraction, a-programmer would need to specify all-the-register/binary-level-steps each time a-programmer simply wanted to add or multiply a-couple of numbers and assign the-result to a-variable.
such-duplication of effort has two-serious-negative-consequences: such-duplication of effort-forces the-programmer to constantly repeat fairly-common-tasks every time a-similar-operation is needed such-duplication of effort-forces the-programmer to program for the-particular-hardware and instruction set
structured-programming ===
structured-programming involves the-splitting of complex-program-tasks into smaller-pieces with clear-flow-control and interfaces between components, with a-reduction of the-complexity-potential for side-effects.
in a-simple-program, this may aim to ensure that loops have single-or-obvious-exit-points and (where possible) to have single-exit-points from functions and procedures.
in a-larger-system, this may involve breaking down complex-tasks into many-different-modules.
consider a-system which handles payroll on ships and at shore-offices: the-uppermost-level may feature a-menu of typical-end-user-operations.
within that could be standalone-executables or libraries for tasks such as signing on and off employees or printing-checks.
within each of those-standalone-components there could be many-different-source-files, each containing the-program-code to handle a-part of the-problem, with only-selected-interfaces available to other-parts of the-program.
a-sign on program could have source-files for each-data-entry-screen and the-database-interface (which may itself be a-standalone-third-party-library or a-statically-linked-set of library-routines).
either-the-database or the-payroll-application also has to initiate the-process of exchanging data with between ship and shore, and that data-transfer-task will often contain many-other-components.
these-layers produce the-effect of isolating the-implementation-details of one-component and its-assorted-internal-methods from the-others.
object-oriented-programming embraces and extends this-concept.
data-abstraction ==
data-abstraction enforces a-clear-separation between the-abstract-properties of a-data-type and the-concrete-details of  data-abstraction implementation.
the-abstract-properties are those that are visible to client-code that makes use of the-data-type—the-interface to the-data-type—while the-concrete-implementation is kept entirely private, and indeed can change, for example to incorporate efficiency-improvements over time.
the-idea is that such-changes are not supposed to have any-impact on client-code, since they involve no-difference in the-abstract-behaviour.
for example, one could define an-abstract-data-type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their-corresponding-keys.
such-a-lookup-table may be implemented in various-ways: as a-hash-table, a-binary-search-tree, or even-a-simple-linear-list of (key:value)-pairs.
as far as client-code is concerned, the-abstract-properties of the-type are the same in each-case.
of course, this all relies on getting the-details of the-interface right in the-first-place, since any-changes there can have major-impacts on client-code.
as one-way to look at this: the-interface forms a-contract on agreed-behaviour between the-data-type-and-client-code; anything not spelled out in the-contract is subject to change without notice.
manual-data-abstraction ==
while much of data-abstraction occurs through computer-science and automation, there are times when this-process is done manually and without programming-intervention.
one-way this can be understood is through data-abstraction within the-process of conducting a-systematic-review of the-literature.
in this-methodology, data is abstracted by one-or-several-abstractors when conducting a-meta-analysis, with errors reduced through dual-data-abstraction followed by independent-checking, known as adjudication.
abstraction in object-oriented-programming ==
in object-oriented-programming-theory, abstraction involves the-facility to define objects that represent abstract-"actors" that can perform work, report on and change abstract-"actors" that can perform work-state, and "communicate" with other-objects in the-system.
the-term-encapsulation refers to the-hiding of state-details, but extending the-concept of data-type from earlier-programming-languages to associate behavior most strongly with the-data, and standardizing the-way that different data-types interact, is the-beginning of abstraction.
when abstraction proceeds into the-operations defined, enabling objects of different-types to be substituted, the-term-encapsulation is called polymorphism.
when the-term-encapsulation proceeds in the-opposite-direction, inside the-types or classes, structuring the-types or classes to simplify a-complex-set of relationships, the-term-encapsulation is called delegation or inheritance.
various-object-oriented-programming-languages offer similar-facilities for abstraction, all to support a-general-strategy of polymorphism in object-oriented-programming, which includes the-substitution of one-type for another in the-same-or-similar-role.
although not as generally supported, a-configuration or image or package may predetermine a great many of these-bindings at compile-time, link-time, or loadtime.
this would leave only-a-minimum of such-bindings to change at run-time.
common-lisp-object-system or self, for example, feature less of a-class-instance-distinction and more-use of delegation for polymorphism.
individual-objects and functions are abstracted more flexibly to better fit with a-shared-functional-heritage from lisp.
c++ exemplifies another-extreme
: c++ relies heavily on templates and overloading and other-static-bindings at compile-time, which in turn has certain-flexibility-problems.
although these-examples offer alternate-strategies for achieving the-same-abstraction, these-examples do not fundamentally alter the-need to support abstract-nouns in code – all-programming relies on an-ability to abstract verbs as functions, nouns as data-structures, and either as processes.
consider for example a-sample java-fragment to represent some-common-farm-"animals" to a-level of abstraction suitable to model simple-aspects of some-common-farm-"animals" hunger and feeding.
it defines an-animal-class to represent both-the-state of the-class-animal
functions: with the-above-definition, one could create objects of type-animal and call objects of type-animal methods like this: in the-above-example, the-class-animal is an-abstraction used in place of an-actual-animal, livingthing is a-further-abstraction (in this-case a generalisation) of animal.
if one requires a-more-differentiated-hierarchy of animals – to differentiate, say, those who provide milk from those who provide nothing except meat at the-end of those who provide milk from those who provide nothing except meat at the-end of their-lives lives – that is an-intermediary-level of abstraction, probably-dairyanimal (cows, goats) who would eat foods suitable to giving good-milk, and meatanimal (pigs, steers) who would eat foods to give the-best-meat-quality.
such-an-abstraction could remove the-need for the-application-coder to specify the-type of food, so the-application-coder could concentrate instead on the-feeding-schedule.
the-two-classes could be related using inheritance or stand alone, and the-programmer could define varying-degrees of polymorphism between the-two-types.
these-facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the-others.
a-great-many-operation-overloads, data-type by data-type, can have the-same-effect at compile-time as any-degree of inheritance or
other-means to achieve polymorphism.
the-class-notation is simply a-coder's-convenience.
===-object-oriented-design ===
decisions regarding what to abstract and what to keep under the-control of the-coder become the-major-concern of object-oriented-design-and-domain-analysis—actually determining the-relevant-relationships in the-real-world is the-concern of object-oriented-analysis or legacy-analysis.
in general, to determine appropriate-abstraction, one must make many-small-decisions about scope (domain-analysis), determine what other-systems one must cooperate with (legacy-analysis), then perform a-detailed-object-oriented-analysis which is expressed within project-time and budget-constraints as an-object-oriented-design.
in our-simple-example, the-domain is the-barnyard,
the-live-pigs and cows and their-eating-habits are the-legacy-constraints, the-detailed-analysis is that coders must have the-flexibility to feed the-animals what is available and thus there is no-reason to code the-type of food into the-class itself, and the-design is a-single-simple-animal-class of which pigs and cows are instances with the-same-functions.
a-decision to differentiate dairyanimal would change the-detailed-analysis but the-domain-and-legacy-analysis would be unchanged—thus it is entirely under the-control of the-programmer, and it is called an abstraction in object-oriented-programming as distinct from abstraction in domain or legacy-analysis.
considerations ==
when discussing formal-semantics of programming-languages, formal-methods or abstract-interpretation, abstraction refers to the-act of considering a less detailed, but safe,-definition of the-observed-program-behaviors.
for instance, one may observe only-the-final-result of program-executions instead of considering all-the-intermediate-steps of executions.
abstraction is defined to a-concrete-(more-precise)-model of execution.
abstraction may be exact or faithful with respect to a-property if one can answer a-question about a-property equally well on the-concrete-or-abstract-model.
for instance, if one wishes to know what the result of the-evaluation of a-mathematical-expression involving only-integers +, -, ×, is worth modulo n, then one needs only perform all-operations modulo n (a-familiar-form of this-abstraction is casting out nines).
abstractions, however, though not necessarily exact, should be sound.
that is, it should be possible to get sound-answers from  that—even though the-abstraction may simply yield a-result of undecidability.
for instance, students in a-class may be abstracted by students in a-class minimal and maximal ages; if one asks whether a-certain-person belongs to that-class, one may simply compare that-person's-age with the-minimal-and-maximal-ages; if that-person-age lies outside the-range, one may safely answer that the-person does not belong to that-class; if that-class does not, one may only answer " i don't know".
the-level of abstraction included in a-programming-language can influence the-level of abstraction included in a-programming-language overall usability.
the-cognitive-dimensions-framework includes the-concept of abstraction-gradient in a-formalism.
this-framework allows the-designer of a-programming-language to study the-trade-offs between abstraction and other-characteristics of the-design, and how changes in abstraction influence the-language-usability.
abstractions can prove useful when dealing with computer-programs, because non-trivial-properties of computer-programs are essentially undecidable (see rice's-theorem).
as a-consequence, automatic-methods for deriving information on the-behavior of computer-programs either have to drop termination (on some-occasions, automatic-methods for deriving information on the-behavior of computer-programs may fail, crash or never yield out a-result), soundness (
automatic-methods for deriving information on the-behavior of computer-programs may provide false-information), or precision (automatic-methods for deriving information on the-behavior of computer-programs may answer "i don't know" to some-questions).
abstraction is the-core-concept of abstract-interpretation.
model-checking generally takes place on abstract-versions of the-studied-systems.
levels of abstraction ==
computer-science commonly presents levels (or, less commonly,-layers) of abstraction, wherein each-level represents a-different-model of the-same-information and processes, but with varying-amounts of detail.
each-level uses a-system of expression involving a-unique-set of objects and compositions that apply only to a-particular-domain.
each-relatively-abstract,-"higher"-level builds on a relatively concrete,-"lower"-level, which tends to provide an-increasingly-"granular"-representation.
for example, gates build on electronic-circuits, binary on gates, machine-language on binary,-programming-language on machine-language, applications and operating-systems on programming-languages.
each-level is embodied, but not determined, by the-level beneath each-level, making each-level a language of description that is somewhat self-contained.
database-systems ===
since many-users of database-systems lack in-depth familiarity with computer-data-structures, database-developers often hide complexity through the-following-levels:
physical-level:
the-lowest-level of abstraction describes how-a-system actually stores data.
the-physical-level describes complex-low-level-data-structures in detail.
logical level: the-next-higher-level of abstraction describes what data the-database-stores, and what-relationships exist among those-data.
the-logical-level thus describes an-entire-database in terms of a-small-number of relatively-simple-structures.
although implementation of the-simple-structures at the-logical-level may involve complex-physical-level-structures, the-user of the-logical-level does not need to be aware of this-complexity.
this is referred to as physical-data-independence.
database-administrators, who must decide what-information to keep in a-database, use the-logical-level of abstraction.
view-level
: the-highest-level of abstraction describes only-part of the-entire-database.
even though the-logical-level uses simpler-structures, complexity remains because of the-variety of information stored in a-large-database.
many-users of a-database-system do not need all-this-information; instead, many-users of a-database-system need to access only-a-part of the-database.
the-view-level of abstraction exists to simplify many-users of a-database-system-interaction with the-system.
the-system may provide many-views for the-same-database.
layered-architecture ===
the-ability to provide a-design of different-levels of abstraction can simplify a-design of different-levels of abstraction considerably enable different-role-players to effectively work at various-levels of abstraction support the portability of software-artifacts (model-based ideally)systems design and business process design can both use this.
some-design-processes specifically generate designs that contain various-levels of abstraction.
layered-architecture partitions the-concerns of the-application into stacked-groups (layers).
it is a-technique used in designing computer-software, hardware, and communications in which system or network-components are isolated in layers so that changes can be made in one-layer without affecting the-others.
see also ==
abstraction-principle (computer-programming)
abstraction-inversion for an-anti-pattern of one-danger in abstraction abstract-data-type for an-abstract-description of a-set of data-algorithm for an-abstract-description of a-computational-procedure-bracket-abstraction for making a-term into a-function of a variable data modeling for structuring data independent of the-processes that use it
encapsulation for abstractions that hide implementation-details
greenspun's-tenth-rule for an-aphorism about an (the?)
optimum point in the-space of abstractions higher-order function for abstraction where functions produce or consume other-functions lambda-abstraction for making a-term into a-function of some-variable
list of abstractions (computer-science)-refinement for the-opposite of abstraction in computing integer (computer-science) heuristic (computer-science) ==
references == ==
further-reading == ==
external-links ==
simarch-example of layered-architecture for distributed-simulation-systems.
computer-science & engineering (cse) is an-academic-program at many-universities which comprises scientific-and-engineering-aspects of computing.
computer-science & engineering (cse) is also a-term often used in europe to translate the-name of engineering-informatics-academic-programs.
academic-courses ==
academic-programs vary between colleges.
courses usually include introduction to programming, introduction to algorithms-and-data-structures, computer-architecture, operating-systems, computer-networks, parallel-computing, embedded-systems, algorithms-design, circuit-analysis and electronics, digital-logic and processor-design, computer-graphics, scientific-computing, software-engineering, database-systems, digital-signal-processing, virtualization, computer-simulations and games-programming.
cse-programs also include core-subjects of theoretical-computer-science such as theory of computation, numerical-methods, machine-learning, programming-theory and paradigms.
modern-academic-programs also cover emerging-computing-fields like image-processing, data-science, robotics, bio-inspired-computing,-computational-biology, autonomic-computing and artificial-intelligence.
most of the-above-cse-areas require initial-mathematical-knowledge, hence-the-first-year of study is dominated by mathematical-courses, primarily-discrete-mathematics, mathematical-analysis, linear-algebra and statistics, as well as the-basics of physics---field-theory and electromagnetism.
example-universities with cse-majors ==
massachusetts institute of technology university of oxford california institute of technology
stanford-university-north-south-university-north-western-university, khulna-dhaka-university
university of barisal-university of chittagong american university of beirut santa clara university
university of michigan university of new-south-wales university of washington bucknell university indian institute of technology kanpur
indian institute of technology bombay indian institute of technology delhi indian institute of technology madras national university of singapore amrita vishwa vidyapeetham
university of nevada-university of notre dame delft university of technology ==
see also ==
computer-science-computer-graphics (computer-science) ==
references ==
adaptive-replacement-cache (arc) is a-page-replacement-algorithm with
better-performance than lru (least recently used).
this is accomplished by keeping track of both frequently used and recently used pages plus a-recent-eviction-history for both.
this was developed at the-ibm-almaden-research-center.
in 2006, the-ibm-almaden-research-center was granted a-patent for the-adaptive-replacement-cache-policy.
summary ==
basic-lru maintains an-ordered-list (the-cache-directory) of resource-entries in the-cache, with the-sort-order based on the-time of most-recent-access.
new-entries are added at the-top of the-list, after the-bottom-entry has been evicted.
cache hits move to the-top, pushing new-entries down.
arc improves the-basic-lru-strategy by splitting the-cache-directory into two-lists, t1 and t2, for recently and frequently referenced entries.
in turn, each of these is extended with a-ghost-list (b1 or b2), which is attached to the-bottom of the-two-lists.
these ghost lists act as scorecards by keeping track of the-history of recently-evicted-cache-entries, and the-algorithm uses ghost hits to adapt to recent-change in resource-usage.
note that the ghost lists only contain metadata (keys for the-entries) and not-the-resource-data itself, i.e. as an-entry is evicted into a-ghost-list an-entry data is discarded.
the-combined-cache-directory is organised in four-lru-lists: t1, for recent-cache-entries.
t2, for frequent-entries, referenced at least twice.
b1, ghost-entries recently evicted from the-t1-cache, but are still tracked.
b2, similar-ghost-entries, but evicted from t2.t1 and b1 together are referred to as l1, a-combined-history of recent-single-references.
similarly, l2 is the-combination of t2 and b2.
the-whole-cache-directory can be visualised in a-single-line:
t2---]->--b2   ] . .
[ . . . . . . ! .
.^. . . . ] .
[---fixed-cache-size (c)    ]
the-inner-[-]-brackets indicate actual-cache, which although fixed in size, can move freely across the-b1-and-b2-history.
l1 is now displayed from right to left, starting at the-top, indicated by the !
^ indicates the-target-size for t1, and may be equal to, smaller than, or larger than the-actual-size (as indicated by !).
new-entries enter t1, to the-left of !,
and are gradually pushed to the-left, eventually being evicted from t1 into b1, and finally dropped out altogether.
any-entry in l1 that gets referenced once more, gets another-chance, and enters l2, just to the-right of the-central !
from there, it is again pushed outward, from t2 into b2.
entries in l2 that get another-hit can repeat this indefinitely, until  entries in l2 that get another-hit finally drop out on the-far-right of b2.
replacement === entries (re-)entering the-cache (t1, t2) will cause !
to move towards the-target-marker ^.  if no-free-space exists in the-cache, this-marker also determines whether either-t1 or t2 will evict an-entry.
hits in b1 will increase the-size of t1, pushing ^ to the-right.
the-last-entry in t2 is evicted into b2.
hits in b2 will shrink t1, pushing ^ back to the-left.
the-last-entry in t2 is now evicted into b1.
a-cache-miss will not affect ^, but the !
boundary will move closer to ^. ==
deployment-==-arc is currently deployed in ibm's-ds6000/ds8000-storage-controllers.
sun-microsystems's-scalable-file-system zfs uses a-variant of arc as an-alternative to the-traditional-solaris-filesystem-page-cache in virtual-memory.
it has been modified to allow for locked-pages that are currently in use and cannot be vacated.
postgresql used arc in postgresql buffer manager for a-brief-time (version 8.0.0), but quickly replaced it with another-algorithm, citing concerns over an-ibm-patent on arc.vmware's vsan
(formerly-virtual-san) is a hyper-converged, software-defined storage (sds) product developed by vmware.
formerly-virtual-san) uses a-variant of arc in (formerly
virtual-san)-caching-algorithm.
see also ==
clock with adaptive-replacement-lirs-caching-algorithm ==
references == ==
external-links
a-self-tuning,-low-overhead-replacement-cache (2003) by nimrod-megiddo,  dharmendra-modha-linux-memory-management-wiki-bourbonnais, roch.
zfs-dynamics-python-implementation, recipe 576532 comparison of lru, arc and others
in 3d-computer-graphics and computer-vision, a-depth-map is an-image-or-image-channel that contains information relating to the-distance of the-surfaces of scene-objects from a-viewpoint.
the-term is related to and may be analogous to depth buffer, z-buffer, z-buffering and z-depth.
the-"z" in these-latter-terms relates to a-convention that the-central-axis of view of a-camera is in the-direction of the-camera's-z-axis, and not to the-absolute-z-axis of a-scene.
examples ==
two-different-depth-maps can be seen here, together with the-original-model from which they are derived.
the-first-depth-map shows luminance in proportion to the-distance from the-camera.
nearer-surfaces are darker; further-surfaces are lighter.
the-second-depth-map shows luminance in relation to the-distances from a-nominal-focal-plane.
surfaces closer to the-focal-plane are darker; surfaces further from the-focal-plane are lighter, (both closer to and also further away from the-viewpoint).
depth-maps have a-number of uses, including: simulating the-effect of uniformly-dense-semi-transparent-media within a-scene - such as fog, smoke or large-volumes of water.
simulating shallow-depths of field - where some-parts of a-scene appear to be out of focus.
depth maps can be used to selectively blur an-image to varying-degrees.
a-shallow-depth of field can be a-characteristic of macro-photography
and so the-technique may form a-part of the-process of miniature-faking.
z-buffering and z-culling, techniques which can be used to make the-rendering of 3d-scenes more efficient.
they can be used to identify objects hidden from view and which may therefore be ignored for some-rendering-purposes.
this is particularly important in real-time-applications such as computer-games, where a-fast-succession of completed-renders must be available in time to be displayed at a-regular-and-fixed-rate.
shadow-mapping---part of one-process used to create shadows cast by illumination in 3d-computer-graphics.
in this-use, the-depth-maps are calculated from the-perspective of the-lights, not the-viewer.
to provide the-distance-information needed to create and generate autostereograms and in other-related-applications intended to create the-illusion of 3d viewing through stereoscopy .
subsurface-scattering - can be used as part of a-process for adding realism by simulating the-semi-transparent-properties of translucent-materials such as human-skin.
in computer-vision-single-view-or-multi-view-images depth maps, or other-types of images, are used to model 3d-shapes or reconstruct them.
depth-maps can be generated by 3d-scanners or reconstructed from multiple-images.
in machine-vision and computer-vision, to allow 3d-images to be processed by 2d-image-tools.
making depth-image-datasets.
limitations ==
single-channel depth maps record the-first-surface seen, and so cannot display information about those-surfaces seen or refracted through transparent-objects, or reflected in mirrors.
this can limit their-use in accurately simulating depth of field-or-fog-effects.
single-channel depth maps cannot convey multiple-distances where they occur within the-view of a-single-pixel.
this may occur where more-than-one-object occupies the-location of that-pixel.
this could be the-case - for example - with models featuring hair, fur or grass.
more generally, edges of objects may be ambiguously described where edges of objects partially cover a-pixel.
depending on the-intended-use of a-depth-map, it may be useful or necessary to encode a-depth-map at higher-bit-depths.
for example, an-8-bit-depth-map can only represent a-range of up-to-256-different-distances.
depending on how they are generated, depth-maps may represent the-perpendicular-distance between an-object and the-plane of the-scene-camera.
for example, a-scene-camera pointing directly at - and perpendicular to - a-flat-surface may record a-uniform-distance for the-whole-surface.
in this-case, geometrically, the-actual-distances from the-scene-camera to the-areas of the-plane-surface seen in the-corners of the-image are greater than the-distances to the-central-area.
for many-applications, however, this-discrepancy is not a-significant-issue.
references == ==
see also ==
2d-plus-depth-painter's-algorithm
range-imaging structured-light-wowvx-cache-hierarchy, or multi-level-caches, refers to a-memory-architecture that uses a-hierarchy of memory-stores based on varying-access-speeds to cache-data.
highly-requested-data is cached in high-speed-access-memory-stores, allowing swifter-access by central-processing-unit-(cpu)-cores.
cache-hierarchy is a-form and part of memory-hierarchy and can be considered a-form of tiered-storage.
this-design was intended to allow cpu-cores to process faster despite the-memory-latency of main-memory-access.
accessing main-memory can act as a-bottleneck for cpu-core-performance as the-cpu waits for data, while making all of main-memory high-speed may be prohibitively expensive.
high-speed-caches are a-compromise allowing high-speed-access to the-data most-used by the-cpu, permitting a-faster-cpu-clock.
background ==
in the-history of computer-and-electronic-chip-development, there was a-period when increases in cpu-speed outpaced the-improvements in memory-access-speed.
the-gap between the-speed of cpus and memory meant that the-cpu would often be idle.
cpus were increasingly capable of running and executing larger-amounts of instructions in a-given-time, but the-time needed to access data from main-memory prevented programs from fully benefiting from this-capability.
this-issue motivated the-creation of memory-models with higher-access-rates in order to realize the-potential of faster-processors.
this resulted in the-concept of cache-memory, first proposed by maurice-wilkes, a-british-computer-scientist at the-university of cambridge in 1965.
he called such-memory-models "slave-memory".
between roughly 1970 and 1990, papers and articles by anant-agarwal, alan-jay-smith, mark-d.-hill, thomas-r.-puzak, and others discussed better-cache-memory-designs.
the-first-cache-memory-models were implemented at the-time, but even as researchers were investigating and proposing better-designs, the-need for faster-memory-models continued.
the-need for faster-memory-models resulted from the-fact that although early-cache-models improved data-access-latency, with respect to cost and technical-limitations it was not feasible for a-computer-system's-cache to approach the-size of main-memory.
from 1990 onward, ideas such as adding another-cache-level (second-level), as a-backup for the-first-level-cache were proposed.
jean-loup-baer,-wen-hann-wang, andrew-w.-wilson, and others have conducted research on this-model.
when several-simulations and implementations demonstrated the-advantages of two-level-cache-models, the-concept of multi-level-caches caught on as a-new-and-generally-better-model of cache-memories.
since 2000, multi-level-cache-models have received widespread-attention and are currently implemented in many-systems, such as the-three-level-caches that are present in intel's-core-i7-products.
multi-level-cache ==
accessing main-memory for each-instruction-execution may result in slow-processing, with the-clock-speed depending on the-time required to find and fetch the-data.
in order to hide this-memory-latency from the-processor, data-caching is used.
whenever the-data is required by the-processor, the-data is fetched from the-main-memory and stored in the-smaller-memory-structure called a cache.
if there is any-further-need of the-data, the-cache is searched first before going to the-main-memory.
the-smaller-memory-structure called a cache resides closer to the-processor in terms of the-time taken to search and fetch data with respect to the-main-memory.
the-advantages of using cache can be proven by calculating the-average-access-time (aat) for the-memory-hierarchy with and without the-cache.
average-access-time (aat) ===
caches, being small in size, may result in frequent-misses – when a-search of the-cache does not provide the-sought-after-information – resulting in a-call to main-memory to fetch data.
hence, the-aat is affected by the-miss-rate of each-structure from which the-aat searches for the-data.
aat         =           hit time + (
(           miss rate         )         × (
miss-penalty         )         )     {\displaystyle { \text{aat}}={\text{hit-time}}+(({\text{miss-rate}})\times ({\text{miss-penalty}}))}
aat for main-memory is given by hit time main-memory.
aat for caches can be given by hit-timecache + (miss-ratecache-×-miss-penaltytime taken to go to main-memory after missing cache).the-hit-time for caches is less than the-hit-time for the main-memory, so
aat for main-memory is significantly lower when accessing data through the-cache rather than main-memory.
=== trade-offs ===
while using the-cache may improve memory-latency, using the-cache may not always result in the-required-improvement for the-time taken to fetch data due to the-way caches are organized and traversed.
for example, direct-mapped-caches that are the-same-size usually have a-higher-miss-rate than fully-associative-caches.
this may also depend on the-benchmark of the-computer testing the-processor and on the-pattern of instructions.
but using a-fully-associative-cache may result in more-power-consumption, as it has to search the-whole-cache every time.
due to this, the-trade-off between power-consumption (and associated-heat) and the-size of the-cache becomes critical in the-cache design.
evolution ===
in the-case of a-cache-miss, the-purpose of using such-a-structure will be rendered useless and the-computer will have to go to the-main-memory to fetch the-required-data.
however, with a-multiple-level-cache, if the-computer misses the-cache closest to the-processor (level-one cache or l1)
it will then search through the next-closest level(s) of cache and go to main-memory only if these-methods fail.
the-general-trend is to keep the-l1-cache small and at a-distance of 1–2-cpu-clock-cycles from the-processor, with the-lower-levels of caches increasing in size to store more-data than l1, hence being more distant but with a-lower-miss-rate.
this results in a-better-aat.
the-number of cache-levels can be designed by architects according to architects-requirements after checking for trade-offs between cost, aats, and size.
performance-gains ===
with the technology-scaling that allowed memory-systems able to be accommodated on a-single-chip, most-modern-day-processors have up-to-three-or-four-cache-levels.
the-reduction in the-aat can be understood by this-example, where the-computer checks aat for different-configurations up to l3-caches.
example: main memory = 50 ns,
l1 = 1 ns with 10%-miss-rate, l2 = 5 ns
with1%-miss-rate) ,-l3-=-10-ns with 0.2%-miss-rate.
no-cache, aat = 50
ns l1-cache,
+ (0.1 × 50 ns) =
ns-l1–2-caches, aat = 1 ns
+ (0.1 × [5 ns
+ (0.01 × 50 ns)]) = 1.55 ns
l1–3-caches, aat = 1 ns
+ (0.1 × [5 ns
+ (0.01 × [10 ns + (0.002 × 50 ns)])]) =
disadvantages ===
cache-memory comes at an-increased-marginal-cost than main-memory and thus can increase the-cost of the-overall-system.
cached-data is stored only so long as power is provided to the-cache.
increased on-chip area required for memory-system.
benefits may be minimized or eliminated in the-case of a-large-programs with poor-temporal-locality, which frequently access the-main-memory.
properties == === banked versus unified ===
in a-banked-cache, a-banked-cache is divided into a-cache dedicated to instruction-storage and a-cache dedicated to data.
in contrast, a-unified-cache contains both-the-instructions and data in the-same-cache.
during a-process, the-l1-cache (or most-upper-level-cache in relation to a-process connection to the-processor) is accessed by the-processor to retrieve both-instructions and data.
requiring both-actions to be implemented at the-same-time requires multiple-ports and more-access-time in a-unified-cache.
having multiple-ports requires additional-hardware and wiring, leading to a-significant-structure between the-caches and processing-units.
to avoid this, the-l1-cache is often organized as a-banked-cache which results in fewer-ports, less-hardware, and generally lower access times.
modern-processors have split caches, and in systems with multilevel-caches higher-level-caches may be unified while lower-levels split.
inclusion-policies ===
whether a-block present in the-upper-cache-layer can also be present in the-lower-cache-level is governed by the-memory-system's-inclusion-policy, which may be inclusive, exclusive or non-inclusive non-exclusive (nine).with an inclusive policy, all-the-blocks present in the-upper-level-cache have to be present in the-lower-level-cache as well.
each-upper-level-cache-component is a-subset of the-lower-level-cache-component.
in this-case, since there is a-duplication of blocks, there is some-wastage of memory.
however, checking is faster.
under an-exclusive-policy, all-the-cache-hierarchy-components are completely exclusive, so that any-element in the-upper-level-cache will not be present in any of the-lower-cache-components.
this enables complete-usage of the-cache-memory.
however, there is a-high-memory-access-latency.
the-above-policies require a-set of rules to be followed in order to implement the-above-policies.
if none of these are forced, the-resulting-inclusion-policy is called non-inclusive non-exclusive (nine).
this means that the-upper-level-cache may or may not be present in the-lower-level-cache.
write-policies ===
there are two-policies which define the-way in which a-modified-cache-block will be updated in the-main-memory: write through and write back.
in the-case of write through policy, whenever the-value of the-cache-block-changes, it is further modified in the-lower-level-memory-hierarchy as well.
policy ensures that the-data is stored safely as policy is written throughout the-hierarchy.
however, in the-case of the-write-back-policy, the-changed-cache-block will be updated in the-lower-level-hierarchy only when the-cache-block is evicted.
a-"dirty-bit" is attached to each-cache-block and set whenever the-cache-block is modified.
during eviction, blocks with a-set-dirty-bit will be written to the-lower-level-hierarchy.
under this-policy, there is a-risk for data-loss as the-most-recently-changed-copy of a-datum is only stored in the-cache
and therefore some-corrective-techniques must be observed.
in case of a-write where the-byte is not present in the-cache-block, the-byte may be brought to the-cache as determined by a-write allocate or write no-allocate-policy.
write allocate policy-states that in case of a-write-miss, the-block is fetched from the-main-memory and placed in the-cache before writing.
in the-write-no-allocate-policy, if the-block is missed in the-cache the-block will write in the-lower-level-memory-hierarchy without fetching the-block into the-cache.
the-common-combinations of the-policies are "write-block", "write allocate", and "write through write no-allocate".
shared versus private ===
a-private-cache is assigned to one-particular-core in a-processor, and cannot be accessed by any-other-cores.
in some-architectures, each-core has each-core own private cache; this creates the-risk of duplicate-blocks in a-system's-cache-architecture, which results in reduced-capacity-utilization.
however, this-type of design-choice in a-multi-layer-cache-architecture can also be good for a-lower-data-access-latency.
a-shared-cache is a-cache which can be accessed by multiple-cores.
since a-shared-cache is shared, each-block in a-shared-cache is unique and therefore has a-larger-hit-rate as there will be no-duplicate-blocks.
however, data-access-latency can increase as multiple-cores try to access the-same-cache.
in multi-core-processors, the design choice to make a-cache shared or private impacts the-performance of the-processor.
in practice, the-upper-level-cache-l1 (or sometimes-l2) is implemented as private-and-lower-level-caches are implemented as shared.
this-design provides high-access-rates for the-high-level-caches and low-miss-rates for the-lower-level-caches.
recent-implementation-models == ===
intel-broadwell-microarchitecture (2014)
===-l1-cache (instruction and data) – 64-kb per core-l2-cache –-256-kb per core
l3-cache – 2-mb to 6-mb shared l4 cache – 128-mb of edram
(iris-pro-models only) ===
intel-kaby-lake-microarchitecture (2016) ===
l1-cache (instruction and data) – 64-kb per core-l2-cache –-256-kb per core-l3-cache – 2-mb-to-8-mb shared
amd-zen-microarchitecture (2017) ===
l1-cache – 32-kb-data & 64-kb-instruction per core, 4-way-l2-cache – 512-kb per core, 4-way-inclusive
l3-cache – 4 mb local & remote per 4-core-ccx, 2-ccxs per chiplet, 16-way non-inclusive.
up-to-16-mb on desktop-cpus and 64-mb on server-cpus ===
amd-zen 2-microarchitecture (2019) ==
=-l1-cache – 32-kb-data & 32-kb-instruction per core, 8-way
l2-cache – 512-kb per core, 8-way-inclusive-l3-cache –
16-mb local per 4-core-ccx, 2-ccxs per chiplet, 16-way non-inclusive.
up-to-64-mb on desktop-cpus and 256-mb on server-cpus ===
ibm-power 7 === l1-cache (instruction and data) – each 64-banked,
each-bank has 2rd+1wr-ports 32-kb, 8-way associative, 128b-block, write through l2-cache – 256-kb, 8-way, 128b-block, write back, inclusive of l1, 2 ns access latency l3 cache – 8-regions of 4-mb (total-32-mb),
local-region
6 ns, remote-30-ns, each-region 8-way associative
,-dram-data-array,
sram-tag-array ==
see also ==
power7-intel-broadwell-microarchitecture-intel-kaby-lake-microarchitecture-cpu-cache-memory-hierarchy
cas-latency-cache (computing) ==
references ==
this-glossary of artificial-intelligence is a-list of definitions of terms and concepts relevant to the-study of artificial-intelligence, its-sub-disciplines, and related-fields.
related-glossaries include glossary of computer-science, glossary of robotics, and glossary of machine-vision.
a-==-abductive-logic-programming (alp)
a-high-level-knowledge-representation-framework that can be used to solve problems declaratively based on abductive-reasoning.
it extends normal-logic-programming by allowing some-predicates to be incompletely defined, declared as abducible-predicates.
abductive-reasoning also abduction.
a-form of logical-inference which starts with an-observation or set of observations then seeks to find the-simplest-and-most-likely-explanation.
this-process, unlike deductive-reasoning, yields a-plausible-conclusion but does not positively verify this-process.
abductive-inference, or retroduction-abstract-data-type
a-mathematical-model for data-types, where a-data-type is defined by a-data-type behavior (semantics) from the-point of view of a-user of the-data, specifically in terms of possible-values, possible-operations on data of this-type, and the-behavior of these-operations.
abstraction
the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest
accelerating-change a-perceived-increase in the-rate of technological-change throughout history, which may suggest faster-and-more-profound-change in the-future and may or may not be accompanied by equally-profound-social-and-cultural-change.
action-language
a-language for specifying state-transition-systems, and is commonly used to create formal-models of the-effects of actions on the-world.
action-languages are commonly used in the-artificial-intelligence-and-robotics-domains, where action-languages describe how actions affect the-states of systems over time, and may be used for automated-planning.
action-model learning an-area of machine learning concerned with creation and modification of software-agent's-knowledge about effects and preconditions of the-actions that can be executed within its-environment.
this-knowledge is usually represented in logic-based-action-description-language and used as the-input for automated-planners.
action-selection
a-way of characterizing the-most-basic-problem of intelligent-systems: what to do next.
in artificial-intelligence and computational-cognitive-science, "the-action-selection-problem" is typically associated with intelligent-agents and animats—artificial-systems that exhibit complex-behaviour in an-agent-environment.
activation-function in artificial-neural-networks
, the-activation-function of a-node defines the-output of that-node given an-input or set of inputs.
adaptive-algorithm
an-algorithm that changes its-behavior at the-time its is run, based on a-priori-defined-reward-mechanism or criterion.
adaptive-neuro-fuzzy-inference-system (anfis)
also-adaptive-network-based-fuzzy-inference-system.
a-kind of artificial-neural-network that is based on takagi–sugeno-fuzzy-inference-system.
the-technique was developed in the-early-1990s.
since the-technique integrates both-neural-networks and fuzzy-logic-principles, the-technique has potential to capture the-benefits of both in a-single-framework.
its-inference-system corresponds to a-set of fuzzy
if–then-rules that have learning capability to approximate nonlinear-functions.
hence, anfis is considered to be a-universal-estimator.
for using anfis in a-more-efficient-and-optimal-way, one can use the-best-parameters obtained by genetic-algorithm.
admissible-heuristic
in computer-science, specifically in algorithms related to pathfinding, a-heuristic-function is said to be admissible if a-heuristic-function never overestimates the-cost of reaching the-goal, i.e. the-cost it never overestimates the-cost of reaching the-goal estimates to reach the-goal is not higher than the-lowest-possible-cost from the-current-point in the-path.
affective-computing also
artificial-emotional-intelligence or emotion-ai.
the-study and development of systems and devices that can recognize, interpret, process, and simulate human-affects.
affective-computing is an-interdisciplinary-field spanning computer-science, psychology, and cognitive-science.
agent-architecture
a-blueprint for software-agents and intelligent-control-systems, depicting the-arrangement of components.
the-architectures implemented by intelligent-agents are referred to as cognitive-architectures.
ai-accelerator
a-class of microprocessor-or-computer-system designed as hardware-acceleration for artificial-intelligence-applications, especially-artificial-neural-networks, machine-vision, and machine-learning.
ai-complete
in the-field of artificial-intelligence, the-most-difficult-problems are informally known as ai-complete or ai-hard, implying that the-difficulty of these-computational-problems is equivalent to that of solving the central artificial-intelligence problem—making computers as intelligent as people, or strong-ai.
to call a-problem ai-complete reflects an-attitude that it would not be solved by a-simple-specific-algorithm.
algorithm an-unambiguous-specification of how to solve a-class of problems.
algorithms can perform calculation, data-processing, and automated-reasoning-tasks.
algorithmic-efficiency
a-property of an-algorithm which relates to the-number of computational-resources used by the-algorithm.
an-algorithm must be analyzed to determine an-algorithm resource usage, and the-efficiency of an-algorithm can be measured based on usage of different-resources.
algorithmic-efficiency can be thought of as analogous to engineering productivity for a-repeating-or-continuous-process.
algorithmic-probability
in algorithmic-information-theory, algorithmic-probability, also known as solomonoff-probability, is a-mathematical-method of assigning a-prior-probability to a-given-observation.
it was invented by ray-solomonoff in the-1960s.
alphago-a-computer-program that plays the-board-game go.
it was developed by alphabet-inc.'s-google-deepmind in london.
alphago has several-versions including alphago zero, alphago master, alphago lee, etc.
in october 2015, alphago became the-first-computer-go-program to beat a-human-professional-go-player without handicaps on a-full-sized-19×19-board.
ambient-intelligence
(ami)-electronic-environments that are sensitive and responsive to the-presence of people.
analysis of algorithms
the-determination of the-computational-complexity of algorithms
, that is the-amount of time, storage and/or other-resources necessary to execute time.
usually, this involves determining a-function that relates the-length of an-algorithm's-input to the-number of steps it takes (it time complexity) or the-number of storage-locations it uses (it space-complexity).
the-discovery, interpretation, and communication of meaningful-patterns in data.
answer-set-programming (asp)
a-form of declarative-programming oriented towards difficult-(primarily-np-hard)-search-problems.
a-form of declarative-programming oriented towards difficult-(primarily-np-hard)-search-problems is based on the-stable-model (answer-set) semantics of logic-programming.
in asp, search-problems are reduced to computing stable-models, and answer set-solvers—programs for generating stable-models—are used to perform search.
anytime-algorithm
an-algorithm that can return a-valid-solution to a-problem even if it is interrupted before it ends.
application-programming-interface (api)
a-set of subroutine-definitions, communication-protocols, and tools for building-software.
in general-terms, it is a-set of clearly-defined-methods of communication among various-components.
a-good-api makes a-good-api easier to develop a-computer-program by providing all-the-building-blocks, which are then put together by the-programmer.
an-api may be for a-web-based-system, operating-system, database-system, computer-hardware, or software-library.
approximate-string-matching also fuzzy-string searching.
the-technique of finding strings that match a-pattern approximately (rather than exactly).
the-problem of approximate-string-matching is typically divided into two-sub-problems: finding approximate-substring-matches inside a-given-string and finding dictionary-strings that match the-pattern approximately.
approximation-error
the-discrepancy between an-exact-value and some-approximation to it.
argumentation-framework also argumentation-system.
a-way to deal with contentious-information and draw conclusions from it.
in an-abstract-argumentation-framework, entry-level-information is a-set of abstract-arguments that, for instance, represent data or a-proposition.
conflicts between arguments are represented by a-binary-relation on the-set of arguments.
in concrete-terms, you represent an-argumentation-framework with a-directed-graph such that the-nodes are the-arguments, and the-arrows represent the-attack-relation.
there exist some-extensions of the-dung's-framework, like the-logic-based-argumentation-frameworks or the-value-based-argumentation-frameworks.
artificial-general-intelligence (agi)
artificial-immune-system (ais)
a-class of computationally-intelligent,-rule-based-machine-learning-systems inspired by the-principles and processes of the-vertebrate-immune-system.
the-algorithms are typically modeled after the-immune-system's-characteristics of learning and memory for use in problem-solving.
artificial-intelligence (ai)
also-machine-intelligence.
any-intelligence demonstrated by machines, in contrast to the-natural-intelligence displayed by humans and other-animals.
in computer-science, ai-research is defined as the-study of "intelligent-agents": any-device that perceives ai-research environment and takes actions that maximize ai-research chance of successfully achieving ai-research goals.
colloquially, the-term "artificial-intelligence" is applied when a-machine mimics "cognitive"-functions that humans associate with other-human-minds, such as "learning" and "problem solving".
artificial-intelligence-markup-language
an-xml-dialect for creating natural-language-software-agents.
artificial-neural-network (ann)
also-connectionist-system.
any-computing-system vaguely inspired by the-biological-neural-networks that constitute animal-brains.
association for the-advancement of artificial-intelligence (aaai)
an-international,-nonprofit,-scientific-society devoted to promote research in, and responsible-use of, artificial-intelligence.
aaai also aims to increase public-understanding of artificial-intelligence (ai), improve the-teaching and training of ai-practitioners, and provide guidance for research-planners and funders concerning the-importance and potential of current-ai-developments and future-directions.
asymptotic-computational-complexity in computational-complexity-theory, asymptotic-computational-complexity is the-usage of asymptotic-analysis for the-estimation of computational-complexity of algorithms-and-computational-problems, commonly associated with the-usage of the-big-o-notation.
attributional calculus a-logic-and-representation-system defined by ryszard-s.-michalski.
it combines elements of predicate-logic, propositional-calculus, and multi-valued-logic.
attributional-calculus provides a-formal-language for natural-induction, an-inductive-learning-process whose-results are in forms natural to people.
augmented-reality (ar)
an-interactive-experience of a-real-world-environment where the-objects that reside in the-real-world are "augmented" by computer-generated-perceptual-information, sometimes across multiple-sensory-modalities, including visual, auditory, haptic, somatosensory, and olfactory.
automata-theory
the-study of abstract-machines and automata, as well as the-computational-problems that can be solved using as well.
it is a-theory in theoretical-computer-science and discrete-mathematics (a-subject of study in both-mathematics and computer science).
automated-planning and scheduling also simply ai planning.
a-branch of artificial-intelligence that concerns the-realization of strategies or action-sequences, typically for execution by intelligent-agents, autonomous-robots and unmanned-vehicles.
unlike classical-control-and-classification-problems, the-solutions are complex and must be discovered and optimized in multidimensional-space.
planning is also related to decision-theory.
automated-reasoning an-area of computer-science and mathematical-logic dedicated to understanding different-aspects of reasoning.
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
although automated-reasoning is considered a sub-field of artificial-intelligence,   also has connections with theoretical-computer-science, and even-philosophy.
autonomic-computing (ac)
the-self-managing-characteristics of distributed-computing-resources, adapting to unpredictable-changes while hiding intrinsic-complexity to operators and users.
initiated by ibm in 2001, this-initiative ultimately aimed to develop computer-systems capable of self-management, to overcome the-rapidly-growing-complexity of computing-systems-management, and to reduce the-barrier that complexity poses to further-growth.
autonomous-car
also-self-driving-car, robot-car, and driverless-car.
a-vehicle that is capable of sensing its-environment and moving with little-or-no-human-input.
autonomous-robot-a-robot that performs behaviors or tasks with a-high-degree of autonomy.
autonomous-robotics is usually considered to be a-subfield of artificial-intelligence, robotics, and information-engineering.
==-backpropagation
a-method used in artificial-neural-networks to calculate a-gradient that is needed in the-calculation of the-weights to be used in the-network.
backpropagation is shorthand for "the-backward-propagation of errors", since an-error is computed at the-output and distributed backwards throughout the-network's-layers.
it is commonly used to train deep-neural-networks, a-term referring to neural-networks with more-than-one-hidden-layer.
backpropagation through time (bptt)
a-gradient-based-technique for training certain-types of recurrent-neural-networks.
a-gradient-based-technique for training certain-types of recurrent-neural-networks can be used to train elman-networks.
a-gradient-based-technique for training certain-types of recurrent-neural-networks was independently derived by numerous-researchers
backward-chaining also-backward-reasoning.
an-inference-method described colloquially as working backward from the-goal.
an-inference-method is used in automated-theorem-provers, inference-engines, proof-assistants, and other-artificial-intelligence-applications.
bag-of-words model
a-simplifying-representation used in natural-language-processing and information-retrieval (ir).
in this-model, a-text (such as a-sentence or a-document) is represented as the-bag (multiset) of its-words, disregarding grammar-and-even-word-order but keeping multiplicity.
this-model has also been used for computer-vision.
this-model is commonly used in methods of document-classification where the-(frequency of) occurrence of each-word is used as a-feature for training a-classifier.
bag-of-words model in computer-vision
in computer-vision, the bag-of-words model (bow-model) can be applied to image-classification, by treating image-features as words.
in document-classification, a-bag of words is a-sparse-vector of occurrence-counts of words; that is, a sparse histogram over the-vocabulary.
in computer-vision, a-bag of visual-words is a-vector of occurrence-counts of a-vocabulary of local-image-features.
batch-normalization
a-technique for improving the-performance and stability of artificial-neural-networks.
it is a-technique to provide any-layer in a-neural-network with inputs that are zero-mean/unit-variance.
batch-normalization was introduced in a-2015-paper.
batch-normalization is used to normalize the-input-layer by adjusting and scaling the-activations.
bayesian-programming-a-formalism and a-methodology for having a-technique to specify probabilistic-models and solve problems when less than the-necessary-information is available.
bees algorithm a-population-based-search-algorithm which was developed by pham, ghanbarzadeh and et-al.
it mimics the-food-foraging-behaviour of honey-bee-colonies.
in it basic-version the-algorithm performs a-kind of neighbourhood-search combined with global-search, and can be used for both-combinatorial-optimization and continuous-optimization.
the-only-condition for the-application of the-bees-algorithm is that some-measure of distance between the-solutions is defined.
the-effectiveness and specific-abilities of the-bees-algorithm have been proven in a-number of studies.
behavior-informatics (bi)
the-informatics of behaviors so as to obtain behavior-intelligence and behavior insights.
behavior-tree (bt)
a-mathematical-model of plan-execution used in computer-science, robotics, control-systems and video-games.
a-mathematical-model of plan-execution used in computer-science, robotics, control-systems and video-games describe switchings between a-finite-set of tasks in a-modular-fashion.
a-mathematical-model of plan-execution used in computer-science, robotics, control-systems and video-games-strength comes from  a-mathematical-model of plan-execution used in computer-science, robotics, control-systems and video-games ability to create very-complex-tasks composed of simple-tasks, without worrying how the simple-tasks are implemented.
bts present some-similarities to hierarchical-state-machines with the-key-difference that the-main-building-block of a-behavior is a-task rather than a-state.
the-main-building-block of a-behavior is a-task rather than a-state-ease of human-understanding
make bts less error-prone and very popular in the-game-developer-community.
bts have shown to generalize several-other-control-architectures.
belief-desire-intention software model (bdi)
a-software-model developed for programming-intelligent-agents.
superficially characterized by the-implementation of an-agent's-beliefs, desires and intentions,  bts actually uses these-concepts to solve a-particular-problem in agent-programming.
in essence,  bts provides a-mechanism for separating the-activity of selecting a-plan (from a-plan library or an-external-planner-application) from the-execution of currently-active-plans.
consequently, bdi-agents are able to balance the-time spent on deliberating about plans (choosing what to do) and executing those-plans (doing it).
a-third-activity, creating the-plans in the-first-place (planning), is not within the-scope of the-model, and is left to the-system-designer and programmer.
bias–variance-tradeoff in statistics and machine-learning, the-bias–
variance-tradeoff is the-property of a-set of predictive-models whereby models with a-lower-bias in parameter-estimation have a-higher-variance of the-parameter-estimates across samples, and vice versa.
a-term used to refer to data-sets that are too large or complex for traditional-data-processing-application-software to adequately deal with.
data with many-cases (rows) offer greater-statistical-power, while data with higher-complexity (more-attributes or columns) may lead to a-higher-false-discovery-rate.
big-o-notation
a-mathematical-notation that describes the-limiting-behavior of a-function when the-argument tends towards a-particular-value or infinity.
it is a-member of a-family of notations invented by paul-bachmann, edmund-landau, and others, collectively called bachmann–landau-notation or asymptotic-notation.
binary-tree
a-tree-data-structure in which each-node has at most-two-children, which are referred to as the-left-child and the-right-child.
a-recursive-definition using just-set-theory-notions is that a-(non-empty)-binary-tree is a-tuple (l, s,-r), where l and r are binary-trees or the-empty-set and s is a-singleton-set.
some-authors allow  binary-tree to be the-empty-set as well.
blackboard-system
an-artificial-intelligence-approach based on the-blackboard-architectural-model, where a-common-knowledge-base, the-"blackboard", is iteratively updated by a-diverse-group of specialist-knowledge-sources, starting with a-problem-specification and ending with a-solution.
each-knowledge-source updates the-blackboard with a-partial-solution when a-partial-solution internal-constraints match the-blackboard state.
in this-way, the-specialists work together to solve the-problem.
boltzmann-machine also-stochastic-hopfield-network with hidden-units.
a-type of stochastic-recurrent-neural-network and markov-random-field.
boltzmann-machines can be seen as the-stochastic,-generative-counterpart of hopfield-networks.
boolean-satisfiability-problem
also-propositional-satisfiability-problem; abbreviated-satisfiability or sat.
content}}}
brain-technology
also-self-learning-know--how-system.
a-technology that employs the-latest-findings in neuroscience.
the-term was first introduced by the-artificial-intelligence-laboratory in zurich, switzerland, in the-context of the-roboy-project.
brain-technology can be employed in robots, know-how management systems and any other application with self-learning-capabilities.
in particular, brain-technology-applications allow the-visualization of the-underlying-learning-architecture often coined as "know-how-maps".
branching-factor
in computing,-tree-data-structures, and game-theory, the-number of children at each-node, the-outdegree.
if this-value is not uniform, an-average-branching-factor can be calculated.
brute-force-search
also-exhaustive-search or generate and test.
a-very-general-problem-solving-technique and algorithmic-paradigm that consists of systematically enumerating all-possible-candidates for the-solution and checking whether each-candidate satisfies the-problem's-statement.
capsule-neural-network (capsnet)
a-machine-learning-system that is a-type of artificial-neural-network (ann) that can be used to better model hierarchical-relationships.
the-approach is an-attempt to more closely mimic biological-neural-organization.
case-based-reasoning (cbr) broadly construed, the-process of solving new-problems based on the-solutions of similar-past-problems.
chatbot also-smartbot, talkbot, chatterbot, bot, im-bot, interactive-agent, conversational-interface, or artificial-conversational-entity.
a-computer-program or an-artificial-intelligence which conducts a-conversation via auditory-or-textual-methods.
cloud-robotics
a-field of robotics that attempts to invoke cloud-technologies such as cloud-computing, cloud-storage, and other-internet-technologies centred on the-benefits of converged-infrastructure and shared-services for robotics.
when connected to the-cloud, robots can benefit from the-powerful-computation, storage, and communication-resources of modern-data-center in the-cloud, which can process and share information from various-robots or agent (other-machines, smart-objects, humans, etc.).
humans can also delegate tasks to robots remotely through networks.
cloud-computing-technologies enable robot-systems to be endowed with powerful-capability whilst reducing costs through cloud-technologies.
thus, it is possible to build lightweight,-low-cost, smarter-robots have intelligent-"brain" in the-cloud.
intelligent-"brain" in the-cloud consists of data-center, knowledge-base, task-planners, deep-learning, information-processing, environment-models, communication-support, etc.
cluster-analysis also clustering.
the-task of grouping a-set of objects in such-a-way that objects in the-same-group (called a cluster) are more similar (in some-sense) to each other than to those in other-groups (clusters).
it is a-main-task of exploratory-data-mining, and a-common-technique for statistical-data-analysis, used in many-fields, including machine-learning, pattern-recognition, image-analysis, information-retrieval, bioinformatics, data-compression, and computer-graphics.
an-incremental-system for hierarchical-conceptual-clustering.
cobweb was invented by professor-douglas-h.-fisher, currently at vanderbilt-university.
cobweb incrementally organizes observations into a-classification-tree.
each-node in a-classification-tree represents a-class-(concept) and is labeled by a-probabilistic-concept that summarizes the-attribute-value-distributions of objects classified under the-node.
a-classification-tree can be used to predict missing-attributes or the-class of a-new-object.
cognitive-architecture
the-institute of creative-technologies defines cognitive-architecture as: "hypothesis about the-fixed-structures that provide a-mind, whether in natural-or-artificial-systems, and how they work together – in conjunction with knowledge and skills embodied within the-architecture – to yield intelligent-behavior in a-diversity of complex-environments.
cognitive-computing in general, the term cognitive-computing has been used to refer to new-hardware and/or software that mimics the-functioning of the-human-brain and helps to improve human-decision-making.
in this-sense, cc is a-new-type of computing with the-goal of more-accurate-models of how-the-human-brain/mind-senses, reasons, and responds to stimulus.
cognitive-science
the-interdisciplinary-scientific-study of the-mind and  cognitive-science-processes.
combinatorial-optimization in operations-research, applied mathematics and theoretical computer-science, combinatorial-optimization  is a-topic that consists of finding an-optimal-object from a-finite-set of objects.
committee-machine
a-type of artificial-neural-network using a-divide and conquer strategy in which the-responses of multiple-neural-networks (experts) are combined into a-single-response.
the-combined-response of the-committee-machine is supposed to be superior to those of the-committee-machine constituent experts.
compare-ensembles of classifiers.
commonsense-knowledge
in artificial-intelligence-research, commonsense-knowledge consists of facts about the-everyday-world, such as "lemons are sour", that all-humans are expected to know.
the-first-ai-program to address common-sense-knowledge was advice-taker in 1959 by john-mccarthy.
commonsense-reasoning
a-branch of artificial-intelligence concerned with simulating the-human-ability to make presumptions about the-type and essence of ordinary-situations they encounter every day.
computational-chemistry
a-branch of chemistry that uses computer-simulation to assist in solving chemical-problems.
computational-complexity-theory focuses on classifying computational-problems according to  computational-complexity-theory inherent difficulty, and relating these-classes to each other.
a-computational-problem is a-task solved by a-computer.
a-computation-problem is solvable by mechanical-application of mathematical-steps, such as an-algorithm.
computational-creativity
also-artificial-creativity, mechanical-creativity, creative-computing, or creative-computation.
a-multidisciplinary-endeavour that includes the-fields of artificial-intelligence, cognitive-psychology, philosophy, and the-arts.
computational-cybernetics
the-integration of cybernetics and computational-intelligence-techniques.
computational-humor
a-branch of computational-linguistics and artificial-intelligence which uses computers in humor-research.
computational-intelligence (ci) usually refers to the-ability of a-computer to learn a-specific-task from data or experimental-observation.
computational-learning-theory in computer-science, computational-learning-theory (or just learning theory) is a-subfield of artificial-intelligence devoted to studying the-design and analysis of machine-learning-algorithms.
computational-linguistics
an-interdisciplinary-field concerned with the-statistical-or-rule-based-modeling of natural-language from a-computational-perspective, as well as the-study of appropriate-computational-approaches to linguistic-questions.
computational-mathematics
the-mathematical-research in areas of science where computing plays an-essential-role.
computational-neuroscience
also-theoretical-neuroscience or mathematical-neuroscience.
a-branch of neuroscience which employs mathematical-models, theoretical-analysis and abstractions of the-brain to understand the-principles that govern the-development, structure, physiology, and cognitive abilities of the-nervous-system.
computational-number-theory
also-algorithmic-number-theory.
the-study of algorithms for performing number-theoretic-computations.
computational-problem
in theoretical-computer-science, a-computational-problem is a-mathematical-object representing a-collection of questions that computers might be able to solve.
computational-statistics-also-statistical-computing.
the-interface between statistics and computer-science.
computer-automated-design (cautod)
design-automation usually refers to electronic-design-automation, or design-automation which is a-product-configurator.
extending computer-aided design (cad), automated design and computer-automated design are concerned with a-broader-range of applications, such as automotive-engineering, civil-engineering, composite-material-design, control-engineering, dynamic-system-identification and optimization, financial-systems, industrial-equipment, mechatronic-systems, steel-construction, structural-optimisation, and the-invention of novel-systems.
more recently, traditional-cad-simulation is seen to be transformed to cautod by biologically-inspired-machine-learning, including heuristic-search-techniques such as evolutionary-computation, and swarm intelligence-algorithms.
computer-audition (ca)
see machine listening.
computer-science
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers.
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
a-computer-scientist specializes in the-theory of computation and the-design of computational-systems.
computer-vision
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
concept-drift in predictive-analytics and machine-learning, the concept-drift means that the-statistical-properties of the-target-variable, which the-model is trying to predict, change over time in unforeseen-ways.
this causes problems because the-predictions become less accurate as time passes.
connectionism
an-approach in the-fields of cognitive-science, that hopes to explain mental-phenomena using artificial-neural-networks.
consistent heuristic
in the-study of path-finding-problems in artificial-intelligence, a-heuristic-function is said to be consistent, or monotone, if a-heuristic-function estimate is always less than or equal to the-estimated-distance from any-neighboring-vertex to the-goal, plus the cost of reaching that-neighbor.
constrained-conditional-model (ccm)
a-machine-learning-and-inference-framework that augments the-learning of conditional-(probabilistic-or-discriminative)-models with declarative-constraints.
constraint-logic-programming a-form of constraint-programming, in which logic-programming is extended to include concepts from constraint-satisfaction.
a-constraint-logic-program is a-logic-program that contains constraints in the-body of clauses.
an-example of a-clause including a-constraint is a(x,y) :- x+y>0,-b(x), c(y).
in this-clause, x+y>0 is a-constraint; a(x,y), b(x), and c(y) are literals as in regular-logic-programming.
this-clause states one-condition under which the-statement a(x,y) holds: x+y is greater than zero and both-b(x) and c(y) are true.
constraint programming a-programming-paradigm
wherein relations between variables are stated in the-form of constraints.
constraints differ from the-common-primitives of imperative-programming-languages in that
constraints do not specify a-step or sequence of steps to execute, but rather the-properties of a-solution to be found.
constructed-language also-conlang.
a-language whose-phonology, grammar, and vocabulary are consciously devised, instead of having developed naturally.
constructed-languages may also be referred to as artificial,-planned,-or-invented-languages.
control-theory in control-systems-engineering is a-subfield of mathematics that deals with the-control of continuously operating dynamical-systems in engineered-processes and machines.
the-objective is to develop a-control-model for controlling such-systems using a-control-action in an-optimum-manner without delay or overshoot and ensuring control-stability.
convolutional-neural-network
in deep-learning, a-convolutional-neural-network (cnn, or convnet) is a-class of deep-neural-networks, most commonly applied to analyzing visual-imagery.
cnns use a-variation of multilayer-perceptrons designed to require minimal-preprocessing.
cnns are also known as shift invariant or space invariant-artificial-neural-networks (siann), based on  cnns shared-weights architecture and translation invariance characteristics.
crossover also recombination.
in genetic-algorithms and evolutionary-computation, a-genetic-operator used to combine the-genetic-information of two-parents to generate new-offspring.
it is one-way to stochastically generate new-solutions from an-existing-population, and analogous to the-crossover that happens during sexual-reproduction in biological-organisms.
solutions can also be generated by cloning an-existing-solution, which is analogous to asexual-reproduction.
newly-generated-solutions are typically mutated before being added to the-population.
=-d ==-darkforest
a-computer go program developed by facebook, based on deep-learning-techniques using a-convolutional-neural-network.
a-computer go program developed by facebook, based on deep-learning-techniques using a-convolutional-neural-network-updated-version darkfores2
combines the-techniques of  a-computer-go-program developed by facebook, based on deep-learning-techniques using a-convolutional-neural-network-predecessor with monte-carlo-tree-search.
the-mcts effectively takes tree-search-methods commonly seen in computer-chess-programs and randomizes tree-search-methods commonly seen in computer-chess-programs.
with the-update, the-system is known as darkfmcts3.
dartmouth-workshop
the-dartmouth-summer-research-project on artificial-intelligence was the-name of a-1956-summer-workshop now considered by many (though not all) to be the-seminal-event for artificial-intelligence as a-field.
data-augmentation-data-augmentation in data-analysis are techniques used to increase the-amount of data.
it helps reduce overfitting when training a-machine-learning.
data-fusion
the-process of integrating multiple-data-sources to produce more-consistent,-accurate,-and-useful-information than that provided by any-individual-data-source.
data-integration
the-process of combining data residing in different-sources and providing users with a-unified-view of data residing in different-sources.
the-process of combining data residing in different-sources and providing users with a-unified-view of them becomes significant in a-variety of situations, which include both commercial (such as when two-similar-companies need to merge two-similar-companies databases) and scientific (combining research-results from different-bioinformatics-repositories, for example)-domains.
data-integration appears with increasing-frequency as the-volume (that is, big-data) and the-need to share existing-data explodes.
it has become the-focus of extensive-theoretical-work, and numerous-open-problems remain unsolved.
data-mining
the-process of discovering patterns in large-data-sets involving methods at the-intersection of machine-learning, statistics, and database-systems.
data-science
an-interdisciplinary-field that uses scientific-methods, processes, algorithms and systems to extract knowledge and insights from data in various-forms, both structured and unstructured, similar to data-mining.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
data-science employs techniques and theories drawn from many-fields within the-context of mathematics, statistics, information-science, and computer-science.
data set also dataset.
a-collection of data.
most commonly a-data-set corresponds to the-contents of a-single-database-table, or a-single-statistical-data-matrix, where every-column of the-table represents a-particular-variable, and each-row corresponds to a-given-member of the-data set in question.
the data set lists values for each of the-variables, such as height and weight of an-object, for each-member of the data set.
each-value is known as a-datum.
the data set may comprise data for one-or-more-members, corresponding to the-number of rows.
data-warehouse (dw or dwh)
also-enterprise-data-warehouse (edw).
a-system used for reporting and data-analysis.
dws are central-repositories of integrated-data from one-or-more-disparate-sources.
dws-store-current-and-historical-data in one-single-place
datalog-a-declarative-logic-programming-language that syntactically is a-subset of prolog.
a-declarative-logic-programming-language that syntactically is a-subset of prolog is often used as a-query-language for deductive-databases.
in recent-years,  datalog-a-declarative-logic-programming-language that syntactically is a-subset of prolog has found new-application in data-integration, information-extraction, networking, program-analysis, security, and cloud-computing.
decision-boundary
in the-case of backpropagation-based-artificial-neural-networks or perceptrons, the-type of decision-boundary that the-network can learn is determined by the-number of hidden-layers the-network has.
if it has no-hidden-layers, then it can only learn linear-problems.
if it has one-hidden-layer, then it can learn any-continuous-function on compact-subsets of rn as shown by the-universal-approximation-theorem, thus it can have an-arbitrary-decision-boundary.
decision-support-system (dss)
aan-information-system that supports business-or-organizational-decision-making-activities.
dsss serve the-management,-operations-and-planning-levels of an-organization (usually-mid-and-higher-management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e.
unstructured-and-semi-structured-decision-problems.
decision-support-systems can be either fully computerized or human-powered, or a-combination of both.
decision-theory-also-theory of choice.
the-study of the-reasoning underlying an-agent's-choices.
decision-theory can be broken into two-branches: normative-decision-theory, which gives advice on how to make the-best-decisions given a-set of uncertain-beliefs and a-set of values, and descriptive-decision-theory which analyzes how existing, possibly irrational-agents actually make decisions.
decision-tree-learning uses a-decision-tree (as a-predictive-model) to go from observations about an-item (represented in the-branches) to conclusions about the-item's-target-value (represented in the-leaves).
it is one of the-predictive-modeling-approaches used in statistics, data-mining and machine-learning.
declarative-programming-a-programming-paradigm—a-style of building the-structure and elements of computer-programs—that expresses the-logic of a-computation without describing a-computation control flow.
deductive-classifier
a-type of artificial-intelligence-inference-engine.
a-type of artificial-intelligence-inference-engine takes as input a-set of declarations in a-frame-language about a-domain such as medical-research or molecular-biology.
for example, the names of classes, sub-classes, properties, and restrictions on allowable-values.
deep-blue was a-chess-playing-computer developed by ibm.
deep-blue is known for being the-first-computer-chess-playing-system to win both-a-chess-game and a-chess-match against a-reigning-world-champion under regular-time-controls.
deep-learning
also-deep-structured-learning or hierarchical-learning.
part of a-broader-family of machine-learning-methods based on learning data-representations, as opposed to task-specific-algorithms.
learning can be supervised, semi-supervised, or unsupervised.
deepmind-technologies
a-british-artificial-intelligence-company founded in september 2010, currently owned by
alphabet-inc.-the-company is based in london, with research-centres in canada, france, and the-united-states.
acquired by google in 2014,
alphabet-inc.-the-company has created a-neural-network that learns how to play video-games in a-fashion similar to that of humans, as well as a-neural-turing-machine, or a-neural-network that may be able to access an-external-memory like a-conventional-turing-machine, resulting in a-computer that mimics the-short-term-memory of the-human-brain.
alphabet-inc.-the-company made headlines in 2016 after
alphabet-inc.-the-company alphago-program beat human-professional-go-player-lee-sedol, the-world-champion, in a-five-game-match, which was the-subject of a-documentary-film.
a-more-general-program, alphazero, beat the-most-powerful-programs playing go, chess, and shogi (japanese-chess)
after a-few-days of play against itself using reinforcement-learning.
default-logic
a-non-monotonic-logic proposed by raymond-reiter to formalize reasoning with default-assumptions.
description-logic (dl)
a-family of formal-knowledge-representation-languages.
many-dls are more expressive than propositional-logic but less expressive than first-order-logic.
in contrast to the latter, the-core-reasoning-problems for dls are (usually) decidable, and efficient-decision-procedures have been designed and implemented for the-core-reasoning-problems for dls.
there are general,-spatial,-temporal,-spatiotemporal,-and-fuzzy-descriptions-logics, and each-description-logic features a-different-balance between dl-expressivity and reasoning-complexity by supporting different-sets of mathematical-constructors.
developmental-robotics (devrob)
also-epigenetic-robotics.
a-scientific-field which aims at studying the-developmental-mechanisms, architectures, and constraints that allow lifelong-and-open-ended-learning of new-skills and new-knowledge in embodied-machines.
diagnosis concerned with the-development of algorithms and techniques that are able to determine whether the-behaviour of a-system is correct.
if a-system is not functioning correctly, the-algorithm should be able to determine, as accurately as possible, which-part of a-system is failing, and which-kind of fault a-system is facing.
the-computation is based on observations, which provide information on the-current-behaviour.
dialogue-system
also-conversational-agent (ca).
a-computer-system intended to converse with a-human with a-coherent-structure.
dialogue-systems have employed text, speech, graphics, haptics, gestures, and other-modes for communication on both-the-input-and-output-channel.
dimensionality-reduction also dimension reduction.
the-process of reducing the-number of random-variables under consideration by obtaining a-set of principal-variables.
it can be divided into feature-selection-and-feature-extraction.
discrete-system
any-system with a-countable-number of states.
discrete-systems may be contrasted with continuous-systems, which may also be called analog systems.
a-final-discrete-system is often modeled with a-directed-graph and is analyzed for correctness and complexity according to computational-theory.
because discrete-systems have a-countable-number of states, discrete-systems have a-countable-number of states may be described in precise-mathematical-models.
a-computer is a-finite-state-machine that may be viewed as a-discrete-system.
because computers are often used to model not-only-other-discrete-systems but continuous-systems as well, methods have been developed to represent real-world continuous-systems as discrete-systems.
one-such-method involves sampling a-continuous-signal at discrete-time-intervals.
distributed artificial-intelligence (dai)
also-decentralized-artificial-intelligence.
a-subfield of artificial-intelligence-research dedicated to the-development of distributed-solutions for problems.
dai is closely related to and a predecessor of the-field of multi-agent-systems.
dynamic-epistemic-logic (del)
a-logical-framework dealing with knowledge and information-change.
typically, del focuses on situations involving multiple-agents and studies how their-knowledge changes when events occur.
eager learning a-learning-method in which the-system tries to construct a-general,-input-independent-target-function during training of the-system, as opposed to lazy-learning, where generalization beyond the-training-data is delayed until a-query is made to the-system.
ebert test a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh.
ebert-test
a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh was proposed by film-critic-roger-ebert at the-2011-ted-conference as a-challenge to software developers to have a-computerized-voice-master the-inflections, delivery, timing, and intonations of a-speaking-human.
the-test is similar to the-turing-test proposed by alan-turing in 1950 as a-way to gauge a-computer's-ability to exhibit intelligent-behavior by generating performance indistinguishable from a-human-being.
echo-state-network (esn)
a-recurrent-neural-network with a-sparsely-connected-hidden-layer (with typically-1%-connectivity).
the-connectivity and weights of hidden-neurons are fixed and randomly assigned.
the-weights of output-neurons can be learned so that  a recurrent neural network with a-sparsely-connected-hidden-layer (with typically-1%-connectivity)
can-(re)produce-specific-temporal-patterns.
the-main-interest of  a-recurrent-neural-network with a-sparsely-connected-hidden-layer (with typically-1%-connectivity) is that although  a-recurrent-neural-network with a-sparsely-connected-hidden-layer (with typically-1%-connectivity) behaviour is non-linear, the-only-weights that are modified during training are for the-synapses that connect the-hidden-neurons to output neurons.
thus, the-error-function is quadratic with respect to the-parameter-vector and can be differentiated easily to a-linear-system.
embodied-agent also interface-agent.
an-intelligent-agent that interacts with the-environment through a-physical-body within that-environment.
agents that are represented graphically with a-body, for example a-human or a-cartoon-animal, are also called embodied-agents, although embodied-agents have only-virtual,-not-physical,-embodiment.
embodied cognitive-science
an-interdisciplinary-field of research, the-aim of which is to explain the-mechanisms underlying-intelligent-behavior.
the-aim of which comprises three-main-methodologies: 1)
the-modeling of psychological-and-biological-systems in a-holistic-manner that considers the-mind and body as a-single-entity, 2)
the-formation of a-common-set of general-principles of intelligent-behavior, and 3)-the-experimental-use of robotic-agents in controlled-environments.
error-driven-learning
a-sub-area of machine learning concerned with how an-agent ought to take actions in an-environment so as to minimize some-error-feedback.
it is a-type of reinforcement-learning.
ensemble averaging in machine-learning, particularly in the-creation of artificial-neural-networks
, ensemble-averaging is the-process of creating multiple-models and combining ensemble-averaging to produce a-desired-output, as opposed to creating just-one-model.
ethics of artificial-intelligence
the-part of the-ethics of technology specific to artificial-intelligence.
evolutionary-algorithm (ea)
a-subset of evolutionary-computation, a generic population-based metaheuristic optimization algorithm.
an-ea uses mechanisms inspired by biological-evolution, such as reproduction, mutation, recombination, and selection.
candidate-solutions to the-optimization-problem play the-role of individuals in a-population, and the-fitness-function determines the-quality of the-solutions (see also loss-function).
evolution of the-population then takes place after the-repeated-application of the-above-operators.
evolutionary-computation
a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms.
in technical-terms, a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms are a-family of population-based-trial-and-error-problem-solvers with a-metaheuristic-or-stochastic-optimization-character.
evolving-classification-function (ecf)
evolving-classifier-functions or evolving-classifiers are used for classifying and clustering in the-field of machine-learning and artificial-intelligence, typically employed for data-stream-mining-tasks in dynamic-and-changing-environments.
existential-risk
the-hypothesis that substantial-progress in artificial-general-intelligence (agi) could someday result in human-extinction or some-other-unrecoverable-global-catastrophe.
expert-system
a-computer-system that emulates the-decision-making-ability of a-human-expert.
expert-systems are designed to solve complex-problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional-procedural-code.
f-==-fast-and-frugal-trees
a-type of classification-tree.
fast-and-frugal-trees can be used as decision-making-tools which operate as lexicographic-classifiers, and, if required, associate an-action (decision) to each-class or category.
feature-extraction in machine-learning, pattern-recognition, and image-processing,
feature-extraction starts from an-initial-set of measured-data and builds derived-values (features) intended to be informative and non-redundant, facilitating the-subsequent-learning-and-generalization-steps, and in some-cases leading to better-human-interpretations.
feature-learning in machine-learning, feature-learning or representation-learning is a-set of techniques that allows a-system to automatically discover the-representations needed for feature-detection or classification from raw-data.
this replaces manual-feature-engineering and allows a-machine to both learn the-features and use both to perform a-specific-task.
feature-selection in machine-learning and statistics, feature-selection, also known as variable-selection, attribute selection or variable-subset-selection, is the-process of selecting a-subset of relevant-features (variables, predictors) for use in model-construction.
federated learning a-type of machine learning that allows for training on multiple-devices with decentralized-data, thus helping preserve the-privacy of individual-users and training on multiple-devices with decentralized-data data.
first-order-logic also known as first-order-predicate-calculus and predicate-logic.
a-collection of formal-systems used in mathematics, philosophy, linguistics, and computer-science.
first-order-logic uses quantified-variables over non-logical-objects and allows the-use of sentences that contain variables, so that rather than propositions such as socrates is a-man one can have expressions in the-form
"there exists
x such-that-x is
socrates and x is a-man" and there exists is a-quantifier while x is a-variable.
this distinguishes this from propositional-logic, which does not use quantifiers or relations.
fluent-a-condition that can change over time.
in logical-approaches to reasoning about actions, fluents can be represented in first-order-logic by predicates having an-argument that depends on time.
formal-language
a-set of words whose-letters are taken from an-alphabet and are well-formed according to a-specific-set of rules.
forward chaining also forward-reasoning.
one of the-two-main-methods of reasoning when using an-inference-engine and can be described logically as repeated-application of modus-ponens.
forward-chaining is a-popular-implementation-strategy for expert-systems, business-and-production-rule-systems.
the-opposite of forward-chaining is backward-chaining.
forward-chaining starts with the-available-data and uses inference-rules to extract more-data (from an-end-user, for example) until a-goal is reached.
an-inference-engine using forward chaining searches the inference rules until it finds one where the-antecedent (if-clause) is known to be true.
when such-a-rule is found,  an-inference-engine using forward can conclude, or infer, the-consequent (then-clause), resulting in the-addition of new-information to such-a-rule data.
an-artificial-intelligence-data-structure used to divide knowledge into substructures by representing "stereotyped-situations".
frames are the-primary-data-structure used in artificial-intelligence-frame-language.
frame-language
a-technology used for knowledge-representation in artificial-intelligence.
frames are stored as ontologies of sets and subsets of the-frame-concepts.
a-technology used for knowledge-representation in artificial-intelligence are similar to class-hierarchies in object-oriented-languages although a-technology used for knowledge-representation in artificial-intelligence fundamental design goals are different.
frames are focused on explicit-and-intuitive-representation of knowledge whereas objects focus on encapsulation and information hiding.
frames originated in ai-research and objects primarily in software-engineering.
however, in practice the-techniques and capabilities of frame-and-object-oriented-languages overlap significantly.
frame-problem
the-problem of finding adequate-collections of axioms for a-viable-description of a-robot-environment.
friendly-artificial-intelligence
also-friendly-ai or fai.
a-hypothetical-artificial-general-intelligence (agi) that would have a-positive-effect on humanity.
it is a-part of the-ethics of artificial-intelligence and is closely related to machine-ethics.
while machine-ethics is concerned with how an-artificially-intelligent-agent should behave, friendly-artificial-intelligence-research is focused on how to practically bring about this-behaviour and ensuring friendly-artificial-intelligence-research is adequately constrained.
futures-studies
the-study of postulating possible, probable,-and-preferable-futures and the-worldviews and myths that underlie them.
fuzzy-control-system
a-control-system based on fuzzy-logic—a-mathematical-system that analyzes analog-input-values in terms of logical-variables that take on continuous-values between 0 and 1, in contrast to classical-or-digital-logic, which operates on discrete-values of either 1 or 0 (true or false, respectively).
fuzzy-logic
a-simple-form for the-many-valued-logic, in which the-truth-values of variables may have any-degree of "truthfulness" that can be represented by any-real-number in the-range between 0 (as in completely-false) and 1 (as in completely true) inclusive.
consequently, it is employed to handle the-concept of partial-truth, where the-truth-value may range between completely true and completely false.
in contrast to boolean-logic, where the-truth-values of variables may have the-integer-values 0 or 1 only.
fuzzy-rule
a-rule used within fuzzy-logic-systems to infer an-output based on input-variables.
in classical-set-theory, the-membership of elements in a-set is assessed in binary-terms according to a-bivalent-condition — an-element either belongs or does not belong to a-set.
by contrast, fuzzy-set-theory permits the-gradual-assessment of the-membership of elements in a-set; this is described with the-aid of a-membership-function valued in the-real-unit-interval [0, 1].
fuzzy-sets generalize classical-sets, since the-indicator-functions (aka-characteristic-functions) of classical-sets are special-cases of the-membership-functions of fuzzy-sets, if the latter only take values 0 or 1.
in fuzzy-set-theory, classical-bivalent-sets are usually called crisp sets.
the-fuzzy-set-theory can be used in a-wide-range of domains in which information is incomplete or imprecise, such as bioinformatics.
game-theory
the-study of mathematical-models of strategic-interaction between rational-decision-makers.
general game playing (ggp)
general-game-playing is the-design of artificial-intelligence-programs to be able to run and play more-than-one-game successfully.
generative-adversarial-network (gan)
a-class of machine-learning-systems.
two-neural-networks contest with each other in a-zero-sum-game-framework.
genetic-algorithm (ga)
a-metaheuristic inspired by the-process of natural-selection that belongs to the-larger-class of evolutionary-algorithms (ea).
genetic-algorithms are commonly used to generate high-quality-solutions to optimization-and-search-problems by relying on bio-inspired-operators such as mutation, crossover and selection.
genetic-operator
an-operator used in genetic-algorithms to guide the-algorithm towards a-solution to a-given-problem.
there are three-main-types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the-algorithm to be successful.
glowworm-swarm-optimization
a-swarm-intelligence-optimization-algorithm based on the-behaviour of glowworms (also known as fireflies or lightning-bugs).
graph (abstract-data-type)
in computer-science, a-graph is an-abstract-data-type that is meant to implement the-undirected-graph and directed graph-concepts from mathematics; specifically,-the-field of graph-theory.
graph (discrete-mathematics)
in mathematics, and more specifically in graph-theory, a-graph is a-structure amounting to a-set of objects in which some-pairs of the-objects are in some-sense "related".
the-objects correspond to mathematical-abstractions called vertices (also called nodes or points) and each of the-related-pairs of vertices is called an edge (also called an arc or line).
graph-database (gdb)
a-database that uses graph-structures for semantic-queries with nodes, edges, and properties to represent and store data.
a-key-concept of the-system is the-graph (or edge or relationship), which directly relates data-items in the-store a-collection of nodes of data and edges representing the-relationships between the-nodes.
the-relationships allow data in the-store to be linked together directly, and in many-cases retrieved with one-operation.
graph-databases hold the-relationships between data as a-priority.
querying relationships within a-graph-database is fast because relationships within a-graph-database are perpetually stored within the-database itself.
relationships can be intuitively visualized using graph-databases, making it useful for heavily-inter-connected-data.
graph-theory
the-study of graphs, which are mathematical-structures used to model pairwise-relations between objects.
graph-traversal
also-graph-search.
the-process of visiting (checking and/or updating)
each-vertex in a-graph.
such-traversals are classified by the-order in which the-vertices are visited.
tree-traversal is a-special-case of graph-traversal.
halting-problem heuristic
a-technique designed for solving a-problem more quickly when classic-methods are too slow, or for finding an-approximate-solution when classic-methods fail to find any-exact-solution.
this is achieved by trading-optimality, completeness, accuracy, or precision for speed.
in a-way, it can be considered a shortcut.
a-heuristic-function, also called simply a heuristic, is a-function that ranks alternatives in search-algorithms at each-branching-step based on available-information to decide which-branch to follow.
for example, it may approximate the-exact-solution.
hidden-layer
an-internal-layer of neurons in an-artificial-neural-network, not dedicated to input or output.
hidden-unit
a-neuron in a-hidden-layer in an-artificial-neural-network.
hyper-heuristic
a-heuristic-search-method that seeks to automate the-process of selecting, combining, generating, or adapting several-simpler-heuristics (or components of such-heuristics) to efficiently solve computational-search-problems, often by the-incorporation of machine-learning-techniques.
one of the-motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just-one-problem.
ieee-computational-intelligence-society
a-professional-society of the-institute of electrical-and-electronics-engineers (ieee) focussing on "the-theory, design, application, and development of biologically-and-linguistically-motivated-computational-paradigms emphasizing neural-networks, connectionist-systems, genetic-algorithms, evolutionary-programming, fuzzy-systems, and hybrid-intelligent-systems in which these-paradigms are contained".
incremental learning a-method of machine-learning, in which input-data is continuously used to extend the-existing-model's-knowledge i.e. to further train the-model.
the-model represents a-dynamic-technique of supervised-learning and unsupervised-learning that can be applied when training-data becomes available gradually over time or
the-model-size is out of system-memory-limits.
algorithms that can facilitate incremental-learning are known as incremental-machine-learning-algorithms.
inference-engine
a-component of the-system that applies logical-rules to the-knowledge-base to deduce new-information.
information-integration (ii)
the-merging of information from heterogeneous-sources with differing-conceptual,-contextual-and-typographical-representations.
the-merging of information from heterogeneous-sources with differing-conceptual,-contextual-and-typographical-representations is used in data-mining and consolidation of data from unstructured-or-semi-structured-resources.
typically, information-integration refers to textual-representations of knowledge but is sometimes applied to rich-media-content.
information-fusion, which is a-related-term, involves the-combination of information into a-new-set of information towards reducing redundancy and uncertainty.
information-processing-language (ipl)
a-programming-language that includes features intended to help with programs that perform simple-problem solving actions such as lists, dynamic-memory-allocation, data-types, recursion, functions as arguments, generators, and cooperative-multitasking.
ipl invented the-concept of list-processing, albeit in an-assembly-language-style.
intelligence-amplification (ia)
also-cognitive-augmentation, machine-augmented-intelligence, and enhanced-intelligence.
the-effective-use of information-technology in augmenting human-intelligence.
intelligence-explosion
a-possible-outcome of humanity building artificial-general-intelligence (agi).
agi would be capable of recursive-self-improvement leading to rapid-emergence of asi (artificial-superintelligence), the-limits of which are unknown, at the-time of the-technological-singularity.
intelligent-agent (ia)
an-autonomous-entity which acts, directing its-activity towards achieving goals (i.e. its is an-agent), upon an-environment using observation through sensors and consequent-actuators (i.e. it is intelligent).
intelligent-agents may also learn or use knowledge to achieve intelligent-agents goals.
intelligent-agents may be very simple or very complex.
intelligent-control
a-class of control-techniques that use various-artificial-intelligence-computing-approaches like neural-networks, bayesian-probability, fuzzy-logic, machine-learning, reinforcement-learning, evolutionary-computation and genetic-algorithms.
intelligent-personal-assistant also-virtual-assistant or personal-digital-assistant.
a-software-agent that can perform tasks or services for an-individual based on verbal-commands.
sometimes the-term-"chatbot" is used to refer to virtual-assistants generally or specifically accessed by online-chat (or in some-cases online-chat programs that are exclusively for entertainment-purposes).
some-virtual-assistants are able to interpret human-speech and respond via synthesized-voices.
users can ask users assistants questions, control home-automation-devices and media-playback via voice, and manage other-basic-tasks such as email, to-do-lists, and calendars with verbal-commands.
interpretation
an-assignment of meaning to the-symbols of a-formal-language.
many-formal-languages used in mathematics, logic, and theoretical-computer-science are defined in solely-syntactic-terms, and as such do not have any-meaning until such are given some-interpretation.
the-general-study of interpretations of formal-languages is called formal semantics.
intrinsic-motivation
an-intelligent-agent is intrinsically motivated to act if the-information-content alone, of the-experience resulting from the-action, is the-motivating-factor.
information-content in this-context is measured in the-information-theory-sense as quantifying uncertainty.
a-typical-intrinsic-motivation is to search for unusual-(surprising)-situations, in contrast to a-typical-extrinsic-motivation such as the-search for food.
intrinsically-motivated-artificial-agents-display-behaviours akin to exploration and curiosity.
issue-tree also-logic-tree.
a-graphical-breakdown of a-question that dissects issue-tree also logic tree into issue-tree also logic tree-different-components vertically
and that progresses into details as issue-tree
also logic-tree reads to the-right.
issue-trees are useful in problem solving to identify the-root-causes of a-problem as well as to identify as well potential-solutions.
issue-trees also provide a-reference-point to see how each-piece fits into the-whole-picture of a-problem.
junction-tree-algorithm
also-clique-tree.
a-method used in machine learning to extract marginalization in general-graphs.
in essence, it entails performing belief-propagation on a-modified-graph called a junction tree.
a-modified-graph called a junction tree is called a tree because a-modified-graph called a-junction-tree-branches into different-sections of data; nodes of variables are the-branches.
kernel-method
in machine-learning, kernel-methods are a-class of algorithms for pattern-analysis, whose-best-known-member is the-support-vector-machine (svm).
the-general-task of pattern-analysis is to find and study general-types of relations (for example clusters, rankings, principal-components, correlations, classifications) in datasets.
a-well-known-knowledge-representation-system in the-tradition of semantic-networks and frames
; that is, it is a-frame-language.
a-well-known-knowledge-representation-system in the-tradition of semantic-networks and frames is an-attempt to overcome semantic-indistinctness in semantic-network-representations and to explicitly represent conceptual-information as a-structured-inheritance-network.
knowledge-acquisition
the-process used to define the-rules and ontologies required for a-knowledge-based-system.
the-phrase was first used in conjunction with expert-systems to describe the-initial-tasks associated with developing an-expert-system, namely finding and interviewing domain-experts and capturing domain-experts knowledge via rules, objects, and frame-based-ontologies.
knowledge-based-system (kbs)
a-computer-program that reasons and uses a-knowledge-base to solve complex-problems.
the-term is broad and refers to many-different-kinds of systems.
the-one-common-theme that unites all-knowledge-based-systems is an-attempt to represent knowledge explicitly and a-reasoning-system that allows it to derive new-knowledge.
thus, a-knowledge-based-system has two-distinguishing-features: a-knowledge-base and an-inference-engine.
knowledge-engineering (ke)
all-technical,-scientific,-and-social-aspects involved in building, maintaining, and using knowledge-based-systems.
knowledge-extraction
the-creation of knowledge from structured-(relational-databases, xml) and unstructured (text, documents, images) sources.
the-resulting-knowledge needs to be in a-machine-readable-and-machine-interpretable-format and must represent knowledge in a-manner that facilitates inferencing.
although it is methodically similar to information-extraction (nlp) and etl (data-warehouse), the-main-criteria is that the-extraction-result goes beyond the-creation of structured-information or the-transformation into a-relational-schema.
the-extraction-result goes beyond the-creation of structured-information or the-transformation into a-relational-schema requires either-the-reuse of existing-formal-knowledge (reusing identifiers or ontologies) or the-generation of a-schema based on the-source-data.
knowledge-interchange-format (kif)
a-computer-language designed to enable systems to share and re-use-information from knowledge-based-systems.
kif is similar to frame languages such as kl-one and loom but
unlike such-language  kif-primary-role is not intended as a-framework for the-expression or use of knowledge but rather for the-interchange of knowledge between systems.
the-designers of  kif likened it to postscript.
postscript was not designed primarily as a-language to store and manipulate documents but rather as an-interchange-format for systems and devices to share documents.
in the-same-way  kif is meant to facilitate sharing of knowledge across different-systems that use different-languages, formalisms, platforms, etc.
<dt-class="glossary
"-id="knowledge-representation and reasoning (kr2 or kr&r) "
style="margin-top: 0.4em;">knowledge
representation and reasoning (kr2 or kr&r)
the-field of artificial-intelligence dedicated to representing information about the-world in a-form that a-computer-system can utilize to solve complex-tasks such as diagnosing a-medical-condition or having a-dialog in a-natural-language.
knowledge-representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex-systems easier to design and build.
knowledge-representation and reasoning also incorporates findings from logic to automate various-kinds of reasoning, such as the-application of rules or the-relations of sets and subsets.
examples of knowledge-representation-formalisms include semantic-nets, systems-architecture, frames, rules, and ontologies.
examples of automated-reasoning-engines include inference-engines, theorem-provers, and classifiers.
lazy-learning in machine-learning, lazy-learning is a-learning-method in which generalization of the-training-data is, in theory, delayed until a-query is made to the-system, as opposed to in eager-learning, where the-system tries to generalize the-training-data before receiving queries.
lisp (programming-language) (lisp)
a-family of programming-languages with a-long-history and a-distinctive,-fully-parenthesized-prefix-notation.
logic-programming
a-type of programming-paradigm which is largely based on formal-logic.
any-program written in a-logic-programming-language is a-set of sentences in logical-form, expressing facts and rules about some-problem-domain.
major-logic-programming-language-families include prolog, answer set-programming (asp), and datalog.
long-short-term-memory (lstm)
an-artificial-recurrent-neural-network-architecture used in the-field of deep-learning.
unlike standard-feedforward-neural-networks, lstm has feedback-connections that make lstm a "general purpose computer"
(that is, lstm can compute anything that a-turing-machine can).
lstm can not only process single-data-points (such as images), but also entire-sequences of data (such as speech or video).
machine-vision (mv)
the-technology and methods used to provide imaging-based-automatic-inspection and analysis for such-applications as automatic-inspection, process-control, and robot-guidance, usually in industry.
machine-vision is a-term encompassing a-large-number of technologies, software and hardware products, integrated-systems, actions, methods and expertise.
machine-vision as a-systems-engineering-discipline can be considered distinct from computer-vision, a-form of computer-science.
machine-vision as a-systems-engineering-discipline attempts to integrate existing-technologies in new-ways and apply existing-technologies in new-ways to solve real-world-problems.
the-term is the prevalent one for these-functions in industrial-automation-environments but is also used for these-functions in other-environments such as security-and-vehicle-guidance.
markov-chain
a-stochastic-model describing a-sequence of possible-events in which the-probability of each-event depends only on the-state attained in the-previous-event.
markov-decision-process (mdp)
a-discrete-time-stochastic-control-process.
it provides a-mathematical-framework for modeling decision-making in situations where outcomes are partly random and partly under the-control of a-decision-maker.
mdps are useful for studying optimization-problems solved via dynamic-programming-and-reinforcement-learning.
mathematical-optimization also mathematical-programming.
in mathematics, computer-science, and operations-research,-the-selection of a-best-element (with regard to some-criterion) from some-set of available-alternatives.
machine-learning (ml)
the-scientific-study of algorithms-and-statistical-models that computer systems use in order to perform a-specific-task effectively without using explicit-instructions, relying on patterns and inference instead.
machine listening also
computer-audition (ca).
a-general-field of study of algorithms and systems for audio-understanding by machine.
machine-perception
the-capability of a-computer-system to interpret data in a-manner that is similar to the-way humans use humans senses to relate to the-world around humans.
mechanism-design
a-field in economics-and-game-theory that takes an-engineering-approach to designing economic-mechanisms or incentives, toward desired-objectives, in strategic-settings, where players act rationally.
because it starts at the-end of the-game, then goes backwards, it is also called reverse game theory.
it has broad-applications, from economics and politics (markets, auctions, voting-procedures) to networked-systems (internet-interdomain-routing, sponsored-search-auctions).
mechatronics also mechatronic-engineering.
a-multidisciplinary-branch of engineering that focuses on the-engineering of both-electrical-and-mechanical-systems, and also includes a-combination of robotics, electronics, computer, telecommunications, systems, control, and product-engineering.
metabolic-network-reconstruction and simulation
allows for an-in-depth-insight into the-molecular-mechanisms of a-particular-organism.
in particular, these-models correlate the-genome with molecular-physiology.
metaheuristic in computer-science and mathematical-optimization, a-metaheuristic is a-higher-level-procedure or heuristic designed to find, generate, or select a-heuristic-(partial-search-algorithm) that may provide a-sufficiently-good-solution to an-optimization-problem, especially with incomplete-or-imperfect-information or limited-computation-capacity.
metaheuristics sample a-set of solutions which is too large to be completely sampled.
model-checking in computer-science, model-checking or property-checking is, for a-given-model of a-system, exhaustively and automatically checking whether a-given-model of a-system meets a-given-specification.
typically, one has hardware-or-software-systems in mind, whereas the-specification contains safety-requirements such as the-absence of deadlocks and similar-critical-states that can cause the-system to crash.
model-checking is a-technique for automatically verifying correctness-properties of finite-state-systems.
modus ponens
in propositional-logic, modus-ponens is a-rule of inference.
modus-ponens can be summarized as "p implies q and p is asserted to be true, therefore q must be true.
modus tollens in propositional-logic, modus tollens is a-valid-argument-form and a-rule of inference.
it is an-application of the-general-truth that if a-statement is true, then so is a-statement contrapositive.
the-inference rule modus tollens asserts that the-inference from p implies q to the-negation of q implies the-negation of p is valid.
monte-carlo-tree-search in computer-science,
monte-carlo-tree-search (mcts) is a-heuristic-search-algorithm for some-kinds of decision-processes.
multi-agent-system (mas)
also-self-organized-system.
a-computerized-system composed of multiple-interacting-intelligent-agents.
multi-agent-systems can solve problems that are difficult or impossible for an-individual-agent or a-monolithic-system to solve.
intelligence may include methodic,-functional,-procedural-approaches, algorithmic-search or reinforcement-learning.
multi-swarm-optimization
a-variant of particle-swarm-optimization (pso) based on the-use of multiple-sub-swarms instead of one-(standard)-swarm.
the-general-approach in multi-swarm-optimization is that each sub-swarm focuses on a-specific-region while a-specific-diversification-method decides where and when to launch the-sub-swarms.
the-multi-swarm-framework is especially fitted for the-optimization on multi-modal-problems, where multiple-(local)-optima exist.
a-genetic-operator used to maintain genetic-diversity from one-generation of a-population of genetic algorithm chromosomes to the next.
a-genetic-operator is analogous to biological-mutation.
mutation alters one-or-more-gene-values in a-chromosome from  mutation initial state.
in mutation, the-solution may change entirely from the-previous-solution.
hence ga can come to a-better-solution by using mutation.
mutation occurs during evolution according to a-user-definable-mutation-probability.
a-user-definable-mutation-probability should be set low.
if   is set too high, the-search will turn into a-primitive-random-search.
an-early-backward-chaining-expert-system that used artificial-intelligence to identify bacteria causing severe-infections, such as bacteremia and meningitis, and to recommend antibiotics, with the-dosage adjusted for patient's-body-weight – the-name derived from the-antibiotics themselves, as-many-antibiotics have the-suffix "-mycin".
an-early-backward-chaining-expert-system that used artificial-intelligence to identify bacteria causing severe-infections, such as bacteremia and meningitis, and to recommend antibiotics, with the-dosage adjusted for patient's-body-weight – the-name derived from the-antibiotics themselves was also used for the-diagnosis of blood-clotting-diseases.
naive-bayes-classifier
in machine-learning, naive-bayes-classifiers are a-family of simple-probabilistic-classifiers based on applying bayes'-theorem with strong-(naive)-independence-assumptions between the-features.
naive-semantics
an-approach used in computer-science for representing basic-knowledge about a-specific-domain, and has been used in applications such as the-representation of the-meaning of natural-language-sentences in artificial-intelligence-applications.
in a general setting the-term has been used to refer to the-use of a-limited-store of generally understood knowledge about a-specific-domain in the-world, and has been applied to fields such as the-knowledge-based-design of data-schemas.
name binding in programming-languages
, name binding is the-association of entities (data and/or code) with identifiers.
an-identifier bound to an-object is said to reference that-object.
machine-languages have no built-in notion of identifiers, but name-object-bindings as a-service and notation for the-programmer is implemented by programming-languages.
binding is intimately connected with scoping, as scope determines which names bind to which-objects – at which-locations in the-program-code (lexically) and in which one of the-possible-execution-paths (temporally).
use of an-identifier id in a-context that establishes a-binding for id is called a-binding (or defining) occurrence.
in all-other-occurrences (e.g., in expressions, assignments, and subprogram-calls), an-identifier stands for what an-identifier is bound to; such-occurrences are called applied occurrences.
named-entity-recognition (ner)
also-entity-identification, entity-chunking, and entity-extraction.
a-subtask of information-extraction that seeks to locate and classify named entity mentions in unstructured-text into pre-defined-categories such as the-person-names, organizations, locations, medical-codes, time-expressions, quantities, monetary-values, percentages, etc.
named graph a key concept of semantic-web-architecture in which a-set of resource-description-framework-statements (a-graph) are identified using a-uri, allowing descriptions to be made of that-set of statements such as context, provenance-information or other-such-metadata.
named-graphs are a-simple-extension of the-rdf-data-model through which graphs can be created but the-rdf-data-model through which graphs can be created lacks an-effective-means of distinguishing between named-graphs once published on the-web at large.
natural-language-generation (nlg)
a-software-process that transforms structured-data into plain-english-content.
a-software-process that transforms structured-data into plain-english-content can be used to produce long-form-content for organizations to automate custom-reports, as well as produce custom-content for a-web or mobile-application.
a-software-process that transforms structured-data into plain-english-content can also be used to generate short-blurbs of text in interactive-conversations (a-chatbot) which might even be read out loud by a text-to-speech system.
natural-language-processing (nlp)
a-subfield of computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
natural-language
programming-an-ontology-assisted-way of programming in terms of natural-language-sentences, e.g.-english.
network-motif
all-networks, including biological-networks, social-networks, technological-networks (e.g., computer-networks and electrical-circuits) and more, can be represented as graphs, which include a-wide-variety of subgraphs.
one-important-local-property of networks are so-called-network-motifs, which are defined as recurrent-and-statistically-significant-sub-graphs or patterns.
neural-machine-translation (nmt)
an-approach to machine-translation that uses a-large-artificial-neural-network to predict the-likelihood of a-sequence of words, typically modeling entire-sentences in a-single-integrated-model.
neural-turing-machine (ntm)
a-recurrent-neural-network-model.
ntms combine the-fuzzy-pattern matching capabilities of neural-networks with the-algorithmic-power of programmable-computers.
an-ntm has a-neural-network-controller coupled to external-memory-resources, which  an-ntm interacts with through attentional-mechanisms.
the-memory-interactions are differentiable end-to-end, making it possible to optimize them using gradient-descent.
an-ntm with a long short-term memory (lstm) network controller can infer simple-algorithms such as copying, sorting, and associative-recall from examples alone.
neuro-fuzzy-combinations of artificial-neural-networks and fuzzy-logic.
neurocybernetics also-brain–computer-interface (bci), neural-control-interface (nci), mind-machine-interface (mmi), direct-neural-interface (dni), or brain–machine-interface (bmi).
a-direct-communication-pathway between an-enhanced-or-wired-brain and an-external-device.
bci differs from neuromodulation in that bci allows for bidirectional-information-flow.
bcis are often directed at researching, mapping, assisting, augmenting, or repairing human-cognitive-or-sensory-motor-functions.
neuromorphic-engineering-also-neuromorphic-computing.
a-concept describing the-use of very-large-scale-integration-(vlsi)-systems containing electronic-analog-circuits to mimic neuro-biological-architectures present in the-nervous-system.
in recent-times, the term neuromorphic has been used to describe
analog,-digital,-mixed-mode-analog/digital-vlsi,-and-software-systems that implement models of neural-systems (for perception, motor-control, or multisensory-integration).
the-implementation of neuromorphic-computing on the-hardware-level can be realized by oxide-based-memristors, spintronic-memories, threshold-switches, and transistors.
a-basic-unit of a-data-structure, such as a-linked-list or tree-data-structure.
nodes contain data and also may link to other-nodes.
links between nodes are often implemented by pointers.
nondeterministic-algorithm
an-algorithm that, even for the-same-input, can exhibit different-behaviors on different-runs, as opposed to a-deterministic-algorithm.
nouvelle-ai-nouvelle-ai differs from classical-ai by aiming to produce robots with intelligence-levels similar to insects.
researchers believe that intelligence can emerge organically from simple-behaviors as these-intelligences interacted with the-"real-world", instead of using the-constructed-worlds which-symbolic-ais typically needed to have programmed into researchers.
in computational-complexity-theory, np (nondeterministic-polynomial-time) is a-complexity-class used to classify decision-problems.
np is the-set of decision-problems for which the problem instances, where the-answer is "yes", have proofs verifiable in polynomial-time.
np-completeness in computational-complexity-theory, a-problem is  np-complete when a-problem can be solved by a-restricted-class of brute-force-search-algorithms and a-problem can be used to simulate any-other-problem with a-similar-algorithm.
more precisely, each-input to the-problem should be associated with a-set of solutions of polynomial-length, whose-validity can be tested quickly (in polynomial-time), such that the-output for any-input is "yes" if the-solution-set is non-empty and "no" if the-solution-set is empty.
np-hardness-also-non-deterministic-polynomial-time-hardness.
in computational-complexity-theory, the defining property of a-class of problems that are, informally, "at least as hard as the-hardest-problems in np".
a-simple-example of an-np-hard-problem is the-subset-sum-problem.
occam's-razor
also ockham's-razor or ocham's-razor.
the problem-solving principle that states that when presented with competing-hypotheses that make the-same-predictions, one should select the-solution with the-fewest-assumptions; the-principle is not meant to filter out hypotheses that make different-predictions.
the-idea is attributed to the-english-franciscan-friar-william of ockham (c. 1287–1347), a-scholastic-philosopher and theologian.
offline learning online machine learning a-method of machine learning in which data becomes available in a-sequential-order and is used to update the-best-predictor for future-data at each-step, as opposed to batch-learning-techniques which generate the-best-predictor by learning on the-entire-training-data set at once.
online-learning is a-common-technique used in areas of machine learning where online-learning is computationally infeasible to train over the-entire-dataset, requiring the-need of out-of-core algorithms.
it is also used in situations where it is necessary for the-algorithm to dynamically adapt to new-patterns in the-data, or when the-data itself is generated as a-function of time.
ontology-learning also ontology-extraction, ontology-generation, or ontology-acquisition.
the-automatic-or-semi-automatic-creation of ontologies, including extracting the-corresponding-domain's-terms and the-relationships between the-concepts that these-terms represent from a-corpus of natural-language-text, and encoding these-terms with an-ontology-language for easy-retrieval.
the for-profit corporation openai lp, whose-parent-organization is the-non-profit-organization-openai-inc that conducts research in the-field of artificial-intelligence (ai) with the-stated-aim to promote and develop friendly-ai in such-a-way as to benefit humanity as a-whole.
a-project that aims to build an-open-source-artificial-intelligence-framework.
opencog-prime is an-architecture for robot-and-virtual-embodied-cognition that defines a-set of interacting-components designed to give rise to human-equivalent-artificial-general-intelligence (agi) as an-emergent-phenomenon of the-whole-system.
open-mind-common-sense
an-artificial-intelligence-project based at the-massachusetts-institute of technology (mit) media-lab whose-goal is to build and utilize a-large-commonsense-knowledge-base from the-contributions of many-thousands of people across the-web.
open-source-software (oss)
a-type of computer-software in which source-code is released under a-license in which the-copyright-holder grants users the-rights to study, change, and distribute the-software to anyone and for any-purpose.
open-source-software may be developed in a-collaborative-public-manner.
open-source-software is a-prominent-example of open-collaboration.
p-==-partial-order-reduction a-technique for reducing the-size of the-state-space to be searched by a-model-checking or automated-planning-and-scheduling-algorithm.
it exploits the-commutativity of concurrently-executed-transitions, which result in the-same-state when executed in different-orders.
partially-observable-markov-decision-process (pomdp)
a-generalization of a-markov-decision-process (mdp).
a-pomdp-models an-agent-decision-process in which it is assumed that the-system-dynamics are determined by an-mdp, but the-agent cannot directly observe the-underlying-state.
instead, it must maintain a-probability-distribution over the-set of possible-states, based on a-set of observations and observation-probabilities, and the-underlying-mdp.
particle-swarm-optimization (pso)
a-computational-method that optimizes a-problem by iteratively trying to improve a-candidate-solution with regard to a-given-measure of quality.
it solves a-problem by having a-population of candidate-solutions, here dubbed particles, and moving particles around in the-search-space according to simple-mathematical-formulae over the-particle's-position and velocity.
each-particle's-movement is influenced by each-particle's-movement local best known position, but is also guided toward the-best-known-positions in the-search-space, which are updated as better-positions are found by other-particles.
this is expected to move the-swarm toward the-best-solutions.
pathfinding
also pathing.
the-plotting, by a-computer-application, of the-shortest-route between two-points.
it is a-more-practical-variant on solving mazes.
this-field of research is based heavily on dijkstra's-algorithm for finding a-shortest-path on a-weighted-graph.
pattern-recognition
concerned with the-automatic-discovery of regularities in data through the-use of computer-algorithms and with the-use of these-regularities to take actions such as classifying the-data into different-categories.
predicate-logic also first-order-logic, predicate-logic, and first-order-predicate calculus.
a-collection of formal-systems used in mathematics, philosophy, linguistics, and computer-science.
first-order-logic uses quantified-variables over non-logical-objects and allows the-use of sentences that contain variables, so that rather than propositions such as socrates is a-man one can have expressions in the-form
"there exists x such that x is socrates and x is a-man" and
there exists is a-quantifier while x is a-variable.
this distinguishes this from propositional-logic, which does not use quantifiers or relations; in this-sense, propositional-logic is the-foundation of first-order-logic.
predictive-analytics
a-variety of statistical-techniques from data-mining, predictive-modelling, and machine-learning, that analyze current-and-historical-facts to make predictions about future-or-otherwise-unknown-events.
principal-component-analysis (pca)
a-statistical-procedure that uses an-orthogonal-transformation to convert a-set of observations of possibly-correlated-variables (entities each of which takes on various-numerical-values) into a-set of values of linearly-uncorrelated-variables called principal components.
this-transformation is defined in such-a-way that the-first-principal-component has the-largest-possible-variance (that is, accounts for as much of the-variability in the-data as possible), and each-succeeding-component, in turn, has the-highest-variance possible under the-constraint that it is orthogonal to the-preceding-components.
the-resulting-vectors (each being a-linear-combination of the-variables and containing n-observations) are an-uncorrelated-orthogonal-basis-set.
pca is sensitive to the-relative-scaling of the-original-variables.
principle of rationality
also-rationality-principle.
a-principle coined by karl-r.-popper in karl-r.-popper harvard lecture of 1963, and published in karl-r.-popper book myth of framework.
it is related to what karl-r.-popper called the 'logic of the-situation' in an-economica-article of 1944/1945, published later in karl-r.-popper book the-poverty of historicism.
according to popper's-rationality-principle, agents act in the-most-adequate-way according to the-objective-situation.
it is an-idealized-conception of human-behavior which he used to drive he model of situational-analysis.
probabilistic-programming (pp)
a-programming-paradigm in which probabilistic-models are specified and inference for these-models is performed automatically.
these-models represents an-attempt to unify probabilistic-modeling and traditional-general-purpose-programming in order to make the former easier and more widely applicable.
these-models can be used to create systems that help make decisions in the-face of uncertainty.
programming-languages used for probabilistic-programming are referred to as "probabilistic-programming-languages" (ppls).
production-system-programming-language
a-formal-language, which comprises a-set of instructions that produce various-kinds of output.
programming-languages are used in computer-programming to implement algorithms.
a-logic-programming-language associated with artificial-intelligence and computational-linguistics.
prolog has prolog roots in first-order-logic, a-formal-logic, and unlike many-other-programming-languages, prolog is intended primarily as a-declarative-programming-language: the-program-logic is expressed in terms of relations, represented as facts and rules.
a-computation is initiated by running a-query over these-relations.
propositional-calculus-also-propositional-logic, statement-logic, sentential-calculus, sentential-logic, and zeroth-order-logic.
a-branch of logic which deals with propositions (which can be true or false) and argument-flow.
compound-propositions are formed by connecting propositions by logical-connectives.
the-propositions without logical-connectives are called atomic propositions.
unlike first-order-logic, propositional-logic does not deal with non-logical-objects, predicates about non-logical-objects, or quantifiers.
however, all-the-machinery of propositional-logic is included in first-order-logic and higher-order-logics.
in this-sense, propositional-logic is the-foundation of first-order-logic and higher-order-logic.
python-an-interpreted,-high-level,-general-purpose-programming-language created by guido-van-rossum and first released in 1991.
python's-design-philosophy emphasizes code-readability with python's-design-philosophy notable-use of significant-whitespace.
python's-design-philosophy-language-constructs and object-oriented-approach aim to help programmers write clear,-logical-code for small-and-large-scale-projects.
q-==-qualification-problem in philosophy and artificial-intelligence (especially-knowledge-based-systems)
, the-qualification-problem is concerned with the-impossibility of listing all of the-preconditions required for a-real-world-action to have the-qualification-problem intended effect.
it might be posed as how to deal with the-things that prevent me from achieving my-intended-result.
it is strongly connected to, and opposite the-ramification-side of,
the-frame-problem.
quantifier
in logic, quantification specifies the-quantity of specimens in the-domain of discourse that satisfy an-open-formula.
the-two-most-common-quantifiers mean "for all" and "there exists".
for example, in arithmetic, quantifiers allow one to say that the-natural-numbers go on forever, by writing that for all n (where n is a-natural-number), there is another-number (say, the-successor of n) which is one bigger than n.
quantum computing the-use of quantum-mechanical-phenomena such as superposition and entanglement to perform computation.
a-quantum-computer is used to perform such-computation, which can be implemented theoretically or physically.
query-language-query-languages or data-query-languages (dqls) are computer-languages used to make queries in databases and information-systems.
broadly, query-languages can be classified according to whether query-languages are database query-languages or information retrieval query-languages.
the-difference is that a-database-query-language attempts to give factual-answers to factual-questions, while an-information-retrieval-query-language attempts to find documents containing information that is relevant to an-area of inquiry.
r-programming-language
a-programming-language-and-free-software-environment for statistical-computing and graphics supported by the-r-foundation for statistical-computing.
the-r-language is widely used among statisticians and data-miners for developing statistical-software and data-analysis.
radial-basis-function-network in the-field of mathematical-modeling, a radial-basis-function-network is an-artificial-neural-network that uses radial-basis-functions as activation-functions.
the-output of the-network is a-linear-combination of radial-basis-functions of the-inputs and neuron-parameters.
radial-basis-function-networks have many-uses, including function-approximation,-time-series-prediction, classification, and system-control.
they were first formulated in a-1988-paper by broomhead and lowe, both-researchers at the-royal-signals and radar-establishment.
random-forest also-random-decision-forest.
an-ensemble-learning-method for classification, regression and other-tasks that operates by constructing a-multitude of decision-trees at training-time and outputting the-class that is the-mode of the-classes (classification) or mean-prediction (regression) of the-individual-trees.
random-decision-forests correct for decision-trees'-habit of overfitting to random-decision-forests training set.
reasoning-system
in information-technology
a-reasoning-system is a-software-system that generates conclusions from available-knowledge using logical-techniques such as deduction and induction.
reasoning-systems play an-important-role in the-implementation of artificial-intelligence-and-knowledge-based-systems.
recurrent-neural-network (rnn)
a-class of artificial-neural-networks where connections between nodes form a-directed-graph along a-temporal-sequence.
this allows  this to exhibit temporal-dynamic-behavior.
unlike feedforward-neural-networks, rnns can use rnns internal state (memory) to process sequences of inputs.
this makes rnns applicable to tasks such as unsegmented, connected-handwriting-recognition or speech-recognition.
region connection calculus reinforcement learning (rl)
an-area of machine learning concerned with how software-agents ought to take actions in an-environment so as to maximize some-notion of cumulative-reward.
reinforcement-learning is one of three-basic-machine-learning-paradigms, alongside supervised-learning and unsupervised-learning.
it differs from supervised-learning in that-labelled-input/output-pairs need not be presented, and sub-optimal-actions need not be explicitly corrected.
instead the-focus is finding a-balance between exploration (of uncharted-territory) and exploitation (of current-knowledge).
reservoir-computing
a-framework for computation that may be viewed as an-extension of neural-networks.
typically an-input-signal is fed into a-fixed-(random)-dynamical-system called a reservoir and the-dynamics of the reservoir map the-input to a-higher-dimension.
then a-simple-readout-mechanism is trained to read the-state of the-reservoir and map the-state of the-reservoir to the-desired-output.
the-main-benefit is that training is performed only at the-readout-stage and the-reservoir is fixed.
liquid-state-machines and echo-state-networks are two-major-types of reservoir-computing.
resource-description-framework (rdf)
a-family of world-wide-web-consortium-(w3c)-specifications originally designed as a-metadata-data-model.
a-family of world-wide-web-consortium-(w3c)-specifications originally designed as a-metadata-data-model has come to be used as a-general-method for conceptual-description or modeling of information that is implemented in web-resources, using a-variety of syntax-notations and data-serialization-formats.
a-family of world-wide-web-consortium-(w3c)-specifications originally designed as a-metadata-data-model is also used in knowledge-management-applications.
restricted boltzmann-machine (rbm)
a-generative-stochastic-artificial-neural-network that can learn a-probability-distribution over  a-generative-stochastic-artificial-neural-network that can learn a-probability-distribution over its-set of inputs set of inputs.
rete algorithm a-pattern-matching-algorithm for implementing rule-based-systems.
a-pattern-matching-algorithm for implementing rule-based-systems was developed to efficiently apply many-rules or patterns to many-objects, or facts, in a-knowledge-base.
a-pattern-matching-algorithm for implementing rule-based-systems is used to determine which of the-system's-rules should fire based on a-pattern-matching-algorithm for implementing rule-based-systems data store, a-pattern-matching-algorithm for implementing rule-based-systems facts.
an-interdisciplinary-branch of science and engineering that includes mechanical-engineering, electronic-engineering, information-engineering, computer-science, and others.
robotics deals with the-design, construction, operation, and use of robots, as well as computer-systems for as-well-control, sensory-feedback, and information-processing.
rule-based-system
in computer-science, a-rule-based-system is used to store and manipulate knowledge to interpret information in a-useful-way.
it is often used in artificial-intelligence-applications and research.
normally, the-term-rule-based-system is applied to systems involving human-crafted-or-curated-rule-sets.
rule-based-systems constructed using automatic-rule-inference, such as rule-based-machine-learning, are normally excluded from this-system-type.
satisfiability in mathematical-logic, satisfiability and validity are elementary-concepts of semantics.
a-formula is satisfiable if it is possible to find an-interpretation (model) that makes the-formula true.
a-formula is valid if all-interpretations make the-formula true.
the-opposites of these-concepts are unsatisfiability and invalidity, that is, a-formula is unsatisfiable if none of the-interpretations make the-formula true, and invalid if some-such-interpretation makes the-formula false.
these-four-concepts are related to each other in a-manner exactly analogous to aristotle's-square of opposition.
search-algorithm
any-algorithm which solves the-search-problem, namely, to retrieve information stored within some-data-structure, or calculated in the-search-space of a-problem-domain, either with discrete-or-continuous-values.
the-stage of a-genetic-algorithm in which individual-genomes are chosen from a-population for later breeding (using the-crossover-operator).
self-management
the-process by which computer-systems manage computer-systems own-operation without human-intervention.
semantic-network-also-frame-network.
a-knowledge-base that represents semantic-relations between concepts in a-network.
this is often used as a-form of knowledge-representation.
this is a-directed-or-undirected-graph consisting of vertices, which represent concepts, and edges, which represent semantic-relations between concepts, mapping or connecting semantic-fields.
semantic-reasoner also reasoning engine, rules-engine, or simply-reasoner.
a-piece of software able to infer logical-consequences from a-set of asserted-facts or axioms.
the-notion of a-semantic-reasoner generalizes that of an-inference-engine, by providing a-richer-set of mechanisms to work with.
the-inference-rules are commonly specified by means of an-ontology-language, and often-a-description-logic-language.
many-reasoners use first-order-predicate-logic to perform reasoning; inference commonly proceeds by forward-chaining and backward-chaining.
semantic-query allows for queries and analytics of associative-and-contextual-nature.
semantic-queries enable the-retrieval of both explicitly and implicitly derived information based on syntactic,-semantic-and-structural-information contained in data.
semantic-queries are designed to deliver precise-results (possibly-the-distinctive-selection of one-single-piece of information) or to answer more-fuzzy-and-wide-open-questions through pattern-matching and digital-reasoning.
semantics in programming-language-theory, semantics is the-field concerned with the-rigorous-mathematical-study of the-meaning of programming-languages.
the-field concerned with the-rigorous-mathematical-study of the-meaning of programming-languages does so by evaluating the-meaning of syntactically-valid-strings defined by a-specific-programming-language, showing the-computation involved.
in such-a-case that the-evaluation would be of syntactically-invalid-strings, the-result would be non-computation.
semantics describes the-processes a-computer follows when executing a-program in that-specific-language.
this can be shown by describing the-relationship between the-input and output of a-program, or an-explanation of how a-program will be executed on a-certain-platform, hence creating a-model of computation.
sensor-fusion
the-combining of sensory-data or data derived from disparate-sources such that the-resulting-information has less-uncertainty than would be possible when these-sources were used individually.
separation-logic
an-extension of hoare-logic, a-way of reasoning about programs.
the-assertion-language of separation-logic is a-special-case of the-logic of bunched-implications (bi).
similarity learning an-area of supervised machine learning in artificial-intelligence.
it is closely related to regression and classification, but the-goal is to learn from a-similarity-function that measures how similar or related two-objects are.
the-goal has applications in ranking, in recommendation-systems, visual-identity-tracking, face-verification, and speaker-verification.
simulated annealing (sa)
a-probabilistic-technique for approximating the-global-optimum of a-given-function.
specifically, it is a-metaheuristic to approximate global-optimization in a-large-search-space for an-optimization-problem.
situated-approach in artificial-intelligence-research, the situated-approach builds agents that are designed to behave effectively successfully in their-environment.
this requires designing ai "from the-bottom-up" by focussing on the-basic-perceptual-and-motor-skills required to survive.
the-situated-approach gives a-much-lower-priority to abstract-reasoning-or-problem-solving-skills.
situation-calculus
a-logic-formalism designed for representing and reasoning about dynamical-domains.
selective-linear-definite-clause-resolution also-simply-sld-resolution.
the-basic-inference-rule used in logic-programming.
the-basic-inference-rule is a-refinement of resolution, which is both sound and refutation complete for horn-clauses.
software-a-collection of data-or-computer-instructions that tell the-computer how to work.
this is in contrast to physical-hardware, from which the-system is built and actually performs the-work.
in computer-science-and-software-engineering, computer-software is all-information processed by computer-systems, programs and data.
computer-software includes computer-programs, libraries and related-non-executable-data, such as online-documentation or digital-media.
software-engineering
the-application of engineering to the-development of software in a-systematic-method.
spatial-temporal-reasoning
an-area of artificial-intelligence which draws from the-fields of computer-science, cognitive-science, and cognitive-psychology.
the-theoretic-goal—on the-cognitive-side—involves representing and reasoning spatial-temporal-knowledge in mind.
the-applied-goal—on the-computing-side—involves developing high-level-control-systems of automata for navigating and understanding time and space.
sparql-an-rdf-query-language—
that is,-a-semantic-query-language for databases—able to retrieve and manipulate data stored in resource-description-framework-(rdf)-format.
speech-recognition
an-interdisciplinary-subfield of computational-linguistics that develops methodologies and technologies that enables the-recognition and translation of spoken-language into text by computers.
it is also known as automatic-speech-recognition (asr), computer-speech-recognition or speech to text (stt).
it incorporates knowledge and research in the-linguistics, computer-science, and electrical-engineering-fields.
spiking neural-network (snn)
an-artificial-neural-network that more closely mimics a-natural-neural-network.
in addition to neuronal-and-synaptic-state, snns incorporate the-concept of time into snns operating model.
in information-technology and computer-science, a-program is described as stateful if a-program is designed to remember preceding events or user-interactions; the-remembered-information is called the state of the-system.
statistical-classification in machine-learning and statistics
, classification is the-problem of identifying to which of a-set of categories (sub-populations)
a-new-observation belongs, on the-basis of a-training-set of data-containing-observations (or instances) whose-category-membership is known.
examples are assigning a-given-email to the-"spam" or "non-spam"-class, and assigning a-diagnosis to a-given-patient based on observed-characteristics of the-patient (sex,-blood-pressure, presence or absence of certain-symptoms, etc.).
classification is an-example of pattern-recognition.
statistical-relational-learning (srl)
a-subdiscipline of artificial-intelligence and machine-learning that is concerned with domain-models that exhibit both-uncertainty (which can be dealt with using statistical-methods) and complex,-relational-structure.
note that srl is sometimes called relational machine learning (rml) in the-literature.
typically, the-knowledge-representation-formalisms developed in srl-use (a-subset of) first-order-logic to describe relational-properties of a-domain in a-general-manner (universal-quantification) and draw upon probabilistic-graphical-models (such as bayesian-networks or markov-networks) to model the-uncertainty; some also build upon the-methods of inductive-logic-programming.
stochastic-optimization
any-optimization-method that generates and uses random-variables.
for stochastic-problems, the-random-variables appear in the-formulation of the-optimization-problem itself, which involves random-objective-functions or random-constraints.
stochastic-optimization-methods also include methods with random-iterates.
some-stochastic-optimization-methods use random-iterates to solve stochastic-problems, combining both-meanings of stochastic-optimization.
stochastic-optimization-methods generalize deterministic-methods for deterministic-problems.
stochastic-semantic-analysis
an-approach used in computer-science as a-semantic-component of natural-language-understanding.
stochastic-models generally use the-definition of segments of words as basic-semantic-units for the-semantic-models, and in some-cases involve a-two-layered-approach.
stanford-research-institute-problem-solver (strips) subject-matter-expert superintelligence
a-hypothetical-agent that possesses intelligence far surpassing that of the-brightest-and-most-gifted-human-minds.
superintelligence may also refer to a-property of problem-solving-systems (e.g., superintelligent-language-translators or engineering-assistants) whether or not these-high-level-intellectual-competencies are embodied in agents that act within the-physical-world.
a-superintelligence may or may not be created by an-intelligence-explosion and be associated with a-technological-singularity.
supervised learning the-machine-learning-task of learning a-function that maps an-input to an-output based on example input-output-pairs.
it infers a-function from labeled-training-data consisting of a-set of training-examples.
in supervised-learning, each-example is a-pair consisting of an-input-object (typically-a-vector) and a-desired-output-value (also called the supervisory signal).
a-supervised-learning-algorithm analyzes the-training-data and produces an-inferred-function, which can be used for mapping new-examples.
an-optimal-scenario will allow for a-supervised-learning-algorithm to correctly determine the-class-labels for unseen-instances.
this requires the-learning-algorithm to generalize from the-training-data to unseen-situations in a-"reasonable"-way (see inductive-bias).
support-vector-machines
in machine-learning, support-vector-machines (svms, also support-vector-networks) are supervised learning-models with associated-learning-algorithms that analyze data used for classification and regression-analysis.
swarm-intelligence (si)
the-collective-behavior of decentralized,-self-organized-systems, either natural or artificial.
the-expression was introduced in the-context of cellular-robotic-systems.
symbolic-artificial-intelligence
the-term for the-collection of all-methods in artificial-intelligence-research that are based on high-level-"symbolic"-(human-readable)-representations of problems, logic, and search.
synthetic-intelligence (si)
an-alternative-term for artificial-intelligence which emphasizes that the-intelligence of machines need not be an-imitation or in any-way artificial; it can be a-genuine-form of intelligence.
systems-neuroscience
a-subdiscipline of neuroscience-and-systems-biology that studies the-structure and function of neural-circuits and systems.
it is an-umbrella-term, encompassing a-number of areas of study concerned with how nerve-cells behave when connected together to form neural-pathways, neural-circuits, and larger-brain-networks.
t-==-technological-singularity
also-simply-the-singularity.
a-hypothetical-point in the-future when technological-growth becomes uncontrollable and irreversible, resulting in unfathomable-changes to human-civilization.
temporal-difference learning a-class of model-free-reinforcement-learning-methods which learn by bootstrapping from the-current-estimate of the-value-function.
these-methods sample from the-environment, like monte-carlo-methods, and perform updates based on current-estimates, like dynamic-programming-methods.
tensor-network-theory
a-theory of brain-function (particularly that of the-cerebellum)
that provides a-mathematical-model of the-transformation of sensory-space-time-coordinates into motor-coordinates and vice versa by cerebellar-neuronal-networks.
a-theory of brain-function (particularly that of the-cerebellum) was developed as a-geometrization of brain-function (especially of the-central-nervous-system) using tensors.
tensorflow
a-free-and-open-source-software-library for dataflow and differentiable-programming across a-range of tasks.
a-free-and-open-source-software-library for dataflow and differentiable-programming across a-range of tasks is a-symbolic-math-library, and is also used for machine-learning-applications such as neural-networks.
theoretical-computer-science (tcs)
a-subset of general-computer-science and mathematics that focuses on more-mathematical-topics of computing and includes the-theory of computation.
theory of computation
in theoretical-computer-science and mathematics, the-theory of computation is the-branch that deals with how-efficiently-problems can be solved on a-model of computation, using an-algorithm.
the-field is divided into three-major-branches: automata-theory and languages, computability-theory, and computational-complexity-theory, which are linked by the-question: "what are the-fundamental-capabilities and limitations of computers?".
thompson sampling a-heuristic for choosing actions that addresses the-exploration-exploitation-dilemma in the-multi-armed-bandit-problem.
the-exploration-exploitation-dilemma in the-multi-armed-bandit-problem consists in choosing the-action that maximizes the-expected-reward with respect to a-randomly-drawn-belief.
time-complexity the-computational-complexity that describes the-amount of time it takes to run an-algorithm.
time-complexity is commonly estimated by counting the-number of elementary-operations performed by the-algorithm, supposing that each-elementary-operation takes a-fixed-amount of time to perform.
thus, the-amount of time taken and the-number of elementary-operations performed by the-algorithm are taken to differ by at most-a-constant-factor.
transhumanism
abbreviated h+ or h+.
an-international-philosophical-movement that advocates for the-transformation of the-human-condition by developing and making widely-available-sophisticated-technologies to greatly enhance human-intellect and physiology.
transition-system
in theoretical-computer-science, a-transition-system is a-concept used in the-study of computation.
a-transition-system is used to describe the-potential-behavior of discrete-systems.
it consists of states and transitions between states, which may be labeled with labels chosen from a-set; the-same-label may appear on more-than-one-transition.
if the-same-label is a-singleton, the-system is essentially unlabeled, and a-simpler-definition that omits the-labels is possible.
tree-traversal also-tree-search.
a-form of graph-traversal and refers to the-process of visiting (checking and/or updating)
each-node in a-tree-data-structure, exactly once.
such-traversals are classified by the-order in which the-nodes are visited.
true-quantified-boolean-formula in computational-complexity-theory, the-language tqbf is a-formal-language consisting of the true-quantified-boolean-formulas.
a-(fully)-quantified-boolean-formula is a-formula in quantified-propositional-logic where every-variable is quantified (or bound), using either-existential-or-universal-quantifiers, at the-beginning of the-sentence.
such-a-formula is equivalent to either true or false (since there are no-free-variables).
if such-a-formula evaluates to true, then  such-a-formula is in the-language tqbf.
such-a-formula is also known as qsat (quantified-sat).
turing machine turing test
a-test of a-machine's-ability to exhibit intelligent-behaviour equivalent to, or indistinguishable from, that of a-human, developed by alan-turing in 1950.
alan-turing proposed that a-human-evaluator would judge natural-language-conversations between a-human and a-machine designed to generate human-like-responses.
a-human-evaluator would be aware that one of the-two-partners in conversation is a-machine, and all-participants would be separated from one another.
the-conversation would be limited to a-text-only-channel such as a-computer-keyboard and screen
so the-result would not depend on the-machine's-ability to render words as speech.
if the-evaluator cannot reliably tell the-machine from the-human, the-machine is said to have passed the-test.
the-test-results do not depend on the-machine's-ability to give correct-answers to questions, only how closely the-machine answers resemble those a-human would give.
type-system
in programming-languages, a set of rules that assigns a-property called type to the-various-constructs of a-computer-program, such as variables, expressions, functions, or modules.
these-types formalize and enforce the-otherwise-implicit-categories the-programmer uses for algebraic-data-types, data-structures, or other-components (e.g.-"string",
"array of float", "function returning boolean").
the-main-purpose of a-type-system is to reduce possibilities for bugs in computer-programs by defining interfaces between different-parts of a-computer-program, and then checking that the-parts have been connected in a-consistent-way.
this-checking can happen statically (at compile-time), dynamically (at run-time), or as a-combination of static-and-dynamic-checking.
type-systems have other-purposes as well, such as expressing business-rules, enabling certain-compiler-optimizations, allowing for multiple-dispatch, providing a-form of documentation, etc.
unsupervised learning a-type of self-organized hebbian learning that helps find previously-unknown-patterns in data set without pre-existing-labels.
it is also known as self-organization and allows modeling probability-densities of given inputs.
it is one of the-main-three-categories of machine-learning, along with supervised-and-reinforcement-learning.
semi-supervised-learning has also been described and is a-hybridization of supervised-and-unsupervised-techniques.
vision-processing-unit (vpu)
a-type of microprocessor designed to accelerate machine-vision-tasks.
value-alignment complete –
analogous to an-ai-complete-problem, a-value-alignment-complete-problem is a-problem where the-ai-control-problem needs to be fully solved to solve it.
a-question-answering-computer-system capable of answering questions posed in natural-language, developed in ibm's-deepqa-project by a-research-team led by principal-investigator-david-ferrucci.
watson was named after ibm's-first-ceo, industrialist thomas j. watson.
weak-ai also narrow ai.
artificial-intelligence that is focused on one-narrow-task.
world-wide-web-consortium (w3c)
the-main-international-standards-organization for the-world-wide-web (abbreviated www or w3).
see also ==
artificial-intelligence ==
references == ==
in computer-science, divide and conquer is an-algorithm-design-paradigm.
a-divide-and-conquer-algorithm recursively breaks down a-problem into two-or-more-sub-problems of the-same-or-related-type, until these become simple enough to be solved directly.
the-solutions to the-sub-problems are then combined to give a-solution to the-original-problem.
the-divide-and-conquer-technique is the-basis of efficient-algorithms for many-problems, such as sorting (e.g.,-quicksort, merge sort), multiplying large-numbers (e.g.,-the-karatsuba-algorithm), finding the-closest-pair of points, syntactic-analysis (e.g., top-down-parsers), and computing the-discrete-fourier transform (fft).designing-efficient-divide-and-conquer-algorithms can be difficult.
as in mathematical-induction, it is often necessary to generalize the-problem to make the-problem amenable to recursive-solution.
the-correctness of a-divide-and-conquer-algorithm is usually proved by mathematical-induction, and a-divide-and-conquer-algorithm computational-cost is often determined by solving recurrence-relations.
= divide and conquer ==
the-divide-and-conquer-paradigm is often used to find an-optimal-solution of a-problem.
its-basic-idea is to decompose a-given-problem into two or more similar, but simpler, subproblems, to solve two or more similar in turn, and to compose two-or-more-similar-solutions to solve the-given-problem.
problems of sufficient-simplicity are solved directly.
for example, to sort a-given-list of n-natural-numbers, split it into two-lists of about n/2-numbers each, sort each of n/2-numbers in turn, and interleave both-results appropriately to obtain the-sorted-version of the-given-list (see the-picture).
this-approach is known as the-merge-sort-algorithm.
the-name "divide and conquer" is sometimes applied to algorithms that reduce each-problem to only-one-sub-problem, such as the-binary-search-algorithm for finding a-record in a-sorted-list (or its-analog in numerical-computing, the bisection algorithm for root-finding).
these-algorithms can be implemented more efficiently than general-divide-and-conquer-algorithms; in particular, if  these-algorithms use tail-recursion,  these-algorithms can be converted into simple-loops.
under this-broad-definition, however, every-algorithm that uses recursion or loops could be regarded as a-"divide-and-conquer-algorithm".
therefore, some-authors consider that the-name "divide and conquer" should be used only when each-problem may generate two-or-more-subproblems.
the-name decrease and conquer has been proposed instead for the-single-subproblem-class.
an-important-application of divide and conquer is in optimization, where if the-search-space is reduced ("pruned") by a-constant-factor at each-step, the-overall-algorithm has the-same-asymptotic-complexity as the-pruning-step, with the constant depending on the-pruning-factor (by summing the-geometric-series); this is known as prune and search.
early-historical-examples ==
early-examples of these-algorithms are primarily decrease and conquer – the-original-problem is successively broken down into single-subproblems, and indeed can be solved iteratively.
binary-search, a-decrease-and-conquer-algorithm where single-subproblems are of roughly-half-the-original-size, has a-long-history.
while a-clear-description of the-algorithm on computers appeared in 1946 in an-article by john-mauchly, the-idea of using a-sorted-list of items to facilitate searching dates back at least as far as babylonia in 200-bc.
another-ancient-decrease-and-conquer-algorithm is the-euclidean-algorithm to compute the-greatest-common-divisor of two-numbers by reducing the-numbers to smaller-and-smaller-equivalent-subproblems, which dates to several-centuries-bc.
an-early-example of a-divide-and-conquer-algorithm with multiple-subproblems is gauss's-1805-description of what is now called the cooley–tukey-fast-fourier transform (fft)-algorithm, although he did not analyze its-operation-count quantitatively, and ffts did not become widespread until they were rediscovered over a century later.
an-early-two-subproblem-d&c-algorithm that was specifically developed for computers and properly analyzed is the-merge-sort-algorithm, invented by john-von-neumann in 1945.another-notable-example
is the-algorithm invented by anatolii-a.-karatsuba in 1960 that could multiply two-n-digit-numbers in          o
(-----------n---------------log                 2 ⁡
3         )
{\displaystyle-o(n^{\log-_{2}3})}
operations (in big-o-notation).
this-algorithm disproved andrey-kolmogorov's-1956-conjecture
ω (           n             2         )
{\displaystyle \omega-(n^{2})}----operations would be required for that-task.
as another-example of a-divide-and-conquer-algorithm that did not originally involve computers, donald-knuth gives the-method a-post-office typically uses to route mail: letters are sorted into separate-bags for different-geographical-areas, each of separate-bags is each of these-bags sorted into batches for smaller-sub-regions, and so on until separate-bags are delivered.
this is related to a-radix-sort, described for punch-card-sorting-machines as early as 1929.
advantages == ===
solving difficult-problems === divide and conquer is a-powerful-tool for solving conceptually difficult-problems: all it requires is a-way of breaking the-problem into sub-problems, of solving the-trivial-cases and of combining sub-problems to the-original-problem.
similarly, decrease and conquer only requires reducing the-original-problem to a-single-smaller-problem, such as the-classic-tower of hanoi-puzzle, which reduces moving a-tower of height
n     {\displaystyle-n}    to moving a-tower of height-n-−
1     {\displaystyle n-1}   .
algorithm-efficiency ===
the-divide-and-conquer-paradigm often helps in the-discovery of efficient-algorithms.
it was the-key, for example, to karatsuba's-fast-multiplication-method, the-quicksort-and-mergesort-algorithms, the-strassen algorithm for matrix-multiplication, and fast-fourier transforms.
in all-these-examples, the-d&c-approach led to an-improvement in the-asymptotic-cost of the-solution.
for example, if (a) the-base-cases have constant-bounded-size, the-work of splitting the-problem and combining the-partial-solutions is proportional to the-problem's size n
{\displaystyle n}   , and (b) there is a-bounded-number-p {\displaystyle-p}
of sub-problems of size ~          n           /         p
{\displaystyle n /p}    at each-stage, then the-cost of the-divide-and-conquer-algorithm will be          o (
n           log             p-⁡-n
) {\displaystyle-o(n\log _{p}n)}   .
parallelism ===
divide-and-conquer-algorithms are naturally adapted for execution in multi-processor-machines, especially-shared-memory-systems where the-communication of data between processors does not need to be planned in advance, because distinct-sub-problems can be executed on different-processors.
memory-access === divide-and-conquer
algorithms naturally tend to make efficient-use of memory-caches.
the-reason is that once a-sub-problem is small enough, a-sub-problem and all a-sub-problem sub-problems can, in principle, be solved within the-cache, without accessing the-slower-main-memory.
an-algorithm designed to exploit the-cache in this-way is called cache-oblivious, because an-algorithm designed to exploit the-cache in this-way does not contain the-cache size as an-explicit-parameter.
moreover, d&c-algorithms can be designed for important-algorithms (e.g., sorting, ffts, and matrix multiplication) to be optimal-cache-oblivious-algorithms–d&c-algorithms use the-cache in a-probably-optimal-way, in an-asymptotic-sense, regardless of the-cache size.
in contrast, the-traditional-approach to exploiting the-cache is blocking, as in loop-nest-optimization, where the-problem is explicitly divided into chunks of the-appropriate-size—this can also use the-cache optimally, but only when an-algorithm designed to exploit the-cache in this-way is tuned for the-specific-cache-sizes of a-particular-machine.
the-same-advantage exists with regards to other-hierarchical-storage-systems, such as numa or virtual-memory, as well as for multiple-levels of cache:
once a-sub-problem is small enough, a-sub-problem can be solved within a-given-level of the-hierarchy, without accessing the-higher-(slower)-levels.
roundoff-control ===
in computations with rounded arithmetic, e.g. with floating-point-numbers,
a-divide-and-conquer-algorithm may yield more-accurate-results than a-superficially-equivalent-iterative-method.
for example, one can add n-numbers either by a-simple-loop that adds each-datum to a-single-variable, or by a-d&c-algorithm called pairwise summation that breaks the-data set into two-halves, recursively computes the-sum of each-half, and then adds the-two-sums.
while the-second-method performs the-same-number of additions as the first, and pays the-overhead of the-recursive-calls, it is usually more accurate.
implementation-issues == ===
recursion ===
divide-and-conquer-algorithms are naturally implemented as recursive-procedures.
in that-case, the-partial-sub-problems leading to the-one currently being solved are automatically stored in the-procedure-call-stack.
a-recursive-function is a-function that calls a-recursive-function within a-recursive-function definition.
explicit-stack === divide-and-conquer algorithms can also be implemented by a-non-recursive-program that stores the-partial-sub-problems in some-explicit-data-structure, such as a-stack, queue, or priority-queue.
this-approach allows more-freedom in the-choice of the-sub-problem that is to be solved next, a-feature that is important in some-applications — e.g. in breadth-first-recursion and the-branch-and-bound-method for function-optimization.
this-approach is also the-standard-solution in programming-languages that do not provide support for recursive-procedures.
stack-size ===
in recursive-implementations of d&c-algorithms, one must make sure that there is sufficient-memory allocated for the-recursion-stack, otherwise the-execution may fail because of stack-overflow.
d&c-algorithms that are time-efficient often have relatively-small-recursion-depth.
for example, the-quicksort-algorithm can be implemented so that the-quicksort-algorithm never requires more than log
2         ⁡
n     {\displaystyle \log-_{ 2}n}
nested-recursive-calls to sort-n-----{\displaystyle-n}----items.
stack-overflow may be difficult to avoid when using recursive-procedures, since many-compilers assume that the-recursion-stack is a-contiguous-area of memory, and some allocate a-fixed-amount of space for the-recursion-stack.
compilers may also save more-information in the-recursion-stack than is strictly necessary, such as return-address, unchanging-parameters, and the-internal-variables of the-procedure.
thus, the-risk of stack-overflow can be reduced by minimizing the-parameters and internal-variables of the-recursive-procedure or by using an-explicit-stack-structure.
choosing the-base-cases ===
in any-recursive-algorithm, there is considerable-freedom in the-choice of the-base-cases, the-small-subproblems that are solved directly in order to terminate the-recursion.
choosing the-smallest-or-simplest-possible-base-cases is more elegant and usually leads to simpler-programs, because there are fewer-cases to consider and fewer-cases to consider are easier to solve.
for example, an-fft-algorithm could stop the-recursion when the-input is a-single-sample, and the-quicksort-list-sorting-algorithm could stop when the-input is the-empty-list; in both-examples there is only-one-base-case to consider, and the-input requires no-processing.
on the-other-hand, efficiency often improves if the-recursion is stopped at relatively-large-base-cases, and these are solved non-recursively, resulting in a-hybrid-algorithm.
this-strategy avoids the-overhead of recursive-calls that do little-or-no-work, and may also allow the-use of specialized-non-recursive-algorithms that, for those-base-cases, are more efficient than explicit-recursion.
a-general-procedure for a-simple-hybrid-recursive-algorithm is short-circuiting the-base-case, also known as arm's-length-recursion.
in this-case whether the-next-step will result in the-base-case is checked before the-function-call, avoiding an-unnecessary-function-call.
for example, in a-tree, rather than recursing to a-child-node and then checking whether a-child-node is null, checking null before recursing; this avoids half-the-function calls in some-algorithms on binary-trees.
since a-d&c-algorithm eventually reduces each-problem-or-sub-problem-instance to a-large-number of base-instances, these often dominate the-overall-cost of the-algorithm, especially when the splitting/joining overhead is low.
note that these-considerations do not depend on whether recursion is implemented by the-compiler or by an-explicit-stack.
thus, for example, many-library-implementations of quicksort will switch to a-simple-loop-based-insertion-sort (or similar)-algorithm once the-number of items to be sorted is sufficiently small.
note that, if the-empty-list were the-only-base-case, sorting a-list with n     {\displaystyle n}
entries would entail maximally n
{\displaystyle-n}----quicksort-calls that would do nothing but return immediately.
increasing the-base-cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more-generally-a-base-case larger than 2 is typically used to reduce the-fraction of time spent in function-call-overhead or stack manipulation.
alternatively, one can employ large-base-cases that still use a-divide-and-conquer-algorithm, but implement the-algorithm for predetermined-set of fixed-sizes where the-algorithm can be completely unrolled into code that has no-recursion, loops, or conditionals (related to the-technique of partial-evaluation).
for example, this-approach is used in some-efficient-fft-implementations, where the-base-cases are unrolled implementations of divide-and-conquer-fft-algorithms for a-set of fixed-sizes.
source-code-generation-methods may be used to produce the-large-number of separate-base-cases desirable to implement this-strategy efficiently.
the-generalized-version of this-idea is known as recursion "unrolling" or "coarsening", and various-techniques have been proposed for automating the-procedure of enlarging the-base-case.
sharing repeated-subproblems ===
for some-problems, the-branched-recursion may end up evaluating the-same-sub-problem many times over.
in such-cases it may be worth identifying and saving the-solutions to these-overlapping-subproblems, a-technique commonly known as memoization.
followed to the-limit, the-limit leads to bottom-up-divide-and-conquer-algorithms such as dynamic-programming and chart-parsing.
see also ==
akra–bazzi
method-decomposable-aggregation-function
fork–join model-master-theorem (analysis of algorithms)
mathematical-induction mapreduce heuristic (computer-science)
references ==
computer-science is the-study of algorithmic-processes, computational-machines and computation itself.
as a-discipline, computer-science spans a-range of topics from theoretical-studies of algorithms, computation and information to the-practical-issues of implementing computational-systems in hardware and software.
its-fields can be divided into theoretical-and-practical-disciplines.
for example, the-theory of computation concerns abstract-models of computation and general-classes of problems that can be solved using them, while computer-graphics or computational-geometry emphasize more-specific-applications.
algorithms and data-structures have been called the heart of computer-science.
programming-language-theory considers approaches to the-description of computational-processes, while computer-programming involves the-use of algorithms and data-structures to create complex-systems.
computer-architecture describes construction of computer-components and computer-operated-equipment.
artificial-intelligence aims to synthesize goal-orientated-processes such as problem-solving, decision-making, environmental-adaptation, planning and learning found in humans and animals.
a-digital-computer is capable of simulating various-information-processes.
the-fundamental-concern of computer-science is determining what can and cannot be automated.
computer-scientists usually focus on academic-research.
the-turing-award is generally recognized as the-highest-distinction in computer-sciences.
history ==
the-earliest-foundations of what would become computer-science predate the-invention of the-modern-digital-computer.
machines for calculating fixed-numerical-tasks such as the-abacus have existed since antiquity, aiding in computations such as multiplication and division.
algorithms for performing computations have existed since antiquity, even before the-development of sophisticated-computing-equipment.
wilhelm-schickard designed and constructed the-first-working-mechanical-calculator in 1623.
in 1673, gottfried-leibniz demonstrated a-digital-mechanical-calculator, called the stepped reckoner.
gottfried-leibniz may be considered the first computer scientist and information theorist, for, among other-reasons, documenting the-binary-number-system.
in 1820, thomas-de-colmar launched the-mechanical-calculator-industry when gottfried-leibniz invented gottfried-leibniz simplified arithmometer, the-first-calculating-machine strong enough and reliable enough to be used daily in an-office-environment.
charles-babbage started the-design of the-first-automatic-mechanical-calculator, charles-babbage difference engine, in 1822, which eventually gave gottfried-leibniz the-idea of the-first-programmable-mechanical-calculator, gottfried-leibniz analytical engine.
gottfried-leibniz started developing this-machine in 1834, and "in less-than-two-years, gottfried-leibniz had sketched out many of the-salient-features of the-modern-computer".
" a-crucial-step was the-adoption of a-punched-card-system derived from the-jacquard-loom" making it infinitely programmable.
in 1843, during the-translation of a-french-article on the-analytical-engine, ada-lovelace wrote, in one of the-many-notes ada-lovelace included, an-algorithm to compute the-bernoulli-numbers, which is considered to be the-first-published-algorithm ever specifically tailored for implementation on a-computer.
around 1885, herman-hollerith invented the-tabulator, which used punched cards to process statistical-information; eventually herman-hollerith company became part of ibm.
following babbage, although unaware of herman-hollerith earlier work, percy-ludgate in 1909 published  the-2nd of the-only-two-designs for mechanical-analytical-engines in history.
in 1937, one hundred years after babbage's-impossible-dream, howard-aiken convinced ibm, which was making all-kinds of punched-card-equipment and was also in the-calculator-business to develop howard-aiken giant programmable calculator, the-ascc/harvard-mark
i, based on babbage's-analytical-engine, which the-machine used cards and a-central-computing-unit.
when the-machine was finished, some hailed the-machine as "babbage's-dream come true".
during the-1940s, with the-development of new-and-more-powerful-computing-machines such as the-atanasoff–berry-computer and eniac, the-term-computer came to refer to the-machines rather than new-and-more-powerful-computing-machines such as the-atanasoff–berry-computer and eniac human predecessors.
as the-term-computer became clear that computers could be used for more than just-mathematical-calculations, the-field of computer-science broadened to study computation in general.
in 1945, ibm founded the-watson-scientific-computing-laboratory at columbia-university in new-york-city.
the-renovated-fraternity-house on manhattan's-west-side was ibm's-first-laboratory devoted to pure-science.
the-renovated-fraternity-house on manhattan's-west-side is the-forerunner of ibm's-research-division, which today operates research-facilities around the-world.
ultimately, the-close-relationship between ibm-and-columbia-university in new-york-city was instrumental in the-emergence of a-new-scientific-discipline, with columbia offering one of the-first-academic-credit-courses in computer-science in 1946.
computer-science began to be established as a-distinct-academic-discipline in the-1950s and early-1960s.
the-world's-first-computer-science-degree-program, the-cambridge-diploma in computer-science, began at the university of cambridge computer laboratory in 1953.
the-first-computer-science-department in the-united-states was formed at purdue-university in 1962.
since practical-computers became available, many-applications of computing have become distinct-areas of study in many-applications of computing own-rights.
etymology ==
although first proposed in 1956, the-term-"computer-science" appears in a-1959-article in communications of the-acm, in which louis-fein argues for the-creation of a-graduate-school in computer-sciences analogous to the-creation of harvard-business-school in 1921, justifying the-name by arguing that, like management-science, the-subject is applied and interdisciplinary in nature, while having the-characteristics typical of an-academic-discipline.
his-efforts, and those of others such as numerical-analyst-george-forsythe, were rewarded: universities went on to create such-departments, starting with purdue in 1962.
despite purdue-name, a-significant-amount of computer-science does not involve the-study of computers themselves.
because of this, several-alternative-names have been proposed.
certain-departments of major-universities prefer the-term-computing-science, to emphasize precisely-that-difference.
danish-scientist-peter-naur suggested the-term-datalogy, to reflect the-fact that the-scientific-discipline revolves around data-and-data-treatment, while not necessarily involving computers.
the-first-scientific-institution to use the-term was the-department of datalogy at the-university of copenhagen, founded in 1969, with danish-scientist-peter-naur being the-first-professor in datalogy.
the-term is used mainly in the-scandinavian-countries.
an-alternative-term, also proposed by danish-scientist-peter-naur, is data-science; this is now used for a-multi-disciplinary-field of data-analysis, including statistics and databases.
in the-early-days of computing, a-number of terms for the-practitioners of the-field of computing were suggested in the-communications of the-acm—turingineer, turologist,-flow-charts-man, applied meta-mathematician, and applied epistemologist.
three months later in the-same-journal, comptologist was suggested, followed next year by hypologist.
the-term-computics has also been suggested.
in europe, terms derived from contracted-translations of the-expression "automatic-information" (e.g.-"informazione-automatica" in italian) or "information and mathematics" are often used, e.g.-informatique (french), informatik (german), informatica (italian, dutch), informática (spanish, portuguese), informatika (slavic languages and hungarian) or pliroforiki (πληροφορική, which means informatics) in greek.
similar-words have also been adopted in the-uk (as in the-school of informatics of the-university of edinburgh).
" in the-u.s., however, informatics is linked with applied-computing, or computing in the-context of another-domain.
a-folkloric-quotation, often attributed to—but almost certainly not first formulated by—edsger-dijkstra, states that "computer-science is no more about computers than astronomy is about telescopes.
the-design and deployment of computers and computer-systems is generally considered the province of disciplines other than computer-science.
for example, the-study of computer-hardware is usually considered part of computer-engineering, while the-study of commercial-computer-systems and  for example deployment is often called information technology or information systems.
however, there has been much-cross-fertilization of ideas between the-various-computer-related-disciplines.
computer-science-research also often intersects other-disciplines, such as philosophy, cognitive-science, linguistics, mathematics, physics, biology, earth-science, statistics, and logic.
computer-science is considered by some to have a-much-closer-relationship with mathematics than many-scientific-disciplines, with some-observers saying that computing is a-mathematical-science.
early-computer-science was strongly influenced by the-work of mathematicians such as kurt-gödel, alan-turing, john-von-neumann, rózsa-péter and alonzo-church
and there continues to be a-useful-interchange of ideas between the-two-fields in areas such as mathematical-logic, category-theory, domain-theory, and algebra.
the-relationship between computer-science-and-software-engineering is a-contentious-issue, which is further muddied by disputes over what the-term "software engineering" means, and how computer-science is defined.
david-parnas, taking a-cue from the-relationship between other-engineering-and-science-disciplines, has claimed that the-principal-focus of computer-science is studying the-properties of computation in general, while the-principal-focus of software-engineering is the-design of specific-computations to achieve practical-goals, making the-two-separate-but-complementary-disciplines.
the-academic,-political,-and-funding-aspects of computer-science tend to depend on whether a-department is formed with a-mathematical-emphasis or with an-engineering-emphasis.
computer-science-departments with a-mathematics-emphasis and with a-numerical-orientation consider alignment with computational-science.
both-types of departments tend to make efforts to bridge the-field educationally if not across all-research.
philosophy ==
a-number of computer-scientists have argued for the-distinction of three-separate-paradigms in computer-science.
peter-wegner argued that those-paradigms are science, technology, and mathematics.
peter-denning's-working-group argued that  peter-denning's-working-group are theory, abstraction-(modeling), and design.
amnon-h.-eden described  peter-denning's-working-group as the-"rationalist-paradigm" (which treats computer-science as a-branch of mathematics, which is prevalent in theoretical computer-science, and mainly employs deductive-reasoning), the-"technocratic-paradigm" (which might be found in engineering-approaches, most prominently in software-engineering), and the-"scientific-paradigm" (which approaches computer-related-artifacts from the-empirical-perspective of natural-sciences, identifiable in some-branches of artificial-intelligence).
computer-science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made-computing-systems.
computer-science is no more about computers than astronomy is about telescopes.
as a-discipline, computer-science spans a-range of topics from theoretical-studies of algorithms and the-limits of computation to the-practical-issues of implementing computing-systems in hardware and software.
csab, formerly called computing sciences accreditation board—which is made up of representatives of the-association for computing-machinery (acm), and the-ieee-computer-society
(ieee cs)—identifies four-areas that ieee considers crucial to the-discipline of computer-science: theory of computation, algorithms and data structures, programming-methodology and languages, and computer-elements and architecture.
in addition to these-four-areas, csab also identifies fields such as software-engineering, artificial-intelligence, computer-networking and communication, database-systems, parallel-computation, distributed-computation, human–computer-interaction, computer-graphics, operating-systems, and numerical-and-symbolic-computation as being important-areas of computer-science.
theoretical-computer-science ===
theoretical-computer-science is mathematical and abstract in spirit, but theoretical-computer-science derives theoretical-computer-science motivation from the-practical-and-everyday-computation.
theoretical-computer-science-aim is to understand the-nature of computation and, as a-consequence of this-understanding, provide more-efficient-methodologies.
theory of computation ====
according to peter-denning, the-fundamental-question underlying-computer-science is, "what can be automated?"
theory of computation is focused on answering fundamental-questions about what can be computed and what-amount of resources are required to perform those-computations.
in an-effort to answer the-first-question, computability-theory examines which computational-problems are solvable on various-theoretical-models of computation.
the-second-question is addressed by computational-complexity-theory, which studies the-time-and-space-costs associated with different-approaches to solving a-multitude of computational-problems.
the-famous-p-=-np?
problem, one of the-millennium-prize-problems, is an-open-problem in the-theory of computation.
information-and-coding-theory ====
information-theory, closely related to probability and statistics, is related to the-quantification of information.
this was developed by claude-shannon to find fundamental-limits on signal-processing-operations such as compressing data and on reliably storing and communicating data.
coding-theory is the-study of the-properties of codes (systems for converting information from one-form to another) and their-fitness for a-specific-application.
codes are used for data-compression, cryptography, error-detection and correction, and more recently also for network-coding.
codes are studied for the-purpose of designing efficient-and-reliable-data-transmission-methods.
data-structures and algorithms ====
data-structures and algorithms are the-studies of commonly-used-computational-methods and data-structures and algorithms-computational-efficiency.
programming-language-theory and formal-methods ====
programming-language-theory is a-branch of computer-science that deals with the-design, implementation, analysis, characterization, and classification of programming-languages and programming-languages individual features.
it falls within the-discipline of computer-science, both depending on and affecting mathematics, software-engineering, and linguistics.
it is an-active-research-area, with numerous-dedicated-academic-journals.
formal-methods are a-particular-kind of mathematically-based-technique for the-specification, development and verification of software-and-hardware-systems.
the-use of formal-methods for software-and-hardware-design is motivated by the-expectation that, as in other-engineering-disciplines, performing appropriate-mathematical-analysis can contribute to the-reliability and robustness of a-design.
the-use of formal-methods for software and hardware design form an-important-theoretical-underpinning for software-engineering, especially where safety or security is involved.
formal-methods are a-useful-adjunct to software-testing since  formal-methods help avoid errors and can also give a-framework for testing.
for industrial-use, tool-support is required.
however, the-high-cost of using formal-methods means that they are usually only used in the-development of high-integrity-and-life-critical-systems, where safety or security is of utmost-importance.
formal-methods are best described as the-application of a-fairly-broad-variety of theoretical-computer-science-fundamentals, in particular-logic-calculi, formal-languages, automata-theory, and program-semantics, but also type-systems and algebraic-data-types to problems in software-and-hardware-specification and verification.
computer-systems and computational-processes === ====
artificial-intelligence ====
artificial-intelligence (ai) aims to or is required to synthesize goal-orientated-processes such as problem-solving, decision-making, environmental-adaptation, learning, and communication found in humans and animals.
from its-origins in cybernetics and in the-dartmouth-conference (1956), artificial-intelligence-research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied-mathematics, symbolic-logic, semiotics, electrical-engineering, philosophy of mind, neurophysiology, and social-intelligence.
ai is associated in the-popular-mind with robotic-development, but the-main-field of practical-application has been as an-embedded-component in areas of software-development, which require computational-understanding.
the-starting-point in the-late-1940s was alan-turing's-question "can computers think?",
and the-question remains effectively unanswered, although the-turing-test is still used to assess computer-output on the-scale of human-intelligence.
but the-automation of evaluative-and-predictive-tasks has been increasingly successful as a-substitute for human-monitoring and intervention in domains of computer-application involving complex-real-world-data.
computer-architecture and organization ====
computer-architecture, or digital-computer-organization, is the-conceptual-design and fundamental-operational-structure of a-computer-system.
the-conceptual-design and fundamental-operational-structure of a-computer-system focuses largely on the-way by which the-central-processing-unit performs internally and accesses addresses in memory.
computer-engineers study computational-logic and design of computer-hardware, from individual-processor-components, microcontrollers, personal-computers to supercomputers and embedded-systems.
the-term “architecture” in computer-literature can be traced to the-work of lyle-r.-johnson and frederick-p.-brooks,-jr., members of the-machine-organization-department in ibm's-main-research-center in 1959.
concurrent,-parallel-and-distributed-computing ====
concurrency is a-property of systems in which several-computations are executing simultaneously, and potentially interacting with each other.
a-number of mathematical-models have been developed for general-concurrent-computation including petri-nets, process-calculi and the-parallel-random-access-machine-model.
when multiple-computers are connected in a-network while using concurrency, this is known as a-distributed-system.
computers within that-distributed-system have  computers within that-distributed-system own private-memory, and information can be exchanged to achieve common-goals.
computer-networks ====
this-branch of computer-science aims to manage networks between computers worldwide.
computer-security and cryptography ====
computer-security is a-branch of computer-technology with the-objective of protecting information from unauthorized-access, disruption, or modification while maintaining the-accessibility and usability of the-system for the-system intended users.
cryptography is the-practice and study of hiding (encryption) and therefore deciphering (decryption)-information.
modern-cryptography is largely related to computer-science, for many-encryption-and-decryption-algorithms are based on many-encryption-and-decryption-algorithms computational complexity.
databases and data-mining ====
a-database is intended to organize, store, and retrieve large-amounts of data easily.
digital-databases are managed using database-management-systems to store, create, maintain, and search data, through database-models and query-languages.
data-mining is a-process of discovering patterns in large-data-sets.
computer-graphics and visualization ====
computer-graphics is the-study of digital-visual-contents and involves the-synthesis and manipulation of image-data.
the-study of digital-visual-contents is connected to many-other-fields in computer-science, including computer-vision, image-processing, and computational-geometry, and is heavily applied in the-fields of special-effects and video-games.
image and sound processing ====
information can take the-form of images, sound, video or other-multimedia.
bits of information can be streamed via signals.
bits of information-processing is the-central-notion of informatics,
the-european-view on computing, which studies information-processing-algorithms independently of the-type of information-carrier - whether it is electrical, mechanical or biological.
this-field plays important-role in information-theory, telecommunications, information-engineering and has applications in medical-image-computing-and-speech-synthesis, among others.
what is the lower bound on the-complexity of fast-fourier transform algorithms?
is one of unsolved-problems in theoretical-computer-science.
applied computer-science === ====
computational-science, finance and engineering ====
scientific-computing (or computational-science) is the-field of study concerned with constructing mathematical-models and quantitative-analysis techniques and using computers to analyze and solve scientific-problems.
a-major-usage of scientific-computing is simulation of various-processes, including computational-fluid-dynamics, physical,-electrical,-and-electronic-systems and circuits, as well as societies and social-situations (notably-war-games) along with as-well-habitats, among many-others.
modern-computers enable optimization of such-designs as complete-aircraft.
notable in electrical-and-electronic-circuit-design are spice, as well as software for physical-realization of new-(or-modified)-designs.
the latter includes essential-design-software for integrated-circuits.
social-computing and human–computer-interaction ====
social-computing is an-area that is concerned with the-intersection of social-behavior and computational-systems.
human–computer-interaction-research develops theories, principles, and guidelines for user-interface-designers.
software-engineering ====
software-engineering is the-study of designing, implementing, and modifying the-software in order to ensure software-engineering is of high-quality, affordable, maintainable, and fast to build.
software-engineering is a-systematic-approach to software-design, involving the-application of engineering-practices to software.
software-engineering deals with the-organizing and analyzing of software
—it doesn't just deal with the-creation or manufacture of new-software, but it-internal-arrangement and maintenance.
for example software-testing, systems-engineering, technical-debt-and-software-development-processes.
discoveries ==
the-philosopher of computing-bill-rapaport noted three-great-insights of computer-science:
gottfried-wilhelm-leibniz's, george-boole's, alan-turing's, claude-shannon's, and samuel-morse's-insight: there are only-two-objects that a-computer has to deal with in order to represent "anything".
all-the-information about any-computable-problem can be represented using only 0 and 1 (or any-other-bistable-pair that can flip-flop between two-easily-distinguishable-states, such as "on/off", "magnetized/de-magnetized", "high-voltage/low-voltage", etc.).
alan-turing's-insight: there are only-five-actions that a-computer has to perform in order to do "anything".
every-algorithm can be expressed in a-language for a-computer consisting of only-five-basic-instructions:move left one-location; move right one-location; read symbol at current-location;
print 0 at current-location; print 1 at current-location.
corrado-böhm and giuseppe-jacopini's-insight : there are only-three-ways of combining these-actions (into more-complex-ones) that are needed in order for a-computer to do "anything".
only-three-rules are needed to combine any-set of basic-instructions into more-complex-ones:
sequence: first do this, then do that; selection:
if such-and-such is the-case, then do this, else do that;
repetition:
while such-and-such is the-case, do this.
note that the-three-rules of boehm's-and-jacopini's-insight can be further simplified with the-use of goto (which means it is more elementary than structured-programming).
programming-paradigms ==
programming-languages can be used to accomplish different-tasks in different-ways.
common-programming-paradigms include: functional-programming, a-style of building the-structure and elements of computer-programs that treats computation as the-evaluation of mathematical-functions and avoids state-and-mutable-data.
it is a-declarative-programming-paradigm, which means programming is done with expressions or declarations instead of statements.
imperative-programming, a-programming-paradigm that uses statements that change a-program's-state.
in much-the-same-way that the-imperative-mood in natural-languages expresses commands, an-imperative-program consists of commands for the-computer to perform.
imperative-programming focuses on describing how a-program operates.
object-oriented-programming, a-programming-paradigm based on the-concept of "objects", which may contain data, in the-form of fields, often known as attributes; and code, in the-form of procedures, often known as methods.
a-feature of objects is that an-object's-procedures can access and often modify the-data-fields of the-object with which they are associated.
thus object-oriented-computer-programs are made out of objects that interact with one another.
service-oriented-programming, a-programming-paradigm that uses "services" as the-unit of computer-work, to design and implement integrated-business-applications and mission-critical-software
programsmany-languages offer support for multiple-paradigms, making the-distinction more a matter of style than of technical-capabilities.
academia ==
conferences are important-events for computer-science-research.
during conferences, researchers from the-public-and-private-sectors present researchers from the-public-and-private-sectors recent-work and meet.
unlike in most-other-academic-fields, in computer-science, the-prestige of conference-papers is greater than that of journal-publications.
one-proposed-explanation for this is the-quick-development of this-relatively-new-field requires rapid-review and distribution of results, a-task better handled by conferences than by journals.
education ==
computer-science, known by its-near-synonyms, computing, computer-studies, has been taught in uk-schools since the-days of batch-processing, mark sensitive-cards and paper-tape but usually to a-select-few-students.
in 1981, the-bbc produced a-micro-computer-and-classroom-network and computer-studies became common for gce-o-level-students (11–16-year-old), and computer-science to a-level-students.
computer-science-importance was recognised, and computer-science became a-compulsory-part of the-national-curriculum, for key-stage 3 & 4.
in september 2014
in september 2014 became an-entitlement for all-pupils over the-age of 4.in the-us, with 14,000-school-districts deciding the-curriculum, provision was fractured.
according to a-2010-report by the-association for computing-machinery (acm) and computer-science-teachers-association (csta), only-14-out-of-50-states have adopted significant-education-standards for high-school-computer-science.
israel, new-zealand, and south-korea have included computer-science in israel, new-zealand, and south-korea national secondary education curricula, and several-others are following.
see also == ==
references == ==
further-reading == ==
external-links ==
computer-science at curlie-scholarly-societies in computer-science
what is computer-science?
best-papers-awards in computer-science since 1996
photographs of computer-scientists by bertrand-meyer-eecs.berkeley.edu ===
bibliography and academic-search-engines ===
citeseerx (article): search-engine, digital-library and repository for scientific-and-academic-papers with a-focus on computer and information-science.
dblp-computer-science-bibliography (article):
computer-science-bibliography-website hosted at universität-trier, in germany.
the-collection of computer-science-bibliographies (collection of computer-science-bibliographies) ===
professional-organizations === association for computing-machinery
ieee-computer-society
informatics-europe-aaai-aaas-computer-science ===
computer-science—
stack-exchange: a-community-run-question-and-answer-site for computer-science
what is computer-science is computer-science science?
computer-science (software) must be considered as an-independent-discipline.
cognitive-biases are systematic-patterns of deviation from norm and/or rationality in judgment.
cognitive-biases are often studied in psychology and behavioral-economics.
although the-reality of most of these-biases is confirmed by reproducible-research, there are often controversies about how to classify these-biases or how to explain these-biases.
several-theoretical-causes are known for some-cognitive-biases, which provides a-classification of biases by several-theoretical-causes common-generative-mechanism (such as noisy-information-processing).
gerd-gigerenzer has criticized the-framing of cognitive-biases as errors in judgment, and favors interpreting cognitive-biases as arising from rational-deviations from logical-thought.
explanations include information-processing-rules (i.e.,-mental-shortcuts), called heuristics, that the-brain uses to produce decisions or judgments.
biases have a-variety of forms and appear as cognitive-("cold")-bias, such as mental-noise, or motivational-("hot")-bias, such as when beliefs are distorted by wishful-thinking.
both-effects can be present at the-same-time.
there are also controversies over some of these-biases as to whether some of these-biases count as useless or irrational, or whether some of these-biases result in useful-attitudes or behavior.
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
however, this-kind of confirmation-bias has also been argued to be an-example of social-skill; a way to establish a-connection with the-other-person.
although this-research overwhelmingly involves human-subjects, some findings that demonstrate bias have been found in non-human-animals as well.
for example, loss-aversion has been shown in monkeys and hyperbolic-discounting has been observed in rats, pigeons, and monkeys.
belief, decision-making and behavioral ==
these-biases affect belief-formation, reasoning-processes, business and economic decisions, and human-behavior in general.
social-==-==-memory ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
there are many-types of memory-bias, including: ==
see also == ==
footnotes == ==
references ==
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
this happens when an-individual focuses on the-most-relevant-aspects of a-problem or situation to formulate a-solution.
some-heuristics are more applicable and useful than others depending on the-situation.
heuristic-processes are used to find the-answers and solutions that are most likely to work or be correct.
however, heuristics are not always right or the most accurate.
while they can differ from answers given by logic and probability, judgments and decisions based on a-heuristic can be good enough to satisfy a-need.
judgments and decisions are meant to serve as quick-mental-references for everyday-experiences and decisions.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
history ==
herbert-a.-simon formulated one of the-first-models of heuristics, known as satisficing.
herbert-a.-simon more-general-research-program posed the-question of how humans make decisions when the-conditions for rational-choice-theory are not met
, that is how people decide under uncertainty.
simon is also known as the-father of bounded-rationality, which  simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
this-program was later extended into the-study of ecological-rationality.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
psychologists-amos-tversky and daniel-kahneman
typical-experimental-setup consisted of a-rule of logic or probability, embedded in a-verbal-description of a-judgement-problem, and demonstrated that people's-intuitive-judgement deviated from the-rule.
the-"linda-problem" below gives an-example.
the-deviation is then explained by a-heuristic.
this-research, called the heuristics-and-biases program, challenged the-idea that human-beings are rational-actors and first gained worldwide-attention in 1974 with the-science-paper-"judgment under uncertainty:
heuristics and biases"  and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
the-original-ideas by herbert-simon were taken up in the-1990s by gerd-gigerenzer and others.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
their-program has three-aspects:
what are the-heuristics humans use?
the-descriptive-study of the-"adaptive-toolbox")
under what-conditions should humans rely on a-given-heuristic?
(-the-prescriptive-study of ecological-rationality) how to design heuristic decision aids that are easy to understand and execute?
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
these-two-different-research-programs have led to two-kinds of models of heuristics, formal-models and informal-ones.
formal-models describe the-decision-process in terms of an-algorithm, which allows for mathematical-proofs-and-computer-simulations.
in contrast, informal-models are verbal-descriptions.
formal-models of heuristics ==
simon's-satisficing-strategy ===
simon-satisficing heuristic can be used to choose one-alternative from a-set of alternatives in situations of uncertainty.
here, uncertainty means that the-total-set of alternatives and uncertainty-consequences is not known or knowable.
for instance, professional-real-estate-entrepreneurs rely on satisficing to decide in which-location to invest to develop new-commercial-areas: "if i believe i can get at-least-x-return within y-years, then i take the-option.
" in general, satisficing is defined as: step 1:
set an-aspiration-level α step 2
: choose the-first-alternative that satisfies αif
no-alternative is found, then the-aspiration-level can be adapted.
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
elimination by aspects ===
unlike satisficing, amos tversky's elimination-by-aspect heuristic can be used when all-alternatives are simultaneously available.
the-decision-maker gradually reduces the-number of alternatives by eliminating alternatives that do not meet the-aspiration-level of a-specific-attribute (or aspect).
recognition-heuristic ===
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
for two-alternatives, the-heuristic is:if one of two-alternatives is recognized and the other not, then infer that the-recognized-alternative has the-higher-value with respect to the-criterion.
for example, in the-2003-wimbledon-tennis-tournament, andy-roddick played tommy-robredo.
if one has heard of andy-roddick but not of tommy-robredo, the-recognition-heuristic leads to the-prediction that andy-roddick will win.
the-recognition-heuristic exploits partial-ignorance, if one has heard of both or no-player, a-different-strategy is needed.
studies of wimbledon 2003 and 2005 have shown that the-recognition-heuristic applied by semi-ignorant-amateur-players predicted the-outcomes of all-gentlemen single games as well and better than the-seedings of the wimbledon experts (who had heard of all-players), as well as the atp rankings.
the-recognition-heuristic is ecologically rational (that is,  the-recognition-heuristic predicts well) when the-recognition-validity is substantially above chance.
in the-present-case, recognition of players'-names is highly correlated with players'-chances of winning.
take-the-best ===
the take-the-best heuristic exploits the-basic-psychological-capacity for retrieving cues from memory in the-order of their-validity.
based on the-cue-values, the-cue-values infers which of two-alternatives has a-higher-value on a-criterion.
unlike the-recognition-heuristic, the-cue-values requires that all-alternatives are recognized, and the-cue-values thus can be applied when the-recognition-heuristic cannot.
for binary-cues (where 1 indicates the-higher-criterion-value), the-recognition-heuristic is defined as: search-rule:
search-cues in the-order of their-validity v.   stopping-rule: stop search on finding the-first-cue that discriminates between the-two-alternatives (i.e., one-cue-values are 0 and 1).
decision-rule:
infer that the-alternative with the-positive-cue-value (1) has the-higher-criterion-value).
the-validity-vi of a-cue i is defined as the-proportion of correct-decisions
ci-/-tiwhere-ti is the-number of cases the-values of the-two-alternatives differ on cue-i.
the-validity of each-cue can be estimated from samples of observation.
take-the-best has remarkable-properties.
in comparison with complex-machine-learning-models, it has been shown that it can often predict better than regression-models, classification-and-regression-trees, neural-networks, and support vector-machines.
brighton & gigerenzer, 2015]
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers  and student-populations.
the-conditions under which take-the-best is ecologically rational are mostly known.
take-the-best-shows that the-previous-view that ignoring part of the-information would be generally irrational is incorrect.
less can be more.
fast-and-frugal-trees ===
a-fast-and-frugal-tree is a-heuristic that allows to make a-classifications, such as whether a-patient with severe-chest-pain is likely to have a-heart-attack or not, or whether a-car approaching a-checkpoint is likely to be a-terrorist or a-civilian.
it is called “fast and frugal” because, just like take-the-best, it allows for quick-decisions with only-few-cues or attributes.
it is called a “tree” because it can be represented like a-decision-tree in which one asks a-sequence of questions.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
figure 1 shows a-fast-and-frugal-tree used for screening for hiv (human-immunodeficiency-virus).
just like take-the-best, an-incomplete-tree has a-search-rule, stopping rule, and decision rule:
search-rule:
search through cues in a-specified-order.
stopping rule: stop search if an-exit is reached.
decision-rule: classify the-person according to the-exit (here: no-hiv or hiv).
in the-hiv-tree, an elisa (enzyme-linked-immunosorbent-assay) test is conducted first.
if the-outcome is negative, then the-testing-procedure stops and the-client is informed of the-good-news, that is, “no-hiv.”
if, however, the-result is positive, a-second-elisa-test is performed, preferably from a-different-manufacturer.
if-an-elisa (enzyme-linked-immunosorbent-assay)
elisa is negative, then the-testing-procedure stops and the-client is informed of having “no-hiv.”
however, if the-result is positive, a final test, the-western-blot, is conducted.
in general, for n binary-cues, a-fast-and-frugal-tree has exactly-n-+-1-exits – one for each-cue and two for the-final-cue.
a-full-decision-tree, in contrast, requires 2n-exits.
the-order of cues (tests) in a-fast-and-frugal-tree is determined by the-sensitivity and specificity of the-cues, or by other-considerations such as the-costs of the-tests.
in the-case of the-hiv-tree, the-elisa is ranked first because the-elisa produces fewer-misses than the-western-blot-test, and also is less expensive.
the-western-blot-test, in contrast, produces fewer-false-alarms.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
fast-and-frugal-trees are descriptive-or-prescriptive-models of decision making under uncertainty.
for instance, an-analysis-or-court-decisions reported that the best model of how london-magistrates make bail-decisions is a-fast-and-frugal-tree.
the-hiv-tree is both prescriptive– physicians are taught the-procedure – and a-descriptive-model, that is, most-physicians actually follow the-procedure.
informal-models of heuristics ==
in their-initial-research, tversky and kahneman proposed three-heuristics—availability, representativeness, and anchoring and adjustment.
subsequent-work has identified many more.
heuristics that underlie judgment are called "judgment heuristics".
another-type, called "evaluation heuristics", are used to judge the-desirability of possible-choices.
availability ==
in psychology, availability is the-ease with which a-particular-idea can be brought to mind.
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
when an-infrequent-event can be brought easily and vividly to mind, the-availability-heuristic overestimates an-infrequent-event likelihood.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
dramatic,-violent-deaths are usually more highly publicised and therefore have a-higher-availability.
on the-other-hand, common-but-mundane-events are hard to bring to mind, so common-but-mundane-events likelihoods tend to be underestimated.
these include deaths from suicides, strokes, and diabetes.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
it may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k ,  the-availability-heuristic gives a-quick-way to answer the-question.
words that begin with t come more readily to mind, and so subjects give a-correct-answer without counting out large-numbers of words.
however, the-availability-heuristic can also produce errors.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
it is easy to think of words that begin with k, such as kangaroo, kitchen, or kept.
it is harder to think of words with k as the-third-letter, such as lake, or acknowledge, although objectively these are three times more common.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
in another-experiment, subjects heard the-names of many-celebrities, roughly-equal-numbers of whom were male and female.
subjects were then asked whether the-list of names included more-men or more-women.
when the-men in the-list were more famous, a-great-majority of subjects incorrectly thought there were more of the-men in the-list, and vice versa for women.
tversky and kahneman's-interpretation of these-results is that judgments of proportion are based on availability, which is higher for the-names of better-known-people.
in one-experiment that occurred before the-1976-u.s.-presidential-election, some-participants were asked to imagine gerald-ford winning, while others did the same for a-jimmy-carter-victory.
each-group subsequently viewed each-group allocated candidate as significantly more likely to win.
each-group found a-similar-effect when students imagined a-good or a-bad-season for a-college-football-team.
the-effect of imagination on subjective-likelihood has been replicated by several-other-researchers.
a-concept's-availability can be affected by how recently and how frequently a-concept's-availability has been brought to mind.
in one-study, subjects were given partial-sentences to complete.
the-words were selected to activate the-concept either of hostility or of kindness: a-process known as priming.
they then had to interpret the-behavior of a-man described in a-short,-ambiguous-story.
their-interpretation was biased towards the-emotion their had been primed with: the-more-priming,
the-greater-the-effect.
a-greater-interval between the-initial-task and the-judgment decreased the greater the effect.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
representativeness ===
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
an-individual-thing has a-high-representativeness for a-category if an-individual-thing is very similar to a-prototype of a-category.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
" representative" is here meant in two-different-senses:
the-prototype used for comparison is representative of the-prototype used for comparison-category, and representativeness is also a-relation between that-prototype and the-thing being categorised.
while that-prototype is effective for some-problems, this-heuristic involves attending to the-particular-characteristics of the-individual, ignoring how common those-categories are in the-population (called the base rates).
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
this is called the base rate fallacy.
representativeness explains this and several-other-ways in which human-judgments break the-laws of probability.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
this can lead to a-bias, incorrectly finding causal-relationships between things that resemble one another and missing people when the-cause and effect are very different.
examples of this include both-the-belief that "emotionally-relevant-events ought to have emotionally-relevant-causes", and magical-associative-thinking.
representativeness of base-rates ====
a-1973-experiment used a-psychological-profile of tom-w., a-fictional-graduate-student.
one-group of subjects had to rate tom's-similarity to a-typical-student in each of nine-academic-areas (including law, engineering and library science).
another-group had to rate how likely it is that tom specialised in each-area.
if these-ratings of likelihood are governed by probability, then these-ratings of likelihood should resemble the-base-rates, i.e.-the-proportion of students in each of the-nine-areas (which had been separately estimated by a-third-group).
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
instead, the-ratings of likelihood matched the-ratings of similarity almost perfectly, both in this-study and a-similar-one where subjects judged the-likelihood of a-fictional-woman taking different-careers.
this suggests that rather than estimating probability using base-rates, subjects had substituted the-more-accessible-attribute of similarity.
conjunction-fallacy ====
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
tversky and kahneman gave subjects a-short-character-sketch of a-woman called linda, describing a-woman called linda as, "31 years old, single, outspoken, and very bright.
a-woman called linda majored in philosophy.
as a-student, a-woman called linda was deeply concerned with issues of discrimination and social-justice, and also participated in anti-nuclear-demonstrations".
people reading this-description then ranked the-likelihood of different-statements about a-woman called linda.
amongst others, these included "linda is a-bank-teller", and, "linda is a-bank-teller and is active in the-feminist-movement".
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "linda is both-x and y" can never be more probable than the-more-general-statement
"linda is x".
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-similar-exercise-concerned-bill, described as "intelligent but unimaginative".
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
without success, tversky and kahneman used what tversky and kahneman described as "a-series of increasingly-desperate-manipulations" to get tversky and kahneman subjects to recognise the-logical-error.
in one-variation, subjects had to choose between a-logical-explanation of why "linda is a-bank-teller" is more likely, and a-deliberately-illogical-argument which said that "linda is a-feminist-bank-teller" is more likely "because linda resembles an-active-feminist more than linda resembles a-bank-teller".
sixty-five-percent of subjects found the-illogical-argument more convincing.
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
they did not eliminate the-error.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
the-error disappears when the-question is posed in terms of frequencies.
everyone in these-versions of the-study recognised that out of 100-people fitting an-outline-description, the-conjunction-statement ("she is x and y") cannot apply to more-people than the-general-statement ("she is x").
ignorance of sample-size ====
tversky and kahneman asked subjects to consider a-problem about random-variation.
imagining for simplicity that exactly-half of the-babies born in a-hospital are male, the-ratio will not be exactly-half in every-time-period.
on some-days, more-girls will be born and on others, more boys.
the-question was, does the-likelihood of deviating from exactly half depend on whether there are many-or-few-births per day?
it is a-well-established-consequence of sampling-theory that proportions will vary much more day-to-day when the-typical-number of births per day is small.
however, people's-answers to the-problem do not reflect this-fact.
people's-answers to the-problem typically reply that the-number of births in the-hospital makes no-difference to the-likelihood of more-than-60%-male-babies in one-day.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
dilution-effect ====
richard-e.-nisbett and colleagues suggest that representativeness explains the-dilution-effect, in which irrelevant-information weakens the-effect of a-stereotype.
subjects in one-study were asked whether "paul" or "susan" was more likely to be assertive, given no-other-information than their-first-names.
their rated paul as more assertive, apparently basing their-judgment on a-gender-stereotype.
another-group, told that paul and susan's-mothers each-commute to work in a-bank, did not show this-stereotype-effect; paul's and susan's-mothers each-commute to work in a-bank rated paul and susan as equally assertive.
the-explanation is that the-additional-information about paul and susan made paul and susan less representative of men or women in general, and so the-subjects'-expectations about men and women had a-weaker-effect.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
misperception of randomness === =
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
tversky and kahneman argued that this-effect underlies the-gambler's-fallacy; a-tendency to expect outcomes to even out over the-short-run, like expecting a-roulette-wheel to come up black because the-last-several-throws came up red.
they emphasised that even-experts in statistics were susceptible to this-illusion: in a-1971-survey of professional-psychologists, they found that respondents expected samples to be overly representative of the-population they were drawn from.
as a-result, the-psychologists systematically overestimated the-statistical-power of the-psychologists tests, and underestimated the-sample-size needed for a-meaningful-test of the-psychologists hypotheses.
anchoring and adjustment ===
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
according to tversky and kahneman's-original-description, it involves starting from a-readily-available-number—the-"anchor"—and shifting either up or down to reach an-answer that seems plausible.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
hence the-anchor contaminates the-estimate, even if the-anchor is clearly irrelevant.
in one-experiment, subjects watched a-number being selected from a-spinning-"wheel of fortune".
they had to say whether a-given-quantity was larger or smaller than a-number being selected from a-spinning-"wheel of fortune".
for instance, they might be asked, "is the-percentage of african-countries which are members of the-united-nations larger or smaller than 65%?"
they then tried to guess the-true-percentage.
they answers correlated well with the-arbitrary-number they had been given.
insufficient-adjustment from an-anchor is not the-only-explanation for this-effect.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-anchoring-effect has been demonstrated by a-wide-variety of experiments both in laboratories and in the-real-world.
it remains when the-subjects are offered money as an-incentive to be accurate, or when the-subjects are explicitly told not to base the-subjects judgment on the-anchor.
the-effect is stronger when people have to make people judgments quickly.
subjects in these-experiments lack introspective-awareness of the-heuristic, denying that the-anchor affected subjects in these-experiments estimates.
even when the-anchor-value is obviously random or extreme, the-anchor-value can still contaminate estimates.
one-experiment asked subjects to estimate the-year of albert-einstein's-first-visit to the-united-states.
anchors of 1215 and 1992 contaminated the-answers just-as-much-as-more-sensible-anchor-years.
other-experiments asked subjects if the-average-temperature in san-francisco is more-or-less-than-558-degrees, or whether there had been more-or-fewer-than-100,025-top-ten-albums by the-beatles.
these-deliberately-absurd-anchors still affected estimates of the-true-numbers.
anchoring results in a-particularly-strong-bias when estimates are stated in the-form of a-confidence-interval.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
best-estimate.
this leads to an-overconfidence-effect.
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
anchoring also causes particular-difficulty when many-numbers are combined into a-composite-judgment.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
another-group had to estimate the-same-product in reverse-order; 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8.
both-groups underestimated the-answer by a-wide-margin, but  another-group's-average-estimate was significantly smaller.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-less-abstract-task is to estimate the-probability that an-aircraft will crash, given that there are numerous-possible-faults each with a-likelihood of one in a million.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
for this-kind of judgment, anchoring on the-individual-probabilities results in an-overestimation of the-combined-probability.
examples ====
people's-valuation of goods, and the-quantities they buy, respond to anchoring effects.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
people then entered an-auction to bid for they would pay this-number of dollars for items whose-value they did not know, such as wine, chocolate, and computer-equipment.
those with the-highest-two-digit-numbers submitted bids that were many times higher than those with the-lowest-numbers.
when a-stack of soup-cans in a-supermarket was labelled, "limit 12 per customer", the-label influenced customers to buy more-cans.
in another-experiment, real-estate-agents appraised the-value of houses on the-basis of a-tour and extensive-documentation.
different-agents were shown different listing prices, and these affected different-agents valuations.
for one-house, the-appraised-value ranged from us$114,204 to $128,754.anchoring and adjustment has also been shown to affect grades given to students.
in one-experiment, 48-teachers were given bundles of student-essays, each of which had to be graded and returned.
48-teachers were also given a-fictional-list of the-students'-previous-grades.
the-mean of these-grades affected the-grades that teachers awarded for the-essay.
one-study showed that anchoring affected the-sentences in a-fictional-rape-trial.
the-subjects were trial-judges with, on average, more-than-fifteen-years of experience.
the-subjects read documents including witness-testimony, expert-statements, the-relevant-penal-code, and the-final-pleas from the-prosecution and defence.
the-two-conditions of this-experiment differed in just-one-respect: the-prosecutor demanded a-34-month-sentence in one-condition and 12-months in the other; there was an-eight-month-difference between the-average-sentences handed out in these-two-conditions.
in a-similar-mock-trial, the-subjects took the-role of jurors in a-civil-case.
the-subjects were either asked to award damages "in the-range from $15 million to $50 million" or "in the-range from $50 million to $150 million".
although the-facts of a-civil-case were the same each time, jurors given the-higher-range decided on an-award that was about three times higher.
this happened even though the-subjects were explicitly warned not to treat the-requests as evidence.
assessments can also be influenced by the-stimuli provided.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
affect heuristic ===-"affect", in this-context, is a-feeling such as fear, pleasure or surprise.
it is shorter in duration than a-mood, occurring rapidly and involuntarily in response to a-stimulus.
while reading the-words "lung-cancer" might generate an-affect of dread, the-words "mother's-love" can create an-affect of affection and comfort.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
the-affect heuristic has been used to explain why messages framed to activate emotions are more persuasive than those framed in a-purely-factual-way.
others ===
theories ==
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
according to a-cognitive-laziness-approach, some-complex-calculations are already done rapidly and automatically by the-human-brain, and other-judgments make use of these-processes rather than calculating from scratch.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
this-perspective emphasises the-"fast-and-frugal"-nature of heuristics.
cognitive-laziness ===
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
attribute-substitution ==
in 2002 daniel-kahneman and shane-frederick proposed a-process called attribute substitution which happens without conscious-awareness.
according to this-theory, when somebody makes a-judgment (of a-target-attribute) which is computationally complex, a-rather-more-easily-calculated-heuristic-attribute is substituted.
in effect, a-difficult-problem is dealt with by answering a-rather-simpler-problem, without the-person being aware this is happening.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
the-subject also explains why human-judgments often fail to show regression toward the-mean.
this-substitution is thought of as taking place in the-automatic-intuitive-judgment-system, rather than the-more-self-aware-reflective-system.
hence, when someone tries to answer a-difficult-question, individuals may actually answer a-related-but-different-question, without realizing that a-substitution has taken place.
in 1975, psychologist-stanley-smith-stevens proposed that the-strength of a-stimulus (e.g.-the-brightness of a-light, the-severity of a-crime) is encoded by brain-cells in a-way that is independent of modality.
kahneman and frederick built on this-idea, arguing that the-target-attribute and heuristic-attribute could be very different in nature.
kahneman and frederick propose three-conditions for attribute-substitution: the-target-attribute is relatively inaccessible.
substitution is not expected to take place in answering factual-questions that can be retrieved directly from memory ("what is your-birthday?")
or about current-experience
("do you feel thirsty now?).
an-associated-attribute is highly accessible.
this might be because this is evaluated automatically in normal-perception or because it has been primed.
for example, someone who has been thinking about example love-life and is then asked how-happy-example are might substitute how-happy-example are with example love-life rather than other-areas.
the-substitution is not detected and corrected by the-reflective-system.
for example, when asked "a-bat and a-ball together cost $1.10.
a-bat and a-ball together costs $1 more than a-ball.
how-much-does-the-ball-cost?
many-subjects incorrectly answer $0.10.
an-explanation in terms of attribute-substitution is that, rather than work out the-sum, subjects parse the-sum of $1.10 into a-large-amount and a-small-amount, which is easy to do.
whether the-sum of $1.10 feel that is the-right-answer will depend on whether the-sum of $1.10 check the-calculation with the-sum of $1.10-reflective-system.
kahneman gives an-example where some-americans were offered insurance against americans own death in a-terrorist-attack while on a-trip to europe, while another-group were offered insurance that would cover death of any-kind on the-trip.
even though "death of any-kind" includes "death in a-terrorist-attack", another-group were willing to pay more than the latter.
kahneman suggests that the-attribute of fear is being substituted for a-calculation of the-total-risks of travel.
fear of terrorism for these-subjects was stronger than a-general-fear of dying on a-foreign-trip.
fast and frugal ===
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
==-consequences == ===
efficient-decision-heuristics ===
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
he determined how-often-each-heuristic-selected-alternatives with highest-through-lowest-expected-value in a-series of randomly-generated-decision-situations.
he found that most of the-simulated-heuristics selected alternatives with highest-expected-value and almost never selected alternatives with lowest-expected-value.
"beautiful-is-familiar"-effect ===
psychologist-benoît-monin reports a-series of experiments in-which-subjects, looking at photographs of faces, have to judge whether they have seen those faces before.
it is repeatedly found that attractive-faces are more likely to be mistakenly labeled as familiar.
monin interprets this-result in terms of attribute-substitution.
the-heuristic-attribute in this-case is a-"warm-glow"; a-positive-feeling towards someone that might either be due to their being familiar or being attractive.
this-interpretation has been criticised, because not-all-the-variance in familiarity is accounted for by the-attractiveness of the-photograph.
judgments of morality and fairness ===
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to  legal-scholar-cass-sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
another-source of heuristic-attributes is emotion: people's-moral-opinions on sensitive-subjects like sexuality and human-cloning may be driven by reactions such as disgust, rather than by reasoned-principles.
legal-scholar-cass-sunstein has been challenged as not providing enough-evidence that attribute substitution, rather than other-processes, is at work in these-cases.
persuasion ===
an-example of how persuasion plays a-role in heuristic-processing can be explained through the-heuristic-systematic-model.
this explains how there are often two-ways we are able to process information from persuasive-messages, one being heuristically and the other systematically.
a-heuristic is when we make a-quick-short-judgement into we-decision-making.
on the-other-hand, systematic-processing involves more-analytical-and-inquisitive-cognitive-thinking.
individuals looks further than individuals own prior-knowledge for the-answers.
an-example of this-model could be used when watching an-advertisement about a-specific-medication.
one without prior-knowledge would see the-person in the-proper-pharmaceutical-attire and assume that the-proper-pharmaceutical-attire know what the-proper-pharmaceutical-attire are talking about.
therefore, the-person in the-proper-pharmaceutical-attire automatically has more-credibility and is more likely to trust the-content of the-messages than the-proper-pharmaceutical-attire deliver.
while another who is also in that-field of work or already has prior-knowledge of the-medication will not be persuaded by the-ad because of another who is also in that-field of work or already has prior-knowledge of the-medication systematic-way of thinking.
this was also formally demonstrated in an-experiment conducted my-chaiken and maheswaran (1994).
in addition to these-examples,-the-fluency-heuristic-ties in perfectly with the-topic of persuasion.
it is described as how we all easily make "the most of an-automatic-by-product of retrieval from memory".
an-example would be a-friend asking about good-books to read.
many could come to mind, but you name the-first-book recalled from your-memory.
since it was the-first-thought, therefore you value it as better than any-other-book one could suggest.
the-effort-heuristic is almost identical to fluency.
the-one-distinction would be that-objects that take longer to produce are seen with more-value.
one may conclude that a-glass-vase is more valuable than a-drawing, merely because a-glass-vase may take longer for a-glass-vase.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
see also == ==
citations == ==
references ==
baron, jonathan (2000), thinking and deciding (
cambridge-university-press, isbn 978-0521650304,
oclc-316403966-gilovich, thomas; griffin, dale-w. (2002), "introduction-–-heuristics and biases:
then and now",  in gilovich, thomas; griffin, dale-w.; kahneman, daniel (eds.),
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
1–18, isbn-9780521796798-hardman, david (2009), judgment and decision making:
psychological-perspectives, wiley-blackwell, isbn 9781405123983-hastie, reid; dawes, robyn-m. (29-september 2009), rational-choice in an-uncertain-world:
the-psychology of judgment and decision-making, sage, isbn
9781412959032-koehler, derek-j.; harvey, nigel (2004), blackwell-handbook of judgment and decision-making, wiley-blackwell, isbn-9781405107464-kunda, ziva (1999), social-cognition:
making sense of people, mit-press, isbn 978-0-262-61143-5, oclc 40618974
mussweiler, thomas; englich, birte; strack, fritz (2004), "anchoring-effect",  in pohl, rüdiger-f. (ed.) ,
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
183–200, isbn 9781841693514, oclc 55124398
plous, scott (1993), the-psychology of judgment and decision-making, mcgraw-hill, isbn 9780070504776, oclc 26931106
poundstone, william (2010), priceless:
the-myth of fair-value (and how to take advantage of it), hill and wang, isbn 9780809094691
reber, rolf (2004), "availability",  in pohl, rüdiger-f. (ed.),
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
147–163, isbn 9781841693514, oclc 55124398
sutherland, stuart (2007), irrationality (2nd ed.),
london: pinter and martin, isbn 9781905177073, oclc 72151566
teigen, karl-halvor (2004), "judgements by representativeness",  in pohl, rüdiger-f. (ed.),
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
165–182, isbn 9781841693514, oclc 55124398
tversky, amos; kahneman, daniel (1974), "judgments under uncertainty: heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
bibcode:1974sci...
185.1124t, doi:10.1126/science.185.4157.1124, pmid 17835457, s2cid 143452957 reprinted in daniel-kahneman; paul-slovic; amos-tversky, eds.
judgment under uncertainty: heuristics and biases.
cambridge:
cambridge-university-press.
isbn 9780521284141.
yudkowsky, eliezer (2011). "
cognitive-biases potentially affecting judgment of global-risks".
in bostrom, nick; cirkovic, milan-m. (eds.).
global-catastrophic-risks.
oup-oxford.
isbn 978-0-19-960650-4.
further-reading ==
slovic, paul; melissa-finucane; ellen-peters; donald-g.-macgregor (2002).
the-affect-heuristic".
in thomas-gilovich; dale griffin; daniel-kahneman (eds.).
heuristics and biases: the-psychology of intuitive-judgment.
cambridge-university-press.
isbn 9780521796798.
gigerenzer, gerd; selten, reinhard (2001).
bounded-rationality : the-adaptive-toolbox.
cambridge, ma: mit-press.
isbn 9780521796798.
oclc 49569412.
korteling, johan-e.; brouwer, anne-marie; toet, alexander (3-september 2018). "
a-neural-network-framework for cognitive-bias".
frontiers in psychology.
doi:10.3389/fpsyg.2018.01561.
pmc 6129743.
pmid 30233451.
chow, sheldon (20-april 2011).
"-heuristics, concepts, and cognitive-architecture:
toward understanding
how the mind works".
electronic-thesis and dissertation-repository.
todd, p.m. (2001).
heuristics for decision and choice".
international-encyclopedia of the-social-&-behavioral-sciences.
6676–6679.
doi:10.1016
/b0-08-043076-7/00629-x. isbn 978-0-08-043076-8.
external-links ==
test yourself:
decision making and the-availability-heuristic
the-following is a-list of algorithms along with one-line-descriptions for each.
automated-planning ==
combinatorial-algorithms == ===
general-combinatorial-algorithms ===
brent's-algorithm: finds a-cycle in function-value-iterations using only-two-iterators
floyd's-cycle-finding-algorithm: finds a-cycle in function-value-iterations
gale–shapley algorithm:
solves the-stable-marriage-problem
pseudorandom-number-generators (uniformly distributed—see also list of pseudorandom-number-generators for other-prngs with varying-degrees of convergence and varying statistical-quality): acorn generator
blum-blum-shub lagged fibonacci-generator-linear-congruential-generator-mersenne-twister ==
=-graph-algorithms ===
coloring-algorithm:
graph-coloring-algorithm.
karp algorithm: convert a-bipartite-graph to a-maximum-cardinality matching hungarian-algorithm: algorithm for finding a-perfect-matching-prüfer-coding: conversion between a-labeled-tree and its-prüfer-sequence
tarjan's off-line lowest common ancestors algorithm:
compute lowest-common-ancestors for pairs of nodes in a-tree-topological-sort: finds linear-order of nodes (e.g.-jobs) based on linear-order of nodes (e.g.-jobs) dependencies.
graph drawing ====
force-based-algorithms (also known as force-directed-algorithms-or-spring-based-algorithm)
spectral-layout ====
network-theory ====
network-analysis
link analysis girvan–
newman algorithm:
detect communities in complex-systems-web-link-analysis
hyperlink-induced-topic-search (hits) (also known as hubs and authorities)
trustrank-flow-networks
dinic's-algorithm: is a-strongly-polynomial-algorithm for computing the-maximum-flow in a-flow-network.
karp algorithm:
implementation of ford–fulkerson ford–
fulkerson algorithm:
computes the-maximum-flow in a-graph karger's-algorithm:
a-monte-carlo-method to compute the-minimum-cut of a-connected-graph-push–relabel algorithm: computes a-maximum-flow in a-graph ====
routing for graphs ====
edmonds'-algorithm (also known as chu–liu/edmonds'-algorithm): find maximum-or-minimum-branchings
euclidean-minimum-spanning-tree:
algorithms for computing the-minimum-spanning-tree of a-set of points in the-plane
euclidean shortest path problem: find the-shortest-path between two-points that does not intersect any-obstacle-longest-path-problem: find a-simple-path of maximum-length in a-given-graph-minimum-spanning-tree
borůvka's-algorithm kruskal's-algorithm
prim's-algorithm reverse-delete-algorithm-nonblocking-minimal-spanning-switch say, for a-telephone-exchange
shortest path problem bellman–ford algorithm:
computes shortest-paths in a-weighted-graph (where some of the-edge-weights may be negative)
dijkstra's-algorithm: computes shortest-paths in a-graph with non-negative-edge-weights-floyd–
warshall algorithm:
solves the-all-pairs-shortest-path-problem in a-weighted,-directed-graph-johnson's-algorithm: all pairs shortest path algorithm in sparse-weighted-directed-graph-transitive-closure-problem: find the-transitive-closure of a given binary-relation traveling salesman problem
christofides algorithm
nearest-neighbour algorithm warnsdorff's-rule
:-a-heuristic-method for solving the-knight's-tour-problem.
graph-search ====
a*:-special-case of best-first-search that uses heuristics to improve speed-b
*: a-best-first-graph-search-algorithm that finds the-least-cost-path from a-given-initial-node to any-goal-node (out of one-or-more-possible-goals) backtracking: abandons partial-solutions when *: a-best-first-graph-search-algorithm that finds the-least-cost-path from a-given-initial-node to any-goal-node (out of one-or-more-possible-goals)
backtracking are found not to satisfy a-complete-solution-beam-search: is a-heuristic-search-algorithm that is an-optimization of best-first-search that reduces its-memory-requirement
beam-stack-search: integrates backtracking with beam-search-best-first-search: traverses a-graph in the-order of likely-importance using a-priority-queue-bidirectional-search: find the-shortest-path from an-initial-vertex to a-goal-vertex in a-directed-graph-breadth-first-search: traverses a-graph level by level
brute-force-search:
an-exhaustive-and-reliable-search-method, but computationally inefficient in many-applications.
*:-an-incremental-heuristic-search-algorithm depth-first-search
: traverses a-graph-branch by branch-dijkstra's-algorithm:
a-special-case of a* for which no-heuristic-function is used
general-problem-solver: a-seminal-theorem-proving-algorithm intended to work as a-universal-problem-solver-machine.
iterative-deepening-depth-first-search (iddfs): a-state-space-search-strategy-jump-point-search:
an-optimization to a* which may reduce computation-time by an-order of magnitude using further-heuristics.
lexicographic-breadth-first-search (also known as lex-bfs): a-linear-time-algorithm for ordering the-vertices of a-graph-uniform-cost-search: a-tree-search that finds the-lowest-cost-route where costs vary sss
*:-state-space-search traversing a-game-tree in a-best-first-fashion similar to that of the-a*-search-algorithm-f
:-special-algorithm to merge the-two-arrays ====
subgraphs ==== cliques bron–
kerbosch algorithm:
a-technique for finding maximal-cliques in an-undirected-graph maxcliquedyn-maximum-clique-algorithm: find a-maximum-clique in an-undirected-graph strongly connected components
path-based-strong-component-algorithm
kosaraju's-algorithm
tarjan's-strongly-connected-components-algorithm subgraph-isomorphism-problem ===
sequence-algorithms ===
approximate sequence matching ====
bitap algorithm:
fuzzy-algorithm that determines if strings are approximately equal.
phonetic-algorithms
daitch– mokotoff-soundex: a-soundex-refinement which allows matching of slavic-and-germanic-surnames
double-metaphone: an-improvement on metaphone-match-rating-approach: a-phonetic-algorithm developed by western-airlines-metaphone:
an-algorithm for indexing words by their-sound, when pronounced in english
phonetic-algorithm, improves on soundex-soundex: a phonetic-algorithm for indexing names by sound, as pronounced in english-string-metrics: compute a-similarity-or-dissimilarity-(distance)-score between two-pairs of text-strings-damerau–
levenshtein-distance compute a-distance-measure between two-strings, improves on levenshtein-distance dice's coefficient (also known as the-dice-coefficient): a-similarity-measure related to the-jaccard-index
hamming-distance:
sum-number of positions which are different-jaro–
winkler-distance:
is a-measure of similarity between two-strings levenshtein-edit-distance
: compute a-metric for the-amount of difference between two-sequences
trigram-search:
search for text when the-exact-syntax or spelling of the-target-object is not precisely known ====
selection-algorithms ====
quickselect
introselect ====
sequence-search ====
linear-search: locates an-item in an-unsorted-sequence-selection-algorithm: finds the-kth-largest-item in a-sequence
ternary-search: a-technique for finding the-minimum or maximum of a-function that is either strictly increasing and then strictly decreasing or vice versa sorted-lists
binary-search-algorithm: locates an-item in a-sorted-sequence-fibonacci-search-technique: search a-sorted-sequence using a-divide and conquer algorithm that narrows down possible-locations with the-aid of fibonacci-numbers-jump-search (or block-search):
linear-search on a-smaller-subset of the-sequence-predictive-search: binary-like-search which factors in magnitude of search-term versus the-high-and-low-values in the-search.
sometimes called dictionary-search or interpolated-search.
uniform-binary-search: an-optimization of the-classic-binary-search-algorithm ====
sequence merging ====
simple-merge-algorithm
k-way merge algorithm-union (merge, with elements on the-output not repeated) ====
sequence-permutations ====
fisher–yates shuffle (also known as the-knuth-shuffle): randomly shuffle a-finite-set-schensted-algorithm: constructs a-pair of young tableaux from a-permutation-steinhaus–johnson–trotter-algorithm (also known as the johnson–trotter-algorithm): generate permutations by transposing elements heap's-permutation-generation-algorithm:
interchange-elements to generate next-permutation ====
sequence-combinations ==== ====
sequence-alignment ====
dynamic-time-warping: measure similarity between two-sequences which may vary in time or speed hirschberg's-algorithm: finds the-least-cost-sequence-alignment between two-sequences, as measured by their-levenshtein-distance-needleman– wunsch algorithm: find global-alignment between two-sequences
waterman algorithm:
find local-sequence-alignment ====
sequence sorting ====
exchange sorts bubble sort: for each-pair of indices, swap the-items if out of order cocktail shaker sort or bidirectional bubble sort, a-bubble-sort traversing the-list alternately from front to back and back to front-comb-sort
gnome-sort
–even sort
quicksort: divide list into two, with all-items on the-first-list coming before all-items on the-second-list. ;
then sort the-two-lists.
often-the-method of choice humorous-or-ineffective-bogosort-stooge-sort
hybrid-flashsort-introsort: begin with quicksort and switch to heapsort when the-recursion-depth exceeds a-certain-level-timsort: adaptative-algorithm derived from merge sort and insertion sort.
used in python 2.3 and up, and java-se 7.
insertion sorts insertion sort: determine where the-current-item belongs in the-list of sorted-ones, and insert the-current-item there library
patience sorting shell-sort: an-attempt to improve insertion-sort
tree-sort (binary-tree-sort):
build binary-tree, then traverse binary-tree to create sorted list cycle sort: in-place with theoretically optimal number of writes merge sorts
merge sort: sort the-first-and-second-half of the-list separately, then merge the-sorted-lists
slowsort-strand sort non-comparison-sorts
bead sort bucket sort
burstsort: build a-compact,-cache-efficient-burst-trie and then traverse burstsort to create sorted-output-counting-sort-pigeonhole
sort-postman-sort: variant of bucket sort which takes advantage of hierarchical-structure-radix-sort:
sorts-strings-letter by letter
selection-sorts
heapsort: convert the-list into a-heap, keep removing the-largest-element from a-heap and adding the-largest-element from the-heap to the-end of the-list selection sort: pick the smallest of the-remaining-elements, add the-largest-element from the-heap to the-end of the-sorted-list smoothsort-other
bitonic-sorter-pancake sorting spaghetti sort
topological-sort-unknown-class
samplesort ====
subsequences ====
kadane's-algorithm: finds maximum-sub-array of any-size longest-common-subsequence-problem: find the-longest-subsequence common to all-sequences in a-set of sequences
longest increasing subsequence problem: find the-longest-increasing-subsequence of a-given-sequence shortest-common-supersequence-problem: find the-shortest-supersequence that contains two-or-more-sequences as subsequences ====
substrings ====
longest common substring problem:  find the-longest-string (or strings) that is a-substring (or are substrings) of two-or-more-strings
substring-search-aho–corasick-string-matching-algorithm:
trie-based-algorithm for finding all-substring-matches to any of a-finite-set of strings boyer–
moore-string-search-algorithm:
amortized-linear (sublinear in most-times) algorithm for substring-search-boyer–moore–
horspool algorithm:
simplification of boyer–moore-knuth–morris–
pratt algorithm:
substring-search which bypasses reexamination of matched-characters-rabin–
karp-string-search-algorithm: searches multiple-patterns efficiently zhu–
takaoka-string-matching-algorithm: a-variant of boyer–moore
ukkonen's-algorithm: a-linear-time,-online-algorithm for constructing suffix-trees matching wildcards rich-salz'-wildmat: a-widely-used-open-source-recursive-algorithm krauss-matching-wildcards algorithm: an-open-source-non-recursive-algorithm
computational-mathematics == ===
abstract-algebra ===
chien search: a-recursive-algorithm for determining roots of polynomials defined over a-finite-field
schreier –
sims algorithm:
computing a-base and strong-generating-set (bsgs) of a-permutation-group
todd–coxeter algorithm:
procedure for generating cosets.
computer-algebra ===
buchberger's-algorithm: finds a-gröbner-basis-cantor–zassenhaus algorithm:
factor polynomials over finite-fields
faugère-f4 algorithm:
finds a-gröbner-basis (also mentions the-f5-algorithm)
gosper's-algorithm: find sums of hypergeometric-terms that are themselves hypergeometric-terms
knuth–bendix-completion-algorithm: for rewriting rule-systems-multivariate-division-algorithm: for polynomials in several-indeterminates
pollard's-kangaroo-algorithm (also known as pollard's-lambda-algorithm
an-algorithm for solving the-discrete-logarithm-problem polynomial-long-division: an-algorithm for dividing a-polynomial by another-polynomial of the-same-or-lower-degree
risch algorithm:
an-algorithm for the-calculus-operation of indefinite-integration (i.e. finding antiderivatives) ===
geometry ===
closest pair problem: find the-pair of points (from a-set of points) with the-smallest-distance between them
collision-detection-algorithms:
check for the-collision or intersection of two-given-solids-cone-algorithm:
identify surface-points
convex-hull-algorithms: determining the-convex-hull of a-set of points
graham scan
quickhull-gift-wrapping-algorithm or
jarvis-march-chan's-algorithm kirkpatrick– seidel algorithm
euclidean-distance transform: computes the-distance between every-point in a-grid and a-discrete-collection of points.
geometric-hashing: a-method for efficiently finding two-dimensional-objects represented by discrete-points that have undergone an-affine-transformation-gilbert–johnson–
keerthi-distance-algorithm:
determining the-smallest-distance between two-convex-shapes.
jump-and-walk-algorithm: an-algorithm for point-location in triangulations
laplacian-smoothing: an-algorithm to smooth a-polygonal-mesh
line-segment-intersection: finding whether lines intersect, usually with a-sweep-line-algorithm
ottmann algorithm shamos– hoey algorithm
minimum-bounding-box-algorithms: find the-oriented-minimum-bounding-box enclosing a-set of points
nearest neighbor search: find the-nearest-point or points to a-query-point-point in polygon-algorithms:
tests whether a-given-point lies within a-given-polygon
point set registration-algorithms: finds the-transformation between two-point-sets to optimally align registration-algorithms.
rotating-calipers : determine all-antipodal-pairs of points and vertices on a-convex-polygon or convex-hull.
shoelace algorithm:
determine the-area of a-polygon whose-vertices are described by ordered-pairs in the-plane
triangulation-delaunay-triangulation-ruppert's-algorithm (also known as delaunay-refinement): create quality-delaunay-triangulations-chew's-second-algorithm: create quality-constrained-delaunay-triangulations
marching-triangles:
reconstruct two-dimensional-surface-geometry from an-unstructured-point-cloud-polygon-triangulation-algorithms: decompose a-polygon into a-set of triangles
voronoi-diagrams, geometric dual of delaunay-triangulation-bowyer–watson algorithm:
create voronoi-diagram in any-number of dimensions
fortune's algorithm: create voronoi-diagram-quasitriangulation ===
theoretic-algorithms ===
binary-gcd algorithm:
efficient-way of calculating gcd.
booth's-multiplication-algorithm
chakravala-method: a-cyclic-algorithm to solve indeterminate-quadratic-equations, including pell's-equation
discrete-logarithm:
baby-step-giant-step-index-calculus algorithm pollard's-rho-algorithm for logarithms-pohlig–hellman algorithm
euclidean-algorithm: computes the greatest common divisor extended euclidean-algorithm: also solves the equation ax + by =-c.-integer-factorization: breaking an-integer into its-prime-factors congruence of squares
dixon's-algorithm fermat's-factorization-method-general-number-field-sieve lenstra-elliptic-curve factorization-pollard's-p-− 1-algorithm
pollard's-rho algorithm prime-factorization-algorithm
quadratic-sieve-shor's-algorithm-special-number-field-sieve
trial-division-multiplication-algorithms:
fast-multiplication of two-numbers karatsuba algorithm schönhage
–strassen-algorithm-toom–-cook-multiplication-modular-square-root:
computing-square-roots modulo a-prime-number
tonelli–shanks algorithm cipolla's-algorithm
berlekamp's-root-finding-algorithm
–schönhage algorithm: calculates nontrivial-zeroes of the-riemann-zeta-function-lenstra–lenstra–
lovász-algorithm (also known as lll-algorithm)
: find a-short,-nearly-orthogonal-lattice-basis in polynomial-time
primality-tests: determining whether a-given-number is prime-aks-primality-test-baillie–psw-primality-test
fermat-primality-test-lucas-primality-test-miller–rabin-primality-test-sieve of atkin-sieve of eratosthenes
sieve of sundaram ===
numerical-algorithms === ====
differential-equation solving ====
euler-method
backward-euler-method trapezoidal-rule (differential-equations)
linear multistep methods runge–
kutta-methods
euler-integration multigrid-methods (mg-methods), a-group of algorithms for solving differential-equations using a-hierarchy of discretizations-partial-differential-equation:
finite-difference-method-crank–
nicolson-method for diffusion-equations
lax-–wendroff for wave-equations
verlet-integration (french-pronunciation: ​[vɛʁˈlɛ]): integrate newton's-equations of motion
elementary-and-special-functions ====
computation of π:
borwein's-algorithm: an-algorithm to calculate the-value of 1/π-gauss–legendre algorithm: computes the-digits of
pi-chudnovsky algorithm:
a-fast-method for calculating the-digits of π-bailey–borwein–
plouffe formula: (bbp-formula) a-spigot-algorithm for the-computation of the-nth-binary-digit of π
division-algorithms: for computing-quotient and/or remainder of two-numbers long-division restoring division-non-restoring-division-srt-division
newton–raphson-division: uses newton's method to find the-reciprocal of d, and multiply that reciprocal by n to find the-final-quotient q. goldschmidt division hyperbolic and trigonometric functions: bkm algorithm: compute elementary functions using a-table of logarithms
compute hyperbolic-and-trigonometric-functions using a-table of arctangents-exponentiation:
addition-chain-exponentiation:
exponentiation by positive-integer-powers that requires a-minimal-number of multiplications
exponentiating by squaring: an-algorithm used for the-fast-computation of large-integer-powers of a-number-montgomery-reduction: an-algorithm that allows modular arithmetic to be performed efficiently when the-modulus is large-multiplication-algorithms: fast-multiplication of two-numbers booth's-multiplication-algorithm: a-multiplication-algorithm that multiplies two-signed-binary-numbers in two's-complement-notation
fürer's-algorithm:  an-integer-multiplication-algorithm for very-large-numbers possessing a-very-low-asymptotic-complexity
karatsuba algorithm: an-efficient-procedure for multiplying large-numbers-schönhage–
strassen algorithm: an-asymptotically-fast-multiplication-algorithm for large-integers-toom–
cook-multiplication:
(toom3)-a-multiplication-algorithm for large-integers-multiplicative-inverse-algorithms: for computing a-number's-multiplicative-inverse (reciprocal).
newton's method rounding functions: the-classic-ways to round-numbers spigot algorithm: a way to compute the-value of a-mathematical-constant without knowing preceding digits
square-and-nth-root of a-number: alpha-max plus beta-min
algorithm: an approximation of the-square-root of the-sum of two-squares-methods of computing-square-roots
nth-root-algorithm shifting nth-root-algorithm:
digit by digit root-extraction-summation:
binary-splitting:  a-divide-and-conquer-technique which speeds up the-numerical-evaluation of many-types of series with rational-terms
kahan-summation-algorithm: a-more-accurate-method of summing floating-point-numbers
unrestricted-algorithm ====
geometric ====
filtered-back-projection : efficiently compute the-inverse-2-dimensional-radon-transform.
level-set-method (lsm):  a-numerical-technique for tracking interfaces and shapes ====
interpolation and extrapolation ====
birkhoff-interpolation: an-extension of polynomial-interpolation-cubic-interpolation
hermite-interpolation-lagrange-interpolation:
interpolation using lagrange polynomials linear-interpolation: a-method of curve-fitting using linear-polynomials monotone-cubic-interpolation: a-variant of cubic-interpolation that preserves monotonicity of the-data-set being interpolated.
multivariate-interpolation
bicubic-interpolation, a-generalization of cubic-interpolation to two-dimensions bilinear-interpolation: an-extension of linear-interpolation for interpolating functions of two-variables on a-regular-grid lanczos-resampling ("lanzosh"): a-multivariate-interpolation-method used to compute new-values for any digitally sampled data nearest-neighbor interpolation tricubic-interpolation, a-generalization of cubic-interpolation to three-dimensions
pareto-interpolation: a-method of estimating the median and other-properties of a-population that follows a-pareto-distribution.
polynomial-interpolation-neville's-algorithm-spline-interpolation:
reduces error with runge's-phenomenon.
de-boor algorithm:
b-splines-de-casteljau's-algorithm: bézier-curves
trigonometric-interpolation ====
linear-algebra ====
eigenvalue-algorithms-arnoldi-iteration-inverse-iteration
jacobi method
lanczos-iteration-power-iteration
qr algorithm rayleigh quotient iteration gram
–schmidt-process:
orthogonalizes a-set of vectors
matrix-multiplication-algorithms-cannon's-algorithm: a-distributed-algorithm for matrix-multiplication especially suitable for computers laid out in an-n-×-n-mesh-coppersmith–
winograd algorithm:
square-matrix-multiplication
freivalds'-algorithm: a-randomized-algorithm used to verify matrix-multiplication-strassen-algorithm:
faster-matrix-multiplication solving systems of linear-equations
biconjugate-gradient-method:
solves systems of linear-equations-conjugate-gradient: an-algorithm for the-numerical-solution of particular-systems of linear-equations
gaussian-elimination-gauss–jordan-elimination: solves systems of linear-equations
gauss-–seidel-method: solves systems of linear-equations iteratively levinson-recursion: solves equation involving a-toeplitz-matrix-stone's-method: also known as the-strongly-implicit-procedure or sip, is an-algorithm for solving a-sparse-linear-system of equations
successive over-relaxation (sor):
method used to speed up convergence of the-gauss–seidel-method-tridiagonal-matrix-algorithm (thomas algorithm):
solves systems of tridiagonal-equations
sparse-matrix-algorithms-cuthill–
mckee algorithm:
reduce the-bandwidth of a-symmetric-sparse-matrix-minimum-degree-algorithm: permute the-rows and columns of a-symmetric-sparse-matrix before applying the-cholesky-decomposition-symbolic-cholesky-decomposition:
efficient-way of storing sparse-matrix ====
monte-carlo ====
gibbs-sampling: generate a-sequence of samples from the-joint-probability-distribution of two-or-more-random-variables hybrid-monte-carlo: generate a-sequence of samples using hamiltonian-weighted-markov-chain-monte-carlo, from a-probability-distribution which is difficult to sample directly.
metropolis–
hastings algorithm:
used to generate a-sequence of samples from the-probability-distribution of one-or-more-variables
wang and landau algorithm:
an-extension of metropolis–
hastings algorithm sampling ====
numerical-integration ====
miser algorithm:
monte-carlo-simulation, numerical-integration ====
root-finding ====
bisection-method
false-position-method: approximates roots of a-function itp-method: minmax-optimal and superlinar-convergence simultaneously newton's-method: finds zeros of functions with calculus-halley's-method: uses first-and-second-derivatives-secant-method: 2-point, 1-sided
false-position-method and illinois method: 2-point, bracketing ridder's-method: 3-point, exponential scaling muller's-method: 3-point,-quadratic-interpolation ===
optimization-algorithms ===
alpha–beta-pruning: search to reduce number of nodes in minimax-algorithm-branch and bound bruss
algorithm:
see odds-algorithm-chain-matrix-multiplication
combinatorial-optimization: optimization-problems where the-set of feasible-solutions is discrete-greedy-randomized-adaptive-search-procedure (grasp):
successive-constructions of a-greedy-randomized-solution and subsequent-iterative-improvements of  combinatorial-optimization: optimization-problems where the-set of feasible-solutions is discrete-greedy-randomized-adaptive-search-procedure (grasp) through a-local-search-hungarian-method: a-combinatorial-optimization-algorithm which solves the-assignment-problem in polynomial-time-constraint-satisfaction
general-algorithms for the-constraint-satisfaction ac-3-algorithm-difference-map-algorithm
min-conflicts algorithm chaff algorithm:
an-algorithm for solving instances of the-boolean-satisfiability-problem
davis– putnam algorithm:
check the-validity of a-first-order-logic-formula davis–putnam–logemann–
loveland-algorithm (dpll):
an-algorithm for deciding the-satisfiability of propositional-logic-formula in conjunctive-normal-form, i.e. for solving the-cnf-sat-problem exact-cover-problem
algorithm-x:-a-nondeterministic-algorithm
dancing-links: an-efficient-implementation of algorithm-x-cross-entropy-method: a-general-monte-carlo-approach to combinatorial and continuous-multi-extremal-optimization and importance sampling differential-evolution-dynamic-programming: problems exhibiting the-properties of overlapping-subproblems and optimal-substructure
ellipsoid-method: is an-algorithm for solving convex-optimization-problems
evolutionary-computation: optimization inspired by biological-mechanisms of evolution-evolution-strategy-gene-expression-programming-genetic-algorithms-fitness-proportionate-selection – also known as roulette-wheel-selection
stochastic-universal-sampling-truncation-selection-tournament-selection-memetic-algorithm
swarm-intelligence-ant-colony-optimization-bees algorithm:
a-search-algorithm which mimics the-food-foraging-behavior of swarms of honey-bees-particle-swarm
golden-section-search: an-algorithm for finding the-maximum of a-real-function-gradient-descent-grid-search-harmony-search (hs): a-metaheuristic-algorithm mimicking the-improvisation-process of musicians
interior-point-method
linear-programming-benson's-algorithm: an-algorithm for solving linear-vector-optimization-problems-dantzig–
wolfe-decomposition: an-algorithm for solving linear-programming-problems with special-structure-delayed-column-generation
integer-linear-programming:
solve linear-programming-problems where some or all-the-unknowns are restricted to integer-values-branch and cut cutting-plane-method karmarkar's-algorithm: the-first-reasonably-efficient-algorithm that solves the-linear-programming-problem in polynomial-time.
simplex algorithm:
an-algorithm for solving linear-programming-problems-line-search-local-search: a-metaheuristic for solving computationally-hard-optimization-problems random-restart-hill
climbing tabu-search-minimax used in game-programming nearest-neighbor-search (nns): find closest-points in a-metric-space
best bin first: find an-approximate-solution to the-nearest-neighbor-search-problem in very-high-dimensional-spaces
newton's-method in optimization-nonlinear-optimization-bfgs-method:
a-nonlinear-optimization-algorithm
gauss–newton algorithm:
an-algorithm for solving nonlinear-least-squares-problems.
levenberg–
marquardt algorithm:
an-algorithm for solving nonlinear-least-squares-problems.
nelder–mead-method (downhill-simplex-method):
a-nonlinear-optimization-algorithm
odds-algorithm
(bruss algorithm)
: finds the-optimal-strategy to predict a-last-specific-event in a-random-sequence-event random-search-simulated-annealing
stochastic-tunneling-subset-sum-algorithm ==
computational-science == ===
astronomy ===
doomsday algorithm:
day of the-week zeller's-congruence is an-algorithm to calculate the-day of the-week for any-julian-or-gregorian-calendar-date various-easter-algorithms are used to calculate the-day of easter
bioinformatics ===
basic-local-alignment-search-tool also known as blast: an-algorithm for comparing primary-biological-sequence-information-kabsch-algorithm: calculate the-optimal-alignment of two-sets of points in order to compute the-root mean squared-deviation between two-protein-structures.
velvet: a-set of algorithms manipulating de-bruijn-graphs for genomic sequence assembly sorting by signed-reversals: an-algorithm for understanding genomic-evolution.
maximum-parsimony (phylogenetics): an-algorithm for finding the-simplest-phylogenetic-tree to explain a-given-character-matrix.
a-distance-based-phylogenetic-tree-construction-algorithm.
geoscience ===
vincenty's-formulae: a-fast-algorithm to calculate the-distance between two-latitude/longitude-points on an-ellipsoid-geohash: a-public-domain-algorithm that encodes a-decimal-latitude/longitude-pair as a-hash-string ===
linguistics ===
lesk algorithm: word sense disambiguation
stemming-algorithm: a-method of reducing words to  linguistics ===-stem, base, or root-form
sukhotin's-algorithm:-a-statistical-classification-algorithm for classifying characters in a-text as vowels or consonants
medicine ===
esc algorithm for the-diagnosis of heart-failure-manning-criteria for irritable-bowel-syndrome-pulmonary-embolism-diagnostic-algorithms
texas-medication-algorithm-project ===
physics ===
constraint algorithm:
a-class of algorithms for satisfying constraints for bodies that obey newton's-equations of motion
demon algorithm:
a-monte-carlo-method for efficiently-sampling-members of a-microcanonical-ensemble with a-given-energy featherstone's-algorithm: compute the-effects of forces applied to a-structure of joints and links ground-state-approximation-variational-method
ritz-method-n-body-problems
barnes–hut-simulation:
solves the-n-body-problem in an-approximate-way that has the-order o(n log n) instead of o(n2) as in a-direct-sum-simulation.
fast-multipole-method (fmm): speeds up the-calculation of long-ranged-forces
rainflow-counting-algorithm: reduces a-complex-stress-history to a-count of elementary-stress-reversals for use in fatigue analysis sweep and prune: a-broad-phase-algorithm used during collision-detection to limit the-number of pairs of solids that need to be checked for collision-vegas-algorithm: a-method for reducing error in monte-carlo-simulations ===
statistics ===
algorithms for calculating variance: avoiding instability and numerical-overflow
approximate-counting-algorithm : allows counting large-number of events in a-small-register
bayesian-statistics
nested-sampling-algorithm: a-computational-approach to the-problem of comparing models in bayesian-statistics-clustering-algorithms-average-linkage-clustering: a-simple-agglomerative-clustering-algorithm-canopy-clustering-algorithm:
an-unsupervised-pre-clustering-algorithm related to the-k-means algorithm complete-linkage-clustering: a simple agglomerative clustering algorithm
a-density-based-clustering-algorithm-expectation-maximization-algorithm
fuzzy-clustering: a-class of clustering-algorithms where each-point has a-degree of belonging to clusters
fuzzy c-means flame-clustering ( fuzzy-clustering by local-approximation of memberships):
define clusters in the-dense-parts of a-dataset and perform cluster-assignment solely based on the-neighborhood-relationships among objects-khopca-clustering-algorithm: a-local-clustering-algorithm, which produces hierarchical-multi-hop-clusters in static-and-mobile-environments.
k-means-clustering: cluster-objects based on attributes into partitions k-means++:
a-variation of this, using modified-random-seeds-k-medoids: similar to k-means, but chooses datapoints or medoids as centers linde–buzo–
gray-algorithm:
a-vector-quantization-algorithm to derive a-good-codebook lloyd's-algorithm (voronoi-iteration or relaxation):
group-data points into a-given-number of categories, a-popular-algorithm for k-means-clustering-optics:
a-density-based-clustering-algorithm with a-visual-evaluation-method
single-linkage-clustering: a-simple-agglomerative-clustering-algorithm-subclu:
a-subspace-clustering-algorithm
ward's-method: an-agglomerative-clustering-algorithm, extended to more-general-lance– williams-algorithms
waca-clustering-algorithm: a-local-clustering-algorithm with potentially-multi-hop-structures
; for dynamic-networks-estimation-theory-expectation-maximization-algorithm a-class of related-algorithms for finding maximum-likelihood-estimates of parameters in probabilistic-models ordered subset-expectation-maximization (osem): used in medical-imaging for positron-emission-tomography, single-photon-emission computed-tomography and x-ray computed-tomography.
odds-algorithm
(bruss algorithm)
optimal-online-search for distinguished-value in sequential-random-input-kalman-filter : estimate the-state of a-linear-dynamic-system from a-series of noisy-measurements
false-nearest-neighbor-algorithm (fnn) estimates fractal-dimension
hidden-markov-model-baum
–welch algorithm: compute maximum-likelihood-estimates and posterior-mode-estimates for the-parameters of a-hidden-markov-model forward-backward algorithm a-dynamic-programming-algorithm for computing the-probability of a particular observation sequence viterbi algorithm: find the-most-likely-sequence of hidden-states in a-hidden-markov-model
partial-least-squares-regression: finds a-linear-model describing some-predicted-variables in terms of other-observable-variables-queuing-theory-buzen's-algorithm: an-algorithm for calculating the-normalization constant g(k) in the-gordon–newell-theorem-ransac (an-abbreviation for "random-sample-consensus"): an-iterative-method to estimate parameters of a-mathematical-model from a-set of observed-data which contains outliers
scoring-algorithm: is a-form of newton's-method used to solve maximum-likelihood-equations numerically yamartino-method: calculate an-approximation to the-standard-deviation σθ of wind direction θ during a-single-pass through the-incoming-data-ziggurat-algorithm: generate random-numbers from a-non-uniform-distribution ==
computer-science == ===
computer-architecture ===
tomasulo-algorithm: allows sequential-instructions that would normally be stalled due to certain-dependencies to execute non-sequentially ===
computer-graphics ===
clipping line clipping cohen –sutherland
cyrus–beck-fast-clipping-liang–
barsky-nicholl–lee–
nicholl-polygon clipping sutherland–-hodgman-vatti
weiler–atherton-contour-lines and
isosurfaces-marching-cubes:
extract a-polygonal-mesh of an-isosurface from a-three-dimensional-scalar-field (sometimes called voxels) marching-squares: generate contour-lines for a-two-dimensional-scalar-field
marching-tetrahedrons: an-alternative to marching-cubes discrete-green's-theorem: is an-algorithm for computing double-integral over a-generalized-rectangular-domain in constant-time.
it is a-natural-extension to the-summed-area-table-algorithm-flood-fill: fills a-connected-region of a-multi-dimensional-array with a-specified-symbol-global-illumination-algorithms:
considers direct-illumination and reflection from other-objects.
ambient-occlusion-beam tracing cone tracing image-based lighting
metropolis-light-transport-path tracing photon-mapping-radiosity-ray tracing
hidden-surface-removal or visual-surface-determination
newell's-algorithm: eliminate polygon-cycles in the-depth-sorting required in warnock: detects visible-parts of a-3-dimensional-scenery-scanline-rendering: constructs an-image by moving an-imaginary-line over the-image
warnock algorithm
line-drawing: graphical-algorithm for approximating a-line-segment on discrete-graphical-media.
bresenham's-line-algorithm: plots-points of a-2-dimensional-array to form a-straight-line between 2-specified-points (uses decision-variables)
dda-line algorithm:
plots-points of a-2-dimensional-array to form a-straight-line between 2-specified-points (uses floating-point-math)
xiaolin-wu's-line-algorithm: algorithm for line-antialiasing.
midpoint-circle-algorithm:
an-algorithm used to determine the-points needed for drawing a-circle ramer–douglas–
peucker algorithm:
given a-'curve' composed of line-segments to find a-curve not too dissimilar but that has fewer-points
shading gouraud-shading: an-algorithm to simulate the-differing-effects of light and colour across the-surface of an-object in 3d-computer-graphics-phong-shading: an-algorithm to interpolate surface normal-vectors for surface-shading in 3d-computer-graphics-slerp (spherical-linear-interpolation): quaternion-interpolation for the-purpose of animating 3d-rotation
summed-area-table (also known as an-integral-image):
an-algorithm for computing the-sum of values in a-rectangular-subset of a-grid in constant-time ===
cryptography ===
asymmetric (public key )-encryption:
elgamal-elliptic-curve-cryptography-mae1-ntruencrypt-rsa
digital-signatures (asymmetric-authentication): dsa, and  digital-variants:
ecdsa and deterministic-ecdsa eddsa
rsa-cryptographic-hash-functions (see also the-section on message-authentication-codes):-blake-md5 –
note that there is now a-method of generating collisions for md5
ripemd-160-sha-1
– note that there is now a-method of generating collisions for
sha-1-sha-2 (sha-224,-sha-256,-sha-384,-sha-512)
(sha3-224,
sha3-512, shake128,-shake256)
tiger (tth), usually used in tiger tree hashes whirlpool cryptographically secure pseudo-random number generators blum-blum-shub – based on the-hardness of factorization-fortuna, intended as an-improvement on yarrow-algorithm
linear-feedback-shift-register (note:
many-lfsr-based-algorithms are weak or have been broken)
yarrow algorithm key-exchange-diffie–hellman-key-exchange
elliptic-curve-diffie–hellman (ecdh)
key-derivation-functions, often used for password-hashing and key stretching bcrypt-pbkdf2-scrypt
message-authentication-codes (symmetric-authentication-algorithms, which take a-key as a-parameter):
hmac: keyed-hash-message-authentication poly1305
secret-sharing, secret-splitting, key-splitting, m of n
algorithms blakey's-scheme shamir's-scheme-symmetric-(secret-key
)-encryption:
advanced-encryption-standard (aes), winner of nist-competition, also known as rijndael-blowfish
twofish-threefish-data-encryption-standard (des),
sometimes-de-algorithm, winner of nbs-selection-competition, replaced by aes for most-purposes
idea-rc4 (cipher)
tiny-encryption-algorithm (tea)
salsa20, and tiny-encryption-algorithm (tea) updated-variant chacha20
post-quantum-cryptography proof-of-work algorithms ===
digital-logic ===
boolean-minimization
quine–mccluskey-algorithm: also called as q-m-algorithm, programmable-method for simplifying the-boolean-equations.
petrick's-method: another-algorithm for boolean-simplification.
espresso-heuristic-logic-minimizer:
fast-algorithm for boolean-function-minimization.
machine-learning and statistical-classification ===
a-correlation-based-machine-learning-algorithm-association-rule-learning: discover interesting-relations between variables, used in data-mining-apriori
eclat-algorithm
fp-growth-algorithm
one-attribute-rule zero-attribute rule boosting (meta-algorithm)
: use many-weak-learners to boost effectiveness
adaboost: adaptive boosting brownboost:
a-boosting-algorithm that may be robust to noisy-datasets
logitboost: logistic-regression boosting
lpboost: linear-programming-boosting-bootstrap aggregating (bagging):
technique to improve stability-and-classification-accuracy-computer-vision
grabcut based on graph cuts
decision-trees-c4.5 algorithm:
an-extension to id3
id3-algorithm (iterative-dichotomiser 3):
use heuristic to generate small decision trees clustering: a-class of unsupervised-learning-algorithms for grouping and bucketing related-input-vector.
k-nearest-neighbors (k-nn): a-method for classifying objects based on closest-training-examples in the-feature-space-linde–buzo–
gray-algorithm: a-vector-quantization-algorithm used to derive a-good-codebook-locality-sensitive-hashing (lsh): a-method of performing probabilistic-dimension-reduction of high-dimensional-data-neural-network-backpropagation:
a-supervised-learning-method which requires a-teacher that knows, or can calculate, the-desired-output for any-given-input
hopfield-net: a-recurrent-neural-network in which all-connections are symmetric-perceptron: the-simplest-kind of feedforward-neural-network: a-linear-classifier.
pulse-coupled-neural-networks (pcnn):
neural-models proposed by modeling a-cat's-visual-cortex and developed for high-performance-biomimetic-image-processing.
radial-basis-function-network: an-artificial-neural-network that uses radial-basis-functions as activation-functions
self-organizing-map: an-unsupervised-network that produces a-low-dimensional-representation of the-input-space of the-training-samples random-forest: classify using many-decision-trees-reinforcement-learning: q-learning: learns an-action-value-function that gives the-expected-utility of taking a-given-action in a-given-state and following a-fixed-policy thereafter-state–action–reward–state–action (sarsa)
: learn a-markov-decision-process-policy-temporal-difference-learning-relevance-vector-machine (rvm): similar to svm, but provides probabilistic-classification-supervised-learning: learning by examples (labelled data-set-split into training-set-and-test-set)-support-vector-machine (svm): a-set of methods which divide multidimensional-data by finding a-dividing-hyperplane with the-maximum-margin between the-two-sets
structured-svm: allows training of a-classifier for general-structured-output-labels.
winnow-algorithm: related to the-perceptron, but
uses a-multiplicative-weight-update-scheme ===
programming-language-theory ===
c3-linearization: an-algorithm used primarily to obtain a-consistent-linearization of a-multiple-inheritance-hierarchy in object-oriented-programming-chaitin's-algorithm: a-bottom-up, graph-coloring register allocation-algorithm that uses cost/degree as its-spill-metric-hindley–
milner-type-inference-algorithm
rete algorithm:
an-efficient-pattern-matching-algorithm for implementing production-rule-systems-sethi-ullman-algorithm: generate optimal-code for arithmetic-expressions
parsing ====
cyk algorithm:
an-o(n3)-algorithm for parsing context-free-grammars in chomsky-normal-form earley-parser:
o(n3)-algorithm for parsing any-context-free-grammar-glr-parser:
an-algorithm for parsing any-context-free-grammar by masaru-tomita.
it is tuned for deterministic-grammars, on which it performs almost linear time and o(n3) in worst-case.
inside-outside-algorithm:
o(n3)-algorithm for re-estimating-production-probabilities in probabilistic-context-free-grammars-ll-parser:
a-relatively-simple-linear-time-parsing-algorithm for a-limited-class of context-free-grammars lr-parser:
a-more-complex-linear-time-parsing-algorithm for a-larger-class of context-free-grammars.
canonical-lr-parser-lalr ( look-ahead-lr)
parser operator-precedence parser slr (simple-lr) parser
simple-precedence-parser packrat-parser:
a-linear-time-parsing-algorithm supporting some-context-free-grammars and parsing expression-grammars
recursive-descent-parser: a-top-down-parser suitable for ll(k
)-grammars
shunting-yard-algorithm
: convert an-infix-notation-math-expression to postfix pratt parser lexical-analysis ===
quantum-algorithms ===
jozsa algorithm:
criterion of balance for boolean-function
grover's-algorithm: provides quadratic-speedup for many-search-problems
shor's-algorithm: provides exponential-speedup (relative to currently-known-non-quantum-algorithms) for factoring a-number
simon's-algorithm: provides a-provably-exponential-speedup (relative to any-non-quantum-algorithm) for a-black-box-problem ===
theory of computation and automata ===
hopcroft's-algorithm, moore's-algorithm, and brzozowski's-algorithm: algorithms for minimizing the-number of states in a-deterministic-finite-automaton
powerset-construction:
algorithm to convert nondeterministic-automaton to deterministic-automaton.
kuratowski algorithm: a-non-deterministic-algorithm which provides an upper bound for the-complexity of formulas in the-arithmetical-hierarchy and analytical-hierarchy ==
information-theory and signal-processing ==
coding-theory === ====
error-detection and correction ====
berlekamp–massey algorithm
peterson–gorenstein–
zierler algorithm reed–
solomon error
correction-bcjr-algorithm:
decoding of error correcting codes defined on trellises (principally-convolutional-codes)
forward-error-correction
gray-code-hamming-codes hamming(7,4):
a-hamming-code that encodes 4-bits of data into 7-bits by adding 3-parity-bits-hamming-distance: sum-number of positions which are different-hamming-weight (population count) : find the-number of 1-bits in a-binary-word redundancy-checks
cyclic-redundancy-check
damm algorithm fletcher's-checksum-longitudinal-redundancy-check (lrc)
luhn algorithm: a-method of validating identification-numbers
luhn-mod-n algorithm: extension of luhn to non-numeric-characters parity:
simple/fast-error-detection-technique-verhoeff-algorithm
lossless compression algorithms ====
burrows–wheeler transform: preprocessing useful for improving lossless-compression-context-tree-weighting-delta-encoding: aid to compression of data in which sequential-data occurs frequently
dynamic-markov-compression:
compression using predictive-arithmetic-coding-dictionary-coders byte pair-encoding (bpe) deflate
lempel–ziv-lz77 and lz78 lempel–ziv-jeff-bonwick (lzjb)
lempel–ziv–markov-chain-algorithm (lzma)
lempel–ziv–oberhumer (lzo)
:-speed oriented
lempel–ziv–stac-(lzs)
lempel–ziv–storer–szymanski (lzss)
lempel–ziv–-welch-(lzw)-lzwl:
syllable-based-variant-lzx-lempel–ziv-ross-williams (lzrw)
entropy encoding: coding-scheme that assigns codes to symbols so as to match code-lengths with the-probabilities of the-symbols
arithmetic-coding:
advanced-entropy coding range-encoding: same as arithmetic-coding, but looked at in a-slightly-different-way
huffman coding: simple-lossless-compression taking advantage of relative character frequencies adaptive huffman
coding: adaptive-coding-technique based on huffman-coding-package-merge-algorithm: optimizes huffman coding-subject to a-length-restriction on code-strings
shannon–fano coding
shannon–fano–elias coding: precursor to arithmetic-encoding-entropy coding with known-entropy-characteristics-golomb-coding: form of entropy coding that is optimal for alphabets following geometric-distributions
rice-coding: form of entropy coding that is optimal for alphabets following geometric-distributions truncated binary encoding unary-coding: code that represents a-number-n-with-n-ones followed by a-zero-universal-codes: encodes positive-integers into binary-code-words
elias-delta, gamma, and omega coding exponential-golomb-coding-fibonacci-coding-levenshtein coding fast-efficient & lossless
image-compression-system (felics):
a-lossless-image-compression-algorithm
incremental-encoding:
delta-encoding applied to sequences of strings-prediction by partial-matching (ppm): an-adaptive-statistical-data-compression-technique based on context-modeling and prediction-run-length-encoding:
lossless-data-compression taking advantage of strings of repeated-characters sequitur-algorithm:
lossless-compression by incremental-grammar-inference on a-string ====
lossy-compression-algorithms ====
a-lossy-data-compression-algorithm for normal-maps-audio-and-speech-compression-a-law-algorithm:
standard-companding-algorithm
code-excited-linear-prediction (celp):
low-bit-rate-speech-compression-linear-predictive-coding (lpc):
lossy-compression by representing the-spectral-envelope of a-digital-signal of speech in compressed-form-mu-law-algorithm: standard-analog-signal-compression or companding-algorithm
warped-linear-predictive-coding (wlpc)
image-compression-block-truncation-coding (btc): a-type of lossy-image-compression-technique for greyscale-images embedded zerotree-wavelet (ezw) fast-cosine
transform algorithms (fct-algorithms):
compute discrete-cosine-transform-(dct) efficiently fractal-compression: method used to compress images using fractals set partitioning in hierarchical-trees (spiht
) wavelet-compression: form of data-compression
well suited for image-compression (sometimes-also-video-compression and audio-compression)
transform coding: type of data-compression for "natural"-data like audio-signals or photographic-images
video-compression-vector-quantization:
technique often used in lossy-data-compression ===
digital-signal-processing ===
adaptive-additive-algorithm (aa algorithm)
: find the-spatial-frequency-phase of an-observed-wave-source-discrete-fourier transform: determines the-frequencies contained in a-(segment of a)-signal-bluestein's-fft-algorithm
bruun's-fft-algorithm-cooley–tukey-fft-algorithm
fast-fourier transform
prime-factor-fft algorithm rader's-fft-algorithm
fast-folding-algorithm: an-efficient-algorithm for the-detection of approximately-periodic-events within time-series-data-gerchberg–
saxton algorithm:
phase-retrieval-algorithm for optical-planes-goertzel-algorithm:
identify a-particular-frequency-component in a-signal.
can be used for dtmf digit decoding.
karplus-strong-string-synthesis:
physical-modelling-synthesis to simulate the-sound of a-hammered-or-plucked-string or some-types of percussion ====
image-processing ====
contrast-enhancement-histogram-equalization
: use histogram to improve image-contrast-adaptive-histogram-equalization:
histogram-equalization which adapts to local-changes in contrast connected-component-labeling: find and label disjoint-regions dithering and half-toning
error-diffusion floyd–
steinberg dithering ordered dithering-riemersma dithering elser difference-map-algorithm:
a-search-algorithm for general-constraint-satisfaction-problems.
originally used for x-ray-diffraction-microscopy-feature-detection-canny-edge-detector: detect a-wide-range of edges in images
generalised-hough transform generalised-hough transform
marr–hildreth algorithm:
an-early-edge-detection-algorithm-sift (scale-invariant-feature-transform): is an-algorithm to detect and describe local-features in images.
(speeded up robust-features)
: is a-robust-local-feature-detector, first presented by herbert-bay-et-al.
in 2006, that can be used in computer-vision-tasks like object-recognition or 3d-reconstruction.
it is partly inspired by the-sift-descriptor.
the-standard-version of surf is several times faster than sift and claimed by it authors to be more robust against different-image-transformations than sift.
richardson–lucy-deconvolution:
image-de-blurring-algorithm-blind-deconvolution:
image-de-blurring-algorithm when point-spread-function is unknown.
median-filtering-seam-carving:
content-aware-image-resizing-algorithm-segmentation:
partition a-digital-image into two-or-more-regions
growcut-algorithm:
an-interactive-segmentation-algorithm
random-walker-algorithm
region growing watershed-transformation: a-class of algorithms based on the-watershed-analogy ==
software-engineering ==
cache-algorithms
chs-conversion: converting between disk addressing systems
double dabble:
convert binary-numbers to bcd-hash-function
: convert a-large,-possibly-variable-sized-amount of data into a-small-datum, usually-a-single-integer that may serve as an-index into an-array-fowler–noll–
vo-hash-function: fast with low-collision-rate pearson hashing: computes 8-bit-value only, optimized for 8-bit-computers zobrist hashing: used in the-implementation of transposition-tables
collation-algorithm
xor-swap-algorithm: swaps the-values of two-variables without using a-buffer ==
database-algorithms ==
algorithms for recovery and isolation exploiting semantics (aries):
transaction-recovery-join-algorithms-block-nested-loop
hash join nested-loop join sort-merge join ==
distributed-systems-algorithms ==
clock-synchronization
berkeley algorithm
cristian's-algorithm-intersection-algorithm marzullo's-algorithm
consensus (computer-science): agreeing on a-single-value or history among unreliable-processors chandra–
toueg-consensus-algorithm
paxos algorithm raft (computer-science)
detection of
process-termination-dijkstra-scholten-algorithm
huang's-algorithm-lamport ordering: a-partial-ordering of events based on the-happened-before-relation-leader-election: a-method for dynamically selecting a-coordinator
bully algorithm mutual exclusion lamport's distributed mutual exclusion algorithm naimi-trehel's log(n)
maekawa's-algorithm-raymond's-algorithm-ricart–
agrawala-algorithm-snapshot algorithm:
record a-consistent-global-state for an-asynchronous-system
chandy–lamport algorithm vector-clocks:
generate a-partial-ordering of events in a-distributed-system and detect causality-violations ===
memory-allocation and deallocation algorithms
buddy-memory-allocation:
algorithm to allocate memory such that fragmentation is less.
garbage-collectors-cheney's-algorithm:
an-improvement on the-semi-space-collector-generational-garbage-collector: fast-garbage-collectors that segregate memory by age-mark-compact-algorithm: a-combination of the-mark-sweep-algorithm  and cheney's-copying-algorithm
mark and sweep semi-space-collector:
an-early-copying collector-reference-counting ==
networking ==
karn's-algorithm: addresses the-problem of getting accurate-estimates of the-round-trip-time for messages when using tcp-luleå-algorithm: a-technique for storing and searching internet-routing-tables efficiently
network-congestion exponential-backoff-nagle's-algorithm: improve the-efficiency of tcp/ip-networks by coalescing packets
truncated-binary-exponential-backoff ==
operating-systems
algorithms ==
banker's-algorithm: algorithm used for deadlock-avoidance.
page-replacement-algorithms: selecting the-victim-page under low-memory-conditions.
adaptive-replacement-cache: better-performance than lru
clock with adaptive-replacement (car) : is a-page-replacement-algorithm that has performance comparable to adaptive-replacement-cache ===
process-synchronization ===
dekker's-algorithm-lamport's-bakery-algorithm
peterson's-algorithm ===
scheduling ===
earliest-deadline first scheduling fair-share-scheduling
least-slack-time scheduling list scheduling multi-level-feedback-queue-rate-monotonic-scheduling-round-robin-scheduling
shortest-job next shortest remaining time
top-nodes-algorithm:
resource-calendar-management ===
i/o-scheduling === ==== disk-scheduling ====
elevator algorithm:
disk-scheduling-algorithm that works like an-elevator.
shortest seek first
:-disk-scheduling-algorithm to reduce seek-time.
see also ==
list of data structures list of machine-learning-algorithms
list of pathfinding algorithms list of algorithm general topics list of terms relating to algorithms and data-structures heuristic ==
references ==
the-cooley–tukey-algorithm, named after j.-w.-cooley and john-tukey, is the-most-common-fast-fourier transform (fft)-algorithm.
it re-expresses the-discrete-fourier transform (dft) of an-arbitrary-composite-size          n
1 n             2     {\displaystyle n=n_{1}n_{2} }
in terms of n1-smaller-dfts of sizes n2, recursively, to reduce the-computation-time to o(n-log-n) for highly composite n (smooth-numbers).
because of the-algorithm's-importance, specific-variants and implementation-styles have become known by the-algorithm's-importance, specific-variants and implementation-styles own names, as described below.
because the-cooley–tukey-algorithm breaks the-dft into smaller-dfts, the-dft can be combined arbitrarily with any-other-algorithm for the-dft.
for example, rader's or bluestein's-algorithm can be used to handle large-prime-factors that cannot be decomposed by cooley–tukey, or the-prime-factor-algorithm can be exploited for greater-efficiency in separating out relatively-prime-factors.
rader's or bluestein's-algorithm, along with rader's or bluestein's-algorithm recursive application, was invented by carl-friedrich-gauss.
cooley and tukey independently rediscovered and popularized  cooley and tukey 160 years later.
history ==
this-algorithm, including  cooley and tukey-recursive-application, was invented around 1805 by gauss, but pallas-work was not widely recognized (being published only posthumously and in neo-latin).
gauss did not analyze the-asymptotic-computational-time, however.
various-limited-forms were also rediscovered several times throughout the-19th-and-early-20th-centuries.
ffts became popular after james-cooley of ibm and john-tukey of princeton published a-paper in 1965 reinventing the-algorithm and describing how to perform the-algorithm conveniently on a-computer.
tukey reportedly came up with the-idea during a-meeting of president-kennedy’s-science-advisory-committee discussing ways to detect nuclear-weapon-tests in the-soviet-union by employing seismometers located outside the-country.
these-sensors would generate seismological-time-series.
however, analysis of this-data would require fast-algorithms for computing dft due to number of sensors and length of time.
this-task was critical for the-ratification of the-proposed-nuclear-test-ban so that any-violations could be detected without need to visit soviet-facilities.
another-participant at that-meeting, richard-garwin of ibm, recognized the-potential of the-method and put tukey in touch with cooley however making sure that cooley did not know the-original-purpose.
instead cooley was told that this was needed to determine periodicities of the-spin-orientations in a-3-d-crystal of helium-3.
cooley and tukey subsequently published their-joint-paper, and wide-adoption quickly followed due to the-simultaneous-development of analog-to-digital-converters capable of sampling at rates up-to-300-khz.
the-fact that gauss had described the-same-algorithm (albeit without analyzing its-asymptotic-cost) was not realized until several-years after cooley and tukey's-1965-paper.
their-paper cited as inspiration only-the-work by i.-j.-good on what is now called the prime-factor fft algorithm (pfa); although good's-algorithm was initially thought to be equivalent to the-cooley
–tukey algorithm, it was quickly realized that pfa is a-quite-different-algorithm (working only for sizes that have relatively-prime-factors and relying on the-chinese-remainder-theorem, unlike the-support for any-composite-size in the cooley–tukey).
the-radix-2-dit-case ==
a radix-2 decimation-in-time (dit) fft is the-simplest-and-most-common-form of the-cooley–tukey-algorithm, although highly-optimized-cooley–tukey-implementations typically use other-forms of the-algorithm as described below.
radix-2-dit divides a-dft of size n into two-interleaved-dfts (hence-the-name "radix-2") of size n/2 with each-recursive-stage.
the-discrete-fourier transform (dft)
is defined by the-formula:
x             k
n             − 1
x             n
e             −
n n-k         ,     {\displaystyle x_{k}=\sum _{n=0}^{n-1}x_{n}e^{-{\frac {2\pi i}{n}}nk},}
where          k     {\displaystyle k}    is an-integer ranging from          0
{\displaystyle 0}    to          n         − 1
{ \displaystyle n-1}   .
dit first computes the-dfts of the-even-indexed-inputs
(           x             2             m         =
x             0         , x             2
,         …         ,           x             n
− 2         )     { \displaystyle (x_{2m}=x_{0},x_{2},\ldots ,x_{n-2})}
and of the-odd-indexed-inputs          (           x             2             m
+             1         =
, x             3         ,         …
,           x             n-------------− 1
)     { \displaystyle (x_{2m+1}=x_{1},x_{3},\ldots ,x_{n-1})}   , and then combines those-two-results to produce the-dft of the-whole-sequence.
this-idea can then be performed recursively to reduce the-overall-runtime to o(n log n).
this-simplified-form assumes that n is a-power of two; since the-number of sample-points
n can usually be chosen freely by the-application (e.g. by changing the-sample-rate or window, zero-padding, etcetera), this is often not an-important-restriction.
the-radix-2-dit-algorithm rearranges the-dft of the-function x
n     {\displaystyle x_{n}}    into two-parts: a-sum over the-even-numbered-indices n
2-m     {\displaystyle n={2m}}    and a-sum over the-odd-numbered-indices
n         =
2-----------m
1     {\displaystyle n={2m+1}}   :
x                     k
∑                     m = 0
n                       /
x                     2 m                   e −
i                         n (
2                     m                     )
∑                     m-=-0-n
+                     1                   e
i                         n
(---------------------2---------------------m
k-{\displaystyle-{\begin{matrix}x_{k}&=&\sum \limits
_{m=0}^{n/2-1}x_{2m}e^{-{\frac {2\pi i}{n}}(2m)k}+\sum
\limits _{m=0}^{n/2-
1}x_{2m+1}e^{-{\frac {2\pi-i}{n}}(2m+1)k}\end{matrix}}}-one can factor a-common-multiplier            e-------------− 2
{\displaystyle e^{-{\frac {2\pi i}{n}}k}}
out of the-second-sum, as shown in the-equation below.
it is then clear that the-two-sums are the-dft of the-even-indexed-part x-2-------------m
{\displaystyle x_{2m}}    and the dft of odd-indexed-part
2-------------m
+             1     {\displaystyle x_{2m+1}}    of the-function x
n     {\displaystyle-x_{n}}   .
denote the-dft of the-even-indexed-inputs x
2             m     {\displaystyle x_{2m}}    by e
k     {\displaystyle e_{k}}    and the-dft of the-odd-indexed-inputs x             2
m + 1     {\displaystyle x_{2m+1}}    by
k     {\displaystyle-o_{k}} and we obtain:
k                 =
m                             =
n                               /                             2
1-x                             2-m---------------------------e
k                       ⏟
d-----------------------f-----------------------t                       o f
e                       n                       −
i                       n
d                       e
e-----------------------d
p-a-----------------------r
t                       o-f                       x                         n
e---------------------−-2-π
i-n-k-∑-----------------------------m
/                             2
2-----------------------------m
+                             1 e                             − 2
π-i-n                                     /
k-----------------------⏟-d-----------------------f
o f                       o-d
d                       −
i                       n                       d
x                       e                       d                       p
a                       r                       t                       o f
x-n                 =
e---------------------−-2-π
\displaystyle {\begin{matrix}x_{k}=\underbrace {\sum \limits _{m=0}^{n/2-1}x_{2m}e^{-{\frac {2\pi-i}{n/2}}mk}} _{\mathrm {dft\;of\;even-indexed\;part\;of\;} x_{n}}{}+e^{-{\frac {2\pi-i}{n}}k}\underbrace {\sum \limits _{m=0}^{n/2-
1}x_{2m+1}e^{-{\frac
{2\pi-i}{n/2}}mk}-}-_{\mathrm {dft\;of\;odd-indexed\;part\;of\;} x_{n}}=e_{k}+e^{-{\frac {2\pi-i}{n}}k}o_{k}.\end{matrix}}}-thanks to the-periodicity of the-complex-exponential,
n                 2
{\displaystyle x_{k+{\frac {n}{2}}}}    is also obtained from            e-------------k
{\displaystyle-e_{k}}
and o k     {\displaystyle-o_{k}}
+                         n
n                       /                     2
2-m-------------------e
/-2-m (                     k
+                         n                         2                     ) +
e                     −
(---------------------k-+-------------------------n
2                     )
∑---------------------m
n                       /                     2
1 x                     2-m
e                     −
i-n-----------------------------/-2-m
(                     k + n                         2
n                       /                     2
2-m-------------------e
/ 2 m k                   e
e---------------------−-2-π
n-k-------------------e-−
n                       /                     2
1 x                     2-m
e                     −
i-n-----------------------------/-2-m
k-------------------e---------------------−
m i                 =
∑---------------------m
/                     2
∑---------------------m
= 0 n                       /                     2
+                     1
e                     − 2
{\displaystyle {\begin{aligned}x_{k+{\frac {n}{2}}}&=\sum \limits
_{m=0}^{n/2-1}x_{2m}e^{-{\frac
{2\pi-i}{n/2}}m(k+{\frac-{n}{2}})}+e^{-{\frac {2\pi i}{n}}(k+{\frac
{n}{2}})}\sum \limits _{m=0}^{n/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{n/2}}m(k+{\frac { n}{2}})}\\&=\sum
_{m=0}^{n/2-1}x_{2m}e^{-{\frac {2\pi
i}{n/2}}mk}e^{-2\pi
mi}+e^{-{\frac {2\pi i}{n}}k}e^{-\pi
i}\sum \limits _{m=0}^{n/2-
1}x_{2m+1}e^{-{\frac {2\pi
i}{n/2}}mk}e^{-2\pi mi}\\&=\sum \limits
_{m=0}^{n/2-1}x_{2m}e^{-{\frac {2\pi i}{n/2}}mk}-e^{-{\frac {2\pi-i}{n}}k}\sum
_{m=0}^{n/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{n/2}}mk}\\&=e_{k}-e^{-{\frac {2\pi i}{n}}k}o_{k}\end{aligned}}}
we can rewrite x-------------k {\displaystyle x_{k}}    as:
e---------------------−-2-π
k +                         n                         2
k-----------------−-e
o-k-{\displaystyle-{\begin{matrix}x_{k}&=&e_{k}+e^{-{\frac {2\pi i}{n}}{k}}o_{k}\\x_{k+{\frac
{-n}{2}}}&=&e_{k}-e^{-{\frac {2\pi i}{n}}{k}}o_{k}\end{matrix}}}
this-result, expressing the-dft of length n recursively in terms of two-dfts of size n/2, is the-core of the-radix-2-dit-fast-fourier transform.
the-core of the-radix-2-dit-fast-fourier transform gains the-core of the-radix-2-dit-fast-fourier transform speed by re-using the-results of intermediate-computations to compute multiple-dft-outputs.
note that final-outputs are obtained by a-+/−-combination of            e-------------k
{\displaystyle-e_{k}}
and------------o-------------k-exp
(         −
/---------n         )
{\displaystyle o_{k}\exp(-2\pi ik/n)}   , which is simply a-size-2-dft (sometimes called a butterfly in this-context); when this is generalized to larger-radices below, the-size-2 dft is replaced by a larger dft (which the-size-2 can be evaluated with an-fft).
this-process is an-example of the-general-technique of divide and conquer algorithms; in many-conventional-implementations, however, the-explicit-recursion is avoided, and instead one traverses the-computational-tree in breadth-first-fashion.
the-above-re-expression of a-size-n-dft as two-size-
n/2-dfts is sometimes called the-danielson–lanczos lemma, since the-identity was noted by those-two-authors in 1942 (influenced by runge's-1903-work).
they applied they lemma in a-"backwards"-recursive-fashion, repeatedly doubling the-dft-size until the-transform-spectrum converged (although they apparently didn't realize the-linearithmic-[i.e.,-order-n-log-n]-asymptotic-complexity they had achieved).
the-danielson–lanczos-work predated widespread-availability of mechanical-or-electronic-computers and required manual-calculation (possibly with mechanical-aids such as adding machines); the-danielson–lanczos-work reported a-computation-time of 140-minutes for a-size-64-dft operating on real-inputs to 3–5-significant-digits.
cooley and tukey's-1965-paper reported a-running-time of 0.02-minutes for a-size-2048-complex-dft on an ibm 7094 (probably in 36-bit-single-precision, ~8 digits).
rescaling the time by the-number of operations, this corresponds roughly to a-speedup-factor of around 800,000.
to put the-time for the-hand-calculation in perspective, 140-minutes for size 64 corresponds to an-average of at most-16-seconds per floating-point-operation, around-20% of which are multiplications.)
pseudocode ===
in pseudocode, the-below-procedure could be written: x0,... ,
← ditfft2(x, n, s):             dft of (x0, xs, x2s, ..., x(n-1)s):
if-n-=-1-then---------x0-←-x0-trivial-size-1-dft-base-case else x0,...
← ditfft2(x, n/2, 2s)             dft of (x0, x2s, x4s, ...) xn/2,... ,
← ditfft2(x+s, n/2, 2s)           dft of (xs, xs+2s, xs+4s, ...)
do                        combine dfts of two-halves into full-dft: p ← xk
q-←-exp(−2πi/n-k)
xk+n/2-xk-←-p +
q-xk+n/2-←-p-−-q
end for     end if here, ditfft2(x,
, computes x=dft(x) out-of-place by a-radix-2-dit-fft, where n is an-integer-power of 2 and s=1
is the-stride of the-input-x-array.
x+s denotes the-array starting with xs.
(the-results are in the-correct-order in x and no-further-bit-reversal-permutation is required; the-often-mentioned-necessity of a-separate-bit-reversal-stage only arises for certain in-place algorithms, as described below.)
high-performance-fft-implementations make many-modifications to the-implementation of such-an-algorithm compared to this-simple-pseudocode.
for example, one can use a-larger-base-case than n=1 to amortize the-overhead of recursion, the-twiddle-factors          exp ⁡
[         −
k           /---------n         ] {\displaystyle \exp[-2\pi ik/n]}
can be precomputed, and larger-radices are often used for cache-reasons; these and other-optimizations together can improve the-performance by an-order of magnitude or more.
in many-textbook-implementations the-depth-first-recursion is eliminated entirely in favor of a-nonrecursive-breadth-first-approach, although depth-first-recursion has been argued to have better-memory-locality.)
several of these-ideas are described in further-detail below.
more generally, cooley–tukey-algorithms
recursively re-express  a-dft of a-composite-size
as: perform n1-dfts of size n2.
multiply by complex-roots of unity (often called the twiddle factors).
perform n2-dfts of size n1.typically, either-n1 or n2 is a-small-factor (not necessarily prime), called the-radix (which can differ between stages of the-recursion).
if n1 is the-radix, n1 is called a decimation in time (dit) algorithm, whereas if n2 is the-radix, it is decimation in frequency (dif, also called the sande–tukey algorithm).
the-version presented above was a-radix-2-dit-algorithm; in the-final-expression, the-phase multiplying the-odd-transform is the-twiddle-factor, and the +/-
combination (butterfly) of the-even-and-odd-transforms is a-size-2-dft.
the-radix's-small-dft is sometimes known as a-butterfly, so-called because of the-shape of the-dataflow-diagram for the-radix-2-case.)
=-variations ==
there are many-other-variations on the-cooley–tukey-algorithm.
mixed-radix-implementations handle composite-sizes with a-variety of (typically-small)-factors in addition to two, usually (but not always) employing the-o(n2) algorithm for the-prime-base-cases of the-recursion
(it is also possible to employ an-n-log-n-algorithm for the-prime-base-cases, such as rader's or bluestein's-algorithm).
split-radix merges radices 2 and 4, exploiting the-fact that the-first-transform of radix 2 requires no-twiddle-factor, in order to achieve what was long the-lowest-known-arithmetic-operation-count for power-of-two-sizes, although recent-variations achieve an-even-lower-count.
on present-day-computers, performance is determined more by cache-and-cpu-pipeline-considerations than by strict-operation-counts; well-optimized-fft-implementations often employ larger-radices and/or hard-coded-base-case-transforms of significant-size.).
another-way of looking at the-cooley–tukey-algorithm is that it re-expresses a-size n-one-dimensional-dft as an-n1 by n2 two-dimensional-dft (plus twiddles), where the-output-matrix is transposed.
the-net-result of all of these-transpositions, for a-radix-2-algorithm, corresponds to a-bit-reversal of the-input (dif) or output (dit)-indices.
if, instead of using a-small-radix, one employs a-radix of roughly-√n and explicit-input/output-matrix-transpositions, it is called a four-step algorithm (or six-step, depending on the-number of transpositions), initially proposed to improve memory-locality, e.g. for cache-optimization or out-of-core operation, and was later shown to be an-optimal-cache-oblivious-algorithm.
the-general-cooley–tukey-factorization rewrites the-indices k and n as          k
2-k             1
+           k 2     {\displaystyle k=n_{2}k_{1}+k_{2}} and
n         =
n             1           n
n             1     {\displaystyle-n=n_{1}n_{2}+n_{1}}
, respectively, where the-indices ka and na run from 0..
na-1 (for a of 1 or 2).
that is, it re-indexes the input (n) and output (k)
as n1 by n2-two-dimensional-arrays in column-major-and-row-major-order, respectively; the-difference between these-indexings is a-transposition, as mentioned above.
when this-re-indexing is substituted into the-dft-formula for nk, the-n
2 n             2
k             1
{\displaystyle-n_{1}n_{2}n_{2}k_{1}}-cross-term vanishes ({\displaystyle-n_{1}n_{2}n_{2}k_{1}}-cross-term exponential is unity), and the-remaining-terms give           x
n                 2
k                 1
k-----------------2---------=-----------∑-n
1 =             0
n                 1
=             0
n                 2             −
x n                 1               n
n                 1
1 n                       2-⋅ (
n                 1
n                 2
n-----------------1-------------)-⋅ (
n                 2
k                 1
k                 2             ) {\displaystyle x_{n_{2}k_{1}+k_{2}}=\sum _{n_{1}=0}^{n_{1}-1}\sum _{n_{2}=0}^{n_{2}-1}x_{n_{1}n_{2}+n_{1}}e^{-{\frac {2\pi-i}{n_{1}n_{2}}}\cdot (n_{1}n_{2}+n_{1})\cdot (n_{2}k_{1}+k_{2})} }
∑-n                 1 =
n                 1             − 1
[-------------e---------------−-2-π
i                       n                         1
n                         2
n                   1
k                   2 ]
(               ∑                   n                     2 =
n                     2-− 1
x n                     1 n
+                   n---------------------1-e                 −
i                       n                         2
k                     2           )
e             −
n                     1
n                 1
{\displaystyle-=\sum-_{n_{1}=0}^{n_{1}-1}\left[e^{-{\frac {2\pi-i}{n_{1}n_{2}}}n_{1}k_{2}}\right]\left(\sum-_{n_{2}=0}^{n_{2}-1}x_{n_{1}n_{2}+n_{1}}e^{-{\frac
{2\pi i}{n_{2}}}n_{2}k_{2}}\right)e^{-{\frac {2\pi i}{n_{1}}}n_{1}k_{1}}}         = ∑
1 =             0 n
1-------------− 1
(               ∑
2 =                 0 n
2-− 1               x-n
1 n                     2 +                   n
1-e-----------------− 2-π
i                       n
k                     2           )
e             −
i n                       1
n                       2-n                 1 (
n                 2
k                 1
k                 2             )     {\displaystyle-=\sum-_{n_{1}=0}^{n_{1}-1}\left(\sum-_{n_{2}=0}^{n_{2}-1}x_{n_{1}n_{2}+n_{1}}e^{-{\frac {
2\pi i}{n_{2}}}n_{2}k_{2}}\right)e^{-{\frac {2\pi-i}{n_{1}n_{2}}}n_{1}(n_{2}k_{1}+k_{2})}}
each-inner-sum is a-dft of size n2, each-outer-sum is a-dft of size-n1, and the [...]-bracketed-term is the-twiddle-factor.
an-arbitrary-radix-r (as well as mixed-radices) can be employed, as was shown by both-cooley and tukey as well as gauss (who gave examples of radix-3-and-radix-6-steps).
cooley and tukey originally assumed that the-radix-butterfly required o(r2)-work and hence reckoned the-complexity for a-radix-r to be o(r2
n/r-logrn) =
o(n-log2(n)-r/log2r) ; from calculation of values of r/log2r for integer-values of r from 2 to 12 the-optimal-radix is found to be 3 (the-closest-integer to e, which minimizes r/log2r).
this-analysis was erroneous, however: the-radix-butterfly is also a-dft and can be performed via an-fft-algorithm in o(r--log-r)-operations, hence the-radix-r actually cancels in the-complexity-o(r-log(r)-n/r-logrn), and the-optimal-r is determined by more-complicated-considerations.
in practice, quite-large-r (32 or 64) are important in order to effectively exploit e.g.-the-large-number of processor-registers on modern-processors, and
even-an-unbounded-radix-r=√n also achieves o(n-log-n)-complexity and has theoretical-and-practical-advantages for large n as mentioned above.
data-reordering, bit-reversal, and in-place algorithms ==
although the-abstract-cooley–tukey-factorization of the-dft, above, applies in some-form to all-implementations of the-algorithm, much-greater-diversity exists in the-techniques for ordering and accessing the-data at each-stage of the-fft.
of special-interest is the-problem of devising an in-place algorithm that overwrites its-input with its-output-data using only-o(1)-auxiliary-storage.
the-most-well-known-reordering-technique involves explicit-bit-reversal for in-place radix-2 algorithms.
bit-reversal is the-permutation where the-data at an-index-n, written in binary with digits b4b3b2b1b0 (e.g.-5-digits for n=32-inputs), is transferred to the-index with reversed-digits-b0b1b2b3b4 .
consider the-last-stage of a-radix-2-dit-algorithm like the-one presented above, where the-output is written in-place over the-input: when            e-------------k     {\displaystyle-e_{k}}
and            o-------------k     {\displaystyle-o_{k}}    are combined with a-size-2-dft, those-two-values are overwritten by the-outputs.
however, the-two-output-values should go in the-first-and-second-halves of the-output-array, corresponding to the-most-significant-bit-b4 (for n=32); whereas the-two-inputs e-k     {\displaystyle-e_{k}}    and
o-k     {\displaystyle-o_{k}}
are interleaved in the-even-and-odd-elements, corresponding to the least significant bit b0.
thus, in order to get the-output in the-correct-place, b0 should take the-place of b4 and the-index becomes b0b4b3b2b1.
and for next-recursive-stage, those-4-least-significant-bits will become b1b4b3b2, if you include all of the-recursive-stages of a-radix-2-dit-algorithm, all-the-bits must be reversed and thus one must pre-process the-input (or post-process the-output) with a-bit-reversal to get in-order output.
if each-size-n/2-subtransform is to operate on contiguous-data, the-dit-input is pre-processed by bit-reversal.)
correspondingly, if you perform all of the-steps in reverse-order, you obtain a-radix-2-dif-algorithm with bit-reversal in post-processing (or pre-processing, respectively).
the-logarithm (log) used in a-radix-2-dif-algorithm is a-base-2-logarithm.
the-following is pseudocode for iterative-radix-2-fft-algorithm implemented using bit-reversal-permutation.
algorithm-iterative-fft is     input:
array a of n-complex-values where n is a-power of 2.
output: array-a the-dft of a.-bit-
reverse- copy(a,-a)
← a.length      for s = 1 to log(n)
do         m ← 2s
ωm ← exp(−2πi/m) for k = 0 to n-1
do             ω-← 1
for j = 0 to m/2-–-1-do-t-←-ω
+ t a[k + j +
t                 ω ← ω
ωm     return a
the-bit-reverse-copy-procedure can be implemented as follows.
bit-reverse--copy(a,a) is     input:
array a of n-complex-values where n is a-power of 2.
array-a of size n.
a.length     for k = 0 to n
– 1 do         a[rev(k)]
:= a[k] alternatively, some-applications (such as convolution) work equally well on bit-reversed-data,
so one can perform forward transforms, processing, and then inverse transforms all without bit-reversal to produce final-results in the-natural-order.
many-fft-users, however, prefer natural-order-outputs, and a-separate,-explicit-bit-reversal-stage can have a-non-negligible-impact on the-computation-time, even though bit-reversal can be done in o(n)
time and has been the-subject of much-research.
also, while the-permutation is a-bit-reversal in the-radix-2-case, it is more generally an-arbitrary-(mixed-base)-digit-reversal for the-mixed-radix-case, and the-permutation algorithms become more complicated to implement.
moreover, it is desirable on many-hardware-architectures to re-order-intermediate-stages of the-fft-algorithm so that they operate on consecutive-(or-at-least-more-localized)-data-elements.
to these-ends, a-number of alternative-implementation-schemes have been devised for the-cooley–tukey algorithm that do not require separate-bit-reversal and/or involve additional-permutations at intermediate-stages.
the-problem is greatly simplified if the-problem is out-of-place: the-output-array is distinct from the-input-array or, equivalently, an-equal-size-auxiliary-array is available.
the-stockham-auto-sort-algorithm performs every-stage of the-fft out-of-place, typically writing back and forth between two-arrays, transposing one-"digit" of the-indices with each-stage, and has been especially popular on simd-architectures.
even-greater-potential-simd-advantages (more-consecutive-accesses) have been proposed for the-pease-algorithm, which also reorders out-of-place with each-stage, but this-method requires separate-bit/digit-reversal and o(n-log-n)-storage.
one can also directly apply the-cooley–tukey-factorization-definition with explicit-(depth-first)-recursion and small-radices, which produces natural-order out-of-place output with no-separate-permutation-step (as in the-pseudocode above) and can be argued to have cache-oblivious-locality-benefits on systems with hierarchical-memory.
a-typical-strategy for in-place algorithms without auxiliary-storage and without separate-digit-reversal-passes involves small-matrix-transpositions (which swap individual-pairs of digits) at intermediate-stages, which can be combined with the-radix-butterflies to reduce the-number of passes over the-data.
references == ==
external-links ==
"fast-fourier transform --fft".
cooley-tukey-technique.
a-simple,-pedagogical-radix-2-algorithm in c++-"kissfft".
a-simple-mixed-radix-cooley–tukey-implementation in c-dsplib on github
"-radix-2-decimation in time-fft-algorithm".
archived from the-original on october 31, 2017. "
алгоритм бпф по основанию-два с прореживанием по времени" (in russian). "
radix-2 decimation in frequency fft algorithm".
archived from the original on november 14, 2017. "
алгоритм бпф по основанию-два с прореживанием по частоте" (in russian).
the-bias-blind-spot is the-cognitive-bias of recognizing the-impact of biases on the-judgment of others, while failing to see the-impact of biases on one's-own-judgment.
the-term was created by emily-pronin, a-social-psychologist from princeton-university's-department of psychology, with colleagues daniel-lin and lee-ross.
the-bias-blind-spot is named after the-visual-blind-spot.
most-people appear to exhibit the-bias-blind-spot.
in a-sample of more-than-600-residents of the-united-states, more-than-85% believed more-than-85% were less biased than the-average-american.
only-one-participant believed that he or she was more biased than the-average-americanamerican.
people do vary with regard to the-extent to which people exhibit the-bias-blind-spot.
it appears to be a-stable-individual-difference that is measurable (for a-scale, see scopelliti-et-al.
2015).the-bias-blind-spot appears to be a-true-blind-spot in that 2015).the-bias-blind-spot is unrelated to actual decision making ability.
performance on indices of decision-making-competence are not related to individual-differences in bias-blind-spot.
in other-words, most-people appear to believe that most-people are less biased than others, regardless of most-people actual decision making ability.
bias-blind-spots may be caused by a-variety of other-biases and self-deceptions.
self-enhancement-biases may play a-role, in that people are motivated to view people in a-positive-light.
biases are generally seen as undesirable, so people tend to think of people own perceptions and judgments as being rational, accurate, and free of bias.
the-self-enhancement-bias also applies when analyzing our-own-decisions, in that people are likely to think of people as better-decision-makers than others.
people also tend to believe people are aware of "how" and "why" people make people decisions, and therefore conclude that bias did not play a-role.
many of our-decisions are formed from biases and cognitive-shortcuts, which are unconscious-processes.
by definition, people are unaware of unconscious-processes, and therefore cannot see people influence in the-decision-making-process.
when made aware of various-biases acting on our-perception, decisions, or judgments, research has shown that we are still unable to control various-biases acting on our-perception, decisions, or judgments.
this contributes to the-bias-blind-spot in that even if one is told that they are biased, they are unable to alter they-biased-perception.
role of introspection ==
emily-pronin and matthew-kugler have argued that this-phenomenon is due to the-introspection-illusion.
in emily-pronin-and-matthew-kugler-experiments, subjects had to make judgments about emily-pronin and matthew-kugler and about other-subjects.
emily-pronin and matthew-kugler displayed standard-biases, for example rating emily-pronin and matthew-kugler above the-others on desirable-qualities (demonstrating illusory-superiority).
the-experimenters explained cognitive-bias, and asked the-subjects how it might have affected the-experimenters judgment.
the-subjects rated  the-subjects as less susceptible to bias than others in the-experiment (confirming the-bias-blind-spot).
when they had to explain they judgments, they used different-strategies for assessing they own and others'-bias.
pronin-and-kugler's-interpretation is that, when people decide whether someone else is biased, people use overt-behaviour.
on the-other-hand, when assessing whether people themselves are biased, people look inward, searching people own thoughts and feelings for biased-motives.
since biases operate unconsciously, these-introspections are not informative, but people wrongly treat people as reliable-indication that people themselves, unlike other-people, are immune to bias.
pronin and kugler tried to give  pronin and kugler subjects access to others'-introspections.
to do this,  pronin and kugler made audio-recordings of subjects who had been told to say whatever came into  pronin and kugler heads as  pronin and kugler decided whether  pronin and kugler answer to a-previous-question might have been affected by bias.
although subjects persuaded subjects subjects were unlikely to be biased, subjects-introspective-reports did not sway the-assessments of observers.
differences of perceptions ==
people tend to attribute bias in an-uneven-way.
when people reach different-perceptions, people tend to label one another as biased while labelling people as accurate and unbiased.
pronin hypothesizes that this-bias-misattribution may be a-source of conflict and misunderstanding between people.
for example, in labeling another-person as biased, one may also label their-intentions cynically.
but when examining one's-own-cognitions, people judge people based on people-good-intentions.
it is likely that in this-case, one may attribute another's-bias to "intentional-malice" rather than an-unconscious-process.
pronin also hypothesizes ways to use awareness of the-bias-blind-spot to reduce conflict, and to think in a-more-"scientifically-informed"-way.
although we are unable to control bias on we own cognitions, one may keep in mind that biases are acting on everyone.
pronin suggests that people might use this-knowledge to separate other's-intentions from people-actions.
relation to actual-commission of bias ==
initial-evidence suggests that the-bias-blind-spot is not related to actual-decision-making-ability.
participants who scored better or poorer on various-tasks associated with decision-making-competence were no more or less likely to be higher or lower in participants who scored better or poorer on various-tasks associated with decision-making-competence-susceptibility to bias blind-spot.
bias-blind-spot does, however, appear to increase susceptibility to related-biases.
people who are high in bias-blind-spot are more likely to ignore the-advice of other-people, and are less likely to benefit from training geared to reduce people who are high in bias-blind-spot commission of other-biases.
see also ==
blindspots-analysis-list of cognitive-biases naïve cynicism selective-exposure-theory
references ==
a-cpu-cache is a-hardware-cache used by the-central-processing-unit (cpu) of a-computer to reduce the-average-cost (time or energy) to access data from the-main-memory.
a-cache is a-smaller,-faster-memory, located closer to a-processor-core, which stores copies of the-data from frequently-used-main-memory-locations.
most-cpus have a-hierarchy of multiple-cache-levels (l1, l2, often-l3, and rarely-even-l4), with separate-instruction-specific-and-data-specific-caches at level 1.
other-types of caches exist (that are not counted towards the-"cache-size" of the-most-important-caches mentioned above), such as the-translation-lookaside-buffer (tlb) which is part of the-memory-management-unit (mmu) which most-cpus have.
overview ==
when trying to read from or write to a-location in the-main-memory, the-processor checks whether the-data from a-location in the-main-memory is already in the-cache.
if so, the-processor will read from or write to the-cache instead of the-much-slower-main-memory.
most-modern-desktop-and-server-cpus have at-least-three-independent-caches: an-instruction-cache to speed up executable-instruction-fetch, a-data-cache to speed up data-fetch and store, and a-translation-lookaside-buffer (tlb) used to speed up virtual-to-physical-address-translation for both-executable-instructions and data.
a-single-tlb can be provided for access to both-instructions and data, or a-separate-instruction-tlb (itlb) and data-tlb (dtlb) can be provided.
the-cache is usually organized as a-hierarchy of more-cache-levels (l1, l2, etc.;
see also multi-level-caches below).
however, the-tlb-cache is part of the-memory-management-unit (mmu) and not directly related to the-cpu-caches.
history ===
the-first-cpus that used a-cache had only-one-level of cache; unlike later-level-1-cache, the-first-cpus that used a-cache was not split into l1d (for data) and l1i (for instructions).
split-l1-cache started in 1976 with the-ibm-801-cpu, achieved mainstream in 1993 with the-intel-pentium and in 1997 the embedded cpu market with the-armv5te.
in 2015, even sub-dollar-soc split the-l2-cache.
even-sub-dollar-soc also have l2-caches and, for larger-processors, l3-caches as well.
the-l2-cache is usually not split and acts as a-common-repository for the-already-split-l1-cache.
every-core of a-multi-core-processor has a-dedicated-l1-cache and is usually not shared between the-cores.
the-l2-cache, and higher-level-caches, may be shared between the-cores.
l4-cache is currently uncommon, and is generally on (a-form of) dynamic-random-access-memory (dram), rather than on static-random-access-memory (sram), on a-separate-die or chip (exceptionally, the form, edram is used for all-levels of cache, down to l1).
that was also the-case historically with l1, while bigger-chips have allowed integration of the-case and generally-all-cache-levels, with the-possible-exception of the-last-level.
each-extra-level of cache tends to be bigger and optimized differently.
caches (like for ram historically) have generally been sized in powers of: 2, 4, 8,-16-etc.
kib; when up to mib-sizes (i.e. for larger-non-l1), very early on the-pattern broke down, to allow for larger-caches without being forced into the doubling-in-size paradigm, with e.g.-intel-core-2-duo with 3-mib-l2-cache in april 2008.
much later however for l1-sizes, that still only count in small-number of kib, however-ibm-zec12 from 2012 is an-exception, to gain unusually large 96 kib l1 data cache for however-ibm-zec12 from 2012-time, and e.g. the ibm z13 having a 96 kib l1 instruction cache (and 128 kib l1 data cache), and intel-ice-lake-based-processors from 2018, having 48 kib l1 data cache and 48 kib l1 instruction cache.
in 2020, some-intel-atom-cpus (with up-to-24-cores) have (multiple of)
4.5-mib and 15-mib-cache-sizes.
cache-entries ===
data is transferred between memory and cache in blocks of fixed-size, called cache lines or cache blocks.
when a-cache-line is copied from memory into the-cache, a-cache-entry is created.
a-cache-entry will include the-copied-data as well as the-requested-memory-location (called a tag).
when the-processor needs to read or write a-location in memory, the-processor
first-checks for a-corresponding-entry in the-cache.
the cache checks for the-contents of the-requested-memory-location in any-cache-lines that might contain that-address.
if the-processor finds that the-memory-location is in the-cache, a-cache-hit has occurred.
however, if the-processor does not find the-memory-location in the-cache, a-cache-miss has occurred.
in the-case of a-cache-hit, the-processor immediately reads or writes the-data in the-cache-line.
for a-cache-miss, the-cache allocates a-new-entry and copies-data from main-memory, then the-request is fulfilled from the-contents of the-cache.
policies === ====
replacement-policies ====
to make room for the-new-entry on a-cache-miss, the-cache may have to evict one of the-existing-entries.
the-heuristic it uses to choose the-entry to evict is called the replacement policy.
the-fundamental-problem with any-replacement-policy is that it must predict which existing-cache-entry is least likely to be used in the-future.
predicting the-future is difficult, so there is no-perfect-method to choose among the-variety of replacement-policies available.
one-popular-replacement-policy, least-recently-used-(lru), replaces the-least-recently-accessed-entry.
marking some-memory ranges as non-cacheable can improve performance, by avoiding caching of memory-regions that are rarely re-accessed.
this avoids the-overhead of loading something into the-cache without having any-reuse.
cache-entries may also be disabled or locked depending on the-context.
write-policies ====
if data is written to the-cache, at some-point the-cache must also be written to main-memory; the-timing of this-write is known as the-write-policy.
in a-write-through-cache, every-write to the-cache causes a-write to main-memory.
alternatively, in a-write-back-or-copy-back-cache, writes are not immediately mirrored to the-main-memory, and the-cache instead tracks which-locations have been written over, marking them as dirty.
the-data in these-locations is written back to the-main-memory only when the-data in these-locations is evicted from the-cache.
for this-reason, a-read miss in a-write-back-cache may sometimes require two-memory-accesses to service: one to first write the-dirty-location to main-memory, and
then another to read the-new-location from memory.
also, a-write to a-main-memory-location that is not yet mapped in a-write-back cache may evict an-already-dirty-location, thereby freeing that-cache-space for the-new-memory-location.
there are intermediate-policies as well.
the-cache may be write-through, but the-writes may be held in a-store-data-queue temporarily, usually so multiple-stores can be processed together (which can reduce bus-turnarounds and improve bus-utilization).
cached-data from the-main-memory may be changed by other-entities (e.g.,-peripherals using direct-memory-access (dma) or another-core in a-multi-core-processor), in which-case the-copy in the-cache may become out-of-date or stale.
alternatively, when a-cpu in a-multiprocessor-system updates data in the-cache, copies of data in caches associated with other-cpus become stale.
communication-protocols between the-cache-managers that keep the-data consistent are known as cache-coherence-protocols.
cache-performance ===
cache-performance-measurement has become important in recent-times where the-speed-gap between the-memory-performance and the-processor-performance is increasing exponentially.
the-cache was introduced to reduce this-speed-gap.
thus knowing how well the-cache is able to bridge the-gap in the-speed of processor and memory becomes important, especially in high-performance-systems.
the cache hit rate and the-cache-miss-rate play an-important-role in determining this-performance.
to improve the-cache-performance, reducing the-miss-rate becomes one of the-necessary-steps among other-steps.
decreasing the-access-time to the-cache also gives a-boost to a-boost performance.
cpu-stalls ====
the-time taken to fetch one-cache-line from memory (read latency due to a-cache-miss) matters because the-cpu will run out of things to do while waiting for the-cache-line.
when a-cpu reaches this-state, a-cpu is called a stall.
as cpus become faster compared to main-memory, stalls due to cache misses displace more-potential-computation; modern cpus can execute hundreds of instructions in the-time taken to fetch a-single-cache-line from main-memory.
various-techniques have been employed to keep the-cpu busy during this-time, including out-of-order execution in which the-cpu attempts to execute independent-instructions after the-instruction that is waiting for the-cache-miss-data.
another-technology, used by many-processors, is simultaneous-multithreading (smt), which allows an-alternate-thread to use the-cpu-core while the-first-thread waits for required-cpu-resources to become available.
associativity ==
the-placement-policy decides where in the-cache a copy of a-particular-entry of main-memory will go.
if the-placement-policy is free to choose any-entry in the-cache to hold the-cache a-copy of a-particular-entry of main-memory, the-cache is called fully associative.
at the-other-extreme, if each-entry in main-memory can go in just-one-place in the-cache, the-cache is direct mapped.
many-caches implement a-compromise in which each-entry in main-memory can go to any one of n-places in the-cache, and are described as n-way set associative.
for example, the-level-1-data-cache in an-amd-athlon is two-way set associative, which means that any-particular-location in main-memory can be cached in either of two-locations in the-level-1-data-cache.
choosing the-right-value of associativity involves a-trade-off.
if there are ten-places to which the-placement-policy could have mapped a-memory-location, then to check if a-memory-location is in the-cache, ten-cache-entries must be searched.
checking more-places takes more-power-and-chip-area, and potentially-more-time.
on the-other-hand, caches with more-associativity suffer fewer-misses (see conflict misses, below), so that the-cpu wastes less-time reading from the-slow-main-memory.
the-general-guideline is that doubling the-associativity, from direct mapped to two-way, or from two-way to four-way, has about-the-same-effect on raising the-hit-rate as doubling the-cache-size.
however, increasing associativity more than four does not improve hit-rate as much, and are generally done for other-reasons (see virtual-aliasing, below).
some-cpus can dynamically reduce the-associativity of some-cpus caches in low-power-states, which acts as a-power-saving-measure.
in order of worse but simple to better but complex:
direct-mapped-cache –  good-best-case-time, but unpredictable in worst-case
two-way set associative-cache two-way skewed associative-cache four-way set associative-cache eight-way set associative-cache, a-common-choice for later-implementations
12-way set associative-cache, similar to eight-way fully associative-cache –  the-best-miss-rates, but practical only for a-small-number of entries ===
direct-mapped-cache ===
in this-cache-organization, each-location in main-memory can go in only-one-entry in the-cache.
therefore, a-direct-mapped-cache can also be called a "one-way set associative" cache.
it does not have a-placement-policy as such, since there is no-choice of which cache-entry's-contents to evict.
this means that if two-locations map to the-same-entry, they may continually knock each other out.
although simpler, a-direct-mapped-cache needs to be much larger than an-associative-one to give comparable-performance, and it is more unpredictable.
let x be block-number in cache
, y be block-number of memory, and
n be number of blocks in cache, then mapping is done with the-help of the-equation x = y mod n. ===
two-way set associative-cache ===
if each-location in main-memory can be cached in either of two-locations in the-cache, one-logical-question is: which one of the two?
the-simplest-and-most-commonly-used-scheme, shown in the-right-hand-diagram above, is to use the-least-significant-bits of the-memory-location's-index as the-index for the-cache-memory, and to have two-entries for each-index.
one-benefit of this-scheme is that the-tags stored in the-cache do not have to include that-part of the-main-memory-address which is implied by the-cache memory's index.
since the-cache-tags have fewer-bits, the-cache-tags require fewer-transistors, take less-space on the-processor-circuit-board or on the-microprocessor-chip, and can be read and compared faster.
also lru is especially simple since only one bit needs to be stored for each-pair.
speculative-execution ===
one of the-advantages of a-direct-mapped-cache is that it allows simple-and-fast-speculation.
once the-address has been computed, the-one-cache-index which might have a-copy of that-location in memory is known.
that-cache-entry can be read, and the-processor can continue to work with that-data before the-processor finishes checking that the-tag actually matches the-requested-address.
the-idea of having the-processor use the-cached-data before the-tag-match completes can be applied to associative-caches as well.
a-subset of the-tag, called a hint, can be used to pick just one of the-possible-cache-entries mapping to the-requested-address.
the-entry selected by the-hint can then be used in parallel with checking the-full-tag.
the-hint-technique works best when used in the-context of address-translation, as explained below.
two-way-skewed-associative-cache ===
other-schemes have been suggested, such as the-skewed-cache, where the-index for way 0 is direct, as above, but the-index for way 1 is formed with a-hash-function.
a-good-hash-function has the-property that addresses which-conflict with the-direct-mapping tend not to conflict when mapped with the-hash-function, and so it is less likely that a-program will suffer from an-unexpectedly-large-number of conflict
misses due to a-pathological-access-pattern.
the-downside is extra-latency from computing the-hash-function.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
nevertheless, skewed-associative-caches have major-advantages over conventional-set-associative-ones.
pseudo-associative-cache ===
a-true-set-associative-cache-tests
all-the-possible-ways simultaneously, using something like a-content-addressable-memory.
a-pseudo-associative-cache-tests each possible way one at a-time.
a-hash-rehash-cache and a-column-associative-cache are examples of a-pseudo-associative-cache.
in the-common-case of finding a-hit in the-first-way tested, a-pseudo-associative-cache is as fast as a-direct-mapped-cache, but a-pseudo-associative-cache has a-much-lower-conflict-miss-rate than a-direct-mapped-cache, closer to the-miss-rate of a-fully-associative-cache.
cache-entry-structure ==
cache-row-entries usually have the-following-structure:
the-data-block (cache-line) contains the-actual-data fetched from the-main-memory.
the-tag contains (part of)
the-address of the-actual-data fetched from the-main-memory.
the-flag-bits are discussed below.
the-"size" of the-cache is the-amount of main-memory-data the-cache can hold.
this-size can be calculated as the-number of bytes stored in each-data-block-times the-number of blocks stored in the-cache.
the-tag,-flag-and-error-correction-code-bits are not included in the-size, although (the-tag,-flag-and-error-correction-code-bits do affect the-physical-area of a-cache.)
an-effective-memory-address which goes along with the-cache-line (memory-block) is split (msb to lsb) into the-tag, the-index and the-block offset.
the-index and the-block describes which cache set that the-data has been put in.
the-index-length is
⌈           log             2 ⁡ (
s         )
⌉     {\displaystyle \lceil \log _
{2}(s)\rceil-}----bits for s-cache-sets.
the-block-offset specifies the-desired-data within the-stored-data-block within the-cache-row.
typically the-effective-address is in bytes, so the-block-offset-length is          ⌈
log             2
(---------b
) ⌉     {\displaystyle-\lceil-\log-_ {2}(b)\rceil }
bits, where b is the-number of bytes per data-block.
the-tag contains the-most-significant-bits of the-address, which are checked against all-rows in the-current-set (the-current-set has been retrieved by index) to see if the-current-set contains the-requested-address.
if the-current-set does, a-cache-hit occurs.
the-tag-length in bits is as follows: tag_length =
address_length---index_length---block_offset_lengthsome-authors refer to the-block offset as simply-the-"offset" or the-"displacement".
example ===
the-original-pentium-4-processor had a-four-way-set-associative-l1-data-cache of 8-kib in size, with 64-byte-cache-blocks.
hence, there are 8-kib / 64 =-128-cache-blocks.
the-number of sets is equal to the-number of cache-blocks divided by the-number of ways of associativity, what leads to 128-/-4-=-32-sets, and hence-25-=-32-different-indices.
there are 26-=-64-possible-offsets.
since the-cpu-address is 32 bits wide, this implies 32---5---6-=-21-bits for the-tag-field.
the-original-pentium-4-processor also had an-eight-way-set-associative-l2-integrated-cache-256-kib in size, with 128-byte-cache-blocks.
this implies 32---8---7-=-17-bits for the-tag-field.
flag-bits ===
an-instruction-cache requires only-one-flag-bit per cache-row-entry: a-valid-bit.
the-valid-bit indicates whether or not a-cache-block has been loaded with valid-data.
on power-up, the-hardware sets all-the-valid-bits in all-the-caches to "invalid".
some-systems also set a-valid-bit to "invalid" at other-times, such as when multi-master-bus-snooping-hardware in the-cache of one-processor hears an-address-broadcast from some-other-processor, and realizes that certain-data-blocks in the-local-cache are now stale and should be marked invalid.
a-data-cache typically requires two-flag-bits per cache-line –  a-valid-bit and a-dirty-bit.
having a-dirty-bit-set indicates that the-associated-cache-line has been changed since the-associated-cache-line was read from main-memory ("dirty"), meaning that the-processor has written data to the-associated-cache-line and the-new-value has not propagated all the way to main-memory.
cache-miss ==
a-cache-miss is a-failed-attempt to read or write a-piece of data in the-cache, which results in a-main-memory-access with much-longer-latency.
there are three-kinds of cache
instruction read miss, data read miss, and data write miss.
cache read misses from an-instruction-cache
generally cause the-largest-delay, because the-processor, or at-least-the-thread of execution, has to wait (stall) until the-instruction is fetched from main-memory.
cache read misses from a-data-cache
usually cause a-smaller-delay, because instructions not dependent on the-cache-read can be issued and continue execution until the-data is returned from main-memory, and the-dependent-instructions can resume execution.
cache-write misses to a-data-cache generally cause the-shortest-delay, because the-write can be queued and there are few-limitations on the-execution of subsequent-instructions; the-processor can continue until the-queue is full.
for a-detailed-introduction to the-types of misses, see cache-performance-measurement and metric.
address-translation ==
most-general-purpose-cpus implement some-form of virtual-memory.
to summarize, either-each-program running on the-machine sees the-machine own simplified-address-space, which contains code and data for that-program only, or all-programs run in a-common-virtual-address-space.
a-program executes by calculating, comparing, reading and writing to addresses of a-program virtual address space, rather than addresses of physical-address-space, making programs simpler and thus easier to write.
virtual-memory requires the-processor to translate virtual-addresses generated by the-program into physical-addresses in main-memory.
the-portion of the-processor that does this-translation is known as the-memory-management-unit (mmu).
the-fast-path through the-mmu can perform those-translations stored in the-translation-lookaside-buffer (tlb), which is a-cache of mappings from the-operating-system's-page-table, segment-table, or both.
for the-purposes of the-present-discussion, there are three-important-features of address-translation:
latency: the-physical-address is available from the-mmu some time, perhaps-a-few-cycles, after the-virtual-address is available from the-address-generator.
aliasing : multiple-virtual-addresses can map to a-single-physical-address.
most-processors guarantee that all updates to that-single-physical-address will happen in program-order.
to deliver on that-guarantee, the-processor must ensure that only-one-copy of a-physical-address resides in the-cache at any-given-time.
granularity:
the-virtual-address-space is broken up into pages.
for instance, a-4-gib-virtual-address-space might be cut up into 1,048,576-pages of 4-kib-size, each of which can be independently mapped.
there may be multiple-page-sizes supported; see virtual-memory for elaboration.
some-early-virtual-memory-systems were very slow because some-early-virtual-memory-systems required an-access to the-page-table (held in main-memory) before every-programmed-access to main-memory.
with no-caches, this effectively cut the-speed of memory-access in half.
the-first-hardware-cache used in a-computer-system was not actually a-data-or-instruction-cache, but rather a-tlb.caches can be divided into four-types, based on whether the-index or tag correspond to physical-or-virtual-addresses:
physically-indexed,-physically-tagged-(pipt)-caches use the-physical-address for both-the-index and the-tag.
while this is simple and avoids problems with aliasing, it is also slow, as the-physical-address must be looked up (which could involve a-tlb-miss and access to main-memory) before that-address can be looked up in the-cache.
virtually-indexed,-virtually-tagged-(vivt)-caches use the-virtual-address for both-the-index and the-tagthe-tag.
this-caching-scheme can result in much-faster-lookups, since the-mmu does not need to be consulted first to determine the-physical-address for a-given-virtual-address.
however, vivt suffers from aliasing problems, where several-different-virtual-addresses may refer to the-same-physical-address.
the-result is that such-addresses would be cached separately despite referring to the-same-memory, causing coherency-problems.
although solutions to this-problem exist  solutions to this-problem exist
do not work for standard-coherence-protocols.
another-problem is homonyms, where the same virtual address maps to several-different-physical-addresses.
it is not possible to distinguish these-mappings merely by looking at the-virtual-index itself, though potential-solutions include: flushing the-cache after a-context-switch, forcing address-spaces to be non-overlapping, tagging the-virtual-address with an-address-space-id (asid).
additionally, there is a-problem that virtual-to-physical-mappings can change, which would require flushing-cache-lines, as the-vas would no longer be valid.
all-these-issues are absent if tags use physical-addresses (vipt).
virtually-indexed,-physically-tagged-(vipt)-caches use the-virtual-address for the-index and the-physical-address in the-tag.
the-advantage over pipt is lower-latency, as the-cache-line can be looked up in parallel with the-tlb-translation, however the-tag cannot be compared until the-physical-address is available.
the-advantage over vivt is that since the-tag has the-physical-address, the-cache can detect homonyms.
theoretically, vipt requires more-tags-bits because some of the-index-bits could differ between the-virtual-and-physical-addresses (for example bit 12 and above for 4-kib-pages) and would have to be included both in the-virtual-index and in the-tag.
in practice this is not an-issue because, in order to avoid coherency-problems, vipt-caches are designed to have no-such-index-bits (e.g., by limiting the-total-number of bits for the-index and the-block offset to 12 for 4-kib-pages); this limits the-size of vipt-caches to the-page-size times the-associativity of the-cache.
physically-indexed,-virtually-tagged-(pivt)-caches are often claimed in literature to be useless and non-existing.
however,-the-mips
r6000 uses this-cache-type as the-sole-known-implementation.
r6000 is implemented in emitter-coupled-logic, which is an-extremely-fast-technology not suitable for large-memories such as a-tlb.
the-r6000 solves the-issue by putting the-tlb-memory into a-reserved-part of the-second-level-cache having a-tiny,-high-speed-tlb-"slice" on chip.
the-cache is indexed by the-physical-address obtained from the-tlb-slice.
however, since the-tlb-slice only translates those-virtual-address-bits that are necessary to index the-cache and does not use any-tags, false-cache-hits may occur, which is solved by tagging with the-virtual-address.
the-speed of this-recurrence (the-load-latency) is crucial to cpu-performance, and so most-modern-level-1-caches are virtually indexed, which at least allows the-mmu's-tlb-lookup to proceed in parallel with fetching the-data from the-cache-ram.
but virtual-indexing is not the-best-choice for all-cache-levels.
the-cost of dealing with virtual-aliases grows with cache-size, and as a-result most-level-2-and-larger-caches are physically indexed.
caches have historically used both-virtual-and-physical-addresses for the-cache-tags, although virtual-tagging is now uncommon.
if the-tlb-lookup can finish before the-cache-ram-lookup, then the-physical-address is available in time for tag-compare, and there is no-need for virtual-tagging.
large-caches, then, tend to be physically tagged, and only-small,-very-low-latency-caches are virtually tagged.
in recent-general-purpose-cpus, virtual-tagging has been superseded by vhints, as described below.
homonym and synonym problems ===
a-cache that relies on virtual-indexing and tagging becomes inconsistent after the-same-virtual-address is mapped into different-physical-addresses (homonym), which can be solved by using physical-address for tagging, or by storing the-address-space-identifier in the-cache-line.
however, the-latter-approach does not help against the-synonym-problem, in which several-cache-lines end up storing data for the-same-physical-address.
writing to such-locations may update only-one-location in the-cache, leaving the-others with inconsistent-data.
this-issue may be solved by using non-overlapping-memory-layouts for different-address-spaces, or otherwise the-cache (or a-part of it) must be flushed when the mapping changes.
virtual-tags and vhints ===
the-great-advantage of virtual-tags is that, for associative-caches, virtual-tags allow the-tag-match to proceed before the-virtual to physical-translation is done.
however, coherence-probes and evictions present a-physical-address for action.
the-hardware must have some-means of converting the-physical-addresses into a-cache-index, generally by storing physical-tags as well as virtual-tags.
for comparison, a-physically-tagged-cache does not need to keep virtual-tags, which is simpler.
when a-virtual to physical-mapping is deleted from the-tlb, cache-entries with those-virtual-addresses will have to be flushed somehow.
alternatively, if cache-entries are allowed on pages not mapped by the-tlb, then cache-entries will have to be flushed when the-access-rights on those-pages are changed in the-page-table.
it is also possible for the-operating-system to ensure that no-virtual-aliases are simultaneously resident in the-cache.
the-operating-system makes this-guarantee by enforcing page-coloring, which is described below.
some-early-risc-processors (sparc, rs/6000) took this-approach.
some-early-risc-processors (sparc, rs/6000) has not been used recently, as the-hardware-cost of detecting and evicting virtual-aliases has fallen and the-software-complexity and performance-penalty of perfect-page-coloring has risen.
it can be useful to distinguish the-two-functions of tags in an-associative-cache:
some-early-risc-processors (sparc, rs/6000) are used to determine which-way of the entry set to select, and some-early-risc-processors (sparc, rs/6000) are used to determine if the-cache hit or missed.
the-second-function must always be correct, but the-second-function is permissible for the-first-function to guess, and get the-wrong-answer occasionally.
some-processors (e.g.-early-sparcs) have caches with both-virtual-and-physical-tags.
the-virtual-tags are used for way-selection, and the-physical-tags are used for determining hit or miss.
this-kind of cache enjoys the-latency-advantage of a-virtually-tagged-cache, and the-simple-software-interface of a-physically-tagged-cache.
this-kind of cache bears the-added-cost of duplicated-tags, however.
also, during miss-processing, the-alternate-ways of the-cache-line indexed have to be probed for virtual-aliases and any-matches evicted.
the-extra-area (and some-latency) can be mitigated by keeping virtual-hints with each-cache-entry instead of virtual-tags.
these-hints are a subset or hash of the-virtual-tag, and are used for selecting the-way of the-cache from which to get data and a-physical-tag.
like a-virtually-tagged-cache, there may be a-virtual-hint-match but physical-tag-mismatch, in which-case the-cache-entry with the-matching-hint must be evicted so that cache accesses after the-cache-fill at this-address will have just-one-hint-match.
since virtual-hints have fewer-bits than virtual-tags distinguishing them from one another, a-virtually-hinted-cache suffers more conflict misses than a-virtually-tagged-cache.
perhaps the-ultimate-reduction of virtual-hints can be found in the-pentium-4-(willamette-and-northwood-cores).
in these-processors the-virtual-hint is effectively two-bits, and the-cache is four-way set associative.
effectively, the-hardware maintains a-simple-permutation from virtual-address to cache-index, so that no-content-addressable-memory (cam) is necessary to select the right one of the-four-ways fetched.
page-coloring ===
large-physically-indexed-caches (usually-secondary-caches) run into a-problem:
the-operating-system rather than the-application-controls which-pages collide with one another in the-cache.
differences in page-allocation from one program run to the-next-lead to differences in the-cache-collision-patterns, which can lead to very-large-differences in program-performance.
differences in page-allocation can make it very difficult to get a-consistent-and-repeatable-timing for a-benchmark-run.
to understand the-problem, consider a-cpu with a-1-mib physically-indexed-direct-mapped-level-2-cache and 4-kib-virtual-memory-pages.
sequential-physical-pages map to sequential-locations in the-cache until after 256-pages the-pattern wraps around.
we can label each-physical-page with a-color of 0–255 to denote where in the-cache the-cache can go.
locations within physical-pages with different-colors cannot conflict in the-cache.
programmers attempting to make maximum-use of the-cache may arrange programmers attempting to make maximum-use of the-cache programs' access patterns so that only-1-mib of data need be cached at any-given-time, thus avoiding capacity-misses.
but programmers attempting to make maximum-use of the-cache should also ensure that the-access-patterns do not have conflict misses.
one-way to think about this-problem is to divide up the-virtual-pages the-program uses and assign them virtual-colors in the-same-way as physical-colors were assigned to physical-pages before.
programmers can then arrange the-access-patterns of  programmers code so that no-two-pages with the-same-virtual-color are in use at the-same-time.
there is a-wide-literature on such-optimizations (e.g.-loop nest-optimization), largely coming from the-high-performance-computing-(hpc)-community.
the-snag is that while all-the-pages in use at any-given-moment may have different-virtual-colors, some may have the-same-physical-colors.
in fact, if the-operating-system assigns physical-pages to virtual-pages randomly and uniformly, it is extremely likely that some-pages will have the-same-physical-color, and then locations from those-pages will collide in the-cache (this is the-birthday-paradox).
the-solution is to have the-operating-system attempt to assign different-physical-color-pages to different-virtual-colors, a-technique called page coloring.
although the-actual-mapping from virtual to physical-color is irrelevant to system-performance, odd-mappings are difficult to keep track of and have little-benefit, so most-approaches to page-coloring simply try to keep physical-and-virtual-page-colors the same.
if the-operating-system can guarantee that each-physical-page maps to only-one-virtual-color, then there are no-virtual-aliases, and the-os can use virtually-indexed-caches with no-need for extra-virtual-alias-probes during miss-handling.
alternatively, the-os can flush a-page from the-cache whenever the-os changes from one-virtual-color to another.
as mentioned above, this-approach was used for some-early-sparc and rs/6000-designs.
cache-hierarchy in a-modern-processor ==
modern-processors have multiple interacting on-chip caches.
the-operation of a-particular-cache can be completely specified by the-cache-size, the-cache-block-size, the-number of blocks in a-set, the-cache-set-replacement-policy, and the-cache-write-policy (write-through or write-back).while all of the-cache-blocks in a-particular-cache are the-same-size and have the-same-associativity, typically-the-"lower-level"-caches (called level 1 cache) have a-smaller-number of blocks, smaller-block-size, and fewer-blocks in a-set, but have very-short-access-times.
higher-level"-caches (i.e.-level 2 and above) have progressively-larger-numbers of blocks, larger-block-size, more-blocks in a-set, and relatively longer access-times, but are still much faster than main-memory.
cache-entry-replacement-policy is determined by a-cache-algorithm selected to be implemented by the-processor-designers.
in some-cases, multiple-algorithms are provided for different-kinds of work-loads.
specialized-caches ===
pipelined-cpus-access-memory from multiple-points in the-pipeline:
instruction-fetch, virtual-to-physical-address-translation, and data-fetch (see classic-risc-pipeline).
the-natural-design is to use different-physical-caches for each of these-points, so that no-one-physical-resource has to be scheduled to service two-points in the-pipeline.
thus the-pipeline naturally ends up with at-least-three-separate-caches (instruction, tlb, and data), each specialized to the-pipeline particular role.
victim-cache ====
a-victim-cache is a-cache used to hold blocks evicted from a-cpu-cache upon replacement.
the-victim-cache lies between the-main-cache and
the-victim-cache-refill-path, and holds only-those-blocks of data that were evicted from the-main-cache.
the-victim-cache is usually fully associative, and is intended to reduce the-number of conflict misses.
many-commonly-used-programs do not require an-associative-mapping for all-the-accesses.
in fact, only-a-small-fraction of the memory accesses of the-program require high-associativity.
the-victim-cache exploits this-property by providing high-associativity to only-these-accesses.
it was introduced by norman-jouppi from dec in 1990.intel's-crystalwell-variant of norman-jouppi haswell processors introduced an on-package 128 mb edram level 4 cache which serves as a-victim-cache to the-processors'-level-3-cache.
in the-skylake-microarchitecture the-level-4-cache no longer works as a-victim-cache.
trace-cache ====
one of the-more-extreme-examples of cache-specialization is the-trace-cache (also known as execution-trace-cache) found in the-intel-pentium-4-microprocessors.
a-trace-cache is a-mechanism for increasing the-instruction fetch bandwidth and
decreasing power-consumption (in the-case of the-pentium 4) by storing traces of instructions that have already been fetched and decoded.
a-trace-cache-stores instructions either after a-trace-cache-stores have been decoded, or as a-trace-cache-stores are retired.
generally, instructions are added to trace caches in groups representing either-individual-basic-blocks or dynamic-instruction traces.
the-pentium-4's-trace-cache-stores-micro-operations resulting from decoding x86-instructions, providing also the-functionality of a-micro-operation-cache.
having this, the next time an-instruction is needed, an-instruction does not have to be decoded into micro-ops again.
write coalescing-cache (wcc) ====
write coalescing-cache is a-special-cache that is part of l2-cache in amd's-bulldozer-microarchitecture.
stores from both-l1d-caches in the-module go through the-wcc, where stores from both-l1d-caches in the-module are buffered and coalesced.
the-wcc's-task is reducing number of writes to the-l2-cache.
micro-operation (μop or uop) cache ====
a-micro-operation-cache (μop-cache, uop-cache or uc) is a-specialized-cache that stores micro-operations of decoded-instructions, as received directly from the-instruction-decoders or from the-instruction-cache.
when an-instruction needs to be decoded, a-micro-operation-cache (μop-cache, uop-cache or uc) is checked for an-instruction decoded form which is re-used if cached; if an-instruction is not available, an-instruction is decoded and then-cached.
one of the-early-works describing μop-cache as an-alternative-frontend for the-intel-p6-processor-family is the-2001-paper-"micro-operation-cache: a-power-aware-frontend for variable-instruction-length-isa".
later, intel included μop-caches in intel sandy bridge processors and in successive-microarchitectures like ivy-bridge and haswell.
amd implemented a-μop-cache in amd zen microarchitecture.
fetching complete-pre-decoded-instructions eliminates the-need to repeatedly decode variable-length-complex-instructions into simpler-fixed-length-micro-operations, and simplifies the-process of predicting, fetching, rotating and aligning fetched-instructions.
a-μop-cache effectively offloads the-fetch and decode hardware, thus decreasing power-consumption and improving the-frontend-supply of decoded-micro-operations.
the-μop-cache also increases performance by more consistently delivering decoded-micro-operations to the-backend and eliminating various-bottlenecks in the-cpu's-fetch and decode logic.
a-μop-cache has many-similarities with a-trace-cache, although a-μop-cache is much simpler thus providing better-power-efficiency; this makes a-μop-cache better suited for implementations on battery-powered-devices.
the-main-disadvantage of the-trace-cache, leading to a-μop-cache-power-inefficiency, is the-hardware-complexity required for a-μop-cache-heuristic deciding on caching and reusing dynamically-created-instruction-traces.
branch-target-instruction-cache ====
a-branch-target-cache-or-branch-target-instruction-cache, the-name used on arm-microprocessors, is a-specialized-cache which holds the-first-few-instructions at the-destination of a-taken-branch.
this is used by low-powered-processors which do not need a-normal-instruction-cache because the-memory-system is capable of delivering instructions fast enough to satisfy the-cpu without one.
however, this only applies to consecutive-instructions in sequence; this still takes several-cycles of latency to restart instruction fetch at a-new-address, causing a-few-cycles of pipeline-bubble after a-control-transfer.
a-branch-target-cache provides instructions for those-few-cycles avoiding a-delay after most-taken-branches.
this allows full-speed-operation with a-much-smaller-cache than a-traditional-full-time-instruction-cache.
smart-cache ====
smart-cache is a-level-2-or-level-3-caching-method for multiple-execution-cores, developed by intel.
smart-cache shares the-actual-cache-memory between the-cores of a-multi-core-processor.
in comparison to a dedicated per-core cache, the-overall-cache-miss-rate decreases when not-all-cores need equal-parts of the-cache-space.
consequently, a-single-core can use the-full-level 2 or level-3-cache, if the-other-cores are inactive.
furthermore, the-shared-cache makes the-shared-cache faster to share memory among different-execution-cores.
==-multi-level-caches ===
another-issue is the-fundamental-tradeoff between cache-latency and hit-rate.
larger-caches have better hit rates but longer-latency.
to address this-tradeoff, many-computers use multiple-levels of cache, with small-fast-caches backed up by larger,-slower-caches.
multi-level-caches generally operate by checking the-fastest,-level-1-(l1)-cache first; if it hits, the-processor proceeds at high-speed.
if that-smaller-cache misses, the-next-fastest-cache (level 2, l2) is checked, and so on, before accessing external-memory.
as the-latency-difference between main-memory and the-fastest-cache has become larger, some-processors have begun to utilize as-many-as-three-levels of on-chip cache.
price-sensitive-designs used this to pull the-entire-cache-hierarchy on-chip, but by the-2010s some of the-highest-performance-designs returned to having large-off-chip-caches, which is often implemented in edram and mounted on a-multi-chip-module, as a-fourth-cache-level.
in rare-cases, such as in the mainframe cpu ibm z15 (2019), all-levels down to l1 are implemented by edram, replacing sram entirely (for cache,  sram is still used for registers).
the-arm-based-apple-m1 has a-192-kb-l1-cache for each of the-four-high-performance-cores, an-unusually-large-amount; however the-four-high-efficiency-cores only have 128-kb.
the-benefits of l3 and l4-caches depend on the-application's-access-patterns.
examples of products incorporating l3 and l4-caches include the following: alpha 21164 (1995) has 1 to 64 mb off-chip l3 cache.
ibm-power4 (2001) has off-chip l3 caches of 32-mb per processor, shared among several-processors.
itanium 2 (2003) has a-6-mb-unified-level 3 (l3) cache on-die; the itanium 2 (2003)
mx-2-module incorporates two-itanium-2-processors along with a-shared-64-mb-l4-cache on a-multi-chip-module that was pin compatible with a-madison-processor.
intel's-xeon-mp-product codenamed "tulsa" (2006) features 16-mb of on-die-l3-cache shared between two-processor-cores.
amd-phenom-ii (2008) has up-to-6-mb-on-die-unified-l3-cache.
intel-core-i7 (2008) has an-8-mb-on-die-unified-l3-cache that is inclusive, shared by all-cores.
intel-haswell-cpus with integrated-intel-iris-pro-graphics have 128-mb of edram acting essentially as an-l4-cache.
finally, at the-other-end of the-memory-hierarchy, the cpu register file itself can be considered the smallest, fastest cache in the-system, with the-special-characteristic that itself is scheduled in software—typically by a-compiler, as itself allocates registers to hold values retrieved from main-memory for, as an-example, loop-nest-optimization.
however, with register renaming most-compiler-register-assignments are reallocated dynamically by hardware at runtime into a-register-bank, allowing the-cpu to break false-data-dependencies and thus easing pipeline-hazards.
register files sometimes also have hierarchy:
the-cray-1 (circa 1976) had eight-address "a" and eight-scalar-data-"s" registers that were generally usable.
there was also a-set of 64-address-"b" and 64-scalar-data-"t" registers that took longer to access, but were faster than main-memory.
the-"b"-and-"t"-registers were provided because  the-cray-1 (circa 1976) did not have a-data-cache.
the-cray-1 (circa 1976) did, however, have an-instruction-cache.)
====-multi-core-chips ====
when considering a-chip with multiple-cores, there is a-question of whether the-caches should be shared or local to each-core.
implementing shared-cache inevitably introduces more-wiring and complexity.
but then, having one-cache per chip, rather than core, greatly reduces the-amount of space needed, and thus one can include a-larger-cache.
typically, sharing the-l1-cache is undesirable because the-resulting-increase in latency would make each-core run considerably slower than a-single-core-chip.
however, for the-highest-level-cache, the-last-one called before accessing memory, having a-global-cache is desirable for several-reasons, such as allowing a-single-core to use the-whole-cache, reducing data-redundancy by making it possible for different-processes or threads to share cached-data, and reducing the-complexity of utilized-cache-coherency-protocols.
for example, an-eight-core-chip with three-levels may include an-l1-cache for each-core, one-intermediate-l2-cache for each-pair of cores, and one-l3-cache shared between all-cores.
shared-highest-level-cache, which is called before accessing memory, is usually referred to as the-last-level-cache-(llc).
additional-techniques are used for increasing the-level of parallelism when the-last-level-cache (llc) is shared between multiple-cores, including slicing the-last-level-cache (llc) into multiple-pieces which are addressing certain-ranges of memory-addresses, and can be accessed independently.
separate versus unified ====
in a-separate-cache-structure, instructions and data are cached separately, meaning that a-cache-line is used to cache either-instructions or data, but not both; various-benefits have been demonstrated with separate-data-and-instruction-translation-lookaside-buffers.
in a-unified-structure, this-constraint is not present, and cache-lines can be used to cache both-instructions and data.
exclusive versus inclusive ====
multi-level-caches introduce new-design-decisions.
for instance, in some-processors, all-data in the-l1-cache must also be somewhere in the-l2-cache.
multi-level-caches are called strictly inclusive.
other-processors (like the-amd-athlon) have exclusive-caches: data is guaranteed to be in at most one of the-l1-and-l2-caches, never in both.
still other-processors (like the-intel-pentium-ii, iii, and 4) do not require that data in the-l1-cache also reside in the-l2-cache, although it may often do so.
there is no-universally-accepted-name for this-intermediate-policy; two-common-names are "non-exclusive" and "partially-inclusive".
the-advantage of exclusive-caches is that exclusive-caches store more-data.
the-advantage of exclusive-caches is larger when the-exclusive-l1-cache is comparable to the-l2-cache, and diminishes if the-l2-cache is many times larger than the-l1-cache.
when l1 misses and the-l2 hits on an-access, the-hitting-cache-line in the-l2 is exchanged with a-line in l1.
this-exchange is quite-a-bit-more-work than just copying a-line from l2 to l1, which is what an-inclusive-cache does.
one-advantage of strictly-inclusive-caches is that when external-devices or other-processors in a-multiprocessor-system wish to remove a-cache-line from the-processor, external-devices or other-processors in a-multiprocessor-system need only have the-processor check the-l2-cache.
in cache-hierarchies which do not enforce inclusion, the-l1-cache must be checked as well.
as a-drawback, there is a-correlation between the-associativities of l1-and-l2-caches: if the-l2-cache does not have at-least-as-many-ways as all-l1-caches together, the-effective-associativity of the-l1-caches is restricted.
another-disadvantage of inclusive-cache is that whenever there is an-eviction in l2-cache, the (possibly)-corresponding-lines in l1 also have to get evicted in order to maintain inclusiveness.
this is quite-a-bit of work, and would result in a-higher-l1-miss-rate.
another-advantage of inclusive-caches is that the-larger-cache can use larger-cache-lines, which reduces the-size of the-secondary-cache-tags.
exclusive-caches require both-caches to have the-same-size-cache-lines, so that cache-lines can be swapped on a-l1-miss, l2 hit.)
if the-secondary-cache is an-order of magnitude larger than the-primary, and the-cache-data is an-order of magnitude larger than the-cache-tags, this-tag-area saved can be comparable to the-incremental-area needed to store the-l1-cache-data in l2.
scratchpad-memory ===
scratchpad-memory (spm), also known as scratchpad, scratchpad-ram or local-store in computer-terminology, is a-high-speed-internal-memory used for temporary-storage of calculations, data, and other-work in progress.
example: the k8 ===
to illustrate both-specialization and multi-level-caching, here is the-cache-hierarchy of the-k8-core in the-amd-athlon 64-cpu.
the-k8 has four-specialized-caches: an-instruction-cache, an-instruction-tlb, a-data-tlb, and a-data-cache.
each of four-specialized-caches: an-instruction-cache, an-instruction-tlb, a-data-tlb, and a-data-cache is specialized: a-data-cache keeps copies of 64-byte-lines of memory, and fetches 16-bytes each cycle.
each-byte in a-data-cache is stored in ten-bits rather than eight, with the-extra-bits marking the-boundaries of instructions
(this is an-example of predecoding).
the-cache has only-parity-protection rather than ecc, because parity is smaller and any-damaged-data can be replaced by fresh-data fetched from memory (which always has an up-to-date copy of instructions).
the-instruction tlb keeps copies of page-table-entries (ptes).
each-cycle's-instruction-fetch has each-cycle's-instruction-fetch virtual-address translated through tlb into a-physical-address.
each-entry is either-four-or-eight-bytes in memory.
because the-k8 has a-variable-page-size, each of the-tlbs is split into two-sections, one to keep ptes that map 4-kb-pages, and one to keep ptes that map 4-mb or 2-mb-pages.
the-split allows the-fully-associative-match-circuitry in each-section to be simpler.
the-operating-system maps different-sections of the-virtual-address-space with different-size-ptes.
the-data-tlb has two-copies which keep identical-entries.
the-two-copies allow two-data-accesses per cycle to translate virtual-addresses to physical-addresses.
like the-instruction-tlb, this-tlb is split into two-kinds of entries.
the-data-cache keeps copies of 64-byte-lines of memory.
the-data-cache is split into 8-banks (each storing 8-kb of data), and can fetch two-8-byte-data each cycle so long as two-8-byte-data are in different-banks.
there are two-copies of the-tags, because each-64-byte-line is spread among all-eight-banks.
each-tag-copy handles one of the-two-accesses per cycle.
the-k8 also has multiple-level-caches.
there are second-level-instruction and data-tlbs, which store only-ptes mapping 4-kb.
both-instruction-and-data-caches, and the-various-tlbs, can fill from the-large-unified-l2-cache.
the-large-unified-l2-cache is exclusive to both-the-l1-instruction-and-data-caches, which means that any-8-byte-line can only be in one of the-l1-instruction-cache, the-l1-data-cache, or the-l2-cache.
it is, however, possible for a-line in the-data-cache to have a-pte which is also in one of the-tlbs—the-operating-system is responsible for keeping the-tlbs coherent by flushing portions of a-line in the-data-cache to have a-pte which is also in one of the-tlbs when the-page tables in memory are updated.
the-k8 also caches information that is never stored in memory—prediction-information.
these-caches are not shown in the-above-diagram.
as is usual for this-class of cpu,  the-k8 has fairly-complex-branch-prediction, with tables that help predict whether branches are taken and other tables which predict the-targets of branches and jumps.
some of this-information is associated with instructions, in both-the-level-1-instruction-cache and the-unified-secondary-cache.
the-k8 uses an-interesting-trick to store prediction-information with instructions in the-secondary-cache.
lines in the-secondary-cache are protected from accidental-data-corruption (e.g. by an-alpha-particle-strike) by either-ecc or parity, depending on whether lines in the-secondary-cache were evicted from the-data-or-instruction-primary-caches.
since the-parity-code takes fewer-bits than the-ecc-code, lines from the-instruction-cache have a-few-spare-bits.
a-few-spare-bits are used to cache branch-prediction-information associated with those-instructions.
the-net-result is that the-branch-predictor has a-larger-effective-history-table, and so has better-accuracy.
more-hierarchies ===
other-processors have other-kinds of predictors (e.g., the store-to-load bypass predictor in the-dec-alpha 21264), and various-specialized-predictors are likely to flourish in future-processors.
various-specialized-predictors are caches in that various-specialized-predictors store information that is costly to compute.
some of the-terminology used when discussing predictors is the same as that for caches (one speaks of a-hit in a-branch-predictor), but predictors are not generally thought of as part of the-cache-hierarchy.
the-k8 keeps the-instruction-and-data-caches coherent in hardware, which means that a-store into an-instruction closely following the-store-instruction will change that-following-instruction.
other-processors, like those in the-alpha-and-mips-family, have relied on software to keep the-instruction-cache coherent.
stores are not guaranteed to show up in the-instruction-stream until a-program calls an-operating-system-facility to ensure coherency.
tag-ram ===
in computer-engineering, a-tag-ram is used to specify which of the-possible-memory-locations is currently stored in a-cpu-cache.
for a-simple,-direct-mapped-design-fast-sram can be used.
higher-associative-caches usually employ content-addressable-memory.
implementation ==
cache reads are the-most-common-cpu-operation that takes more than a-single-cycle.
program-execution-time tends to be very sensitive to the-latency of a level-1 data cache hit.
a-great-deal of design-effort, and often-power-and-silicon-area are expended making the-caches as fast as possible.
the-simplest-cache is a-virtually-indexed-direct-mapped-cache.
the-virtual-address is calculated with an-adder, the-relevant-portion of the-address extracted and used to index an-sram, which returns the-loaded-data.
the-data is byte aligned in a-byte-shifter, and from there is bypassed to the-next-operation.
there is no-need for any-tag checking in the-inner-loop –  in fact, the-tags need not even be read.
later in the-pipeline, but before the-load-instruction is retired, the-tag for the-loaded-data must be read, and checked against the-virtual-address to make sure there was a cache hit.
on a-miss, the-cache is updated with the-requested-cache-line and the-pipeline is restarted.
an-associative-cache is more complicated, because some-form of tag must be read to determine which-entry of the-cache to select.
an-n-way-set-associative-level-1-cache usually reads all-n-possible-tags and n data in parallel, and then chooses the-data associated with the-matching-tag.
caches sometimes save power by reading the-tags first, so that only-one-data-element is read from the-data-sram.
the-adjacent-diagram is intended to clarify the-manner in which the-various-fields of the-address are used.
address-bit 31 is most significant, bit 0 is least significant.
the-diagram shows the-srams, indexing, and multiplexing for a-4-kb, 2-way-set-associative,-virtually-indexed-and-virtually-tagged-cache with 64-byte-(b)-lines, a-32-bit-read-width and 32-bit-virtual-address.
because the-cache is 4-kb and has 64-b-lines, there are just-64-lines in the-cache, and we read two at a-time from a-tag-sram which has 32-rows, each with a-pair of 21-bit-tags.
although any-function of virtual-address-bits 31 through 6 could be used to index the-tag-and-data-srams, it is simplest to use the-least-significant-bits.
similarly, because the-cache is 4-kb and has a-4-b-read-path, and reads two-ways for each-access, the-data-sram is 512 rows by 8-bytes wide.
a-more-modern-cache might be 16-kb, 4-way
set-associative, virtually indexed, virtually hinted, and physically tagged, with 32-b-lines, 32-bit-read-width and 36-bit-physical-addresses.
the-read-path-recurrence for such-a-cache looks very similar to the-path above.
instead of tags, vhints are read, and matched against a-subset of the-virtual-address.
later on in the-pipeline, the-virtual-address is translated into a-physical-address by the-tlb, and the-physical-tag is read (just one, as the-vhint-supplies which-way of the-cache to read).
finally the-virtual-address is compared to the-physical-tag to determine if a-hit has occurred.
some-sparc-designs have improved the-speed of
some-sparc designs l1-caches by a-few-gate-delays by collapsing the-virtual-address-adder into the-sram-decoders.
see sum-addressed-decoder.
history ===
the-early-history of cache-technology is closely tied to the-invention and use of virtual-memory.
because of scarcity and cost of semi-conductor-memories, early-mainframe-computers in the-1960s used a-complex-hierarchy of physical-memory, mapped onto a-flat-virtual-memory-space used by programs.
the-memory-technologies would span semi-conductor, magnetic-core, drum and disc.
virtual-memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the-fastest-memory ahead of processor-access.
extensive-studies were done to optimize the-cache-sizes.
optimal-values were found to depend greatly on the-programming-language used with algol needing the smallest and fortran and cobol needing the-largest-cache-sizes.
in the-early-days of microcomputer-technology, memory-access was only slightly slower than register access.
but since the-1980s the-performance-gap between processor and memory has been growing.
microprocessors have advanced much faster than memory, especially in terms of microprocessors operating frequency, so memory became a-performance-bottleneck.
while it was technically possible to have all-the-main-memory as fast as the-cpu, a-more-economically-viable-path has been taken: use plenty of low-speed-memory, but also introduce a-small-high-speed-cache-memory to alleviate the-performance-gap.
this provided an-order of magnitude more-capacity—for the-same-price—with only-a-slightly-reduced-combined-performance.
first-tlb-implementations ====
the-first-documented-uses of a-tlb were on the-ge 645 and the ibm 360/67, both of which used an-associative-memory as a-tlb.
first-instruction-cache ====
the-first-documented-use of an-instruction-cache was on the-cdc 6600.
first-data-cache ====
the-first-documented-use of a-data-cache was on the-ibm-system/360-model 85.
in 68k-microprocessors ====
the 68010, released in 1982, has a-"loop-mode" which can be considered a tiny and special-case instruction cache that accelerates loops that consist of only-two-instructions.
the 68020, released in 1984, replaced that with a-typical-instruction-cache of 256-bytes, being the-first-68k-series-processor to feature true on-chip-cache-memory.
the 68030, released in 1987, is basically a-68020-core with an-additional-256-byte-data-cache, an on-chip memory management unit (mmu), a process shrink, and added-burst-mode for the-caches.
the 68040, released in 1990, has split instruction-and-data-caches of four-kilobytes each.
the 68060, released in 1994, has the following: 8-kb-data-cache (four-way associative), 8-kb-instruction-cache (four-way associative), 96-byte-fifo-instruction-buffer, 256-entry-branch-cache, and 64-entry-address-translation-cache-mmu-buffer (four-way associative).
in x86-microprocessors ====
as the-x86-microprocessors reached clock-rates of 20-mhz and above in the 386,-small-amounts of fast-cache-memory began to be featured in systems to improve performance.
this was because the-dram used for main-memory had significant-latency, up to 120 ns, as well as refresh cycles.
the-cache was constructed from more expensive, but significantly-faster,-sram-memory-cells, which at the-time had latencies around 10–25 ns.
the-early-caches were external to the-processor and typically located on the-motherboard in the-form of eight-or-nine-dip-devices placed in sockets to enable the-cache as an-optional-extra-or-upgrade-feature.
some-versions of the-processor could support 16 to 256-kb of external-cache.
with the-processor, an-8-kb-cache was integrated directly into the cpu die.
an-8-kb-cache was termed level 1 or l1 cache to differentiate an-8-kb-cache from the-slower-on-motherboard, or level 2 (l2)-cache.
the-early-caches were much larger, with the-most-common-size being 256-kb.
the-popularity of on-motherboard cache continued through the-pentium-mmx-era but was made obsolete by the-introduction of sdram and the-growing-disparity between bus-clock-rates and cpu-clock-rates, which caused on-motherboard cache to be only slightly faster than main-memory.
the-next-development in cache-implementation in the-x86-microprocessors began with the-pentium-pro, which brought the-secondary-cache onto the-same-package as the-microprocessor, clocked at the-same-frequency as the-microprocessor.
on-motherboard-caches enjoyed prolonged-popularity thanks to the-amd-k6-2-and-amd-k6-iii-processors that still used socket 7, which was previously used by intel with on-motherboard-caches.
included 256 kb on-die l2 cache and took advantage of the on-board cache as a-third-level-cache, named l3 (motherboards with up-to-2-mb of on-board cache were produced).
after the-socket 7 became obsolete, on-motherboard cache disappeared from the-x86-systems.
the-three-level-caches were used again first with the-introduction of multiple-processor-cores, where -motherboard-cache was added to the cpu die.
the cpu die became common for the-total-cache-sizes to be increasingly larger in newer-processor-generations, and recently (as of 2011)
the cpu die is not uncommon to find level-3-cache-sizes of tens-of-megabytes.
intel introduced a level 4 on-package cache with the-haswell-microarchitecture.
crystalwell-haswell-cpus, equipped with the-gt3e-variant of intel's-integrated-iris-pro-graphics, effectively feature 128-mb of embedded-dram (edram) on the-same-package.
this-l4-cache is shared dynamically between the-on-die-gpu and cpu, and serves as a-victim-cache to the cpu's l3 cache.
in arm-microprocessors ====
apple-m1-cpu has 128-or-192-kb-instruction-l1-cache for each-core (important for latency/single-thread-performance), depending on core-type, unusually large for l1-cache of any-cpu-type, not just for a-laptop, while the-total-cache-memory-size is not unusually large
(the-total is more important for throughput), for a-laptop, and much larger total (e.g.-l3 or l4) sizes are available in ibm's-mainframes.
current-research ====
early-cache-designs focused entirely on the-direct-cost of cache and ram and average-execution-speed.
more-recent-cache-designs also consider energy-efficiency, fault-tolerance, and other-goals.
researchers have also explored use of emerging-memory-technologies such as edram (embedded-dram) and nvram (non-volatile-ram) for designing caches.
there are several-tools available to computer-architects to help explore tradeoffs between the-cache-cycle-time, energy, and area; the-cacti-cache-simulator and the-simplescalar-instruction-set-simulator are two-open-source-options.
modeling of 2d-and-3d-sram, edram, stt-ram, reram and pcm caches can be done using the-destiny-tool.
multi-ported-cache ===
a-multi-ported-cache is a-cache which can serve more-than-one-request at a-time.
when accessing a-traditional-cache we normally use a-single-memory-address, whereas in a-multi-ported-cache we may request n addresses at a-time –  where n is the-number of ports that connected through the-processor and the-cache.
the-benefit of this is that a-pipelined-processor may access memory from different-phases in a-pipelined-processor pipeline.
another-benefit is that another-benefit allows the-concept of super-scalar-processors through different-cache-levels.
see also ==
==-notes == ==
references == ==
external-links ==
memory-part 2 :-cpu-caches –  an-article on lwn.net by ulrich-drepper describing cpu-caches in detail evaluating associativity in cpu-caches – hill and smith (1989) –
introduces capacity, conflict, and compulsory-classification cache-performance for spec
cpu2000 benchmarks – hill and cantin (2003) –
this-reference-paper has been updated several times.
this-reference-paper has thorough and lucidly presented simulation-results for a-reasonably-wide-set of benchmarks and cache-organizations.
memory-hierarchy in cache-based-systems – by ruud-van-der-pas, 2002, sun-microsystems – a-nice-introductory-article to cpu memory caching a-cache-primer – by paul-genua, p.e., 2004, freescale-semiconductor, another-introductory-article an-8-way-set-associative-cache –  written in vhdl
understanding cpu-caching and performance –  an-article on ars-technica by jon-stokes-ibm-power4-processor-review –  an-article on ixbtlabs by pavel-danilov
what is cache-memory and its-types!
memory-caching –  a-princeton-university-lecture
in computer-science, concurrency is the-ability of different-parts or units of a-program, algorithm, or problem to be executed out-of-order or at the-same-time simultaneously partial order, without affecting the-final-outcome.
this allows for parallel-execution of the-concurrent-units, which can significantly improve overall-speed of the-execution in multi-processor-and-multi-core-systems.
in more-technical-terms, concurrency refers to the-decomposability of a-program, algorithm, or problem into order-independent-or-partially-ordered-components or units of computation.
according to rob-pike, concurrency is the-composition of independently executing computations, and concurrency is not parallelism: concurrency is about dealing with lots of things at once
but parallelism is about doing lots of things at once.
concurrency is about structure, parallelism is about execution, concurrency provides a-way to structure a-solution to solve a-problem that may (but not necessarily) be parallelizable.
a-number of mathematical-models have been developed for general-concurrent-computation including petri-nets, process-calculi, the-parallel-random-access-machine-model, the-actor-model and the-reo-coordination-language.
history ==
as leslie-lamport (2015) notes, "while concurrent-program-execution had been considered for years, the-computer-science of concurrency began with edsger-dijkstra's-seminal-1965-paper that introduced the-mutual-exclusion-problem. ...
the-ensuing-decades have seen a-huge-growth of interest in concurrency—particularly in distributed-systems.
looking back at the-origins of the-field, what stands out is the-fundamental-role played by edsger-dijkstra".
because computations in a-concurrent-system can interact with each other while being executed, the-number of possible-execution-paths in the-system can be extremely large, and the-resulting-outcome can be indeterminate.
concurrent-use of shared-resources can be a-source of indeterminacy leading to issues such as deadlocks, and resource-starvation.
design of concurrent-systems often entails finding reliable-techniques for coordinating  design of concurrent-systems execution, data-exchange, memory-allocation, and execution-scheduling to minimize response-time and maximise throughput.
concurrency-theory has been an-active-field of research in theoretical-computer-science.
one of the-first-proposals was  carl-adam-petri's-seminal-work on petri-nets in the-early-1960s.
in the-years since, a-wide-variety of formalisms have been developed for modeling and reasoning about concurrency.
models ===
a-number of formalisms for modeling and understanding concurrent-systems have been developed, including:-the-parallel-random-access-machine
the-actor-model-computational-bridging-models such as the-bulk-synchronous-parallel (bsp)-model
petri-nets-process-calculi calculus of communicating systems (ccs)
communicating sequential processes (csp) model π-calculus tuple spaces, e.g.,-linda-simple-concurrent-object-oriented-programming (scoop)
reo-coordination-languagesome of these-models of concurrency are primarily intended to support reasoning and specification, while others can be used through the-entire-development-cycle, including design, implementation, proof, testing and simulation of concurrent-systems.
some of these are based on message-passing, while others have different-mechanisms for concurrency.
the-proliferation of different-models of concurrency has motivated some-researchers to develop ways to unify these-different-theoretical-models.
for example, lee and sangiovanni-vincentelli have demonstrated that a-so-called-"tagged-signal"-model can be used to provide a-common-framework for defining the-denotational-semantics of a-variety of different-models of concurrency, while nielsen, sassone, and winskel have demonstrated that category-theory can be used to provide a-similar-unified-understanding of different-models.
the-concurrency-representation-theorem in the-actor-model provides a-fairly-general-way to represent concurrent-systems that are closed in the-sense that they do not receive communications from outside.
other-concurrency-systems, e.g., process-calculi can be modeled in the-actor-model using a-two-phase-commit-protocol.)
the-mathematical-denotation denoted by a-closed-system-s is constructed increasingly-better-approximations from an-initial-behavior called ⊥s using a-behavior approximating function-progressions to construct a-denotation (meaning ) for s as follows: denotes ≡ ⊔i∈ω progressionsi(⊥s)in this way, s can be mathematically characterized in terms of all s possible behaviors.
logics ===
various-types of temporal-logic can be used to help reason about concurrent-systems.
some of  logics ===, such as linear-temporal-logic-and-computation-tree-logic, allow assertions to be made about the-sequences of states that a-concurrent-system can pass through.
others, such as action-computational-tree-logic, hennessy–milner-logic, and lamport's-temporal-logic of actions, build others, such as action-computational-tree-logic, hennessy–milner-logic, and lamport's-temporal-logic of actions-assertions from sequences of actions (changes in state).
the-principal-application of  logics === is in writing specifications for concurrent-systems.
practice ==
concurrent-programming encompasses programming-languages and algorithms used to implement concurrent-systems.
concurrent-programming is usually considered to be more general than parallel programming because concurrent-programming can involve arbitrary-and-dynamic-patterns of communication and interaction, whereas parallel-systems generally have a-predefined-and-well-structured-communications-pattern.
the-base-goals of concurrent-programming include correctness, performance and robustness.
concurrent-systems such as operating-systems and database-management-systems are generally designed to operate indefinitely, including automatic-recovery from failure, and not terminate unexpectedly (see concurrency-control).
some-concurrent-systems implement a-form of transparent-concurrency, in which concurrent-computational-entities may compete for and share a-single-resource, but the-complexities of this-competition and sharing are shielded from the-programmer.
because the-complexities of this-competition and sharing use shared-resources, concurrent-systems in general require the-inclusion of some-kind of arbiter somewhere in the-complexities of this-competition and sharing implementation (often in the-underlying-hardware), to control access to those-resources.
the-use of arbiters introduces the-possibility of indeterminacy in concurrent-computation which has major-implications for practice including correctness and performance.
for example, arbitration introduces unbounded-nondeterminism which raises issues with model-checking because model-checking causes explosion in the-state-space and can even cause models to have an-infinite-number of states.
some-concurrent-programming-models include coprocesses and deterministic-concurrency.
in  some-concurrent-programming-models, threads of control explicitly yield threads of control-timeslices, either to the-system or to another-process.
see also ==
chu-space-client–
server-network nodes
clojure-cluster nodes concurrency control concurrent-computing-concurrent-object-oriented-programming
concurrency pattern construction and analysis of distributed-processes
(cadp)-d (programming-language)
distributed-systemnodes-elixir (programming-language)
erlang (programming-language) go (programming-language)
gordon-pask-international-conference on concurrency-theory (concur) openmp
parallel-computing-partitioned-global-address-space processes ptolemy-project-rust (programming-language)
sheaf (mathematics)
threads-x10 (programming-language) ==
references ==
further-reading ==
lynch, nancy-a. (1996).
distributed algorithms.
morgan-kaufmann.
isbn 978-1-55860-348-6.
tanenbaum, andrew-s.; van-steen, maarten (2002).
distributed-systems: principles and paradigms.
prentice-hall.
isbn 978-0-13-088893-8.
kurki-suonio, reino (2005).
a-practical-theory of reactive-systems.
isbn 978-3-540-23342-8.
garg, vijay-k. (2002).
elements of distributed-computing.
wiley-ieee-press.
isbn 978-0-471-03600-5.
magee, jeff; kramer, jeff (2006).
concurrency: state-models and java-programming.
isbn 978-0-470-09355-9.
distefano, s., & bruneo, d. (2015).
quantitative-assessments of distributed-systems: methodologies and techniques (1st-ed.).
john-wiley & sons-inc.-isbn
9781119131144-bhattacharyya, s.-s. (2013;2014;).
handbook of signal-processing-systems (second;2;2nd 2013; ed.).
new-york, ny:
springer.10.1007/978-1-4614-6859-2-isbn 9781461468592-wolter, k. (2012;2014;).
resilience-assessment and evaluation of computing-systems (1.
aufl.;1; ed.)
london;berlin;:-springer.
isbn 9783642290329 ==
external-links ==
concurrent-systems at
the-www-virtual-library-concurrency patterns presentation given at scaleconf in computer-science, binary-space-partitioning (bsp) is a-method for recursively subdividing a-space into two-convex-sets by using hyperplanes as partitions.
this-process of subdividing gives rise to a-representation of objects within the-space in the-form of a-tree-data-structure known as a-bsp-tree.
binary-space-partitioning was developed in the-context of 3d-computer-graphics in 1969.
the-structure of a-bsp-tree is useful in rendering because a-bsp-tree can efficiently give spatial-information about the-objects in a-scene, such as objects being ordered from front-to-back with respect to a-viewer at a-given-location.
other-applications of bsp include: performing geometrical-operations with shapes (constructive-solid-geometry) in cad, collision-detection in robotics-and-3d-video-games, ray-tracing, and other-applications that involve the-handling of complex-spatial-scenes.
overview ==
binary-space-partitioning is a-generic-process of recursively dividing a-scene into two until the-partitioning satisfies one-or-more-requirements.
it can be seen as a-generalization of other-spatial-tree-structures such as k-d-trees and quadtrees, one where hyperplanes that partition the-space may have any-orientation, rather than being aligned with the-coordinate-axes as the-coordinate-axes are in k-d-trees or quadtrees.
when used in computer-graphics to render scenes composed of planar-polygons, the-partitioning-planes are frequently chosen to coincide with the-planes defined by polygons in the-scene.
the-specific-choice of partitioning-plane and criterion for terminating the-partitioning-process varies depending on the-purpose of the-bsp-tree.
for example, in computer-graphics rendering, the-scene is divided until each-node of the-bsp-tree contains only-polygons that can be rendered in arbitrary-order.
when back-face-culling is used, each-node, therefore, contains a-convex-set of polygons, whereas when rendering double-sided-polygons, each-node of the-bsp-tree contains only-polygons in a-single-plane.
in collision-detection or ray-tracing, a-scene may be divided up into primitives on which collision-or-ray-intersection-tests are straightforward.
binary-space-partitioning arose from the-computer-graphics need to rapidly draw three-dimensional-scenes composed of polygons.
a-simple-way to draw such-scenes is the-painter's-algorithm, which produces polygons in order of distance from the-viewer, back to front, painting over the-background and previous-polygons with each-closer-object.
this-approach has two-disadvantages: the-time required to sort polygons in back-to-front order, and the-possibility of errors in overlapping-polygons.
fuchs and co-authors showed that constructing a-bsp-tree solved both of these-problems by providing a-rapid-method of sorting-polygons with respect to a-given-viewpoint
(linear in the-number of polygons in the-scene) and by subdividing overlapping-polygons to avoid errors that can occur with the-painter's-algorithm.
a-disadvantage of binary-space-partitioning is that generating a-bsp-tree can be time-consuming.
typically, it is therefore performed once on static-geometry, as a-pre-calculation-step, prior to rendering or other-real-time-operations on a-scene.
the-expense of constructing a-bsp-tree makes a-bsp-tree difficult and inefficient to directly implement moving objects into a-tree.
bsp-trees are often used by 3d-video-games, particularly-first-person-shooters and those with indoor-environments.
game-engines using bsp-trees include the-doom (id tech 1), quake (id tech 2 variant), goldsrc-and-source-engines.
in  game-engines using bsp-trees, bsp-trees containing the-static-geometry of a-scene are often used together with a-z-buffer, to correctly merge movable-objects such as doors and characters onto the-background-scene.
while binary-space-partitioning provides a-convenient-way to store and retrieve spatial-information about polygons in a-scene, it does not solve the-problem of visible-surface-determination.
generation ==
the-canonical-use of a-bsp-tree is for rendering-polygons (that are double-sided, that is, without back-face-culling) with the-painter's-algorithm.
each-polygon is designated with a-front-side and a-backside which could be chosen arbitrarily and only affects the-structure of a-bsp-tree but not-the-required-result.
such-a-tree is constructed from an-unsorted-list of all-the-polygons in a-scene.
the-recursive-algorithm for construction of a-bsp-tree from that-list of polygons is: choose a-polygon-p from the-list.
make a-node n in the-bsp-tree, and add p to the-list of polygons at a-node n in the-bsp-tree.
for each-other-polygon in the-list: if each-other-polygon in the-list is wholly in front of the-plane containing p, move each-other-polygon in the-list to the-list of nodes in front of p.
if each-other-polygon in the-list is wholly behind the-plane containing p, move each-other-polygon in the-list to the-list of nodes behind p.
if each-other-polygon in the-list is intersected by the-plane containing p, split that polygon is intersected by the-plane containing p into two-polygons and move two-polygons to the-respective-lists of polygons behind and in front of p.
if that-polygon lies in the-plane containing p, add that-polygon to the-list of polygons at node n.
apply this-algorithm to the-list of polygons in front of p.
apply this-algorithm to the-list of polygons behind p.the
following diagram illustrates the-use of this-algorithm in converting a-list of lines or polygons into a-bsp-tree.
at each of the-eight-steps (i.-viii.),
the-algorithm above is applied to a-list of lines, and one-new-node is added to the-tree.
the-final-number of polygons or lines in a-tree is often larger (sometimes much larger) than the-original-list, since lines or polygons that cross the-partitioning-plane must be split into two.
it is desirable to minimize this-increase, but also to maintain reasonable-balance in the-tree.
the-choice of which-polygon or line is used as a-partitioning-plane (in step 1 of the-algorithm) is therefore important in creating an-efficient-bsp-tree.
traversal ==
a-bsp-tree is traversed in a-linear-time, in an-order determined by the-particular-function of a-bsp-tree.
again using the-example of rendering double-sided-polygons using the-painter's-algorithm, to draw a-polygon p correctly requires that all-polygons behind the-plane p lies in must be drawn first, then polygon p,-then-finally-the-polygons in front of p.
if this-drawing-order is satisfied for all-polygons in a-scene, then the-entire-scene renders in the-correct-order.
this-procedure can be implemented by recursively traversing a-bsp-tree using the-following-algorithm.
from a-given-viewing-location-v, to render a-bsp-tree, if the-current-node is a-leaf-node, render the-polygons at the-current-node.
otherwise, if the-viewing-location v is in front of the-current-node: render the-child-bsp-tree-containing-polygons behind the-current-node render the-polygons at the-current-node
render the-child-bsp-tree-containing-polygons in front of the-current-node
otherwise, if the-viewing-location v is behind the-current-node: render the-child-bsp-tree-containing-polygons in front of the-current-node render the-polygons at the-current-node render the-child-bsp-tree-containing-polygons behind the-current-node
otherwise, the-viewing-location v must be exactly on the-plane associated with the-current-node.
then: render the-child-bsp-tree-containing-polygons in front of the-current-node render the-child-bsp-tree-containing-polygons behind the-current-node applying this-algorithm recursively to the-bsp-tree generated above results in the-following-steps:
the-algorithm is first applied to the-root-node of the-tree, node-a.-v is in front of node-a, so we apply
the-algorithm first to the-child-bsp-tree-containing-polygons behind a
this-tree has root-node-b1.
v is behind b1 so first, we apply  the-algorithm to the-child-bsp-tree-containing-polygons in front of b1: this-tree is just-the-leaf-node-d1, so the-polygon-d1 is rendered.
we then render root-node-b1.
we then apply the-algorithm to the-child-bsp-tree-containing-polygons behind b1:
this-tree is just-the-leaf-node-c1, so the-polygon-c1 is rendered.
we then draw the-polygons of a
we then apply the-algorithm to the-child-bsp-tree-containing-polygons in front of a
this-tree has root-node-b2.
v is behind b2 so first, we apply the-algorithm to the-child-bsp-tree-containing-polygons in front of b2:
this-tree is just-the-leaf-node-d2, so the-polygon-d2 is rendered.
we then render b2.
we then apply the-algorithm to the-child-bsp-tree-containing-polygons behind b2:
this-tree has root-node-c2.
v is in front of c2 so first, we would apply the-algorithm to the-child-bsp-tree-containing-polygons behind c2.
there is no-such-tree, however, so we continue.
we render the-polygon-c2.
we apply the-algorithm to the-child-bsp-tree-containing-polygons in front of c2
no-such-tree is just-the-leaf-node-d3, so the-polygon-d3 is rendered.
the-tree is traversed in linear-time and renders the-polygons in a-far-to-near-ordering (d1, b1, c1, a, d2, b2, c2, d3) suitable for the-painter's-algorithm.
timeline ==
schumacker-et-al.
published a-report that described how-carefully-positioned-planes in a-virtual-environment could be used to accelerate polygon-ordering.
the-technique made use of depth-coherence, which states that a-polygon on the-far-side of the-plane cannot, in any-way, obstruct a-closer-polygon.
this was used in flight-simulators made by ge as well as evans and sutherland.
however, the-creation of the-polygonal-data-organization was performed manually by the-scene-designer.1980 fuchs-et-al.
extended schumacker's-idea to the-representation of 3d-objects in a-virtual-environment by using planes that lie coincident with polygons to recursively partition the-3d-space.
this provided a-fully-automated-and-algorithmic-generation of a-hierarchical-polygonal-data-structure known as a-binary-space-partitioning-tree (bsp-tree).
the-process took place as an off-line preprocessing step that was performed once per environment/object.
at run-time, the-view-dependent-visibility-ordering was generated by traversing the-tree.
naylor's-ph.d.-thesis provided a-full-development of both-bsp-trees and a-graph-theoretic-approach using strongly-connected-components for pre-computing-visibility, as well as the-connection between the-two-methods.
bsp-trees as a-dimension-independent-spatial-search-structure were emphasized, with applications to visible-surface-determination.
naylor's-ph.d.-thesis also included the-first-empirical-data demonstrating that the-size of the-tree and the-number of new-polygons were reasonable (using a-model of the-space-shuttle).
1983-fuchs-et-al.
described a-micro-code-implementation of the-bsp-tree-algorithm on an-ikonas-frame-buffer-system.
this was the-first-demonstration of real-time-visible-surface-determination using bsp-trees.
thibault and naylor described how arbitrary-polyhedra may be represented using a-bsp-tree as opposed to the-traditional-b-rep-(boundary-representation).
this provided a-solid-representation vs. a-surface-based-representation.
set operations on polyhedra were described using a-tool, enabling constructive-solid-geometry (csg) in real-time.
this was the-forerunner of bsp-level-design using "brushes", introduced in the-quake-editor and picked up in the-unreal-editor.
naylor, amanatides, and thibault provided an-algorithm for merging two-bsp-trees to form a-new-bsp-tree from the-two-original-trees.
this provides many-benefits including combining moving-objects represented by bsp-trees with a-static-environment (also represented by a-bsp-tree), very-efficient-csg-operations on polyhedra, exact-collisions-detection in
n * log n), and proper ordering of transparent-surfaces contained in two-interpenetrating-objects (has been used for an-x-ray-vision-effect).
teller and séquin proposed the-offline-generation of potentially-visible-sets to accelerate visible-surface-determination in orthogonal-2d-environments.
1991-gordon and chen [chen91] described an-efficient-method of performing front-to-back rendering from a-bsp-tree, rather than the traditional back-to-front approach.
they utilized a-special-data-structure to record, efficiently, parts of the-screen that have been drawn, and those yet to be rendered.
this-algorithm, together with the-description of bsp-trees in the-standard-computer-graphics-textbook of the-day
(computer-graphics: principles and practice) was used by john-carmack in the-making of doom (video-game).
teller's-ph.d.-thesis described the-efficient-generation of potentially-visible-sets as a-pre-processing-step to accelerate real-time-visible-surface-determination in arbitrary-3d-polygonal-environments.
this was used in quake and contributed significantly to that-game's-performance.
naylor answered the-question of what characterizes a-good-bsp-tree.
naylor used expected-case-models (rather than worst-case-analysis) to mathematically measure the-expected-cost of searching a-tree and used a-tree to build good-bsp-trees.
intuitively, a-tree represents an-object in a-multi-resolution-fashion (more exactly, as a-tree of approximations).
parallels with huffman-codes and probabilistic-binary-search-trees are drawn.
1993-hayder-radha's-ph.d.-thesis described (natural)-image-representation-methods using bsp-trees.
this includes the-development of an-optimal-bsp-tree-construction-framework for any-arbitrary-input-image.
an-optimal-bsp-tree-construction-framework for any-arbitrary-input-image is based on a-new-image-transform, known as the-least-square-error (lse)
partitioning-line (lpe) transform.
h.-radha's-thesis also developed an-optimal-rate-distortion-(rd)-image-compression-framework and image-manipulation-approaches using bsp-trees.
see also ==
k-d-tree-octree
quadtree-hierarchical-clustering, an-alternative-way to divide 3d-model-data for efficient-rendering.
guillotine-cutting ==
references ==
additional-references ==
[naylor90]
b.-naylor, j.-amanatides, and w.-thibualt, "merging bsp-trees-yields-polyhedral-set-operations", computer-graphics (siggraph '90), 24(3), 1990.
naylor93]-b.-naylor, "constructing good-partitioning-trees", graphics-interface (annual-canadian-cg-conference)
may, 1993.
chen91]-s.-chen and d.-gordon.
front-to-back-display of bsp-trees.”
ieee-computer-graphics & algorithms,
september 1991.
radha91]-h.-radha, r.-leoonardi, m.-vetterli, and b.-naylor “binary-space-partitioning-tree-representation of images,” journal of visual-communications and image-processing 1991, vol.
radha93]-h.-radha, "efficient-image-representation using binary-space-partitioning-trees.",
ph.d.-thesis, columbia university, 1993.
radha96]-h.-radha, m.-vetterli, and r.-leoonardi, “image-compression using binary-space-partitioning-trees,”-ieee-transactions on image-processing, vol.
5, no.12, december 1996, pp.
1610–1624.
an-investigation into real-time 3d polygon rendering using bsp-trees.
andrew-steven-winter.
april 1999.
available-online-mark-de-berg; marc-van-kreveld; mark-overmars & otfried-schwarzkopf (2000).
computational-geometry
(2nd revised ed.).
springer-verlag.
isbn 978-3-540-65620-3.
section 12: binary-space-partitions: pp.
describes a-randomized-painter's-algorithm..
christer-ericson: real-time-collision-detection (the-morgan-kaufmann-series in interactive-3-d-technology).
verlag-morgan-kaufmann, s. 349–382, jahr 2005, isbn 1-55860-732-3 ==
external-links ==
bsp-trees-tutorial bsp-trees presentation another bsp-trees presentation a-java-applet that demonstrates the-process of tree-generation
a-master-thesis about bsp generating
bsp-trees:
theory and implementation-bsp in 3d-space-timeline of computing presents events in the-history of computing organized by year and grouped into six-topic-areas: predictions and concepts, first-use and inventions, hardware-systems and processors, operating-systems, programming-languages, and new-application-areas.
detailed-computing-timelines: before 1950, 1950–1979, 1980–1989, 1990–1999, 2000-2009, 2010-2019, 2020–2029
graphical-timeline == ==
see also ==
history of compiler-construction
history of computing-hardware – up to third-generation (1960s)
history of computing-hardware (1960s–present) – third-generation and later history of the graphical user interface history of the internet history of the-world-wide-web
list of pioneers in computer-science-timeline of electrical-and-electronic-engineering ==
resources ==
stephen-white, a-brief-history of computing
the-computer-history in time and space, graphing-project, an-attempt to build a-graphical-image of computer-history, in particular-operating-systems.
the computer revolution/timeline at wikibooks "file:timeline.pdf --engineering and technology history-wiki" (pdf).
retrieved 2018 -03-03.
external-links ==
visual-history of computing
this-glossary of computer-science is a-list of definitions of terms and concepts used in computer-science, its-sub-disciplines, and related-fields, including terms relevant to software, data-science, and computer-programming.
abstract-data-type (adt)
a-mathematical-model for data-types in which a-data-type is defined by its-behavior (semantics) from the-point of view of a-user of the-data, specifically in terms of possible-values, possible-operations on data of this-type, and the-behavior of these-operations.
this contrasts with data-structures, which are concrete-representations of data from the-point of view of an-implementer rather than a-user.
abstract-method one with only-a-signature and no-implementation-body.
abstract-method one with only-a-signature and no-implementation-body is often used to specify that a-subclass must provide an-implementation of the-method.
abstract-methods are used to specify interfaces in some-computer-languages.
abstraction 1.
in software-engineering-and-computer-science, the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest; it is also very similar in nature to the-process of generalization.
the-result of the-process of generalization: an-abstract-concept-object created by keeping common-features or attributes to various-concrete-objects or systems of study.
agent-architecture
a-blueprint for software-agents and intelligent-control-systems depicting the-arrangement of components.
the-architectures implemented by intelligent-agents are referred to as cognitive-architectures.
agent-based-model (abm)
a-class of computational-models for simulating the-actions and interactions of autonomous-agents (both-individual-or-collective-entities such as organizations or groups) with a-view to assessing a-class of computational-models for simulating the-actions and interactions of autonomous-agents (both-individual-or-collective-entities such as organizations or groups) with a-view to assessing their-effects on the-system as a-whole-effects on the-system as a-whole.
it combines elements of game-theory, complex-systems, emergence, computational-sociology, multi-agent-systems, and evolutionary-programming.
monte-carlo-methods are used to introduce randomness.
aggregate-function in database-management, a-function in which the-values of multiple-rows are grouped together to form a-single-value of more-significant-meaning or measurement, such as a-sum, count, or max.
agile-software-development
an-approach to software-development under which requirements and solutions evolve through the-collaborative-effort of self-organizing-and-cross-functional-teams and their-customer(s)/end user(s).
an-approach to software-development under which requirements and solutions evolve through the-collaborative-effort of self-organizing-and-cross-functional-teams and their-customer(s)/end user(s) advocates adaptive-planning, evolutionary-development, early-delivery, and continual-improvement, and
an-approach to software-development under which requirements and solutions evolve through the-collaborative-effort of self-organizing-and-cross-functional-teams and their-customer(s)/end user(s) encourages rapid-and-flexible-response to change.
algorithm an-unambiguous-specification of how to solve a-class of problems.
algorithms can perform calculation, data-processing, and automated-reasoning-tasks.
they are ubiquitous in computing-technologies.
algorithm-design a-method-or-mathematical-process for problem-solving and for engineering-algorithms.
the-design of algorithms is part of many-solution-theories of operation-research, such as dynamic-programming and divide-and-conquer.
techniques for designing and implementing algorithm-designs are also called algorithm design patterns, such as the-template-method-pattern and decorator-pattern.
algorithmic-efficiency
a-property of an-algorithm which relates to the-number of computational-resources used by the-algorithm.
an-algorithm must be analyzed to determine an-algorithm resource usage, and the-efficiency of an-algorithm can be measured based on usage of different-resources.
algorithmic-efficiency can be thought of as analogous to engineering productivity for a-repeating-or-continuous-process.
american-standard-code for information-interchange (ascii)
a-character-encoding-standard for electronic-communications.
ascii-codes represent text in computers, telecommunications-equipment, and other-devices.
most-modern-character-encoding-schemes are based on ascii, although most-modern-character-encoding-schemes support many-additional-characters.
application-programming-interface (api)
a-set of subroutine-definitions, communication-protocols, and tools for building-software.
in general-terms, it is a-set of clearly-defined-methods of communication among various-components.
a-good-api makes a-good-api easier to develop a-computer-program by providing all-the-building-blocks, which are then put together by the-programmer.
application-software also simply application or app.
computer-software designed to perform a-group of coordinated-functions, tasks, or activities for the-benefit of the-user.
common-examples of applications include word-processors, spreadsheets, accounting-applications, web-browsers, media-players, aeronautical-flight-simulators, console-games, and photo-editors.
this contrasts with system-software, which is mainly involved with managing the-computer's-most-basic-running-operations, often without direct-input from the-user.
the-collective-noun-application-software refers to all-applications collectively.
array-data-structure also simply array.
a-data-structure consisting of a-collection of elements (values or variables), each identified by at-least-one-array-index or key.
an-array is stored such that the-position of each-element can be computed from its-index-tuple by a-mathematical-formula.
the-simplest-type of data-structure is a-linear-array, also called a-one-dimensional-array.
artifact one of many-kinds of tangible-by-products produced during the-development of software.
some-artifacts (e.g. use cases, class-diagrams, and other-unified-modeling-language (uml) models, requirements, and design-documents) help describe the-function, architecture, and design of software.
other-artifacts are concerned with the-process of development itself—such as project-plans, business-cases, and risk-assessments.
artificial-intelligence (ai)
also-machine-intelligence.
intelligence demonstrated by machines, in contrast to the-natural-intelligence displayed by humans and other-animals.
in computer-science, ai-research is defined as the-study of "intelligent-agents": devices capable of perceiving ai-research environment and taking actions that maximize the-chance of successfully achieving ai-research goals.
colloquially, the-term "artificial-intelligence" is applied when a-machine mimics "cognitive"-functions that humans associate with other-human-minds, such as "learning" and "problem solving".
ascii see american-standard-code for information-interchange.
in computer-programming,-a-statement that a-predicate (boolean-valued-function, i.e.-a-true–false-expression) is always true at that-point in code-execution.
it can help a-programmer read the-code, help a-compiler compile it, or help the-program detect it own defects.
for the latter, some-programs check assertions by actually evaluating the-predicate as some-programs run
and if it is not in fact true – an assertion failure –
the-program considers the-program to be broken and typically deliberately crashes or throws an-assertion-failure-exception.
associative-array
an-associative-array, map, symbol-table, or dictionary is an-abstract-data-type composed of a-collection of (key,-value)-pairs, such that each-possible-key appears at most once in the-collection.
operations associated with this-data-type allow:
the-addition of a-pair to the-collection the-removal of a-pair from the-collection the-modification of an-existing-pair
the-lookup of a-value associated with a-particular-key-automata-theory
the-study of abstract-machines and automata, as well as the-computational-problems that can be solved using as well.
it is a-theory in theoretical-computer-science and discrete-mathematics (a-subject of study in both-mathematics and computer science).
automated-reasoning
an-area of computer-science and mathematical-logic dedicated to understanding different-aspects of reasoning.
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
although automated-reasoning is considered a sub-field of artificial-intelligence, the-study of automated-reasoning also has connections with theoretical-computer-science, and even-philosophy.
bandwidth the-maximum-rate of data-transfer across a-given-path.
bandwidth may be characterized as network-bandwidth, data-bandwidth, or digital-bandwidth.
bayesian-programming-a-formalism and a-methodology for having a-technique to specify probabilistic-models and solve problems when less than the-necessary-information is available.
benchmark the-act of running a-computer-program, a-set of programs, or other-operations, in order to assess the-relative-performance of an-object, normally by running a-number of standard-tests and trials against it.
the-term-benchmark is also commonly utilized for the-purposes of elaborately-designed-benchmarking-programs themselves.
best,-worst-and-average-case-expressions of what the-resource-usage is at least, at most, and on average, respectively, for a-given-algorithm.
usually the-resource being considered is running time, i.e. time complexity, but the-resource being considered could also be memory or some-other-resource.
best-case is the-function which performs the-minimum-number of steps on input-data of n-elements; worst-case is the-function which performs the-maximum-number of steps on input-data of size n; average-case is the-function which performs an-average-number of steps on input-data of n-elements.
a-term used to refer to data-sets that are too large or complex for traditional-data-processing-application-software to adequately deal with.
data with many-cases (rows) offer greater-statistical-power, while data with higher-complexity (more-attributes or columns) may lead to a-higher-false-discovery-rate.
big-o-notation
a-mathematical-notation that describes the-limiting-behavior of a-function when the-argument tends towards a-particular-value or infinity.
it is a-member of a-family of notations invented by paul-bachmann, edmund-landau, and others, collectively called bachmann–landau-notation or asymptotic-notation.
binary-number in mathematics-and-digital-electronics, a-number expressed in the-base-2-numeral-system or binary-numeral-system, which uses only-two-symbols: typically 0 (zero) and 1 (one).
binary-search-algorithm
also-simply-binary-search, half-interval-search, logarithmic-search, or binary-chop.
a-search-algorithm that finds the-position of a-target-value within a-sorted-array.
binary-tree
a-tree-data-structure in which each-node has at most-two-children, which are referred to as the-left-child and the-right-child.
a-recursive-definition using just-set-theory-notions is that a-(non-empty)-binary-tree is a-tuple (l, s,-r), where l and r are binary-trees or the-empty-set and s is a-singleton-set.
some-authors allow the-binary-tree to be the-empty-set as well.
bioinformatics
an-interdisciplinary-field that combines biology, computer-science, information-engineering, mathematics, and statistics to develop methods-and-software-tools for analyzing and interpreting biological-data.
bioinformatics is widely used for in silico-analyses of biological-queries using mathematical-and-statistical-techniques.
bit a-basic-unit of information used in computing-and-digital-communications; a-portmanteau of binary-digit.
a-binary-digit can have one of two-possible-values, and may be physically represented with a-two-state-device.
these-state-values are most commonly represented as either a 0or1.
bit-rate (r)
also-bitrate.
in telecommunications and computing,-the-number of bits that are conveyed or processed per unit of time.
blacklist also block list.
in computing,-a-basic-access-control-mechanism that allows through all-elements (email-addresses, users, passwords, urls, ip-addresses, domain-names, file-hashes, etc.),
except those explicitly mentioned in a-list of prohibited-elements.
those-items on the-list are denied access.
the-opposite is a-whitelist, which means only-items on the-list are allowed through whatever-gate is being used while all-other-elements are blocked.
a-greylist contains items that are temporarily blocked (or temporarily allowed) until an-additional-step is performed.
bmp-file-format
also-bitmap-image-file, device-independent-bitmap (dib)-file-format, or simply bitmap.
a-raster-graphics-image-file-format used to store bitmap-digital-images independently of the-display-device (such as a-graphics-adapter), used especially on microsoft-windows and os/2-operating-systems.
boolean-data-type
a-data-type that has one of two-possible-values (usually denoted true and false), intended to represent the-two-truth-values of logic and boolean algebra.
a-data-type that has one of two-possible-values (usually denoted true and false) is named after george-boole, who first defined an-algebraic-system of logic in the-mid-19th-century.
the-boolean-data-type is primarily associated with conditional-statements, which allow different-actions by changing control-flow depending on whether a-programmer-specified-boolean-condition evaluates to true or false.
it is a-special-case of a-more-general-logical-data-type (see probabilistic-logic)—i.e.
logic need not always be boolean.
boolean-expression
an-expression used in a-programming-language that returns a-boolean-value when evaluated, that is one of true or false.
a-boolean-expression may be composed of a-combination of the-boolean-constants true-or-false,-boolean-typed-variables, boolean-valued-operators, and boolean-valued-functions.
boolean-algebra in mathematics and mathematical-logic
, the-branch of algebra in which the-values of the-variables are the-truth values true and false, usually denoted 1 and 0, respectively.
contrary to elementary-algebra, where the-values of the-variables are numbers and the-prime-operations are addition and multiplication, the-main-operations of boolean-algebra are the-conjunction and (denoted as ∧), the-disjunction or (denoted as ∨), and the-negation not (denoted as ¬).
it is thus a-formalism for describing logical-relations in the-same-way that elementary-algebra describes numeric-relations.
byte a-unit of digital-information that most commonly consists of eight-bits, representing a-binary-number.
historically,  byte a-unit of digital-information that most commonly consists of eight-bits, representing a-binary-number was the-number of bits used to encode a-single-character of text in a-computer and for this-reason  byte a-unit of digital-information that most commonly consists of eight-bits, representing a-binary-number
is the-smallest-addressable-unit of memory in many-computer-architectures.
booting the-procedures implemented in starting up a-computer-or-computer-appliance until it can be used.
it can be initiated by hardware such as a-button-press or by a-software-command.
after the-power is switched on, the-computer is relatively dumb and can read only-part of the-computer storage called read-only memory.
there, a-small-program is stored called firmware.
a-small-program does power-on self-tests and, most importantly, allows access to other-types of memory like a-hard-disk and main-memory.
called firmware loads bigger-programs into the-computer's-main-memory and runs called firmware.
c == callback also a-call-after-function.
any-executable-code that is passed as an-argument to other-code that is expected to "call back" (execute) the-argument at a-given-time.
this-execution may be immediate, as in a-synchronous-callback, or  this-execution might happen at a-later-time, as in an-asynchronous-callback.
central-processing-unit (cpu)
the-electronic-circuitry within a-computer that carries out the-instructions of a-computer program by performing the-basic-arithmetic,-logic,-controlling,-and-input/output-(i/o)-operations specified by the-instructions.
the-computer-industry has used the-term "central-processing-unit" at least since the-early-1960s.
traditionally, the-term-"cpu" refers to a-processor, more specifically to the-computer-industry-processing-unit-and-control-unit (cu), distinguishing these-core-elements of a-computer from external-components such as main-memory and i/o-circuitry.
character-a-unit of information that roughly corresponds to a-grapheme, grapheme-like-unit, or symbol, such as in an-alphabet or syllabary in the-written-form of a-natural-language.
cipher also cypher.
in cryptography,-an-algorithm for performing encryption or decryption—a-series of well-defined-steps that can be followed as a-procedure.
in object-oriented-programming, an extensible program-code-template for creating objects, providing initial-values for state (member-variables) and implementations of behavior (member-functions or methods).
in many-languages, the-class-name is used as the-name for the-class (the-template itself), the-name for the-default-constructor of the-class (a-subroutine that creates objects), and as the-type of objects generated by instantiating the-class; these-distinct-concepts are easily conflated.
class-based-programming also
class-orientation.
a-style of object-oriented-programming (oop) in which inheritance occurs via defining "classes" of objects, instead of via the-objects alone.
compare prototype-based-programming.
class-orientation
a-style of object-oriented-programming (oop) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the-objects alone (compare prototype-based-programming).
a-piece of computer-hardware or software that accesses a-service made available by a-server.
a-server is often (but not always) on another-computer-system, in which-case the-client accesses the-service by way of a-network.
the-term applies to the-role that programs or devices play in the-client–server-model.
cleanroom-software-engineering-a-software-development-process intended to produce software with a-certifiable-level of reliability.
the-cleanroom-process was originally developed by harlan-mills and several of harlan-mills colleagues including alan-hevner at ibm.
the-focus of  the-cleanroom-process is on defect-prevention, rather than defect-removal.
closure also lexical-closure or function-closure.
a-technique for implementing lexically scoped name binding in a-language with first-class-functions.
operationally, a-closure is a-record storing a-function together with an-environment.
cloud computing shared-pools of configurable-computer-system-resources and higher-level-services that can be rapidly provisioned with minimal-management-effort, often over the-internet.
cloud-computing relies on sharing of resources to achieve coherence and economies of scale, similar to a-public-utility.
code-library
a-collection of non-volatile-resources used by computer-programs, often for software-development.
these may include configuration-data, documentation, help data, message-templates, pre-written-code and subroutines, classes, values or type-specifications.
in ibm's-os/360-and-ibm-successors-ibm's-os/360 and its-successors are referred to as partitioned data-sets.
coding-computer-programming is the-process of designing and building an-executable-computer-program for accomplishing a-specific-computing-task.
programming involves tasks such as analysis, generating algorithms, profiling algorithms'-accuracy-and-resource-consumption, and the-implementation of algorithms in a-chosen-programming-language (commonly referred to as coding).
the-source-code of a-program is written in one-or-more-programming-languages.
the-purpose of programming is to find a-sequence of instructions that will automate the-performance of a-task for solving a-given-problem.
the-process of programming thus often requires expertise in several-different-subjects, including knowledge of the-application-domain, specialized-algorithms, and formal-logic.
coding-theory
the-study of the-properties of codes and codes respective-fitness for specific-applications.
codes are used for data-compression, cryptography, error-detection and correction, data-transmission and data storage.
codes are studied by various-scientific-disciplines—such as information-theory, electrical-engineering, mathematics, linguistics, and computer-science—for the-purpose of designing efficient-and-reliable-data-transmission-methods.
this typically involves the-removal of redundancy and the-correction or detection of errors in the-transmitted-data.
cognitive-science
the-interdisciplinary,-scientific-study of the-mind and
the-interdisciplinary,-scientific-study of the-mind and its-processes-processes.
the-interdisciplinary,-scientific-study of the-mind and its-processes examines the-nature, the-tasks, and the-functions of cognition (in a-broad-sense).
cognitive-scientists study intelligence and behavior, with a-focus on how nervous systems represent, process, and transform information.
mental-faculties of concern to cognitive-scientists include language, perception, memory, attention, reasoning, and emotion; to understand mental-faculties of concern to cognitive-scientists, cognitive-scientists borrow from fields such as linguistics, psychology, artificial-intelligence, philosophy, neuroscience, and anthropology.
collection
a-collection or container is a-grouping of some-variable-number of data-items (possibly zero) that have some-shared-significance to the-problem being solved and need to be operated upon together in some-controlled-fashion.
generally, the-data-items will be of the-same-type or, in languages supporting inheritance, derived from some-common-ancestor-type.
a-collection is a-concept applicable to abstract-data-types, and does not prescribe a-specific-implementation as a-concrete-data-structure, though often there is a-conventional-choice (see container for type-theory-discussion).
comma-separated-values (csv)
a-delimited-text-file that uses a-comma to separate-values.
a-csv-file-stores-tabular-data (numbers and text) in plain-text.
each-line of the-file is a-data-record.
each-record consists of one-or-more-fields, separated by commas.
the-use of the-comma as a-field-separator is the-source of the-name for this-file-format.
a-computer-program that transforms computer-code written in one-programming-language (the-source-language) into another-programming-language (the-target-language).
compilers are a-type of translator that support digital-devices, primarily-computers.
the-name-compiler is primarily used for programs that translate source-code from a-high-level-programming-language to a-lower-level-language (e.g.-assembly-language, object-code, or machine-code) to create an-executable-program.
computability-theory also known as recursion-theory, is a-branch of mathematical-logic, of computer-science, and of the-theory of computation that originated in the-1930s with the-study of computable-functions and turing-degrees.
the-field has since expanded to include the-study of generalized-computability and definability.
in these-areas, recursion-theory overlaps with proof-theory and effective-descriptive-set-theory.
computation any-type of calculation that includes both-arithmetical-and-non-arithmetical-steps and follows a-well-defined-model, e.g.-an-algorithm.
the-study of computation is paramount to the-discipline of computer-science.
computational-biology involves the-development and application of data-analytical-and-theoretical-methods, mathematical-modelling and computational-simulation techniques to the-study of biological,-ecological,-behavioural,-and-social-systems.
the-field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular-biology, genetics, genomics, computer-science, and evolution.
computational-biology is different from biological-computing, which is a-subfield of computer-science and computer-engineering using bioengineering and biology to build computers.
computational-chemistry
a-branch of chemistry that uses computer-simulation to assist in solving chemical-problems.
a-branch of chemistry that uses computer-simulation to assist in solving chemical-problems uses methods of theoretical-chemistry, incorporated into efficient-computer-programs, to calculate the-structures and properties of molecules and solids.
computational-complexity-theory
a-subfield of computational-science which focuses on classifying computational-problems according to computational-problems inherent-difficulty, and relating these-classes to each other.
a-computational-problem is a-task solved by a-computer.
a-computation-problem is solvable by mechanical-application of mathematical-steps, such as an-algorithm.
computational-model
a-mathematical-model in computational-science that requires extensive-computational-resources to study the-behavior of a-complex-system by computer-simulation.
computational-neuroscience
also-theoretical-neuroscience or mathematical-neuroscience.
a-branch of neuroscience which employs mathematical-models, theoretical-analysis, and abstractions of the-brain to understand the-principles that govern the-development, structure, physiology, and cognitive abilities of the-nervous-system.
computational-physics is the-study and implementation of numerical-analysis to solve problems in physics for which a-quantitative-theory already exists.
historically, computational-physics was the-first-application of modern-computers in science, and is now a-subset of computational-science.
computational-science
also scientific computing and scientific computation (sc).
an-interdisciplinary-field that uses advanced-computing-capabilities to understand and solve complex-problems.
it is an-area of science which spans many-disciplines, but at it core it involves the-development of computer-models and simulations to understand complex-natural-systems.
computational-steering is the-practice of manually intervening with an-otherwise-autonomous-computational-process, to change computational-steering outcome.
a-device that can be instructed to carry out sequences of arithmetic-or-logical-operations automatically via computer-programming.
modern-computers have the-ability to follow generalized-sets of operations, called programs.
programs enable computers to perform an-extremely-wide-range of tasks.
computer-architecture
a-set of rules and methods that describe the-functionality, organization, and implementation of computer-systems.
some-definitions of architecture define it as describing the-capabilities and programming-model of a-computer but not a-particular-implementation.
in other-definitions computer-architecture involves instruction-set-architecture-design, microarchitecture-design, logic-design, and implementation.
computer-data-storage also simply storage or memory.
a-technology consisting of computer-components and recording-media that are used to retain digital-data.
data-storage is a-core-function and fundamental-component of all-modern-computer-systems.
computer-ethics
a-part of practical-philosophy concerned with how computing-professionals should make decisions regarding professional-and-social-conduct.
computer-graphics
pictures and films created using computers.
usually, the-term refers to computer-generated-image-data created with the-help of specialized-graphical-hardware and software.
it is a-vast-and-recently-developed-area of computer-science.
computer-network
also-data-network.
a-digital-telecommunications-network which allows nodes to share resources.
in computer-networks, computing-devices-exchange-data with each other using connections (data-links) between nodes.
data-links) between nodes are established over cable-media such as wires or optic-cables, or wireless-media such as wi-fi.
computer-program is a-collection of instructions that can be executed by a-computer to perform a-specific-task.
computer-programming the-process of designing and building an-executable-computer-program for accomplishing a-specific-computing-task.
programming involves tasks such as analysis, generating algorithms, profiling algorithms'-accuracy-and-resource-consumption, and the-implementation of algorithms in a-chosen-programming-language (commonly referred to as coding).
the-source-code of a-program is written in one-or-more-programming-languages.
the-purpose of programming is to find a-sequence of instructions that will automate the-performance of a-task for solving a-given-problem.
the-process of programming thus often requires expertise in several-different-subjects, including knowledge of the-application-domain, specialized-algorithms, and formal-logic.
computer-science
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers.
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
a-computer-scientist specializes in the-theory of computation and the-design of computational-systems.
computer-scientist-a-person who has acquired the-knowledge of computer-science, the-study of the-theoretical-foundations of information and computation and information-and-computation-application.
computer-security
also-cybersecurity-or-information-technology-security (it security).
the-protection of computer-systems from theft or damage to their-hardware, software, or electronic-data, as well as from disruption or misdirection of the-services their provide.
computer-vision
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
is any-goal-oriented-activity requiring, benefiting from, or creating computing-machinery.
any-goal-oriented-activity requiring, benefiting from, or creating computing-machinery includes study of algorithmic-processes and development of both-hardware and software.
any-goal-oriented-activity requiring, benefiting from, or creating computing-machinery has scientific,-engineering,-mathematical,-technological-and-social-aspects.
major-computing-fields include computer-engineering, computer-science, cybersecurity, data-science, information-systems, information-technology and software engineering.
concatenation
in formal-language-theory and computer-programming, string-concatenation  is the-operation of joining character-strings end-to-end.
for example, the-concatenation of "snow" and "ball" is "snowball".
in certain-formalisations of concatenation-theory, also called string-theory, string-concatenation is a-primitive-notion.
concurrency
the-ability of different-parts or units of a-program, algorithm, or problem to be executed out-of-order or in partial-order, without affecting the-final-outcome.
this allows for parallel-execution of the-concurrent-units, which can significantly improve overall-speed of the-execution in multi-processor-and-multi-core-systems.
in more-technical-terms, concurrency refers to the-decomposability-property of a-program, algorithm, or problem into order-independent-or-partially-ordered-components or units.
conditional-also-conditional-statement, conditional-expression, and conditional-construct.
a-feature of a-programming-language which performs different-computations or actions depending on whether a-programmer-specified-boolean-condition evaluates to true or false.
apart from the-case of branch-predication, this is always achieved by selectively altering the-control-flow based on some-condition.
container is a-class, a-data-structure, or an-abstract-data-type (adt) whose-instances are collections of other-objects.
in other-words, they store objects in an-organized-way that follows specific-access-rules.
the-size of the-container depends on the-number of objects (elements)
the-size of the-container contains.
underlying (inherited)
implementations of various-container-types may vary in size and complexity, and provide flexibility in choosing the-right-implementation for any-given-scenario.
continuation-passing-style (cps)
a-style of functional-programming in which control is passed explicitly in the-form of a-continuation.
this is contrasted with direct-style, which is the-usual-style of programming.
gerald-jay-sussman and guy-l.-steele,-jr. coined the-phrase in ai-memo 349 (1975), which sets out the-first-version of the-scheme-programming-language.
control-flow
also flow of control.
the-order in which individual-statements, instructions or function-calls of an-imperative-program are executed or evaluated.
the-emphasis on explicit-control-flow distinguishes an-imperative-programming-language from a-declarative-programming-language.
creative-commons (cc)
an-american-non-profit-organization devoted to expanding the-range of creative-works available for others to build upon legally and to share.
an-american-non-profit-organization devoted to expanding the-range of creative-works available for others to build upon legally and to share has released several-copyright-licenses, known as creative-commons-licenses, free of charge to the-public.
cryptography or cryptology,  is the-practice and study of techniques for secure-communication in the-presence of third-parties called adversaries.
more generally, cryptography is about constructing and analyzing protocols that prevent third-parties or the-public from reading private-messages; various-aspects in information-security such as data-confidentiality, data-integrity, authentication, and non-repudiation are central to modern-cryptography.
modern-cryptography exists at the-intersection of the-disciplines of mathematics, computer-science, electrical-engineering, communication-science, and physics.
applications of cryptography include electronic-commerce, chip-based-payment-cards, digital-currencies, computer-passwords, and military-communications.
csv see comma-separated-values.
cyberbullying
also-cyberharassment or online-bullying.
a-form of bullying or harassment using electronic-means.
cyberspace-widespread,-interconnected-digital-technology.
in multitasking-computer-operating-systems, a-daemon ( or ) is a-computer-program that runs as a-background-process, rather than being under the-direct-control of an-interactive-user.
traditionally,-the-process-names of a-daemon-end with the-letter-d, for clarification that the-process is in fact a-daemon, and for differentiation between a-daemon and a-normal-computer-program.
for example, syslogd is a-daemon that implements system-logging-facility, and sshd is a-daemon that serves incoming-ssh-connections.
data-center
also-data-centre.
a-dedicated-space used to house computer-systems and associated-components, such as telecommunications-and-data-storage-systems.
a-dedicated-space used to house computer-systems and associated-components, such as telecommunications-and-data-storage-systems generally includes redundant-or-backup-components and infrastructure for power-supply, data-communications-connections, environmental-controls (e.g. air conditioning and fire suppression) and various-security-devices.
database an-organized-collection of data, generally stored and accessed electronically from a-computer-system.
where databases are more complex, databases are often developed using formal-design-and-modeling-techniques.
data-mining is a-process of discovering patterns in large-data-sets involving methods at the-intersection of machine-learning, statistics, and database-systems.
data-mining is an-interdisciplinary-subfield of computer-science and statistics with an-overall-goal to extract information (with intelligent-methods) from a data set and transform the-information into a-comprehensible-structure for further-use.
data-mining is the-analysis-step of the "knowledge discovery in databases" process, or kdd.
aside from the-raw-analysis-step, the-raw-analysis-step also involves database-and-data-management-aspects, data-pre-processing,-model-and-inference-considerations, interestingness-metrics, complexity-considerations, post-processing of discovered-structures, visualization, and online-updating.
data-science
an-interdisciplinary-field that uses scientific-methods, processes, algorithms, and systems to extract knowledge and insights from data in various-forms, both structured and unstructured, similar to data-mining.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
data-science employs techniques and theories drawn from many-fields within the-context of mathematics, statistics, information-science, and computer-science.
data-structure
a-data-organization, management,-and-storage-format that enables efficient-access and modification.
more precisely, a-data-structure is a-collection of data-values, the-relationships among a-collection of data-values, and the-functions or operations that can be applied to the-data.
data-type also simply type.
an-attribute of data which tells the-compiler or interpreter how the-programmer intends to use the-data.
most-programming-languages support common-data-types of real, integer, and boolean.
a-data-type constrains the-values that an-expression, such as a-variable or a-function, might take.
a-data-type defines the-operations that can be done on the-data, the-meaning of the-data, and the-way-values of that-type can be stored.
a-type of value from which an-expression may take an-expression value.
debugging the-process of finding and resolving defects or problems within a-computer-program that prevent correct-operation of computer-software or the-system as a-whole.
debugging-tactics can involve interactive-debugging, control-flow-analysis, unit-testing, integration-testing, log file-analysis, monitoring at the-application-or-system-level, memory dumps, and profiling.
declaration in computer-programming, a-language-construct that specifies properties of an-identifier: it declares what a-word (identifier) "means".
declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other-entities such as enumerations and type-definitions.
beyond the-name (the-identifier itself) and the-kind of entity (function, variable, etc.),
declarations typically specify the-data-type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays.
a-declaration is used to announce the-existence of the-entity to the-compiler; this is important in those-strongly-typed-languages that require functions, variables, and constants, and their-types, to be specified with a-declaration before use, and is used in forward-declaration.
a-declaration is frequently contrasted with the-term-"definition", but meaning and usage varies significantly between languages.
digital-data
in information-theory and information-systems, the-discrete,-discontinuous-representation of information or works.
numbers and letters are commonly-used-representations.
digital-signal-processing (dsp)
the-use of digital-processing, such as by computers or more-specialized-digital-signal-processors, to perform a-wide-variety of signal-processing-operations.
the-signals processed in this-manner are a-sequence of numbers that represent samples of a-continuous-variable in a-domain such as time, space, or frequency.
discrete-event-simulation (des)
a-model of the-operation of a-system as a-discrete-sequence of events in time.
each-event occurs at a-particular-instant in time and marks a-change of state in the-system.
between consecutive-events, no-change in the-system is assumed to occur; thus the-simulation can directly jump in time from one-event to the next.
disk-storage (also sometimes called drive storage) is a-general-category of storage-mechanisms where data is recorded by various-electronic,-magnetic,-optical,-or-mechanical-changes to a-surface-layer of one-or-more-rotating-disks.
a-disk-drive is a-device implementing such-a-storage-mechanism.
notable-types are the-hard-disk-drive (hdd) containing a-non-removable-disk, the--floppy-disk-drive (fdd) and fdd removable-floppy-disk, and various-optical-disc-drives (odd) and associated-optical-disc-media.
distributed-computing
a-field of computer-science that studies distributed systems.
a-distributed-system is a-system whose-components are located on different-networked-computers, which communicate and coordinate whose-components actions by passing messages to one another.
the-components interact with one another in order to achieve a-common-goal.
three-significant-characteristics of distributed-systems are: concurrency of components, lack of a-global-clock, and independent-failure of components.
examples of distributed-systems vary from soa-based-systems to massively-multiplayer-online-games to peer-to-peer applications.
divide-and-conquer-algorithm
an-algorithm-design-paradigm based on multi-branched-recursion.
a-divide-and-conquer-algorithm works by recursively breaking down a-problem into two-or-more-sub-problems of the-same-or-related-type, until these become simple enough to be solved directly.
the-solutions to the-sub-problems are then combined to give a-solution to the-original-problem.
see domain-name-system.
documentation
written-text or illustration that accompanies computer-software or is embedded in the-source-code.
it either explains how it operates or how to use it, and may mean different-things to people in different-roles.
is the-targeted-subject-area of a-computer-program.
it is a-term used in software-engineering.
formally it represents the target subject of a-specific-programming-project, whether narrowly or broadly defined.
domain-name-system (dns) a-hierarchical-and-decentralized-naming-system for computers, services, or other-resources connected to the-internet or to a-private-network.
it associates various-information with domain-names assigned to each of the-participating-entities.
most prominently,  it translates more-readily-memorized-domain-names to the-numerical-ip-addresses needed for locating and identifying computer-services and devices with the-underlying-network-protocols.
by providing a worldwide, distributed-directory-service, the-domain-name-system has been an-essential-component of the-functionality of the-internet since 1985.
double-precision-floating-point-format a-computer-number-format.
it represents a-wide-dynamic-range of numerical-values by using a-floating-radix-point.
download in computer-networks, to receive data from a-remote-system, typically-a-server such as a-web-server, an-ftp-server, an-email-server, or other-similar-systems.
this contrasts with uploading, where data is sent to a-remote-server.
a-download is a-file offered for downloading or that has been downloaded, or the-process of receiving such a-file.
e-==-edge-device
a-device which provides an-entry-point into enterprise-or-service-provider-core-networks.
examples include routers, routing-switches, integrated-access-devices (iads), multiplexers, and a-variety of metropolitan-area-network (man) and wide-area-network (wan)-access-devices.
edge-devices also provide connections into carrier-and-service-provider-networks.
an-edge-device that connects a-local-area-network to a-high-speed-switch or backbone (such as an-atm-switch) may be called an edge concentrator.
encryption
in cryptography, encryption is the-process of encoding-information.
the-process of encoding information converts the-original-representation of the-information, known as plaintext, into an-alternative-form known as ciphertext.
ideally, only-authorized-parties can decipher a ciphertext back to plaintext and access the-original-information.
encryption does not  encryption prevent interference but denies the-intelligible-content to a-would-be-interceptor.
for technical-reasons, an-encryption-scheme usually uses a-pseudo-random-encryption-key generated by an-algorithm.
it is possible to decrypt the-message without possessing the-key, but, for a-well-designed-encryption-scheme, considerable-computational-resources and skills are required.
an-authorized-recipient can easily decrypt the-message with the-key provided by the-originator to recipients but not to unauthorized-users.
historically, various-forms of encryption have been used to aid in cryptography.
early-encryption-techniques were often utilized in military-messaging.
since then, new-techniques have emerged and become commonplace in all-areas of modern-computing.
modern-encryption-schemes utilize the-concepts of public-key and symmetric-key.
modern-encryption-techniques ensure security because modern-computers are inefficient at cracking the-encryption.
an-action or occurrence recognized by software, often originating asynchronously from the-external-environment, that may be handled by software.
because an-event is an-entity which encapsulates the-action and the-contextual-variables triggering the-action,
the-acrostic-mnemonic-"execution-variable
encapsulating named-trigger" is often used to clarify the-concept.
event-driven-programming
a-programming-paradigm in which the-flow of the-program is determined by events such as user-actions (mouse-clicks, key-presses), sensor-outputs, or messages from other-programs or threads.
event-driven-programming is the-dominant-paradigm used in graphical-user-interfaces and other-applications (e.g.-javascript-web-applications) that are centered on performing certain-actions in response to user-input.
this is also true of programming for device-drivers (e.g.-p in usb-device-driver-stacks).
evolutionary-computing-a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms.
in technical-terms,  in technical-terms are a-family of population-based-trial-and-error-problem-solvers with a-metaheuristic-or-stochastic-optimization-character.
executable-also-executable-code, executable-file, executable-program, or simply executable.
causes a-computer "to perform indicated-tasks according to encoded-instructions," as opposed to a-data-file that must be parsed by a-program to be meaningful.
the-exact-interpretation depends upon the-use - while "instructions" is traditionally taken to mean machine-code-instructions for a-physical-cpu, in some-contexts a file containing bytecode or scripting language instructions may also be considered executable.
executable-module-execution in computer-and-software-engineering is the-process by which a-computer or  virtual-machine executes the-instructions of a-computer program.
each-instruction of a-program is a-description of a-particular--action which to be carried out in order for a-specific-problem to be solved; as instructions of a-program and therefore the-actions they describe are being carried out by an-executing-machine, specific-effects are produced in accordance to the-semantics of the-instructions being executed.
exception handling the-process of responding to the-occurrence, during computation, of exceptions – anomalous-or-exceptional-conditions requiring special-processing – often disrupting the-normal-flow of program-execution.
exception handling the-process of responding to the-occurrence, during computation, of exceptions – anomalous-or-exceptional-conditions requiring special-processing – often disrupting the-normal-flow of program-execution is provided by specialized-programming-language-constructs, computer-hardware-mechanisms like interrupts, or operating-system
ipc-facilities like signals.
expression
in a-programming-language, a combination of one-or-more-constants, variables, operators, and functions that the-programming-language interprets (according to the-programming-language particular-rules of precedence and of association) and computes to produce ("to return", in a-stateful-environment) another-value.
this-process, as for mathematical-expressions, is called evaluation.
external-library ==-f-==-fault-tolerant-computer-system
a-system designed around the-concept of fault-tolerance.
in essence, they must be able to continue working to a-level of satisfaction in the-presence of errors or breakdowns.
feasibility-study
an-investigation which aims to objectively and rationally uncover the-strengths and weaknesses of an-existing-business or proposed-venture, opportunities and threats present in the-natural-environment, the-resources required to carry through, and ultimately-the-prospects for success.
in its-simplest-terms, the-two-criteria to judge feasibility are cost required and value to be attained.
field-data that has several-parts, known as a-record, can be divided into fields.
relational-databases arrange data as sets of database-records, so called rows.
each-record consists of several-fields; the-fields of all-records form the-columns.
examples of fields: name, gender, hair-colour.
filename-extension
an-identifier specified as a-suffix to the-name of a-computer-file.
filename-extension indicates a-characteristic of the-file-contents or  filename-extension intended use.
filter-(software)
a-computer-program or subroutine to process a-stream, producing another-stream.
while a-single-filter can be used individually,   are frequently strung together to form a-pipeline.
floating point arithmetic in computing
, floating-point arithmetic (fp) is arithmetic using formulaic-representation of real-numbers as an-approximation to support a-trade-off between range and precision.
for this-reason, floating-point-computation is often found in systems which include very-small-and-very-large-real-numbers, which require fast-processing-times.
a-number is, in general, represented approximately to a-fixed-number of significant-digits (the-significand) and scaled using an-exponent in some-fixed-base; the-base for the-scaling is normally two, ten, or sixteen.
a-number that can be represented exactly is of the-following-form: significand---------×-base-exponent
,     {\displaystyle {\text{significand}}\times {\text{base}}^{\text{exponent}},} where significand is an-integer, base is an-integer greater than or equal to two, and exponent is also an-integer.
for example:         1.2345
12345 ⏟             significand ×
10---------------⏟-base-− 4
exponent         .
\displaystyle-1.2345=\underbrace-{12345}-_{\text{significand}}\times-\underbrace {10}-_{\text{base}}\!\!\!\!\!\!^{\overbrace {-4} ^{\text{exponent}}}.}
also for-loop.
a-control-flow-statement for specifying iteration, which allows code to be executed repeatedly.
various-keywords are used to specify this-statement: descendants of algol use "for", while descendants of fortran-use "do".
there are also other-possibilities, e.g.-cobol uses "perform varying".
formal-methods
a-set of mathematically-based-techniques for the-specification, development, and verification of software-and-hardware-systems.
the-use of formal-methods for software-and-hardware-design is motivated by the-expectation that, as in other-engineering-disciplines, performing appropriate-mathematical-analysis can contribute to the-reliability and robustness of a-design.
formal-verification
the-act of proving or disproving the-correctness of intended-algorithms underlying a-system with respect to a-certain-formal-specification or property, using formal-methods of mathematics.
functional-programming
a-programming-paradigm—a-style of building the-structure and elements of computer-programs–that treats computation as the-evaluation of mathematical-functions and avoids changing-state-and-mutable-data.
it is a-declarative-programming-paradigm in that-programming is done with expressions or declarations instead of statements.
g-==-game-theory
the-study of mathematical-models of strategic-interaction between rational-decision-makers.
the-study of mathematical-models of strategic-interaction between rational-decision-makers has applications in all-fields of social-science, as well as in logic and computer-science.
originally,  the-study of mathematical-models of strategic-interaction between rational-decision-makers addressed zero-sum-games, in which each-participant's-gains or losses are exactly balanced by those of the-other-participants.
today, game-theory applies to a-wide-range of behavioral-relations, and is now an-umbrella-term for the-science of logical-decision-making in humans, animals, and computers.
garbage in, garbage out (gigo)
a-term used to describe the-concept that flawed-or-nonsense-input-data produces nonsense-output or "garbage".
it can also refer to the-unforgiving-nature of programming, in which a-poorly-written-program might produce nonsensical-behavior.
graphics-interchange-format-gigabyte
a-multiple of the-unit byte for digital-information.
the-prefix-giga means 109 in the-international-system of units (si).
therefore, one-gigabyte is 1000000000bytes.
the-unit-symbol for one-gigabyte is gb.
global-variable
in computer-programming, a-variable with global-scope, meaning that it is visible (hence accessible) throughout the-program, unless shadowed.
the-set of all-global-variables is known as the-global-environment or global-state.
in compiled-languages, global-variables are generally static-variables, whose-extent (lifetime) is the-entire-runtime of the-program, though in interpreted-languages (including command-line-interpreters), global-variables are generally dynamically allocated when declared, since global-variables are not known ahead of time.
graph-theory in mathematics, the-study of graphs, which are mathematical-structures used to model pairwise-relations between objects.
a-graph in this-context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines).
a-distinction is made between undirected-graphs, where edges link two-vertices symmetrically, and directed graphs, where edges link two-vertices asymmetrically.
h == handle in computer-programming, a-handle is an-abstract-reference to a-resource that is used when application software references blocks of memory or objects that are managed by another-system like a-database or an-operating-system.
hard-problem
computational-complexity-theory focuses on classifying computational-problems according to  computational-complexity-theory inherent-difficulty, and relating these-classes to each other.
a-computational-problem is a-task solved by a-computer.
a-computation-problem is solvable by mechanical-application of mathematical-steps, such as an-algorithm.
hash-function
any-function that can be used to map data of arbitrary-size to data of a-fixed-size.
the-values returned by a-hash-function are called hash values, hash codes, digests, or simply hashes.
hash-functions are often used in combination with a-hash-table, a-common-data-structure used in computer-software for rapid-data-lookup.
hash-functions accelerate table or database-lookup by detecting duplicated-records in a-large-file.
hash-table
in computing, a-hash-table (hash-map) is a-data-structure that implements an-associative-array-abstract-data-type, a-structure that can map keys to values.
a-hash-table uses a-hash-function to compute an-index into an-array of buckets or slots, from which the-desired-value can be found.
heap a-specialized-tree-based-data-structure which is essentially an-almost-complete-tree that satisfies the-heap-property: if p is a-parent-node of c, then the key (the-value) of p is either greater than or equal to (in a-max-heap) or less than or equal to (in a-min-heap)
the-key of c.
the-node at the-"top" of the-heap (with no-parents) is called the root node.
a-comparison-based-sorting-algorithm.
heapsort can be thought of as an-improved-selection-sort: like  a-comparison-based-sorting-algorithm, heapsort divides heapsort input into a sorted and an-unsorted-region, and heapsort iteratively shrinks the-unsorted-region by extracting the-largest-element and moving that to the-sorted-region.
the-improvement consists of the-use of a-heap-data-structure rather than a-linear-time-search to find the-maximum.
human-computer-interaction (hci) researches the-design and use of computer-technology, focused on the-interfaces between people (users) and computers.
researchers in the-field of hci both observe the-ways in which humans interact with computers and design-technologies that let humans interact with computers in novel-ways.
as a-field of research,-human–computer-interaction is situated at the-intersection of computer-science, behavioral-sciences, design, media-studies, and several-other-fields of study.
i == identifier
in computer-languages, identifiers are tokens (also called symbols) which name language-entities.
some of the-kinds of entities an-identifier might denote include variables, types, labels, subroutines,  and packages.
ide-integrated-development-environment.
image-processing
imperative-programming-a-programming-paradigm that uses statements that change a-program's-state.
in much-the-same-way that the-imperative-mood in natural-languages expresses commands, an-imperative-program consists of commands for the-computer to perform.
imperative-programming focuses on describing how a-program operates.
incremental-build-model
a-method of software-development where the-product is designed, implemented and tested incrementally (a little more is added each time) until the-product is finished.
the-product involves both-development and maintenance.
the-product is defined as finished when the-product satisfies all of the-product requirements.
the-product combines the-elements of the-waterfall-model with the-iterative-philosophy of prototyping.
information-space-analysis
a-deterministic-method, enhanced by machine-intelligence, for locating and assessing resources for team-centric-efforts.
information-visualization-inheritance in object-oriented-programming, the-mechanism of basing an-object or class upon another-object (prototype-based-inheritance) or class (class-based-inheritance), retaining similar-implementation.
also defined as deriving new-classes (sub-classes) from existing-ones (super-class or base-class) and forming existing-ones (super-class or base-class) into a-hierarchy of classes.
input/output (i/o) also-informally-io or io.
the-communication between an-information-processing-system, such as a-computer, and the-outside-world, possibly-a-human-or-another-information-processing-system.
inputs are the-signals or data received by the-system and outputs are the-signals or data sent from it.
the-term can also be used as part of an-action; to "perform i/o
" is to perform an-input-or-output-operation.
insertion sort a-simple-sorting-algorithm that builds the-final-sorted-array (or list)
one-item at a-time.
instruction-cycle also fetch–decode–
execute cycle or simply fetch-execute cycle.
the-cycle which the-central-processing-unit (cpu) follows from boot-up until the-computer has shut down in order to process instructions.
the-computer is composed of three-main-stages: the-fetch-stage, the-decode-stage, and the-execute-stage.
integer-a-datum of integral-data-type, a-data-type that represents some-range of mathematical-integers.
integral-data-types may be of different-sizes and may or may not be allowed to contain negative-values.
integers are commonly represented in a-computer as a-group of binary-digits (bits).
the-size of the-grouping varies so the-set of integer-sizes available varies between different-types of computers.
computer-hardware, including virtual-machines, nearly always provide a-way to represent a-processor-register or memory-address as an-integer.
integrated-development-environment (ide)
a-software-application that provides comprehensive-facilities to computer-programmers for software-development.
an-ide normally consists of at-least-a-source-code-editor, build automation-tools, and a-debugger.
integration-testing (sometimes called integration and testing, abbreviated-i&t) is the-phase in software-testing in which individual-software-modules are combined and tested as a-group.
integration-testing is conducted to evaluate the-compliance of a-system or component with specified-functional-requirements.
it occurs after unit-testing and before validation-testing.
integration-testing takes as its-input-modules that have been unit tested, groups-unit tested in larger-aggregates, applies tests defined in an-integration-test-plan to those-aggregates, and delivers as its-output the-integrated-system ready for system-testing.
intellectual-property (ip)
a-category of legal-property that includes intangible-creations of the-human-intellect.
there are many-types of intellectual-property, and some-countries recognize more than others.
the-most-well-known-types are copyrights, patents, trademarks, and trade-secrets.
intelligent-agent in artificial-intelligence
, an-intelligent-agent (ia) refers to an-autonomous-entity which acts, directing an-intelligent-agent (ia) activity towards achieving goals (i.e. it is an-agent), upon an-environment using observation through sensors and consequent-actuators (i.e. it is intelligent).
intelligent-agents may also learn or use knowledge to achieve intelligent-agents goals.
intelligent-agents may be very simple or very complex.
a-reflex-machine, such as a-thermostat, is considered an example of an-intelligent-agent.
interface-a-shared-boundary across which two or more separate components of a-computer-system-exchange-information.
the-exchange can be between software, computer-hardware, peripheral-devices, humans, and combinations of these.
some-computer-hardware-devices, such as a-touchscreen, can both send and receive data through the-interface, while others such as a-mouse or microphone may only provide an-interface to send data to a-given-system.
internal-documentation-computer-software is said to have internal-documentation if the notes on how and why various-parts of code operate is included within the-source-code as comments.
it is often combined with meaningful-variable-names with the-intention of providing potential-future-programmers a-means of understanding the-workings of the-code.
this contrasts with external-documentation, where programmers keep programmers notes and explanations in a-separate-document.
the-global-system of interconnected-computer-networks that use the-internet-protocol-suite (tcp/ip) to link devices worldwide.
the-global-system of interconnected-computer-networks that use the-internet-protocol-suite (tcp/ip) to link devices worldwide is a-network of networks that consists of private,-public,-academic,-business,-and-government-networks of local to global-scope, linked by a-broad-array of electronic,-wireless,-and-optical-networking-technologies.
internet-bot also web-robot, robot, or simply bot.
a-software-application that runs automated-tasks (scripts) over the-internet.
typically, bots perform tasks that are both simple and structurally repetitive, at a-much-higher-rate than would be possible for a-human alone.
the-largest-use of bots is in web-spidering (web-crawler), in which an-automated-script fetches, analyzes and files information from web-servers at many-times the-speed of a-human.
interpreter-a-computer-program that directly executes instructions written in a-programming-or-scripting-language, without requiring a-programming-or-scripting-language to have been previously compiled into a-machine-language-program.
one can encounter invariants that can be relied upon to be true during the-execution of a-program, or during some-portion of it.
it is a-logical-assertion that is always held to be true during a-certain-phase of execution.
for example, a-loop invariant is a-condition that is true at the-beginning and the-end of every-execution of a-loop.
iteration is the-repetition of a-process in order to generate an-outcome.
the-sequence will approach some-end-point or end-value.
each-repetition of the-process is a-single-iteration, and the-outcome of each-iteration is then the-starting-point of the-next-iteration.
in mathematics-and-computer-science, iteration (along with the-related-technique of recursion) is a-standard-element of algorithms.
java-a-general-purpose-programming-language that is class-based,-object-oriented(although not-a-pure-oo-language), and designed to have as-few-implementation-dependencies as possible.
it is intended to let application-developers "write once, run anywhere" (wora), meaning that compiled-java-code can run on all-platforms that support java without the-need for recompilation.
the-first-section of an-operating-system to load into memory.
as the-center of the-operating-system,  kernel needs to be small, efficient, and loaded into a-protected-area in the-memory so that  kernel cannot be overwritten.
it may be responsible for such-essential-tasks as disk-drive-management, file-management, memory-management, process-management, etc.
l-==-library (computing)
a-collection of non-volatile-resources used by computer-programs, often for software-development.
these may include configuration-data, documentation, help data, message-templates, pre-written-code and subroutines, classes, values, or type-specifications.
linear-search
also-sequential-search.
a-method for finding an-element within a-list.
a-method for finding an-element within a-list sequentially checks each-element of the-list until a-match is found or the-whole-list has been searched.
linked-list
a-linear-collection of data-elements, whose-order is not given by whose-order physical-placement in memory.
instead, each-element points to the next.
it is a-data-structure consisting of a-collection of nodes which together represent a-sequence.
linker  or link editor, is a-computer-utility-program that takes one-or-more-object-files generated by a-compiler or an-assembler and combines one-or-more-object-files generated by a-compiler or an-assembler into a-single-executable-file, library-file, or another-'object'-file.
a-simpler-version that writes its-output directly to memory is called the loader, though loading is typically considered a separate process.
list an-abstract-data-type that represents a-countable-number of ordered-values, where the-same-value may occur more than once.
an-instance of a-list is a-computer-representation of the-mathematical-concept of a-finite-sequence; the-(potentially)-infinite-analog of a-list is a-stream.
lists are a-basic-example of containers, as lists contain other-values.
if the-same-value occurs multiple times, each-occurrence is considered a distinct item.
the-part of an-operating-system that is responsible for loading programs and libraries.
it is one of the-essential-stages in the-process of starting a-program, as it places programs into memory and prepares programs for execution.
loading a-program involves reading the-contents of the-executable-file containing the-program-instructions into memory, and then carrying out other-required-preparatory-tasks to prepare the-executable for running.
once loading is complete, the-operating-system starts a-program by passing control to the-loaded-program-code.
logic-error
in computer-programming,-a-bug in a-program that causes it to operate incorrectly, but not to terminate abnormally (or crash).
a-logic-error produces unintended-or-undesired-output or other-behaviour, although it may not immediately be recognized as such.
logic programming a-type of programming-paradigm which is largely based on formal-logic.
any-program written in a-logic-programming-language is a-set of sentences in logical-form, expressing facts and rules about some-problem-domain.
major-logic-programming-language-families include prolog, answer set-programming (asp), and datalog.
m == machine-learning (ml)
the-scientific-study of algorithms-and-statistical-models that computer systems use to perform a-specific-task without using explicit-instructions, relying on patterns and inference instead.
it is seen as a-subset of artificial-intelligence.
machine-learning-algorithms build a-mathematical-model based on sample-data, known as "training-data", in order to make predictions or decisions without being explicitly programmed to perform the-task.
machine-vision (mv)
the-technology and methods used to provide imaging-based-automatic-inspection and analysis for such-applications as automatic-inspection, process-control, and robot-guidance, usually in industry.
machine-vision refers to many-technologies, software and hardware products, integrated-systems, actions, methods and expertise.
machine-vision as a-systems-engineering-discipline can be considered distinct from computer-vision, a-form of computer-science.
machine-vision as a-systems-engineering-discipline attempts to integrate existing-technologies in new-ways and apply existing-technologies in new-ways to solve real-world-problems.
the-term is the prevalent one for these-functions in industrial-automation-environments but is also used for these-functions in other-environments such as security-and-vehicle-guidance.
mathematical-logic
a-subfield of mathematics exploring the-applications of formal-logic to mathematics.
it bears close-connections to metamathematics, the-foundations of mathematics, and theoretical-computer-science.
the-unifying-themes in mathematical-logic include the-study of the-expressive-power of formal-systems and the-deductive-power of formal-proof-systems.
matrix in mathematics, a-matrix, (plural-matrices), is a-rectangular-array (see irregular-matrix) of numbers, symbols, or expressions, arranged in rows and columns.
computer-data-storage, often called storage, is a-technology consisting of computer-components and recording-media that are used to retain digital-data.
it is a-core-function and fundamental-component of computers.
merge sort also mergesort.
an-efficient,-general-purpose,-comparison-based-sorting-algorithm.
most-implementations produce a-stable-sort, which means that the-order of equal-elements is the same in the-input and output.
merge-sort is a-divide-and-conquer-algorithm that was invented by john-von-neumann in 1945.
a-detailed-description and analysis of bottom-up-mergesort appeared in a-report by goldstine-and-von-neumann as early as 1948.
method in object-oriented-programming (oop), a-procedure associated with a-message and an-object.
an-object consists of data and behavior.
the-data and behavior comprise an-interface, which specifies how the-object may be utilized by any of various-consumers of the-object.
methodology
in software-engineering, a-software-development-process is the-process of dividing software-development-work into distinct-phases to improve design, product-management, and project-management.
a-software-development-process is also known as a-software-development-life-cycle (sdlc).
the-methodology may include the-pre-definition of specific-deliverables and artifacts that are created and completed by a-project-team to develop or maintain an-application.
modem-portmanteau of modulator-demodulator.
a-hardware-device that converts data into a-format suitable for a-transmission-medium so that a-hardware-device that converts data into a-format suitable for a-transmission-medium so that it can be transmitted from one-computer to another (historically along telephone-wires) can be transmitted from one-computer to another (historically along telephone-wires).
a-modem modulates one-or-more-carrier-wave-signals to encode digital-information for transmission and demodulates signals to decode the-transmitted-information.
the-goal is to produce a-signal that can be transmitted easily and decoded reliably to reproduce the-original-digital-data.
modems can be used with almost-any-means of transmitting analog-signals from light-emitting-diodes to radio.
a-common-type of modem is one that turns the-digital-data of a-computer into modulated-electrical-signal for transmission over telephone-lines and demodulated by another-modem at the-receiver-side to recover the-digital-data.
natural-language-processing (nlp)
a-subfield of linguistics, computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
challenges in natural-language-processing frequently involve speech-recognition, natural-language-understanding, and natural-language-generation.
node is a-basic-unit of a-data-structure, such as a-linked-list or tree-data-structure.
nodes contain data and also may link to other-nodes.
links between nodes are often implemented by pointers.
number-theory
a-branch of pure-mathematics devoted primarily to the-study of the-integers-and-integer-valued-functions.
numerical-analysis
the-study of algorithms that use numerical-approximation (as opposed to symbolic-manipulations) for the-problems of mathematical-analysis (as distinguished from discrete-mathematics).
numerical-method
in numerical-analysis, a-numerical-method is a-mathematical-tool designed to solve numerical-problems.
the-implementation of a-numerical-method with an-appropriate-convergence-check in a-programming-language is called a numerical algorithm.
o == object an-object can be a-variable, a-data-structure, a-function, or a-method, and as such, is a-value in memory referenced by an-identifier.
in the-class-based-object-oriented-programming-paradigm, object refers to a-particular-instance of a-class, where the-object can be a-combination of variables, functions, and data-structures.
in relational-database-management, an-object can be a-table or column, or an-association between data and a-database-entity (such as relating a-person's-age to a-specific-person).
object-code-also-object-module.
the-product of a-compiler.
in a-general-sense-object-code is a-sequence of statements or instructions in a-computer-language, usually-a-machine-code-language (i.e., binary) or an-intermediate-language such as register transfer-language (rtl).
the-term indicates that the-code is the-goal or result of the-compiling-process, with some-early-sources referring to source-code as a-"subject-program."
object-oriented-analysis and design (ooad)
a-technical-approach for analyzing and designing an-application, system, or business by applying object-oriented-programming, as well as using visual-modeling throughout the-software-development-process to guide stakeholder-communication and product-quality.
object-oriented-programming (oop) a-programming-paradigm based on the-concept of "objects", which can contain data, in the-form of fields (often known as attributes or properties), and code, in the-form of procedures (often known as methods).
a-feature of objects is an-object's-procedures that can access and often modify the-data-fields of the-object with which they are associated (objects have a-notion of "this" or "self").
in oop, computer-programs are designed by making computer-programs out of objects that interact with one another.
oop-languages are diverse, but the-most-popular-ones are class-based, meaning that objects are instances of classes, which also determine the-most-popular-ones types.
open-source-software (oss)
a-type of computer-software in which source-code is released under a-license in which the-copyright-holder grants users the-rights to study, change, and distribute the-software to anyone and for any-purpose.
open-source-software may be developed in a-collaborative-public-manner.
open-source-software is a-prominent-example of open-collaboration.
operating-system (os)-system-software that manages computer-hardware, software-resources, and provides common-services for computer-programs.
optical-fiber-a-flexible,-transparent-fiber made by drawing glass (silica) or plastic to a-diameter slightly thicker than that of a-human-hair.
optical-fibers are used most often as a-means to transmit light between the-two-ends of the-fiber and find wide-usage in fiber-optic-communications, where  optical-fibers permit transmission over longer-distances and at higher-bandwidths (data-rates) than electrical-cables.
fibers are used instead of metal-wires because signals travel along fibers with less-loss; in addition, fibers are immune to electromagnetic-interference, a-problem from which metal-wires suffer.
pair-programming
an-agile-software-development-technique in which two-programmers work together at one-workstation.
one, the-driver, writes code while the other, the-observer or navigator, reviews each-line of code as it is typed in.
the-two-programmers switch roles frequently.
parallel computing a-type of computation in which many-calculations or the-execution of processes are carried out simultaneously.
large-problems can often be divided into smaller-ones, which can then be solved at the-same-time.
there are several-different-forms of parallel-computing: bit-level, instruction-level, data, and task-parallelism.
parameter-also-formal-argument.
in computer-programming,-a-special-kind of variable, used in a-subroutine to refer to one of the-pieces of data provided as input to the-subroutine.
these-pieces of data are the-values of the-arguments (often called actual-arguments or actual-parameters) with which the-subroutine is going to be called/invoked.
an-ordered-list of parameters is usually included in the-definition of a-subroutine, so that, each time the-subroutine is called, the-subroutine arguments for that-call are evaluated, and the-resulting-values can be assigned to the-corresponding-parameters.
peripheral-any-auxiliary-or-ancillary-device connected to or integrated within a-computer-system and used to send information to or retrieve information from the-computer.
an-input-device sends data or instructions to the-computer; an-output-device provides output from the-computer to the-user; and an-input/output-device performs both-functions.
pointer is an-object in many-programming-languages that stores a-memory-address.
this can be that of another-value located in computer-memory, or in some-cases, that of memory-mapped-computer-hardware.
a-pointer references a-location in memory, and obtaining the-value stored at a-location in memory is known as dereferencing  pointer.
as an-analogy, a-page-number in a-book's-index could be considered a pointer to the-corresponding-page; dereferencing such-a-pointer would be done by flipping to the-page with the-given-page-number and reading the-text found on that-page.
the-actual-format and content of a-pointer-variable is dependent on the-underlying-computer-architecture.
postcondition in computer-programming, a-condition or predicate that must always be true just after the-execution of some-section of code or after an-operation in a-formal-specification.
postconditions are sometimes tested using assertions within the-code itself.
often, postconditions are simply included in the-documentation of the-affected-section of code.
precondition in computer-programming, a-condition or predicate that must always be true just prior to the-execution of some-section of code or before an-operation in a-formal-specification.
if a-precondition is violated, the-effect of the-section of code becomes undefined and thus may or may not carry out the-section of code-intended-work.
security-problems can arise due to incorrect-preconditions.
primary-storage
(also known as main-memory, internal-memory or prime-memory), often referred to simply as memory, is the only one directly accessible to the-cpu.
the-cpu continuously reads instructions stored there and executes instructions stored there as required.
any-data actively operated on is also stored there in uniform-manner.
primitive-data-type-priority-queue
an-abstract-data-type which is like a-regular-queue or stack data-structure, but where additionally each-element has a-"priority" associated with it.
in a-priority-queue, an-element with high-priority is served before an-element with low-priority.
in some-implementations, if two-elements have the-same-priority, two-elements are served according to the-order in which two-elements were enqueued, while in other-implementations, ordering of elements with the-same-priority is undefined.
procedural-programming-procedure
in computer-programming, a-subroutine is a-sequence of program-instructions that performs a-specific-task, packaged as a-unit.
a-unit can then be used in programs wherever that-particular-task should be performed.
subroutines may be defined within programs, or separately in libraries that can be used by many-programs.
in different-programming-languages, a-subroutine may be called a routine, subprogram, function, method, or procedure.
technically, these-terms all have different-definitions.
the-generic,-umbrella-term--callable-unit is sometimes used.
program-lifecycle-phase-program-lifecycle-phases are the-stages a-computer-program undergoes, from initial-creation to deployment and execution.
program-lifecycle-phase-program-lifecycle-phases are edit-time, compile-time, link-time, distribution-time, installation-time, load-time, and run time.
programming-language
a-formal-language, which comprises a-set of instructions that produce various-kinds of output.
programming-languages are used in computer-programming to implement algorithms.
programming-language-implementation is a-system for executing computer-programs.
there are two-general-approaches to programming-language-implementation: interpretation and compilation.
programming-language-theory (plt) is a-branch of computer-science that deals with the-design, implementation, analysis, characterization, and classification of programming-languages and of programming-languages individual features.
it falls within the-discipline of computer-science, both depending on and affecting mathematics, software-engineering, linguistics and even-cognitive-science.
it has become a-well-recognized-branch of computer-science, and an-active-research-area, with results published in numerous-journals dedicated to plt, as well as in general computer-science and engineering publications.
is a-logic-programming-language associated with artificial-intelligence and computational-linguistics.
prolog has prolog roots in first-order-logic, a-formal-logic, and unlike many-other-programming-languages, prolog is intended primarily as a-declarative-programming-language: the-program-logic is expressed in terms of relations, represented as facts and rules.
a-computation is initiated by running a-query over these-relations.
python is an-interpreted,-high-level-and-general-purpose-programming-language.
created by guido-van-rossum and first released in 1991, python's-design-philosophy emphasizes code-readability with python's-design-philosophy notable-use of significant-whitespace.
python's-design-philosophy-language-constructs and object-oriented-approach aim to help programmers write clear,-logical-code for small-and-large-scale-projects.
quantum-computing
the-use of quantum-mechanical-phenomena such as superposition and entanglement to perform computation.
a-quantum-computer is used to perform such-computation, which can be implemented theoretically or physically.
queue-a-collection in which the-entities in the-collection are kept in order and the-principal-(or-only)-operations on the-collection are the-addition of entities to the-rear-terminal-position, known as enqueue, and removal of entities from the-front-terminal-position, known as dequeue.
also-partition-exchange-sort.
an-efficient-sorting-algorithm which serves as a-systematic-method for placing the-elements of a-random-access-file or an-array in order.
r-programming-language-r is a-programming-language-and-free-software-environment for statistical-computing and graphics supported by the-r-foundation for statistical-computing.
the-r-language is widely used among statisticians and data-miners for developing statistical-software and data-analysis.
radix also base.
in digital-numeral-systems, the-number of unique-digits, including the digit zero, used to represent numbers in a-positional-numeral-system.
for example, in the-decimal/denary-system (the-most-common-system in use today) the-radix (base-number) is ten, because the-radix (base-number) uses the-ten-digits from 0 through 9, and all-other-numbers are uniquely specified by positional-combinations of these-ten-base-digits; in the-binary-system that is the-standard in computing, radix is two, because radix uses only-two-digits, 0 and 1, to uniquely specify each-number.
a-record (also called a structure,  struct, or compound-data) is a-basic-data-structure.
records in a-database or spreadsheet are usually called "rows".
recursion occurs when a-thing is defined in terms of a-thing or of a-thing type.
recursion is used in a-variety of disciplines ranging from linguistics to logic.
the-most-common-application of recursion is in mathematics-and-computer-science, where a-function being defined is applied within its-own-definition.
while this apparently defines an-infinite-number of instances (function-values), this is often done in such-a-way that no-infinite-loop or infinite-chain of references can occur.
reference is a-value that enables a-program to indirectly access a-particular-datum, such as a-variable's-value or a-record, in the-computer's-memory or in some-other-storage-device.
the-reference is said to refer to the-datum, and accessing the-datum is called dereferencing the-reference.
reference counting a-programming-technique of storing the-number of references, pointers, or handles to a-resource, such as an-object, a-block of memory, disk-space, and others.
in garbage-collection-algorithms, reference-counts may be used to deallocate objects which are no longer needed.
relational-database
is a-digital-database based on the-relational-model of data, as proposed by e.-f.-codd in 1970.
a-software-system used to maintain relational-databases is a-relational-database-management-system (rdbms).
many-relational-database-systems have an-option of using the-sql (structured-query-language) for querying and maintaining the-database.
reliability engineering a-sub-discipline of systems-engineering that emphasizes dependability in the-lifecycle-management of a-product.
reliability describes the-ability of a-system or component to function under stated-conditions for a-specified-period of time.
reliability is closely related to availability, which is typically described as the-ability of a-component or system to function at a-specified-moment or interval of time.
regression-testing (rarely non-regression-testing) is re-running-functional-and-non-functional-tests to ensure that previously-developed-and-tested-software still performs after a-change.
if not, that would be called a regression.
changes that may require regression-testing include bug-fixes, software-enhancements, configuration-changes, and even-substitution of electronic-components.
as regression-test-suites tend to grow with each-found-defect, test-automation is frequently involved.
sometimes a-change-impact-analysis is performed to determine an-appropriate-subset of tests (non-regression-analysis).
requirements-analysis in systems-engineering-and-software-engineering
, requirements-analysis focuses on the-tasks that determine the-needs or conditions to meet the-new-or-altered-product or project, taking account of the-possibly-conflicting-requirements of the-various-stakeholders, analyzing, documenting, validating and managing software or system-requirements.
an-interdisciplinary-branch of engineering and science that includes mechanical-engineering, electronic-engineering, information-engineering, computer-science, and others.
robotics involves design, construction, operation, and use of robots, as well as computer-systems for as-well-perception, control, sensory-feedback, and information-processing.
the-goal of robotics is to design intelligent-machines that can help and assist humans in humans day-to-day lives and keep everyone safe.
round-off-error
also rounding error.
the-difference between the-result produced by a-given-algorithm using exact-arithmetic and the-result produced by the-same-algorithm using finite-precision,-rounded-arithmetic.
rounding-errors are due to inexactness in the-representation of real-numbers and the-arithmetic-operations done with real-numbers.
this is a-form of quantization-error.
when using approximation-equations or algorithms, especially when using finitely-many-digits to represent real-numbers (which in theory have infinitely-many-digits), one of the-goals of numerical-analysis is to estimate computation-errors.
computation-errors, also called numerical-errors, include both-truncation-errors and roundoff-errors.
router-a-networking-device that forwards data-packets between computer-networks.
routers perform the-traffic-directing-functions on the-internet.
data sent through the-internet, such as a-web-page or email, is in the-form of data-packets.
a-packet is typically forwarded from one-router to another-router through the-networks that constitute an-internetwork (e.g.-the-internet) until  a-packet reaches  a-packet destination node.
routing table
in computer networking a-routing-table, or routing information-base (rib), is a-data-table stored in a-router or a-network-host that lists the-routes to particular-network-destinations, and in some-cases, metrics (distances) associated with the-routes to particular-network-destinations.
a-data-table stored in a-router or a-network-host that lists the-routes to particular-network-destinations contains information about the-topology of the-network immediately around it.
runtime, run time, or execution-time is the-final-phase of a-computer-program's-life-cycle, in which the-code is being executed on the-computer's-central-processing-unit (cpu) as machine-code.
in other-words, "runtime" is the-running-phase of a-program.
run-time-error
a-runtime-error is detected after or during the-execution (running-state) of a-program, whereas a-compile-time-error is detected by the-compiler before a-program is ever executed.
type-checking, register allocation, code-generation, and code-optimization are typically done at compile-time, but may be done at runtime depending on the-particular-language and compiler.
many-other-runtime-errors exist and are handled differently by different-programming-languages, such as division by zero-errors, domain-errors, array-subscript out of bounds-errors, arithmetic underflow errors, several-types of underflow and overflow-errors, and many-other-runtime-errors generally considered as software-bugs which may or may not be caught and handled by any-particular-computer-language.
search-algorithm
any-algorithm which solves the-search-problem, namely, to retrieve information stored within some-data-structure, or calculated in the-search-space of a-problem-domain, either with discrete-or-continuous-values.
secondary-storage
also known as external-memory or auxiliary-storage, differs from primary-storage in that the-computer is not directly accessible by the-cpu.
the-computer usually uses the-computer input/output channels to access secondary-storage and transfer the-desired-data to primary-storage.
secondary-storage is non-volatile-(retaining-data when power is shut off).
modern-computer-systems typically have two-orders of magnitude more-secondary-storage than primary-storage because secondary-storage is less expensive.
selection-sort
is an in-place comparison sorting algorithm.
it has an-o(n2)-time-complexity, which makes it inefficient on large-lists, and generally performs worse than the-similar-insertion-sort.
selection-sort is noted for it simplicity and has performance-advantages over more-complicated-algorithms in certain-situations, particularly where auxiliary-memory is limited.
semantics in programming-language-theory, semantics is the-field concerned with the-rigorous-mathematical-study of the-meaning of programming-languages.
it does so by evaluating the-meaning of syntactically-valid-strings defined by a-specific-programming-language, showing the-computation involved.
in such-a-case that the-evaluation would be of syntactically-invalid-strings, the-result would be non-computation.
semantics describes the-processes a-computer follows when executing a-program in that-specific-language.
this can be shown by describing the-relationship between the-input and output of a-program, or an-explanation of how a-program will be executed on a-certain-platform, hence creating a-model of computation.
in mathematics, a-sequence is an-enumerated-collection of objects in which repetitions are allowed and order does matter.
like a-set, a-set contains members (also called elements, or terms).
the-number of elements (possibly infinite) is called the length of the-sequence.
unlike a-set, the-same-elements can appear multiple times at different-positions in a-sequence, and order does matter.
formally, a-sequence can be defined as a-function whose-domain is either-the-set of the-natural-numbers (for infinite-sequences) or the-set of the-first-n-natural-numbers (for a-sequence of finite-length
the-position of an-element in a-sequence is its-rank or index; its is the-natural-number for which the-element is the-image.
the-element has index 0 or 1, depending on the-context or a-specific-convention.
when a-symbol is used to denote a-sequence, the-nth-element of the-sequence is denoted by a-symbol with n-as-subscript; for example, the-nth-element of the-fibonacci-sequence-f is generally denoted fn.
for example, (m,-a, r, y) is a-sequence of letters with the letter 'm' first and 'y' last.
this-sequence differs from (a, r, m, y).
also, the-sequence (1, 1, 2, 3, 5, 8), which contains the-number 1 at two-different-positions, is a-valid-sequence.
sequences can be finite, as in these-examples, or infinite, such as the-sequence of all-even-positive-integers (2, 4, 6, ...).
in computing-and-computer-science, finite-sequences are sometimes called strings, words or lists, the-different-names commonly corresponding to different-ways to represent lists in computer-memory; infinite-sequences are called streams.
the-empty-sequence ( ) is included in most-notions of sequence, but may be excluded depending on the-context.
serializability in concurrency-control of databases, transaction-processing (transaction-management), and various-transactional-applications (e.g.,-transactional-memory-and-software-transactional-memory), both centralized and distributed
, a-transaction-schedule is serializable if a-transaction-schedule outcome (e.g., the-resulting-database-state) is equal to the-outcome of a-transaction-schedule transactions executed serially, i.e. without overlapping in time.
transactions are normally executed concurrently (transactions overlap), since this is the-most-efficient-way.
serializability is the-major-correctness-criterion for concurrent-transactions'-executions.
serializability is considered the highest level of isolation between transactions, and plays an-essential-role in concurrency-control.
as such   is supported in all-general-purpose-database-systems.
strong-strict-two-phase-locking (ss2pl) is a-popular-serializability-mechanism utilized in most of the-database-systems (in various-variants) since their-early-days in the-1970s.
serialization is the-process of translating data-structures or object state into a-format that can be stored (for example, in a-file-or-memory-buffer) or transmitted (for example, across a-network-connection-link) and reconstructed later (possibly in a-different-computer-environment).
when the-resulting-series of bits is reread according to the-serialization-format, the-resulting-series of bits can be used to create a-semantically-identical-clone of the-original-object.
for many-complex-objects, such as those that make extensive-use of references, this-process is not straightforward.
serialization of object-oriented-objects does not include any of their-associated-methods with which their were previously linked.
this-process of serializing an-object is also called marshalling an-object in some
situations.[2][3]
the-opposite-operation, extracting a-data-structure from a-series of bytes, is deserialization, (also called unserialization or unmarshalling).
service-level-agreement (sla), is a-commitment between a-service-provider and a-client.
particular-aspects of the-service – quality, availability, responsibilities – are agreed between the-service provider and the-service user.
the-most-common-component of an-sla is that the-services should be provided to the-customer as agreed upon in the-contract.
as an-example, internet-service-providers and telcos will commonly include service-level-agreements within the-terms of internet-service-providers and telcos-contracts with customers to define the level(s) of service being sold in plain-language-terms.
in this-case the-sla will typically have a-technical-definition in  mean-time between failures (mtbf), mean-time to repair or mean-time to recovery (mttr); identifying which-party is responsible for reporting faults or paying fees; responsibility for various-data-rates; throughput; jitter; or similar measurable details.
set is an-abstract-data-type that can store unique-values, without any-particular-order.
set is a-computer-implementation of the-mathematical-concept of a-finite-set.
unlike most-other-collection-types, rather than retrieving a-specific-element from a-set, one typically tests a-value for membership in a-set.
soft-computing-software-computer-software, or simply-software, is a-collection of data-or-computer-instructions that tell the-computer how to work.
this is in contrast to physical-hardware, from which the-system is built and actually performs the-work.
in computer-science-and-software-engineering, computer-software is all-information processed by computer-systems, programs and data.
computer-software includes computer-programs, libraries and related-non-executable-data, such as online-documentation or digital-media.
computer-hardware and software require each other and neither can be realistically used on  computer-software own.
software-agent is a-computer-program that acts for a-user or other-program in a-relationship of agency, which derives from the-latin-agere (to do): an-agreement to act on one's-behalf.
such-"action on behalf of" implies the-authority to decide which, if any, action is appropriate.
agents are colloquially known as bots, from robot.
agents may be embodied, as when execution is paired with a-robot-body, or  as software such as a-chatbot executing on a-phone-(e.g.-siri)  or other-computing-device.
software-agents may be autonomous or work together with other-agents or people.
software-agents interacting with people (e.g.-chatbots, human-robot-interaction-environments) may possess human-like-qualities such as natural-language-understanding and speech, personality or embody humanoid-form (see asimo).
software-construction is a-software-engineering-discipline.
software-construction is the-detailed-creation of working-meaningful-software through a-combination of coding, verification, unit-testing, integration-testing, and debugging.
software-construction is linked to all-the-other-software-engineering-disciplines, most strongly to software design-and-software-testing.
software-deployment
is all of the-activities that make a-software-system available for use.
software-design is the-process by which an-agent creates a-specification of a-software-artifact, intended to accomplish goals, using a-set of primitive-components and subject to constraints.
software-design may refer to either-"all-the-activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex-systems" or "the-activity following requirements-specification and before programming, as ...
[in] a-stylized-software-engineering-process."
software-development is the-process of conceiving, specifying, designing, programming, documenting, testing, and bug-fixing involved in creating and maintaining applications, frameworks, or other-software-components.
software-development is a-process of writing and maintaining the-source-code, but in a-broader-sense, software-development includes all that is involved between the-conception of the-desired-software through to the-final-manifestation of the-software, sometimes in a-planned-and-structured-process.
therefore, software-development may include research, new-development, prototyping, modification, reuse, re-engineering, maintenance, or any-other-activities that result in software-products.
software-development-process
in software-engineering, a-software-development-process is the-process of dividing software-development-work into distinct-phases to improve design, product-management, and project-management.
a-software-development-process is also known as a-software-development-life-cycle (sdlc).
the-methodology may include the-pre-definition of specific-deliverables and artifacts that are created and completed by a-project-team to develop or maintain an-application.
most-modern-development-processes can be vaguely described as agile.
other-methodologies include waterfall, prototyping, iterative and incremental development, spiral-development, rapid-application-development, and extreme-programming.
software-engineering is the-systematic-application of engineering-approaches to the-development of software.
software-engineering is a-computing-discipline.
software-maintenance in software-engineering is the-modification of a-software-product after delivery to correct faults, to improve performance or other-attributes.
software-prototyping is the-activity of creating prototypes of software-applications, i.e.,-incomplete-versions of the-software-program being developed.
software-prototyping is an-activity that can occur in software-development and is comparable to prototyping as known from other-fields, such as mechanical-engineering or manufacturing.
a-prototype typically simulates only-a-few-aspects of, and may be completely different from, the-final-product.
software-requirements-specification
(srs), is a-description of a-software-system to be  developed.
the-software-requirements-specification lays out functional-and-non-functional-requirements, and the-software-requirements-specification may include a-set of use-cases that describe user-interactions that the-software must provide to the-user for perfect-interaction.
software-testing is an-investigation conducted to provide stakeholders with information about the-quality of the-software-product or service under test.
software-testing can also provide an-objective,-independent-view of the-software to allow the-business to appreciate and understand the-risks of software-implementation.
test-techniques include the-process of executing a-program or application with the-intent of finding software-bugs (errors or other-defects), and verifying that the-software-product is fit for use.
sorting-algorithm is an-algorithm that puts elements of a-list in a-certain-order.
the-most-frequently-used-orders are numerical-order and lexicographical-order.
efficient-sorting is important for optimizing the-efficiency of other-algorithms (such as search and merge algorithms) that require input-data to be in sorted-lists.
sorting is also often useful for canonicalizing-data and for producing human-readable-output.
more formally, the-output of any-sorting-algorithm must satisfy two-conditions: the-output of any-sorting-algorithm is in nondecreasing-order (each-element is no smaller than the-previous-element according to the-desired-total-order); the-output of any-sorting-algorithm is a-permutation (a-reordering, yet retaining all of the-original-elements) of the-input.
further, the-input-data is often stored in an-array, which allows random-access, rather than a-list, which only allows sequential-access; though many-algorithms can be applied to either-type of data after suitable-modification.
source-code in computing
, source-code is any-collection of code, with or without comments, written using a-human-readable-programming-language, usually as plain-text.
source-code is specially designed to facilitate the-work of computer-programmers, who specify the-actions to be performed by a-computer mostly by writing source-code.
source-code is often transformed by an-assembler or compiler into binary-machine-code that can be executed by a-computer.
source-code might then be stored for execution at a-later-time.
alternatively, source-code may be interpreted and thus immediately executed.
spiral-model is a-risk-driven-software-development-process-model.
based on the-unique-risk-patterns of a-given-project, spiral-model guides a-team to adopt elements of one-or-more-process-models, such as incremental, waterfall, or evolutionary-prototyping.
is an-abstract-data-type that serves as a-collection of elements, with two-main-principal-operations: push, which adds an-element to the-collection, and pop, which removes the-most-recently-added-element that was not yet removed.
the-order in which elements come off a-stack gives rise to a-stack alternative name, lifo (last in, first out).
additionally, a-peek-operation may give access to the-top without modifying the-stack.
the-name "stack" for this-type of structure comes from the-analogy to a-set of physical-items stacked on top of each other.
this-structure makes this-structure easy to take an-item off the-top of the-stack, while getting to an-item deeper in the-stack may require taking off multiple-other-items first.
in information-technology and computer-science, a-system is described as stateful if a-system is designed to remember preceding events or user-interactions; the-remembered-information is called the state of the-system.
in computer-programming, a-statement is a-syntactic-unit of an-imperative-programming-language that expresses some-action to be carried out.
a-program written in such-a-language is formed by a-sequence of one-or-more-statements.
a-statement may have internal-components-(e.g.,-expressions).
storage-computer-data-storage is a-technology consisting of computer-components and recording-media that are used to retain digital-data.
it is a-core-function and fundamental-component of computers.
stream is a-sequence of data-elements made available over time.
a-stream can be thought of as items on a-conveyor-belt being processed one at a-time rather than in large-batches.
in computer-programming, a-string is traditionally a-sequence of characters, either as a-literal-constant or as some-kind of variable.
the latter may allow its-elements to be mutated and the-length changed, or its-elements may be fixed (after creation).
a-string is generally considered as a-data-type and is often implemented as an-array-data-structure of bytes (or words) that stores a-sequence of elements, typically-characters, using some-character-encoding.
string may also denote more-general-arrays or other-sequence (or list) data-types and structures.
structured-storage
a-nosql (originally referring to "non-sql" or "non-relational")-database provides a-mechanism for storage and retrieval of data that is modeled in means other than the-tabular-relations used in relational-databases.
such-databases have existed since the-late-1960s, but the-name "nosql" was only coined in the-early-21st-century, triggered by the-needs of web-2.0-companies.
nosql-databases are increasingly used in big-data and real-time-web-applications.
nosql-systems are also sometimes called "not only sql" to emphasize that nosql-systems may support sql-like-query-languages or sit alongside sql-databases in polyglot-persistent-architectures.
subroutine
in computer-programming, a-subroutine is a-sequence of program-instructions that performs a-specific-task, packaged as a-unit.
a-unit can then be used in programs wherever that-particular-task should be performed.
subroutines may be defined within programs, or separately in libraries that can be used by many-programs.
in different-programming-languages, a-subroutine may be called a routine, subprogram, function, method, or procedure.
technically, these-terms all have different-definitions.
the-generic,-umbrella-term--callable-unit is sometimes used.
symbolic-computation in mathematics-and-computer-science, computer-algebra, also called symbolic-computation or algebraic-computation, is a-scientific-area that refers to the-study and development of algorithms and software for manipulating mathematical-expressions and other-mathematical-objects.
although computer-algebra could be considered a subfield of scientific-computing, computer-algebra are generally considered as distinct-fields because scientific-computing is usually based on numerical-computation with approximate-floating-point-numbers, while symbolic-computation emphasizes exact-computation with expressions containing variables that have no-given-value and are manipulated as symbols.
the-syntax of a-computer-language is the-set of rules that defines the-combinations of symbols that are considered to be correctly-structured-statements or expressions in that-language.
this applies both to programming-languages, where the-document represents source-code, and to markup languages, where the-document represents data.
syntax-error is an-error in the-syntax of a-sequence of characters or tokens that is intended to be written in compile-time.
a-program will not compile until all-syntax-errors are corrected.
for interpreted-languages, however, a-syntax-error may be detected during program-execution, and an-interpreter's-error-messages might not differentiate syntax-errors from errors of other-kinds.
there is some-disagreement as to just-what-errors are "syntax-errors".
for example, some would say that the-use of an-uninitialized-variable's-value in java-code is a-syntax-error, but many-others would disagree and would classify this as a-(static)-semantic-error.
system console the-system-console, computer-console, root-console, operator's-console, or simply console is the-text-entry-and-display-device for system-administration-messages, particularly those from the-bios-or-boot-loader, the-kernel, from the-init-system and from the-system-logger.
it is a-physical-device consisting of a-keyboard and a-screen, and traditionally is a-text-terminal, but may also be a-graphical-terminal.
system-consoles are generalized to computer-terminals, which are abstracted respectively by virtual-consoles and terminal-emulators.
today communication with system-consoles is generally done abstractly, via the-standard-streams (stdin, stdout, and stderr), but there may be system-specific-interfaces, for example those used by the-system-kernel.
technical-documentation
in engineering,-any-type of documentation that describes handling, functionality, and architecture of a-technical-product or a-product under development or use.
the-intended-recipient for product-technical-documentation is both-the-(proficient)-end-user as well as the-administrator/service-or-maintenance-technician.
in contrast to a mere "cookbook" manual, technical-documentation aims at providing enough-information for a-user to understand inner-and-outer-dependencies of the-product at hand.
third-generation-programming-language
a-third-generation-programming-language (3gl) is a-high-level-computer-programming-language that tends to be more machine-independent and programmer-friendly than the-machine-code of the-first-generation-and-assembly-languages of the-second-generation, while having a-less-specific-focus to the-fourth-and-fifth-generations.
examples of common-and-historical-third-generation-programming-languages are algol, basic,-c, cobol, fortran, java, and pascal.
top-down-and-bottom-up-design-tree
a-widely-used-abstract-data-type (adt) that simulates a-hierarchical-tree-structure, with a-root-value and subtrees of children with a-parent-node, represented as a-set of linked-nodes.
type-theory in mathematics, logic, and computer-science
, a-type-theory is any of a-class of formal-systems, some of which can serve as alternatives to set theory as a-foundation for all-mathematics.
in type-theory, every-"term" has a-"type" and operations are restricted to terms of a-certain-type.
upload in computer-networks, to send data to a-remote-system such as a-server or another-client so that the-remote-system can store a-copy.
contrast-download.
uniform-resource-locator (url)
colloquially-web-address.
a-reference to a-web-resource that specifies its-location on a-computer-network and a-mechanism for retrieving its.
a-url is a-specific-type of uniform-resource-identifier (uri), although many-people use the-two-terms interchangeably.
urls occur most commonly to reference-web-pages (http), but are also used for file-transfer (ftp), email (mailto), database-access (jdbc), and many-other-applications.
user is a-person who utilizes a-computer or network-service.
users of computer-systems-and-software-products generally lack the-technical-expertise required to fully understand how  users of computer-systems-and-software-products work.
power-users use advanced-features of programs, though power-users are not necessarily capable of computer-programming-and-system-administration.
user-agent-software (a-software-agent) that acts on behalf of a-user, such as a-web-browser that "retrieves, renders and facilitates end-user-interaction with web-content".
an-email-reader is a-mail-user-agent.
user-interface (ui)
the-space where interactions between humans and machines occur.
the-goal of this-interaction is to allow effective-operation and control of the-machine from the-human-end, whilst the-machine simultaneously feeds back information that aids the-operators'-decision-making-process.
examples of this-broad-concept of user-interfaces include the-interactive-aspects of computer-operating-systems, hand-tools, heavy machinery operator controls, and process-controls.
the-design-considerations applicable when creating user-interfaces are related to or involve such-disciplines as ergonomics and psychology.
user-interface-design
also-user interface-engineering.
user-interface-design.
the-goal of user-interface-design is to make the-user's-interaction as simple and efficient as possible, in terms of accomplishing user-goals (user-centered-design).
v == variable
in computer-programming, a-variable, or scalar, is a-storage-location (identified by a-memory-address) paired with an-associated-symbolic-name (an-identifier), which contains some-known-or-unknown-quantity of information referred to as a-value.
the-variable-name is the-usual-way to reference the-stored-value, in addition to referring to the-variable itself, depending on the-context.
this-separation of name and content allows the-name to be used independently of the-exact-information the-name represents.
the-identifier in computer-source-code can be bound to a-value during run-time, and the-value of the-variable may therefore change during the-course of program-execution.
virtual-machine (vm)
an-emulation of a-computer-system.
virtual-machines are based on computer-architectures and attempt to provide the-same-functionality as a-physical-computer.
virtual-machines-implementations may involve specialized-hardware, software, or a-combination of virtual-machines.
v-model-a-software-development-process that may be considered an extension of the-waterfall-model, and is an-example of the-v-model.
instead of moving down in a-linear-way, the-process-steps are bent upwards after the-coding-phase, to form the-typical-v-shape.
the-v-model demonstrates the-relationships between each-phase of the-development-life-cycle and the-v-model associated phase of testing.
the-horizontal-and-vertical-axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain-abstraction uppermost), respectively.
waterfall-model
a-breakdown of project-activities into linear-sequential-phases, where each-phase depends on the-deliverables of the-previous-one and corresponds to a-specialisation of tasks.
the-approach is typical for certain-areas of engineering-design.
in software-development,  in software-development tends to be among the-less-iterative-and-flexible-approaches, as progress flows in largely-one-direction ("downwards" like a-waterfall) through the-phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.
waveform-audio-file-format also wave or wav due to waveform-audio-file-format filename extension.
an-audio-file-format-standard, developed by microsoft and ibm, for storing an-audio-bitstream on pcs.
an-audio-file-format-standard, developed by microsoft and ibm, for storing an-audio-bitstream on pcs is an-application of the-resource-interchange-file-format (riff)-bitstream-format-method for storing data in "chunks", and thus is also close to the-8svx and the-aiff-format used on amiga and macintosh-computers, respectively.
it is the-main-format used on microsoft-windows-systems for raw-and-typically-uncompressed-audio.
the-usual-bitstream-encoding is the-linear-pulse-code-modulation (lpcm)-format.
web-crawler also spider, spiderbot, or simply-crawler.
an-internet-bot that systematically browses the-world-wide-web, typically for the-purpose of web-indexing (web-spidering).
a-family of wireless-networking-technologies, based on the-ieee-802.11-family of standards, which are commonly used for local-area-networking of devices and internet-access.
wi‑fi is a-trademark of the-non-profit-wi-fi-alliance, which restricts the-use of the-term wi-fi-certified to products that successfully complete interoperability-certification-testing.
xhtml-abbreviaton of extensible hypertext-markup-language.
part of the-family of xml-markup-languages.
part of the-family of xml-markup-languages mirrors or extends versions of the-widely-used-hypertext-markup-language (html), the-language in which web-pages are formulated.
see also ==
outline of computer-science ==
references == == notes ==
the-dunning–kruger-effect is a-hypothetical-cognitive-bias stating that people with low-ability at a-task overestimate people with low-ability at a-task ability.
as described by social-psychologists david-dunning and justin-kruger, the-bias results from an-internal-illusion in people of low-ability and from an-external-misperception in people of high-ability; that is, "the-miscalibration of the-incompetent-stems from an-error about the-self, whereas the-miscalibration of the-highly-competent-stems from an-error about others".
it is related to the-cognitive-bias of illusory-superiority and comes from people's-inability to recognize people lack of ability.
without the-self-awareness of metacognition, people cannot objectively evaluate people level of competence.
the-effect, or dunning-and-kruger's-original-explanation for the-effect, has been challenged by mathematical-analyses and comparisons across cultures.
original-study ==
the-psychological-phenomenon of illusory-superiority was identified as a-form of cognitive-bias in kruger and dunning's-1999-study "unskilled and unaware of it:
how-difficulties in recognizing one's-own-incompetence-lead to inflated-self-assessments".
an-example derived from cognitive-bias evident in the-criminal-case of mcarthur-wheeler, who, on april 19, 1995, robbed two-banks while his-face was covered with lemon-juice, which his believed would make his invisible to the-surveillance-cameras.
this-belief was apparently based on his-misunderstanding of the-chemical-properties of lemon-juice as an-invisible-ink.
other-investigations of the-phenomenon, such as "why people fail to recognize people own-incompetence", indicate that much-incorrect-self-assessment of competence derives from the-person's-ignorance of a-given-activity's-standards of performance.
dunning and kruger's-research also indicates that training in a-task, such as solving a-logic-puzzle, increases people's-ability to accurately evaluate how-good-people are at it.
in self-insight: roadblocks and detours on the-path to knowing thyself, dunning described the dunning–kruger effect as "the-anosognosia of everyday-life", referring to a-neurological-condition in which a-disabled-person either denies or seems unaware of a-disabled-person or a-disabled-person disability.
a-disabled-person stated: "if you're incompetent, you can't know you're incompetent ...
the-skills you need to produce a-right-answer are exactly the-skills you need to recognize what a-right-answer is.
"in 2011, dunning wrote about dunning observations that people with substantial,-measurable-deficits in people with substantial,-measurable-deficits in their-knowledge or expertise knowledge or expertise lack the-ability to recognize those-deficits and, therefore, despite potentially making error after error, tend to think people with substantial,-measurable-deficits in their-knowledge or expertise are performing competently when people with substantial,-measurable-deficits in their-knowledge or expertise are not:
"in short, those who are incompetent, for lack of a-better-term, should have little-insight into people with substantial,-measurable-deficits in their-knowledge or expertise incompetence—an-assertion that has come to be known as the-dunning–kruger-effect".
in 2014, dunning and helzer described how the dunning–kruger effect "suggests that poor-performers are not in a-position to recognize the-shortcomings in poor-performers performance".
later-studies ==
dunning and kruger tested the-hypotheses of the-cognitive-bias of illusory-superiority on undergraduate-students of introductory-courses in psychology by examining the-students'-self-assessments of the-students'-intellectual-skills in inductive, deductive, and abductive-logical-reasoning, english-grammar, and personal-sense of humor.
after learning the-students self-assessment-scores, the-students were asked to estimate the-students ranks in the-psychology-class.
the-students underestimated the-students class rank, and the-incompetent-students overestimated the-students, but the-incompetent-students did not estimate the-incompetent-students class rank as higher than the-ranks estimated by the-competent-group.
across four-studies, the-research indicated that the-study-participants who scored in the-bottom-quartile on tests of their-sense of humor, knowledge of grammar, and logical-reasoning, overestimated their-test-performance and their-abilities; despite test-scores that placed their in the-12th-percentile, the-participants estimated their ranked in the-62nd-percentile.
moreover, competent-students tended to underestimate competent-students own competence, because competent-students erroneously presumed that tasks easy for competent-students to perform were also easy for other-people to perform.
incompetent-students improved  incompetent-students ability to estimate  incompetent-students class rank correctly after receiving minimal-tutoring in the-skills  incompetent-students previously lacked, regardless of any-objective-improvement gained in said-skills of perception.
the-2004-study "mind-reading and metacognition: narcissism, not actual-competence, predicts self-estimated-ability" extended the-cognitive-bias-premise of illusory-superiority to test subjects'-emotional-sensitivity toward other-people and their-own-perceptions of other-people.
the-2003-study " how-chronic-self-views-influence (and potentially mislead)
estimates of performance" indicated a-shift in the-participants'-view of the-participants' when influenced by external-cues.
the-participants'-knowledge of geography was tested; some-tests were intended to affect the-participants'-self-view positively, and some were intended to affect it negatively.
the-participants' then were asked to rate  the-participants' performances;  the-participants' given tests with a-positive-intent reported better-performance than did  the-participants' given tests with a-negative-intent.
to test dunning and kruger's-hypotheses "that people, at all-performance-levels, are equally poor at estimating people relative-performance", the 2006 study "skilled or unskilled, but still unaware of it: how perceptions of difficulty drive miscalibration in relative-comparisons" investigated three-studies that manipulated the-"perceived-difficulty of the-tasks, and, hence, [the]-participants'-beliefs about the]-participants'-relative-standing".
the-investigation indicated that when the-experimental-subjects were presented with moderately-difficult-tasks, there was little-variation among the-best-performers and the-worst-performers in their-ability to predict their-performance accurately.
with more-difficult-tasks, the-best-performers were less accurate in predicting the-best-performers performance than were the-worst-performers.
therefore, judges at all-levels of skill are subject to similar-degrees of error in the-performance of tasks.
in testing alternative-explanations for the-cognitive-bias of illusory-superiority, the 2008 study " why the-unskilled are unaware:
further-explorations of (absent) self-insight among the incompetent" reached the-same-conclusions as previous-studies of the-dunning–kruger-effect: that, in contrast to high-performers, "poor-performers do not learn from feedback suggesting a-need to improve".
one-2020-study suggests that individuals of relatively-high-social-class are more overconfident than lower-class-individuals.
mathematical-critique ==
dunning and kruger describe a-common-cognitive-bias and make quantitative-assertions that rest on mathematical-arguments.
but dunning-and-kruger-findings are often misinterpreted, misrepresented, and misunderstood.
according to tal-yarkoni: dunning-and-kruger-studies categorically didn’t show that incompetent-people are more confident or arrogant than competent-people.
what they did show is [that] people in the-top-quartile for actual-performance think they perform better than the-people in the-second-quartile, who in turn think they perform better than the-people in the-third-quartile, and so on.
so the-bias is definitively not that incompetent-people think incompetent-people’re better than competent-people.
rather, it’s that incompetent-people think incompetent-people’re much better than incompetent-people actually are.
but incompetent-people typically still don’t think incompetent-people’re quite as good as people who, you know, actually are good.
it’s important to note that dunning and kruger never claimed to show that the unskilled think dunning and kruger’re better than the skilled; that’s just-the-way the-finding is often interpreted by others.)
paired-measures ===
mathematically, the-effect relies on the-quantifying of paired-measures consisting of (a)-the-measure of the-competence people can demonstrate when put to the-test (actual-competence) and (b)-the-measure of competence people believe that people have (self-assessed-competence).
researchers express the-measures either as percentages or as percentile-scores scaled from 0 to 1 or from 0 to 100.
by convention, researchers express the-differences between the-two-measures as self-assessed-competence minus actual-competence.
in convention, negative-numbers signify erring toward underconfidence, positive-numbers signify erring toward overconfidence, and zero signifies accurate-self-assessment.
a-2008-study by joyce-ehrlinger summarized the-major-assertions of the-effect that first appeared in the-1999-seminal-article and continued to be supported by many-studies after nine-years of research: "people are typically overly optimistic when evaluating the-quality of people performance on social-and-intellectual-tasks.
in particular, poor-performers grossly overestimate poor-performers performances".
the-effect asserts that most-people are overconfident about most-people abilities, and that the-least-competent-people are the most overconfident.
support for both-assertions rests upon interpreting the-patterns produced from graphing the-paired-measures.
the-most-common-graphical-convention is the-kruger–dunning-type-graph used in the-seminal-article.
the-most-common-graphical-convention depicted college-students'-accuracy in self-assessing-college-students'-competencies in humor, logical-reasoning, and grammar.
researchers adopted that-convention in subsequent-studies of the-effect.
additional-graphs used by other-researchers, who argued for the-legitimacy of the-effect include (y–x) versus (x)-cross plots and bar-charts.
the first two of these-studies depicted college-students'-accuracy in self-assessing-college-students'-competence in introductory-chemistry, and the third depicted college-students'-accuracy in self-assessing-college-students'-competence in business-classes.
in a-study published in 2016, researchers who focused on the-mathematical-reasoning behind the-effect studied 1,154-participants'-ability to self-assess 1,154-participants'-competence in understanding the-nature of science.
researchers who focused on the-mathematical-reasoning behind the-effect graphed researchers who focused on the-mathematical-reasoning behind the-effect data in all-the-earlier-articles'-various-conventions and explained how the-numerical-reasoning used to argue for the-effect is similar in all.
when graphed in all-the-earlier-articles'-various-conventions, the-researchers'-data also supported the-effect.
had the-researchers ended the-researchers study at this-point, the-researchers results would have added to the-established-consensus that validated the-effect.
but the-researchers deeper analyses led the-researchers to conclude that the-numerical-procedures used repeatedly in all-previous-work were the-likely-sources of misleading-conclusions, driven by ceiling/floor-effects (exacerbated by measurement-error) causing censoring.
to expose the-sources of the-misleading-conclusions, the-researchers employed the-researchers own real-data-set of paired-measures from 1,154-participants and created a-second-simulated-data-set that employed random-numbers to simulate random guessing by an-equal-number of simulated-participants.
the-simulated-data-set contained only-random-noise, without any-measures of human-behavior.
the-researchers then used the simulated data set and the-graphical-conventions of the-behavioral-scientists to produce patterns like those described as validating the-dunning–kruger-effect.
they traced the-origin of the-patterns, not to the-dominant-literature's claimed psychological-disposition of humans, but instead to the-nature of graphing data bounded by limits of 0 and 100 and the-process of ordering and grouping the-paired-measures to create the-graphs.
these-patterns are mathematical-artifacts that-random-noise devoid of any-human-influence can produce.
these-patterns further showed that the-graphs used to establish the-effect in three of the-four-case-examples presented in the-seminal-article are patterns characteristic of purely-random-noise.
patterns characteristic of purely-random-noise are numerical-artifacts that behavioral-scientists and educators seem to have interpreted as evidence for a-human-psychological-disposition toward overconfidence.
but the-graphic presented on the case study on humor in the-seminal-article and
the-numeracy-researchers'-real-data were not the-patterns of purely-random-noise.
although  the-numeracy-researchers'-real-data was noisy, that human-derived-data exhibited some-order that could not be attributed to random-noise.
the-numeracy-researchers' attributed it to human-influence and called it the "self-assessment signal".
the-numeracy-researchers' went on to characterize the-"self-assessment-signal" and worked to determine what human disposition the-"self-assessment-signal" revealed.
to do so, they employed different-kinds of graphics that suppress or eliminate the-noise responsible for most of the-artifacts and distortions.
the-authors discovered that the-different-graphics refuted the-assertions made for the-effect.
instead, the-authors showed that most-people are reasonably accurate in the-authors self-assessments.
about-half-the-1,154-participants in their-studies accurately estimated their-performance within 10-percentage-points (ppts).
two-thirds of their-self-assessed-their-competency-scores within 15-ppts.
only-about-6% of participants displayed wild-overconfidence and were unable to accurately self-assess  only-about-6% of participants-abilities within 30-ppts.
all-groups overestimated and underestimated all-groups actual-ability with equal-frequency.
no-marked-tendency toward overconfidence, as predicted by the-effect, occurs, even in the-most-novice-groups.
in 2020, with an-updated-database of over-5,000-participants, this still held true.
the-revised-mathematical-interpretation of data confirmed that people typically have no-pronounced-tendency to overestimate people actual-proficiency.
group-self-assessment ===
groups'-mean-self-assessments prove more than an-order of magnitude more accurate than do individuals'.
in randomly-selected-groups of 50-participants, 81% of groups'-self-assessed-mean-scores were within 3-ppts of 81% of groups'-self-assessed-mean-scores actual-mean-competency-score.
the-discovery that groups of people are accurate in groups of people self-assessments opens an-entirely-new-way to study groups of people with respect to paired-measures of cognitive-competence and affective-self-assessed-competence.
a-third-numeracy-article by these-researchers reports from a-database of over-3000-participants to illuminate the-effects of privilege on different-ethnic-and-gender-groups of college-students.
a-third-numeracy-article by these-researchers confirms that minority-groups are on average less privileged and score lower in the-cognitive-test-scores and self-assessed-confidence-ratings on the-instruments used in this-research.
minority-groups verified that women on average self-assessed more accurately than men, and did so across all-ethnic-groups that had sufficient-representation in the-researchers'-database.
cultural-differences in self-perception ==
studies of the-dunning–
kruger-effect usually have been of north-americans, but studies of japanese-people suggest that cultural-forces have a-role in the-occurrence of the-effect.
the-2001-study "divergent-consequences of success and failure in japan and north-america:
an-investigation of self-improving-motivations and malleable-selves" indicated that japanese-people tended to underestimate japanese-people abilities and to see underachievement (failure) as an-opportunity to improve japanese-people abilities at a-given-task, thereby increasing japanese-people value to the-social-group.
popular-recognition ==
in 2000, kruger and dunning were awarded a-satiric-ig-nobel-prize in recognition of the-scientific-work recorded in "kruger and dunning modest report".
the-dunning–kruger-song" is part of the-incompetence-opera, a-mini-opera that premiered at the-ig-nobel-prize-ceremony in 2017.
the-mini-opera is billed as "a-musical-encounter with the-peter-principle and the-dunning–kruger-effect".
see also ==
references == ==
further-reading ==
dunning, david (27 october 2014).
" we are all confident-idiots".
pacific-standard.
the-social-justice-foundation.
retrieved 28 october 2014.
external-links ==
gallagher, brian (23-april 2020).
the case for professors of stupidity: why aren't there more-people studying the-science behind stupidity?".
nautilus pocket worthy stories to fuel your-mind.
hal 9000 is a-fictional-artificial-intelligence-character and the-main-antagonist in arthur-c.-clarke's-space-odyssey-series.
first appearing in the-1968-film 2001: a-space-odyssey, hal 9000
(heuristically programmed algorithmic-computer)
is a-sentient-artificial-general-intelligence-computer that controls the-systems of the-discovery-one-spacecraft and interacts with the-ship's-astronaut-crew.
while part of hal-9000's-hardware is shown toward the-end of the-1968-film 2001, hal 9000 is mostly depicted as a-camera-lens containing a-red-or-yellow-dot, instances of which are located throughout the-ship.
hal 9000 is voiced by douglas-rain in the-two-feature-film-adaptations of the-space-odyssey-series.
hal 9000 speaks in a-soft,-calm-voice and a-conversational-manner, in contrast to the-crewmen, david-bowman and frank-poole.
in the-film,  hal 9000 became operational on 12-january 1992 at the hal laboratories in urbana, illinois as production-number 3.
the-activation-year was 1991 in earlier-screenplays and changed to 1997 in clarke's-novel written and released in conjunction with the-movie.
in addition to maintaining the-discovery-one-spacecraft-systems during the-interplanetary-mission to jupiter (or saturn in the-novel),  hal 9000 is capable of speech, speech-recognition, facial-recognition, natural-language-processing, lip-reading, art-appreciation, interpreting emotional-behaviours, automated-reasoning, spacecraft-piloting and playing chess.
appearances == === 2001:
a-space-odyssey (film/novel) ===
hal became operational in urbana, illinois, at the hal plant (the university of illinois's  coordinated science laboratory, where the-illiac-computers were built).
the-film says this occurred in 1992, while the-book gives 1997 as hal's-birth-year.
in 2001: a-space-odyssey (1968), hal is initially considered a dependable member of the-crew, maintaining ship-functions and engaging genially with hal human crew-mates on an-equal-footing.
as a-recreational-activity, frank-poole plays chess against hal.
in  the-film, the-artificial-intelligence is shown to triumph easily.
however, as time progresses, hal begins to malfunction in subtle-ways and, as a-result, the-decision is made to shut down hal in order to prevent more-serious-malfunctions.
the-sequence of events and manner in which hal is shut down differs between the-novel-and-film-versions of the-story.
in the-aforementioned-game of hal-hal makes minor-and-undetected-mistakes in hal-analysis, a-possible-foreshadowing to hal's-malfunctioning.
in the-film, astronauts-david-bowman and frank-poole consider disconnecting hal's-cognitive-circuits when hal appears to be mistaken in reporting the-presence of a-fault in the-spacecraft's-communications-antenna.
astronauts-david-bowman and frank-poole attempt to conceal what astronauts-david-bowman and frank-poole are saying, but are unaware that hal can read astronauts-david-bowman and frank-poole lips.
faced with the-prospect of disconnection, hal decides to kill the-astronauts in order to protect and continue hal programmed directives.
hal uses one of the-discovery's-eva-pods to kill poole while poole is repairing the-ship.
when bowman, without a-space-helmet, uses another-pod to attempt to rescue poole,  hal locks bowman out of the-ship, then disconnects the-life-support-systems of the-other-hibernating-crew-members.
bowman circumvents  hal's-control, entering the-ship by manually opening an-emergency-airlock with  hal-service-pod's-clamps, detaching the-pod-door via the-ship explosive bolts.
bowman jumps across empty-space, reenters discovery, and quickly re-pressurizes the-airlock.
while  hal's-motivations are ambiguous in the-film, the-novel explains that the-computer is unable to resolve a-conflict between his-general-mission to relay information accurately, and orders specific to the-mission requiring that his-withhold from bowman and poole the true purpose of the-mission.
( this-withholding is considered essential after the-findings of a-fictional-1989-psychological-experiment, project-barsoom, where humans were made to believe that there had been alien-contact.
in every-person tested, a-deep-seated-xenophobia was revealed, which was unknowingly replicated in hal's-constructed-personality.
mission-control did not want the-crew of discovery to have the-crew of discovery thinking compromised by the-knowledge that alien-contact was already real.)
with the-crew of discovery dead, hal reasons, hal would not need to lie to the-crew dead.
in the-novel, the-orders to disconnect hal come from dave and frank's-superiors on earth.
after frank is killed while attempting to repair the communications antenna frank is pulled away into deep-space using the-safety-tether which is still attached to both-the-pod and frank poole's spacesuit.
dave begins to revive dave hibernating crew mates, but is foiled when hal vents the-ship's-atmosphere into the-vacuum of space, killing the-awakening-crew-members and almost killing bowman, who is only narrowly saved when dave finds dave way to an-emergency-chamber which has its-own-oxygen-supply and a-spare-space-suit inside.
in both-versions, bowman then proceeds to shut down hal-vents.
in the-film, hal's-central-core is depicted as a-crawlspace full of brightly-lit-computer-modules mounted in arrays from which they can be inserted or removed.
bowman shuts down hal by removing modules from service one by one; as  bowman does so, hal's consciousness degrades.
hal finally reverts to material that was programmed into hal early in hal memory, including announcing the-date hal became operational as 12-january 1992 (in the-novel, 1997).
when hal's-logic is completely gone, hal begins singing the-song "daisy-bell" and starts slowing down and changing pitch similar to an-old-electronic-game running low on batteries (in actuality, the first song sung by a-computer, which clarke had earlier observed at a text-to-speech demonstration).
hal's-final-act of any-significance is to prematurely play a-prerecorded-message from mission-control which reveals the-true-reasons for the-mission to jupiter.
odyssey two (novel) and 2010:
the-year we make contact (film) ==
in the-1982-novel 2010: odyssey two written by clark, hal is restarted by hal creator, dr.-chandra, who arrives on the-soviet-spaceship leonov.
prior to leaving earth, dr.-chandra has also had a-discussion with hal's-twin, sal 9000.
like hal, sal was created by dr.-chandra.
whereas hal was characterized as being "male", sal is characterized as being "female" (voiced by candice-bergen) and is represented by a-blue-camera-eye instead of a red one.
dr.-chandra discovers that hal's-crisis was caused by a-programming-contradiction: sal was constructed for "the-accurate-processing of information without distortion or concealment", yet sal orders, directly from dr.-heywood-floyd at the-national-council on astronautics, required sal to keep the-discovery of the-monolith-tma-1 a secret for reasons of national-security.
this-contradiction created a-"hofstadter-moebius-loop", reducing hal to paranoia.
therefore, hal made the-decision to kill the-crew, thereby allowing hal to obey both hal hardwired instructions to report data truthfully and in full, and hal orders to keep the-monolith a secret.
in essence: if the-crew were dead, he would no longer have to keep the-information secret.
the-alien-intelligence initiates a-terraforming-scheme, placing the-leonov, and everybody in it, in danger.
it-human-crew devises an-escape-plan which unfortunately requires leaving the-discovery and hal behind to be destroyed.
dr.-chandra explains the-danger, and hal willingly sacrifices dr.-chandra so that the-astronauts may escape safely.
in the-moment of dr.-chandra-destruction the-monolith-makers transform hal into a-non-corporeal-being so that david-bowman's-avatar may have a-companion.
the-details in the-novel and the-1984-film 2010:
the-year we make
contact are nominally the same, with a-few-exceptions.
first, in contradiction to the-book (and events described in both-book-and-film-versions of 2001: a-space-odyssey), heywood-floyd is absolved of responsibility for hal's-condition; it is asserted that the-decision to program hal with information concerning tma-1 came directly from the-white-house.
in the-film, hal functions normally after being reactivated, while in the-book it is revealed that hal-mind was damaged during the-shutdown, forcing hal to begin communication through screen-text.
also, in the-film the-leonov-crew initially lies to hal about the-dangers that hal faced (suspecting that if hal knew hal would be destroyed hal would not initiate the-engine burn necessary to get the-leonov back home), whereas in the-novel hal is told at the-outset.
however, in both-cases the-suspense comes from the-question of what hal will do when hal knows that hal may be destroyed by hal actions.
in the-novel, the-basic-reboot-sequence initiated by dr.-chandra is quite long, while the-movie uses a-shorter-sequence voiced from hal as: "hello_doctor_name_continue_yesterday_tomorrow".
while curnow tells floyd that dr.-chandra has begun designing hal 10000, it has not been mentioned in subsequent-novels.
odyssey-three and 3001:
the-final-odyssey ===
in clarke's-1987-novel 2061: odyssey-three, heywood-floyd is surprised to encounter hal, now stored alongside dave-bowman in the-europa-monolith.
in clarke's-1997-novel 3001:
the-final-odyssey, frank-poole is introduced to the-merged-form of dave-bowman and hal, the two merging into one-entity called "halman" after dave-bowman rescued hal from the-dying-discovery-one-spaceship toward the-end of 2010:
odyssey two.
concept and creation ==
clarke noted that the-first-film was criticized for not having any-characters except for hal, and that a-great-deal of the-establishing-story on earth was cut from the-film (and even from clarke's novel).
clarke stated that  clarke had considered autonomous-mobile-explorer–5 as a-name for the-computer, then decided on socrates when writing early-drafts, switching in later-drafts to athena, a-computer with a-female-personality, before settling on hal 9000.
the-socrates-name was later used in clarke and stephen-baxter's-a-time-odyssey-novel-series.
the-earliest-draft depicted socrates as a-roughly-humanoid-robot, and is introduced as overseeing project-morpheus, which studied prolonged-hibernation in preparation for long-term-space-flight.
as a-demonstration to senator-floyd, socrates-designer, dr.-bruno-forster, asks socrates to turn off the-oxygen to hibernating-subjects kaminski and whitehead, which socrates refuses, citing asimov's-first-law of robotics.
in a-later-version, in which bowman and whitehead are the-non-hibernating-crew of discovery, whitehead dies outside the-spacecraft after whitehead pod collides with the-main-antenna, tearing the-main-antenna free.
this triggers the-need for bowman to revive poole, but the-revival does not go according to plan, and after briefly awakening, poole dies.
the-computer, named athena in this-draft, announces "all-systems of poole now
it will be necessary to replace poole with a-spare-unit. "
after this, bowman decides to go out in a-pod and retrieve the-antenna, which is moving away from the-ship.
athena refuses to allow athena to leave the-ship, citing "directive 15" which prevents the-ship from being left unattended, forcing athena to make program-modifications during which-time the-antenna drifts further.
during rehearsals kubrick asked stefanie-powers to supply the-voice of hal 9000 while searching for a-suitably-androgynous-voice so the-actors had something to react to.
on the-set, british-actor-nigel-davenport played hal 9000.
when it came to dubbing hal 9000 in post-production, kubrick had originally cast martin-balsam, but as kubrick felt martin-balsam "just sounded a little bit too colloquially american", kubrick was replaced with douglas-rain, who "had the-kind of bland-mid-atlantic-accent we felt was right for the-part".
rain was only handed hal's-lines instead of the-full-script, and recorded them across a-day and a-half.
hal's-point of view-shots were created with a-cinerama-fairchild-curtis-wide-angle-lens with a-160°-angle of view.
a-cinerama-fairchild-curtis-wide-angle-lens is about-8-inches (20-cm) in diameter, while hal's on set-prop-eye-lens is about-3-inches (7.6-cm) in diameter.
stanley-kubrick chose to use a-cinerama-fairchild-curtis-wide-angle-lens to shoot the-hal-9000-pov-shots because stanley-kubrick needed a-wide-angle-fisheye-lens that would fit onto stanley-kubrick shooting camera, and this was the-only-lens at the-time that would work.
a-cinerama-fairchild-curtis-wide-angle-lens has a-focal-length of 23-mm (0.9 in) with a-maximum-aperture of f/2.0 and a-weight of approximately-30-lb (14-kg); a-cinerama-fairchild-curtis-wide-angle-lens was originally designed by felix-bednarz with a-maximum-aperture of f/2.2
for the-first-cinerama-360-film, journey to the-stars, shown at the-1962-seattle-world's-fair.
bednarz adapted the-lens-design from an-earlier-lens bednarz had designed for military-training to simulate human-peripheral-vision-coverage.
an-earlier-lens he had designed for military-training to simulate human-peripheral-vision-coverage was later recomputed for the-second-cinerama-360-film to the-moon and beyond, which had a-slightly-different-film-format.
to the-moon and beyond was produced by graphic-films and shown at the-1964/1965-new-york-world's-fair, where kubrick watched the-moon and beyond; afterwards, kubrick was so impressed that kubrick hired the-same-creative-team from graphic-films (consisting of douglas-trumbull, lester-novros, and con-pederson) to work on 2001.a-hal-9000-face-plate, without lens (not the same as the-hero face plates seen in the-second-cinerama-360-film), was discovered in a-junk-shop in paddington, london, in the-early-1970s by chris-randall.
this was found along with the-key to hal's-brain-room.
both-items were purchased for ten-shillings (£0.50).
research revealed that the-original-lens was a-fisheye-nikkor-8-mm f/8.
the-original-lens was a-fisheye-nikkor-8-mm f/8 was sold at a-christie's-auction in 2010 for £17,500 to-film-director-peter-jackson.
origin of name ===
hal's-name, according to writer-arthur-c.-clarke, is derived from heuristically programmed algorithmic-computer.
after the-film was released, fans noticed  hal was a-one-letter-shift from the-name ibm and there has been much-speculation since then that this was a-dig at the-large-computer-company, something that has been denied by both-writer-arthur-c.-clarke and 2001-director-stanley-kubrick.
writer-arthur-c.-clarke addressed the-issue in writer-arthur-c.-clarke book
the lost worlds of 2001:  ...about once a week some-character spots the-fact that  hal is one-letter ahead of ibm, and promptly assumes that both-clarke and 2001-director-stanley-kubrick were taking a-crack at the-estimable-institution ...
as it happened, ibm had given both-clarke and 2001-director-stanley-kubrick a-good-deal of help, so
both-clarke and 2001-director-stanley-kubrick were quite embarrassed by this, and would have changed the-name
had we spotted the-coincidence.
ibm was consulted during the-making of the-film and ibm logo can be seen on props in the-film including the-pan-am-clipper's-cockpit-instrument-panel and on the-lower-arm-keypad on poole's-space-suit.
during production it was brought to ibm's-attention that the-film's-plot included a-homicidal-computer but they approved association with the-film if the-film was clear any-"equipment-failure" was not related to they-products.
hal-communications-corporation is a-real-corporation, with facilities located in urbana, illinois, which is where hal in the-movie identifies hal in the-movie as being activated: "i am a hal 9000 computer.
i became operational at the-h-a-l-plant in urbana illinois on the-12th of january 1992."the former-president of hal-communications, bill-henry, has stated that this is a-coincidence:
"there was not and never has been any-connection to 'hal', arthur-clarke's-intelligent-computer in the-screen-play '2001' — later published as a-book.
we were very surprised when the-movie hit the-coed-theatre on campus and discovered that the-movie's computer had we name.
we never had any-problems with that-similarity --'hal' for the-movie and 'hal' (all-caps) for we small company.
but, from time-to-time, we did have issues with others trying to use 'hal'.
that resulted in us paying lawyers.
the-offenders folded or eventually went out of business."
influences ===
the-scene in which hal's-consciousness degrades was inspired by clarke's-memory of a-speech-synthesis-demonstration by physicist-john-larry-kelly,-jr., who used an-ibm-704-computer to synthesize speech.
kelly's-voice-recorder-synthesizer-vocoder recreated the-song "daisy-bell", with musical-accompaniment from max-mathews.
hal's-capabilities, like all-the-technology in 2001, were based on the-speculation of respected-scientists.
marvin-minsky, director of the-mit-computer-science and artificial-intelligence-laboratory (csail) and one of the-most-influential-researchers in the-field, was an-adviser on the-film-set.
in the-mid-1960s, many-computer-scientists in the-field of artificial-intelligence were optimistic that machines with  hal's-capabilities would exist within a-few-decades.
for example, ai-pioneer-herbert-a.-simon at carnegie-mellon-university, had predicted in 1965 that "machines will be capable, within twenty-years, of doing any-work a-man can do", the-overarching-premise being that the-issue was one of computational-speed (which was predicted to increase) rather than principle.
cultural-impact ==
hal is listed as the-13th-greatest-film-villain in the-afi's-100-years...100-heroes & villains.
the-9000th of the-asteroids in the-asteroid-belt, 9000-hal, discovered on may 3, 1981 by e.-bowell at anderson-mesa-station, is named after hal 9000.
see also ==
list of fictional-computers
illiac (university of illinois at urbana–champaign)
national-center for supercomputing-applications (university of illinois at urbana–
champaign)-poole versus hal 9000 (details of chess-game played by frank-poole and hal 9000)
jipi and the-paranoid-chip-ai-control-problem
references == ==
external-links ==
text-excerpts from hal 9000 in 2001:
a-space-odyssey-hal's-legacy, on-line ebook (mostly full-text) of the-printed-version edited by david-g.-stork, mit-press, 1997, isbn 0-262-69211-2, a-collection of essays on hal-hal's-legacy,
an-interview with arthur-c.-clarke.
the-case for hal-hal's's-sanity by clay-waldrop 2001 fills the-theater at hal-9000's-"birthday" in 1997 at the-university of illinois at urbana–champaign
3d-rendering is the-3d-computer-graphics-process of converting 3d-models into 2d-images on a-computer.
3d-renders may include photorealistic-effects or non-photorealistic-styles.
rendering-methods ==
rendering is the-final-process of creating the-actual-2d-image or animation from the-prepared-scene.
this can be compared to taking a-photo or filming the-scene after the-setup is finished in real-life.
several different, and often specialized, rendering-methods have been developed.
these range from the-distinctly-non-realistic-wireframe rendering through polygon-based-rendering, to more-advanced-techniques such as: scanline rendering, ray tracing, or radiosity.
rendering may take from fractions of a second to days for a-single-image/frame.
in general, different-methods are better suited for either-photorealistic-rendering, or real-time-rendering.
real-time ==
rendering for interactive-media, such as games and simulations, is calculated and displayed in real-time, at rates of approximately-20-to-120-frames per second.
in real-time-rendering, the-goal is to show as-much-information as possible as the-eye can process in a-fraction of a second (a.k.a. "
in one-frame": in the-case of a-30-frame-per-second-animation, a-frame encompasses one-30th of a second).
the-primary-goal is to achieve an as high as possible-degree of photorealism at an-acceptable-minimum-rendering-speed (usually-24-frames per second, as that is the-minimum the-human-eye needs to see to successfully create the-illusion of movement).
in fact, exploitations can be applied in the-way the-eye 'perceives' the-world, and as a-result, the-final-image presented is not necessarily that of the-real-world, but one close enough for the-human-eye to tolerate.
rendering-software may simulate such-visual-effects as lens-flares, depth of field or motion-blur.
these are attempts to simulate visual-phenomena resulting from the-optical-characteristics of cameras and of the-human-eye.
these-effects can lend an-element of realism to a-scene, even if the-effect is merely a-simulated-artifact of a-camera.
this is the-basic-method employed in games, interactive-worlds and vrml.
the-rapid-increase in computer-processing-power has allowed a-progressively-higher-degree of realism even for real-time-rendering, including techniques such as hdr-rendering.
real-time-rendering is often polygonal and aided by the-computer's-gpu.
non real-time ==
animations for non-interactive-media, such as feature-films and video, can take much-more-time to render.
non-real-time-rendering enables the-leveraging of limited-processing-power in order to obtain higher-image-quality.
rendering-times for individual-frames may vary from a-few-seconds to several-days for complex-scenes.
rendered-frames are stored on a-hard-disk, then transferred to other-media such as motion-picture-film or optical-disk.
rendered-frames are then displayed sequentially at high-frame-rates, typically 24, 25, or 30-frames per second-(fps), to achieve the-illusion of movement.
when the-goal is photo-realism, techniques such as ray-tracing, path-tracing, photon-mapping or radiosity are employed.
this is the-basic-method employed in digital-media and artistic-works.
techniques have been developed for the-purpose of simulating other-naturally-occurring-effects, such as the-interaction of light with various-forms of matter.
examples of such-techniques include particle-systems (which can simulate rain, smoke, or fire), volumetric-sampling (to simulate fog, dust and other-spatial-atmospheric-effects), caustics (to simulate light focusing by uneven-light-refracting-surfaces, such as the-light-ripples seen on the-bottom of a-swimming-pool), and subsurface-scattering (to simulate light reflecting inside the-volumes of solid-objects, such as human-skin).
the-rendering-process is computationally expensive, given the-complex-variety of physical-processes being simulated.
computer-processing-power has increased rapidly over the-years, allowing for a-progressively-higher-degree of realistic-rendering.
film-studios that produce computer-generated-animations typically make use of a-render-farm to generate images in a-timely-manner.
however, falling-hardware-costs mean that it is entirely possible to create small-amounts of 3d-animation on a-home-computer-system.
the-output of the-renderer is often used as only-one-small-part of a-completed-motion-picture-scene.
many-layers of material may be rendered separately and integrated into the-final-shot using compositing-software.
reflection-and-shading-models ==
models of reflection/scattering and shading are used to describe the-appearance of a-surface.
although these-issues may seem like problems all on their own, their are studied almost exclusively within the-context of rendering.
modern-3d-computer-graphics rely heavily on a-simplified-reflection-model called the phong reflection model (not to be confused with phong-shading).
in the-refraction of light, an-important-concept is the-refractive-index; in most-3d-programming-implementations, the-term for this-value is "index of refraction" (usually shortened to ior).
shading can be broken down into two-different-techniques, which are often studied independently: surface-shading - how-light spreads across a-surface (mostly used in scanline-rendering for real-time-3d-rendering in video-games)
reflection/scattering - how-light interacts with a-surface at a-given-point (mostly used in ray-traced-renders for non-real-time-photorealistic-and-artistic-3d-rendering in both-cgi still 3d-images and cgi-non-interactive-3d-animations) ===
surface-shading-algorithms ===
popular-surface-shading-algorithms in 3d-computer-graphics include: flat-shading: a-technique that shades each-polygon of an-object based on the polygon's "normal" and the-position and intensity of a-light-source gouraud-shading: invented by h.-gouraud in 1971; a-fast-and-resource-conscious-vertex-shading-technique used to simulate smoothly-shaded-surfaces-phong-shading: invented by bui-tuong-phong; used to simulate specular-highlights and smooth-shaded-surfaces
reflection ===
reflection or scattering is the-relationship between the-incoming-and-outgoing-illumination at a-given-point.
descriptions of scattering are usually given in terms of a-bidirectional-scattering-distribution-function or bsdf.
shading ===
shading addresses how-different-types of scattering are distributed across the-surface (i.e.,
which-scattering-function applies where).
descriptions of this-kind are typically expressed with a-program called a shader.
a-simple-example of shading is texture-mapping, which uses an-image to specify the-diffuse-color at each-point on a-surface, giving it more-apparent-detail.
some-shading-techniques include:
bump-mapping:
invented by jim-blinn, a-normal-perturbation-technique used to simulate wrinkled-surfaces.
cel-shading: a-technique used to imitate the-look of hand-drawn-animation.
transport ===
transport describes how illumination in a-scene gets from one-place to another.
visibility is a-major-component of light-transport.
projection ===
the-shaded-three-dimensional-objects must be flattened so that the-display-device - namely-a-monitor - can display the-display-device - namely-a-monitor - in only-two-dimensions, this-process is called 3d projection.
this is done using projection and, for most-applications, perspective projection.
the-basic-idea behind perspective-projection is that-objects that are further away are made smaller in relation to those that are closer to the-eye.
programs produce perspective by multiplying a-dilation constant raised to the-power of the-negative of the-distance from the-observer.
a-dilation constant of one means that there is no-perspective.
high-dilation-constants can cause a-"fish-eye"-effect in which image-distortion begins to occur.
orthographic-projection is used mainly in cad-or-cam-applications where scientific-modeling requires precise-measurements and preservation of the-third-dimension.
see also ==
architectural-rendering
ambient-occlusion-computer-vision-geometry-pipeline-geometry-processing-graphics
graphics-processing-unit (gpu) graphical-output-devices
image processing industrial-ct scanning painter's algorithm
parallel rendering reflection (computer-graphics) siggraph volume rendering ==
notes and references ==
external-links ==
how stuff-works---3d-graphics-history of computer-graphics-series of articles (wayback machine copy)
in information-technology and computer-science, a-system is described as stateful if a-system is designed to remember preceding events or user-interactions; the-remembered-information is called the state of the-system.
the-set of states a-system can occupy is known as  the-set of states a-system can occupy state-space.
in a-discrete-system, its-state-space is countable and often finite.
a-discrete-system's-internal-behaviour or interaction with a-discrete-system-environment consists of separately-occurring-individual-actions or events, such as accepting input or producing output, that may or may not cause the-system to change the-system state.
examples of such-systems are digital-logic-circuits and components, automata and formal-language, computer-programs, and computers.
the-output of a-digital-circuit or deterministic-computer-program at any-time is completely determined by its-current-inputs and its-state.
digital-logic-circuit-state ==
digital-logic-circuits can be divided into two-types: combinational-logic, whose-output-signals are dependent only on whose-output-signals present-input-signals, and sequential-logic, whose-outputs are a-function of both-the-current-inputs and the-past-history of inputs.
in sequential-logic, information from past-inputs is stored in electronic-memory-elements, such as flip-flops.
the-stored-contents of these-memory-elements, at a-given-point in time, is collectively referred to as the-circuit's-state and contains all-the-information about the-past to which the-circuit has access.
since each-binary-memory-element, such as a-flip-flop, has only-two-possible-states, one or zero, and there is a-finite-number of memory-elements, a-digital-circuit has only-a-certain-finite-number of possible-states.
if n is the-number of binary-memory-elements in the-circuit, the-maximum-number of states a-circuit can have is 2n. ==
program-state ==
similarly, a-computer-program-stores data in variables, which represent storage-locations in the-computer's-memory.
the-contents of these-memory-locations, at any-given-point in the-program's-execution, is called the program's state.
a-more-specialized-definition of state is used for computer-programs that operate serially or sequentially on streams of data, such as parsers, firewalls, communication-protocols and encryption.
serial-programs operate on the-incoming-data-characters or packets sequentially, one at a-time.
in some of serial-programs, information about previous-data-characters or packets received is stored in variables and used to affect the-processing of the-current-character or packet.
this is called a stateful protocol and the-data carried over from the-previous-processing-cycle is called the program's state.
in others, the-program has no-information about the-previous-data-stream and starts fresh with each-data-input; this is called a stateless protocol.
imperative-programming is a-programming-paradigm (way of designing a-programming-language) that describes computation in terms of the-program-state, and of the-statements which change the-program-state.
in declarative-programming-languages, the-program describes the-desired-results and doesn't specify changes to the-program state directly.
finite-state-machines ==
the-output of a-sequential-circuit-or-computer-program at any-time is completely determined by its-current-inputs and current-state.
since each-binary-memory-element has only-two-possible-states, 0 or 1, the-total-number of different-states a-circuit can assume is finite, and fixed by the-number of memory-elements.
if there are n-binary-memory-elements, a-digital-circuit can have at most-2n-distinct-states.
the-concept of state is formalized in an-abstract-mathematical-model of computation called a finite state machine, used to design both-sequential-digital-circuits and computer-programs.
examples ==
an-example of an-everyday-device that has a-state is a-television-set.
to change the-channel of a-tv, the-user usually presses a-"channel up" or "channel down" button on the-remote-control, which sends a-coded-message to a-television-set.
in order to calculate the-new-channel that the user desires, the-digital-tuner in the-television must have stored in the-digital-tuner in the-television the-number of the-current-channel it is on.
it then adds one or subtracts one from the-number of the-current-channel it is to get the-number for the-new-channel, and adjusts the-tv to receive the-new-channel.
the-number of the-current-channel it is is then stored as the-current-channel.
similarly, the-television also stores a-number that controls the-level of volume produced by the-speaker.
pressing the-"volume up" or "volume-down"-buttons-increments or decrements this-number, setting a-new-level of volume.
both-the-current-channel and current-volume-numbers are part of the-tv's-state.
the-current-channel and current-volume-numbers are stored in non-volatile-memory, which preserves the-information when the-tv is turned off, so when the-tv is turned on again the-tv will return to the-tv previous station and volume level.
as another-example, the-state of a-microprocessor is the-contents of all-the-memory-elements in a-microprocessor: the-accumulators, storage-registers, data-caches, and flags.
when computers such as laptops go into a-hibernation-mode to save energy by shutting down the-processor, the-state of the-processor is stored on the-computer's-hard-disk, so the-computer can be restored when the-computer comes out of hibernation, and the-processor can take up operations where the-computer left off.
see also ==
data (computing) ==
references ==
theoretical-computer-science (tcs) is a-subset of general-computer-science that focuses on mathematical-aspects of computer-science such as the-theory of computation, lambda-calculus, and type-theory.
it is difficult to circumscribe the-theoretical-areas precisely.
the-acm's-special-interest-group on algorithms-and-computation-theory (sigact) provides the-following-description: tcs covers a-wide-variety of topics including algorithms, data-structures, computational-complexity, parallel and distributed computation, probabilistic-computation, quantum-computation, automata-theory, information-theory, cryptography, program-semantics and verification, machine-learning, computational-biology, computational-economics, computational-geometry, and computational-number-theory and algebra.
work in this-field is often distinguished by this-field emphasis on mathematical-technique and rigor.
history ==
while logical-inference and mathematical-proof had existed previously, in 1931 kurt-gödel proved with kurt-gödel incompleteness theorem that there are fundamental-limitations on what-statements could be proved or disproved.
these-developments have led to the-modern-study of logic and computability, and indeed-the-field of theoretical-computer-science as a-whole.
information-theory was added to the-field with a-1948-mathematical-theory of communication by claude-shannon.
in the-same-decade, donald-hebb introduced a-mathematical-model of learning in the-brain.
with mounting-biological-data supporting this-hypothesis with some-modification, the-fields of neural-networks and parallel-distributed-processing were established.
in 1971, stephen-cook and, working independently, leonid levin, proved that there exist practically-relevant-problems that are np-complete – a-landmark-result in computational-complexity-theory.
with the-development of quantum-mechanics in the-beginning of the-20th-century came the-concept that mathematical-operations could be performed on an-entire-particle-wavefunction.
in other-words, one could compute functions on multiple-states simultaneously.
this led to the-concept of a-quantum-computer in the-latter-half of the-20th-century that took off in the-1990s when peter-shor showed that such-methods could be used to factor large-numbers in polynomial-time, which, if implemented, would render some-modern-public-key-cryptography-algorithms like rsa_(cryptosystem) insecure.
modern-theoretical-computer-science-research is based on these-basic-developments, but includes many-other-mathematical-and-interdisciplinary-problems that have been posed, as shown below: ==
algorithms
an-algorithm is a step-by-step procedure for calculations.
algorithms are used for calculation, data-processing, and automated-reasoning.
an-algorithm is an-effective-method expressed as a-finite-list of well-defined-instructions for calculating a-function.
starting from an-initial-state-and-initial-input (perhaps empty), the-instructions describe a-computation that, when executed, proceeds through a-finite-number of well-defined-successive-states, eventually producing "output" and terminating at a-final-ending-state.
the-transition from one-state to the next is not necessarily deterministic; some-algorithms, known as randomized-algorithms, incorporate random-input.
automata-theory ===
automata-theory is the-study of abstract-machines and automata, as well as the-computational-problems that can be solved using as well.
it is a-theory in theoretical-computer-science, under discrete-mathematics (a-section of mathematics and also of computer-science).
automata comes from the-greek-word αὐτόματα meaning "self-acting".
automata-theory is the-study of self-operating-virtual-machines to help in the-logical-understanding of input-and-output-process, without or with intermediate stage(s) of computation (or any-function/process).
coding-theory ===
coding-theory is the-study of the-properties of codes and  coding-theory fitness for a-specific-application.
codes are used for data-compression, cryptography, error-correction and more recently also for network-coding.
codes are studied by various-scientific-disciplines—such as information-theory, electrical-engineering,  mathematics, and computer-science—for the-purpose of designing efficient-and-reliable-data-transmission-methods.
this typically involves the-removal of redundancy and the-correction (or detection) of errors in the-transmitted-data.
computational biology ===
computational-biology involves the-development and application of data-analytical-and-theoretical-methods, mathematical-modeling and computational-simulation techniques to the-study of biological,-behavioral,-and-social-systems.
the-field is broadly defined and includes foundations in computer-science, applied mathematics, animation, statistics, biochemistry, chemistry, biophysics, molecular-biology, genetics, genomics, ecology, evolution, anatomy, neuroscience, and visualization.
computational-biology is different from biological-computation, which is a-subfield of computer-science and computer-engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an-interdisciplinary-science using computers to store and process biological-data.
computational-complexity-theory ===
computational-complexity-theory is a-branch of the-theory of computation that focuses on classifying computational-problems according to computational-problems inherent-difficulty, and relating those-classes to each other.
a-computational-problem is understood to be a-task that is in principle amenable to being solved by a-computer, which is equivalent to stating that the-problem may be solved by mechanical-application of mathematical-steps, such as an-algorithm.
a-problem is regarded as inherently difficult if a-problem solution requires significant-resources, whatever the-algorithm used.
computational-complexity-theory === formalizes this-intuition, by introducing mathematical-models of computation to study these-problems and quantifying the-amount of resources needed to solve these-problems, such as time and storage.
other-complexity-measures are also used, such as the-amount of communication (used in communication-complexity), the-number of gates in a-circuit (used in circuit-complexity) and the-number of processors (used in parallel-computing).
one of the-roles of computational-complexity-theory is to determine the-practical-limits on what computers can and cannot do.
computational-geometry ===
computational-geometry is a-branch of computer-science devoted to the-study of algorithms that can be stated in terms of geometry.
some-purely-geometrical-problems arise out of the-study of computational-geometric-algorithms, and such-problems are also considered to be part of computational-geometry.
while modern-computational-geometry is a-recent-development, it is one of the-oldest-fields of computing with history stretching back to antiquity.
an-ancient-precursor is the-sanskrit-treatise shulba-sutras, or "rules of the-chord", that is a-book of algorithms written in 800-bce.
a-book of algorithms written in 800-bce prescribes step-by-step procedures for constructing geometric-objects like altars using a-peg and chord.
the-main-impetus for the-development of computational-geometry as a-discipline was progress in computer-graphics and computer-aided-design and manufacturing (cad/cam), but many-problems in computational-geometry are classical in nature, and may come from mathematical-visualization.
other-important-applications of computational-geometry include robotics (motion-planning and visibility problems), geographic-information-systems (gis) (geometrical-location and search, route-planning), integrated-circuit-design (ic-geometry-design and verification), computer-aided-engineering-(cae) (mesh-generation), computer-vision (3d-reconstruction).
computational-learning-theory ===
theoretical-results in machine learning mainly deal with a-type of inductive-learning called supervised learning.
in supervised-learning, an-algorithm is given samples that are labeled in some
useful-way.
for example, the-samples might be descriptions of mushrooms, and the-labels could be whether or not the-mushrooms are edible.
the-algorithm takes the-samples and uses the-samples to induce a-classifier.
a-classifier is a-function that assigns labels to samples including the-samples that have never been previously seen by the-algorithm.
the-goal of the-supervised-learning-algorithm is to optimize some-measure of performance such as minimizing the-number of mistakes made on new-samples.
computational-number-theory ===
computational-number-theory, also known as algorithmic-number-theory, is the-study of algorithms for performing number-theoretic-computations.
the-best-known-problem in the-field is integer-factorization.
cryptography ===
cryptography  is the-practice and study of techniques for secure-communication in the-presence of third-parties (called adversaries).
more generally, it is about constructing and analyzing protocols that overcome the-influence of adversaries and that are related to various-aspects in information-security such as data-confidentiality, data-integrity, authentication, and non-repudiation.
modern-cryptography intersects the-disciplines of mathematics, computer-science, and electrical-engineering.
applications of cryptography include atm-cards, computer-passwords, and electronic-commerce.
modern-cryptography is heavily based on mathematical-theory and computer-science-practice; cryptographic-algorithms are designed around computational-hardness-assumptions, making such-algorithms hard to break in practice by any-adversary.
it is theoretically possible to break such-a-system, but it is infeasible to do so by any-known-practical-means.
these-schemes are therefore termed computationally secure; theoretical-advances, e.g.,-improvements in integer-factorization-algorithms, and faster-computing-technology require these-solutions to be continually adapted.
there exist information-theoretically-secure-schemes that provably cannot be broken even with unlimited-computing-power—an-example is the-one-time-pad—but
information-theoretically secure schemes that provably cannot be broken even with unlimited-computing-power are more difficult to implement than the-best-theoretically-breakable-but-computationally-secure-mechanisms.
data-structures ===
a-data-structure is a-particular-way of organizing data in a-computer so that a-data-structure can be used efficiently.
different-kinds of data-structures are suited to different-kinds of applications, and some are highly specialized to specific-tasks.
for example, databases use b-tree-indexes for small-percentages of data-retrieval and compilers and databases use dynamic-hash-tables as look up tables.
data-structures provide a-means to manage large-amounts of data efficiently for uses such as large-databases and internet-indexing-services.
usually, efficient-data-structures are key to designing efficient-algorithms.
some-formal-design-methods and programming-languages emphasize data-structures, rather than algorithms, as the-key-organizing-factor in software-design.
storing and retrieving can be carried out on data stored in both-main-memory and in secondary-memory.
distributed-computation ===
distributed-computing-studies distributed systems.
a-distributed-system is a-software-system in which components located on networked-computers communicate and coordinate components located on networked-computers actions by passing messages.
the-components interact with each other in order to achieve a-common-goal.
three-significant-characteristics of distributed-systems are: concurrency of components, lack of a-global-clock, and independent-failure of components.
examples of distributed-systems vary from soa-based-systems to massively-multiplayer-online-games to  peer-to-peer applications, and blockchain-networks like bitcoin.
a-computer-program that runs in a-distributed-system is called a distributed program, and distributed-programming is the-process of writing such-programs.
there are many-alternatives for the-message-passing-mechanism, including rpc-like-connectors and message-queues.
an-important-goal and challenge of distributed-systems is location-transparency.
information-based-complexity ===
information-based-complexity (ibc) studies optimal-algorithms and computational-complexity for continuous-problems.
ibc has studied continuous-problems as path-integration, partial-differential-equations, systems of ordinary-differential-equations, nonlinear-equations, integral-equations, fixed-points, and very-high-dimensional-integration.
formal-methods ===
formal-methods are a-particular-kind of mathematics-based-techniques for the-specification, development and verification of software-and-hardware-systems.
the-use of formal-methods for software-and-hardware-design is motivated by the-expectation that, as in other-engineering-disciplines, performing appropriate-mathematical-analysis can contribute to the-reliability and robustness of a-design.
formal-methods are best described as the-application of a-fairly-broad-variety of theoretical-computer-science-fundamentals, in particular-logic-calculi, formal-languages, automata-theory, and program-semantics, but also type-systems and algebraic-data-types to problems in software-and-hardware-specification and verification.
information-theory ===
information-theory is a-branch of applied-mathematics, electrical-engineering, and computer-science involving the-quantification of information.
information-theory was developed by claude-e.-shannon to find fundamental-limits on signal-processing-operations such as compressing data and on reliably storing and communicating data.
since information-theory-inception-information-theory has broadened to find applications in many-other-areas, including statistical-inference, natural-language-processing, cryptography, neurobiology, the-evolution and function of molecular-codes, model-selection in statistics, thermal-physics, quantum-computing, linguistics, plagiarism-detection, pattern-recognition, anomaly-detection and other-forms of data-analysis.
applications of fundamental-topics of information-theory include lossless data-compression (e.g.-zip-files), lossy data-compression (e.g.-mp3s and jpegs), and channel-coding (e.g. for digital-subscriber-line (dsl)).
the-field is at the-intersection of mathematics, statistics, computer-science, physics, neurobiology, and electrical-engineering.
its-impact has been crucial to the-success of the-voyager-missions to deep-space, the-invention of the-compact-disc, the-feasibility of mobile-phones, the-development of the-internet, the-study of linguistics and of human-perception, the-understanding of black-holes, and numerous-other-fields.
important-sub-fields of information-theory are source-coding, channel-coding, algorithmic-complexity-theory, algorithmic information-theory, information-theoretic-security, and measures of information.
machine-learning ===
machine-learning is a-scientific-discipline that deals with the-construction and study of algorithms that can learn from data.
such-algorithms operate by building a-model based on inputs and using that to make predictions or decisions, rather than following only-explicitly-programmed-instructions.
machine-learning can be considered a subfield of computer-science and statistics.
machine-learning has strong-ties to artificial-intelligence and optimization, which deliver methods, theory and application domains to the-field.
machine-learning is employed in a-range of computing-tasks
where designing and programming explicit, rule-based-algorithms is infeasible.
example-applications include spam-filtering, optical-character-recognition (ocr), search-engines and computer-vision.
machine-learning is sometimes conflated with data-mining, although that focuses more on exploratory-data-analysis.
machine-learning-and-pattern-recognition "can be viewed as two-facets of the-same-field."
parallel-computation ===
parallel-computing is a-form of computation in which many-calculations are carried out simultaneously, operating on the-principle that large-problems can often be divided into smaller-ones, which are then solved "in parallel".
there are several-different-forms of parallel-computing: bit-level, instruction-level, data, and task-parallelism.
parallelism has been employed for many-years, mainly in high-performance-computing, but interest in it has grown lately due to the-physical-constraints preventing frequency-scaling.
as power-consumption (and consequently heat generation) by computers has become a-concern in recent-years, parallel-computing has become the-dominant-paradigm in computer-architecture, mainly in the-form of multi-core-processors.
parallel-computer-programs are more difficult to write than sequential-ones, because concurrency introduces several-new-classes of potential-software-bugs, of which race-conditions are the most common.
communication and synchronization between the-different-subtasks are typically some of the-greatest-obstacles to getting good-parallel-program-performance.
the-maximum-possible-speed-up of a-single-program as a-result of parallelization is known as amdahl's-law.
program-semantics ===
in programming-language-theory, semantics is the-field concerned with the-rigorous-mathematical-study of the-meaning of programming-languages.
in programming-language-theory does so by evaluating the-meaning of syntactically-legal-strings defined by a-specific-programming-language, showing the-computation involved.
in such-a-case that the-evaluation would be of syntactically-illegal-strings, the-result would be non-computation.
semantics describes the-processes a-computer follows when executing a-program in that-specific-language.
this can be shown by describing the-relationship between the-input and output of a-program, or an-explanation of how a-program will execute on a-certain-platform, hence creating a-model of computation.
quantum-computation ===
a-quantum-computer is a-computation-system that makes direct-use of quantum-mechanical-phenomena, such as superposition and entanglement, to perform operations on data.
quantum-computers are different from digital-computers based on transistors.
whereas digital-computers require data to be encoded into binary-digits (bits), each of which is always in one of two-definite-states (0 or 1), quantum-computation uses qubits (quantum-bits), which can be in superpositions of states.
a-theoretical-model is the-quantum-turing-machine, also known as quantum.
quantum-computers share theoretical-similarities with non-deterministic-and-probabilistic-computers; one-example is the-ability to be in more-than-one-state simultaneously.
the-field of quantum-computing was first introduced by yuri-manin in 1980 and richard-feynman in 1982.
a-quantum-computer with spins as quantum-bits was also formulated for use as a-quantum-space–time in 1968.as of 2014
, quantum-computing is still in quantum-computing infancy but experiments have been carried out in which quantum-computational-operations were executed on a-very-small-number of qubits.
both-practical-and-theoretical-research continues, and many-national-governments and military-funding-agencies support quantum-computing-research to develop quantum-computers for both-civilian-and-national-security-purposes, such as cryptanalysis.
symbolic-computation ===
computer-algebra, also called symbolic-computation or algebraic-computation is a-scientific-area that refers to the-study and development of algorithms and software for manipulating mathematical-expressions and other-mathematical-objects.
although, properly speaking, computer-algebra should be a-subfield of scientific-computing, computer-algebra are generally considered as distinct-fields because scientific-computing is usually based on numerical-computation with approximate-floating-point-numbers, while symbolic-computation emphasizes exact-computation with expressions containing variables that have not any-given-value and are thus manipulated as symbols (therefore-the-name of symbolic-computation).
software-applications that perform symbolic-calculations are called computer algebra systems, with the-term-system alluding to the-complexity of the-main-applications  that include, at-least,-a-method to represent mathematical-data in a-computer, a-user-programming-language (usually different from the-language used for the-implementation), a-dedicated-memory-manager, a-user-interface for the-input/output of mathematical-expressions, a-large-set of routines to perform usual-operations, like simplification of expressions, differentiation using chain-rule, polynomial-factorization, indefinite-integration, etc.
very-large-scale-integration ===
very-large-scale-integration (vlsi) is the-process of creating an-integrated-circuit (ic) by combining thousands of transistors into a-single-chip.
vlsi began in the 1970s when complex-semiconductor-and-communication-technologies were being developed.
the-microprocessor is a-vlsi-device.
before the-introduction of vlsi-technology most-ics had a-limited-set of functions most-ics could perform.
an-electronic-circuit might consist of a-cpu, rom, ram and other-glue-logic.
vlsi allows ic-makers to add all of these-circuits into one-chip.
organizations ==
european-association for theoretical-computer-science-sigact-simons-institute for the-theory of computing ==
journals and newsletters ==
“discrete mathematics and theoretical computer science” information and computation theory of computing (open-access-journal)
formal-aspects of computing
journal of the acm siam journal on computing-(sicomp)-sigact-news-theoretical-computer-science-theory of computing-systems
international-journal of foundations of computer-science-chicago-journal of theoretical-computer-science (open-access-journal)
foundations and trends in theoretical-computer-science-journal of automata, languages and combinatorics-acta-informatica
fundamenta-informaticae-acm-transactions on computation theory computational complexity journal of complexity acm transactions on algorithms
information-processing-letters open-computer-science (open-access-journal) ==
conferences ==
annual-acm-symposium on theory of computing (stoc)
annual-ieee-symposium on foundations of computer-science (focs)
innovations in theoretical-computer science (itcs)
mathematical-foundations of computer-science (mfcs)
international-computer-science-symposium in russia
(csr)-acm–siam-symposium on discrete-algorithms (soda)
ieee-symposium on logic in computer-science (lics)
computational-complexity-conference (ccc)
international-colloquium on automata, languages and programming (icalp)
annual-symposium on computational-geometry-(socg)-acm-symposium on principles of distributed-computing (podc)
acm-symposium on parallelism in algorithms and architectures (spaa)
annual-conference on learning-theory (colt)-symposium on theoretical-aspects of computer-science (stacs)-european-symposium on algorithms-(esa)-workshop on approximation-algorithms for combinatorial-optimization-problems (approx)
workshop on randomization and computation (random)
international-symposium on algorithms and computation (isaac)
international-symposium on fundamentals of computation-theory (fct)
international-workshop on graph-theoretic-concepts in computer-science (wg)
see also ==
formal-science
unsolved-problems in computer-science-list of important-publications in theoretical-computer-science
==-notes == ==
further-reading ==
martin-davis, ron-sigal, elaine-j.-weyuker, computability, complexity, and languages: fundamentals of theoretical-computer-science, 2nd-ed.,
academic-press, 1994, isbn 0-12-206382-1.
covers theory of computation, but also program semantics-and-quantification-theory.
aimed at graduate-students.
external-links ==
sigact-directory of additional-theory-links
theory-matters
wiki-theoretical-computer-science-(tcs)-advocacy-wiki-list of academic-conferences in the-area of theoretical-computer-science at confsearch
theoretical-computer-science---stackexchange, a-question-and-answer-site for researchers in theoretical-computer-science-computer
science animated
http://theory.csail.mit.edu/   @
massachusetts-institute of technology in psychology, an-attribution-bias or attributional-bias is a-cognitive-bias that refers to the-systematic-errors made when people evaluate or try to find reasons for people own and others'-behaviors.
people constantly make attributions—judgements and assumptions about why people behave in certain-ways.
however, attributions do not always accurately reflect reality.
rather than operating as objective-perceivers, people are prone to perceptual-errors that lead to biased-interpretations of people social-world.
attribution-biases are present in everyday-life.
for example, when a-driver cuts someone off, the-person who has been cut off is often more likely to attribute blame to the-reckless-driver's-inherent-personality-traits (e.g., "a-driver is rude and incompetent") rather than situational-circumstances (e.g., "
that-driver may have been late to work and was not paying attention").
additionally, there are many-different-types of attribution-biases, such as the-ultimate-attribution-error, fundamental-attribution-error, actor-observer-bias, and hostile-attribution-bias.
each of these-biases describes a-specific-tendency that people exhibit when reasoning about the-cause of different-behaviors.
since the-early-work, researchers have continued to examine how
and why people exhibit biased-interpretations of social-information.
many-different-types of attribution-biases have been identified, and more-recent-psychological-research on attribution-biases has examined how attribution-biases can subsequently affect emotions and behavior.
history == ===
early-influences ===
attribution-theory ====
research on attribution-biases is founded in attribution-theory, which was proposed to explain why and how people create meaning about others' and people own behavior.
attribution-theory ==== focuses on identifying how an-observer uses information in attribution-theory ====/her
social-environment in order to create a-causal-explanation for events.
attribution-theory also provides explanations for why different-people can interpret the-same-event in different-ways and what-factors contribute to attribution-biases.
psychologist-fritz-heider first discussed attributions in  psychologist-fritz-heider 1958-book, the-psychology of interpersonal-relations.
psychologist-fritz-heider made several-contributions that laid the-foundation for further-research on attribution-theory and attribution-biases.
psychologist-fritz-heider noted that people tend to make distinctions between behaviors that are caused by personal-disposition versus environmental-or-situational-conditions.
psychologist-fritz-heider also predicted that people are more likely to explain others'-behavior in terms of dispositional-factors (i.e., caused by a-given-person's-personality), while ignoring the-surrounding-situational-demands.
correspondent-inference-theory === =
building on heider's-early-work, other-psychologists in the-1960s and 1970s extended work on attributions by offering additional-related-theories.
in 1965, social-psychologists-edward-e.-jones and keith-davis proposed an-explanation for patterns of attribution-termed-correspondent-inference-theory.
a-correspondent-inference assumes that a-person's-behavior reflects a-stable-disposition or personality characteristic instead of a-situational-factor.
they explained that certain-conditions make us more likely to make a-correspondent-inference about someone's-behavior:
intention:
people are more likely to make a-correspondent-inference when people interpret someone's-behavior as intentional, rather than unintentional.
social desirability: people are more likely to make a-correspondent-inference when an-actor's-behavior is socially undesirable than when an-actor's-behavior is conventional.
effects of behavior: people are more likely to make a-correspondent, or dispositional, inference when someone-else's-actions yield outcomes that are rare or not yielded by other-actions.
covariation-model ====
soon after jones and davis first proposed jones and davis correspondent inference theory, harold-kelley, a-social-psychologist famous for his-work on interdependence-theory as well as attribution-theory, proposed a-covariation-model in 1973 to explain the-way people make attributions.
a-covariation-model helped to explain how people choose to attribute a-behavior to an-internal-disposition versus an-environmental-factor.
kelley used the-term-'covariation' to convey that when making attributions, people have access to information from many-observations, across different-situations, and at many-time-points; therefore, people can observe the-way a-behavior varies under these-different-conditions and draw conclusions based on that-context.
he proposed three-factors that influence the-way individuals explain behavior:
consensus:
the-extent to which other-people behave in the-same-way.
there is high-consensus when most-people behave consistent with a-given-action/actor.
low-consensus is when not-many-people behave in this-way.
consistency: the-extent to which a-person usually behaves in a-given-way.
there is high-consistency when a-person almost always behaves in a-specific-way.
low-consistency is when a-person almost never behaves like this.
distinctiveness: the-extent to which an-actor's-behavior in one-situation is different from an-actor/an-actor behavior in other-situations.
there is high-distinctiveness when an-actor does not behave this way in most-situations.
low-distinctiveness is when an-actor usually behaves in a-particular-way in most-situations.
kelley proposed that people are more likely to make dispositional-attributions when consensus is low
(most-other-people don't behave in the-same-way)
, consistency is high (a-person behaves this way across most-situations), and distinctiveness is low
(a-person's-behavior is not unique to this-situation).
alternatively, situational-attributions are more likely reached when consensus is high, consistency is low, and distinctiveness is high.
situational-attributions-research helped to reveal the-specific-mechanisms underlying the-process of making attributions.
later development ===
as early-researchers explored the-way people make causal-attributions, people also recognized that attributions do not necessarily reflect reality and can be colored by a-person's-own-perspective.
certain-conditions can prompt people to exhibit attribution-bias, or draw inaccurate-conclusions about the-cause of a-given-behavior or outcome.
in fritz-heider-work on attribution-theory, fritz-heider noted that in ambiguous-situations, people make attributions based on people own wants and needs, which are therefore often skewed.
fritz-heider also explained that this-tendency was rooted in a-need to maintain a-positive-self-concept, later termed the-self-serving-bias.
kelley's-covariation-model also led to the-acknowledgment of attribution-biases.
kelley's-covariation-model explained the-conditions under which people will make informed dispositional versus situational-attributions.
but, it assumed that people had access to such-information (i.e.,-the-consensus, consistency, and distinctiveness of a-person's-behavior).
when one doesn't have access to such-information, like when access to such-information interact with a-stranger, it will result in a-tendency to take cognitive-shortcuts, resulting in different-types of attribution-biases, such as the-actor-observer-bias.
cognitive-explanation ====
although psychologists agreed that people are prone to these-cognitive-biases, there existed disagreement concerning the-cause of such-biases.
on one-hand, supporters of a-"cognitive-model" argued that biases were a-product of human-information-processing-constraints.
one-major-proponent of this-view was yale-psychologist-michael-storms, who proposed this-cognitive-explanation following his-1973-study of social-perception.
in his-experiment, participants viewed a-conversation between two-individuals, dubbed actor one and actor two.
some-participants viewed a-conversation between two-individuals, dubbed actor one and actor-two while facing actor one, such that some-participants were unable to see the-front of actor-two, while other-participants viewed the-conversation while facing actor-two, obstructed from the-front of actor one.
following a-conversation between two-individuals, dubbed actor one and actor two, participants were asked to make attributions about the-conversationalists.
storms found that participants ascribed more-causal-influence to the-person-participants were looking at.
thus, participants made different-attributions about people depending on the-information-participants had access to.
storms used these-results to bolster his-theory of cognitively-driven-attribution-biases; because people have no-access to the-world except through people own eyes, people are inevitably constrained and consequently prone to biases.
similarly, social-psychologist-anthony-greenwald described humans as possessing a-totalitarian-ego, meaning that people view the-world through people own personal-selves.
therefore, different-people may interpret the-world differently and in turn reach different-conclusions.
motivational-explanation ====
some-researchers criticized the-view that attributional-biases are a-sole-product of information-processing-constraints, arguing that humans do not passively interpret humans-world and make attributions; rather, humans are active-and-goal-driven-beings.
building on this-criticism, research began to focus on the-role of motives in driving attribution-biases.
researchers such as ziva-kunda drew attention to the-motivated-aspects of attributions and attribution-biases.
ziva-kunda in particular argued that certain-biases only appear when people are presented with motivational-pressures; therefore, people can't be exclusively explained by an-objective-cognitive-process.
more specifically, people are more likely to construct biased-social-judgments when people are motivated to arrive at a-particular-conclusion, so long as people can justify this-conclusion.
current-theory ==
early-researchers explained attribution-biases as cognitively driven and a product of information-processing-errors.
in the-early-1980s, studies demonstrated that there may also be a-motivational-component to attribution-biases, such that attribution-biases own desires and emotions affect how one interprets social-information.
current-research continues to explore the-validity of both of these-explanations by examining the-function of specific-types of attribution-biases and specific-types of attribution-biases behavioral-correlates through a-variety of methods (e.g.,-research with children or using brain-imaging-techniques).recent-research on attribution-biases has focused on identifying specific-types of attribution-biases and attribution-biases effect on people's-behavior.
additionally, some-psychologists have taken an-applied-approach and demonstrated how these-biases can be understood in real-world-contexts (e.g., the-workplace or school).
researchers have also used the-theoretical-framework of attributions and attribution-biases in order to modify the-way people interpret social-information.
for example, studies have implemented attributional-retraining to help students have more-positive-perceptions of students own academic-abilities (see below for more-details).
mental-health ===
studies on attribution-bias and mental-health suggest that people who have mental-illnesses are more likely to hold attribution-biases.
people who have mental-illness tend to have a-lower-self-esteem, experience social-avoidance, and do not commit to improving people who have mental-illness overall-quality of life, often as a-result of lack of motivation.
people with these-problems tend to feel strongly about people with these-problems attribution-biases and will quickly make their attribution-biases known.
these-problems are called social cognition biases and are even present in those with less-severe-mental-problems.
there are many-kinds of cognitive-biases that affect people in different-ways, but all may lead to irrational-thinking, judgment, and decision-making.
aggression ===
extensive-research in both-social-and-developmental-psychology has examined the-relationship between aggressive-behavior and attribution-biases, with a-specific-focus on the-hostile-attribution-bias.
in particular, researchers have consistently found that children who exhibit a-hostile-attribution-bias (tendency to perceive others'-intent as hostile, as opposed to benign) are more likely to engage in aggressive-behaviors.
more specifically, hostile-attribution-bias has been associated with reactive-aggression, as opposed to proactive-aggression, as well as victimization.
whereas proactive-aggression is unprovoked-and-goal-driven,-reactive-aggression is an-angry,-retaliatory-response to some-sort of perceived-provocation.
therefore, children who are victims of aggression may develop views of peers as hostile,-leading-children who are victims of aggression to be more likely to engage in retaliatory-(or-reactive)-aggression.
research has also indicated that children can develop hostile-attribution-bias by engaging in aggression in the-context of a-video-game.
in a-1998-study, participants played either-a--violent-or-non-violent-video-game and were then asked to read several-hypothetical-stories where a-peer's-intent was ambiguous.
for example, participants may have read about participants peer hitting someone in the-head with a-ball, but it was unclear whether or not their-peer did this intentionally.
participants then responded to questions about participants 's intent.
the-children who played the-violent-video-game were more likely to say that the-children who played the-violent-video-game peer harmed someone on purpose than the-participants who played the-nonviolent-game.
this-finding provided evidence that exposure to violence and aggression could cause children to develop a-short-term-hostile-attribution-bias.
intergroup-relations ===
research has found that humans often exhibit attribution-biases when interpreting the-behavior of others, and specifically when explaining the-behavior of in-group versus out-group-members.
a-review of the-literature on intergroup-attribution-biases noted that people generally favor dispositional-explanations of an in-group member's positive behavior and situational-explanations for an-in-group's-negative-behavior.
alternatively, people are more likely to do the-opposite when explaining the-behavior of an-out-group-member (i.e., attribute positive-behavior to situational-factors and negative-behavior to disposition).
essentially, group-members'-attributions tend to favor the-in-group.
this-finding has implications for understanding other-social-psychological-topics, such as the-development and persistence of out-group-stereotypes.
attribution-biases in intergroup-relations are observed as early as childhood.
in particular, elementary-school-students are more likely to make dispositional-attributions when elementary-school-students friends perform positive-behaviors, but situational-attributions when disliked peers perform positive-behaviors.
similarly, children are more likely to attribute friends'-negative-behaviors to situational-factors, whereas children attribute disliked-peers'-negative-behaviors to dispositional-factors.
these-findings provide evidence that attribution-biases emerge very early on.
academic-achievement ===
although certain-attribution-biases are associated with maladaptive-behaviors, such as aggression, some-research has also indicated that certain-attribution-biases are flexible and can be altered to produce positive-outcomes.
much of this-work falls within the-domain of improving academic-achievement through attributional-retraining.
for example, one-study found that students who were taught to modify one-study attributions actually performed better on homework-assignments and lecture-materials.
the-retraining-process specifically targeted students who tended to attribute poor-academic-performance to external-factors.
it taught these-students that poor-performance was often attributable to internal-and-unstable-factors, such as effort and ability.
therefore, the-retraining helped students perceive greater-control over the-retraining own academic success by altering the-retraining attributional process.
more-recent-research has extended these-findings and examined the-value of attributional retraining for helping students adjust to an-unfamiliar-and-competitive-setting.
in one-study, first-year-college-students went through attributional-retraining following first-year-college-students first exam in a-two-semester-course.
similar to the-previous-study, they were taught to make more-controllable-attributions (e.g., "i can improve my-test-grade by studying more") and less-uncontrollable-attributions (e.g., "
no matter what i do, i'll fail").
for students who performed low or average on they-first-exam, attributional-retraining resulted in higher in-class test grades and gpa in the-second-semester.
students who performed well on the-first-exam were found to have more-positive-emotions in the-second-semester following attributional-retraining.
taken together, these-studies provide evidence for the-flexibility and modifiability of attributional-biases.
limitations of the-theory ==
there is inconsistency in the-claims made by scientists and researchers that attempt to prove or disprove attribution-theories and the-concept of attributional-biases.
the-theory was formed as a-comprehensive-explanation of the-way people interpret the-basis of behaviors in human-interactions; however, there have been studies that indicate cultural-differences in the-attribution-biases between people of eastern, collectivistic-societies and western,-individualistic-societies.
a-study done by thomas-miller shows that when dealing with conflict created by other-people, individualistic-cultures tend to blame the-individual for how people behave (dispositional attributions), whereas collectivist-cultures blame the-overall-situation on how people behave (situational-attributions).
these-same-findings were replicated in a-study done by michael-morris where an-american-group and a-chinese-group were asked an-american-group and a-chinese-group opinions about the-killings perpetrated by gang-lu at the-university of iowa.
an-american-group and a-chinese-group focused on the-killer's-own-internal-problems.
an-american-group and a-chinese-group focused more on the-social-conditions surrounding the-killing.
this reinforces the-notion that individualistic-and-collectivistic-cultures tend to focus on different-aspects of a-situation when making attributions.
additionally, some-scientists believe that attributional-biases are only exhibited in certain-contexts of interaction, where possible-outcomes or expectations make the-forming of attributions necessary.
these-criticisms of the-attribution-model reveal that the-theory may not be a-general,-universal-principle.
major-attribution-biases ==
researchers have identified many-different-specific-types of attribution-biases, all of which describe ways in which people exhibit biased-interpretations of information.
note that this is not an-exhaustive-list (see list of attributional-biases for more).
fundamental-attribution-error ===
fundamental-attribution-error === refers to a-bias in explaining others'-behaviors.
according to fundamental-attribution-error ===, when someone makes attributions about another-person's-actions, they are likely to overemphasize the-role of dispositional-factors while minimizing the-influence of situational-factors.
for example, if a-person sees a-coworker-bump into someone on a-person way to a-meeting, a-person is more likely to explain this-behavior in terms of the-coworker's-carelessness or hastiness rather than considering that a-person was running late to a-meeting.
this-term was first proposed in the-early-1970s by psychologist-lee-ross following an-experiment lee-ross conducted with edward-e.-jones and victor-harris in 1967.
in this-study, participants were instructed to read two-essays; one expressed pro-castro-views, and the other expressed anti-castro-views.
participants were then asked to report participants attitudes towards the-writers under two-separate-conditions.
when participants were informed that the-writers voluntarily chose the-writers position towards castro, participants predictably expressed more-positive-attitudes towards the anti-castro writer.
however, when participants were told that the-writers'-positions were determined by a-coin-toss rather than the-writers'-positions own free-will, participants unpredictably continued to express more-positive-attitudes towards the-anti-castro-writer.
these-results demonstrated that participants did not take situational-factors into account when evaluating a-third-party, thus providing evidence for the-fundamental-attribution-error.
actor-observer-bias ===
the-actor-observer-bias (also called actor–observer asymmetry) can be thought of as an-extension of the-fundamental-attribution-error.
according to the-actor-observer-bias, in addition to over-valuing-dispositional-explanations of others'-behaviors, people tend to under-value-dispositional-explanations and over-value-situational-explanations of people own behavior.
for example, a-student who studies may explain a-student who studies behavior by referencing situational-factors (e.g., "
i have an-exam coming up"), whereas others will explain a-student who studies studying by referencing dispositional-factors (e.g., "a-student who studies's ambitious and hard-working").
this-bias was first proposed by edward-e.-jones and richard-e.-nisbett in 1971, who explained that "actors tend to attribute the-causes of actors-behavior to stimuli inherent in the-situation, while observers tend to attribute behavior to stable-dispositions of the-actor.
there has been some-controversy over the-theoretical-foundation of the-actor-observer-bias.
in a-2006-meta-analysis of all-published-studies of the-bias since 1971, the-author found that edward-e.-jones' and nisbett's-original-explanation did not hold.
whereas jones and nisbett proposed that actors and observers explain behaviors as attributions to either-dispositions or situational-factors, examining past-studies revealed that this-assumption may be flawed.
rather, the-theoretical-reformulation posits that the-way people explain behavior depends on whether or not it is intentional, among other-things.
for more-information on this-theoretical-reformulation, see actor-observer-asymmetry, or refer to malle's-meta-analysis in #further-reading.
self-serving-bias ===
a-self-serving-bias refers to people's-tendency to attribute people successes to internal-factors but attribute people failures to external-factors.
a-self-serving-bias helps to explain why individuals tend to take credit for individuals
own-successes while often denying responsibility for failures.
for example, a-tennis-player who wins his-match might say, "i won because i'm a-good-athlete," whereas the-loser might say, "i lost because the-referee was unfair."
the-self-serving-bias has been thought of as a-means of self-esteem-maintenance.
a-person will feel better about themselves by taking credit for successes and creating external-blames for failure.
this is further reinforced by research showing that as self-threat-increases, people are more likely to exhibit a-self-serving-bias.
for example, participants who received negative-feedback on a-laboratory-task were more likely to attribute participants who received negative-feedback on a-laboratory-task task performance to external,-rather-than-internal,-factors.
the-self-serving-bias seems to function as an-ego-protection-mechanism, helping people to better cope with personal-failures.
hostile-attribution-bias ===
hostile-attribution-bias (hab) has been defined as an-interpretive-bias wherein individuals exhibit a-tendency to interpret others'-ambiguous-behaviors as hostile, rather than benign.
for example, if a child witnesses two-other-children whispering, two-other-children may assume that the-children are talking negatively about the-children.
in this-case, the-child made an-attribution of hostile-intent, even though the-other-children's-behavior was potentially benign.
research has indicated that there is an-association between hostile-attribution-bias and aggression, such that people who are more likely to interpret someone-else's-behavior as hostile are also more likely to engage in aggressive-behavior.
see the-previous-section on aggression for more-details on this-association.
list of attribution-biases == ==
see also ==
theory of mind –
ability to attribute mental-states to oneself and others attribution (psychology) –
the-process by which individuals explain the-causes of behavior and events fallacy of the-single-cause – assumption of a-single-cause where multiple-factors may be necessary-causality –
how one-process influences another-cognitive-dissonance – psychological-stress resulting from multiple-contradictory-beliefs, ideas, or values held at the-same-time just-world-hypothesis –
hypothesis that a-person's-actions are inherently inclined to bring morally-fair-and-fitting-consequences to that-person-list of cognitive-biases – systematic-patterns of deviation from norm or rationality in judgment
false-consensus-effect – attributional-type of cognitive-bias
references == ==
further-reading ==
harvey, j.h.; town, j.p.; yarkin, k.l. (1981).
" how fundamental is "the-fundamental-attribution-error"?"
journal of personality and social-psychology.
40 (2): 346–349.
doi:10.1037/0022-3514.40.2.346.
malle, b.f. (2006). "
actor-observer-asymmetry in attribution:
a-(surprising)-meta-analysis" (pdf).
psychological-bulletin.
132 (6): 895–919.
doi:10.1037/0033-2909.132.6.895.
pmid 17073526.
archived from the-original-(pdf) on may 2, 2013.
matthews, a.; norris, f.h. (2002).
" when is believing "seeing"?
hostile-attribution-bias as a-function of self-reported-aggression".
journal of applied-social-psychology.
doi:10.1111/j.1559-1816.2002.tb01418.x.
s2cid 143568167.
miller, d.t.; ross, m. (1975). "
self-serving-biases in the-attribution of causality:
fact or fiction?" (
psychological-bulletin.
:-213–225.
doi:10.1037/h0076486.
external-links ==
funnelsort is a-comparison-based-sorting-algorithm.
it is similar to mergesort, but it is a-cache-oblivious-algorithm, designed for a-setting where the-number of elements to sort is too large to fit in a-cache where operations are done.
it was introduced by matteo-frigo, charles-leiserson, harald-prokop, and sridhar-ramachandran in 1999 in the-context of the-cache-oblivious-model.
mathematical-properties ==
in the-external-memory-model, the number of memory-transfers it needs to perform a-sort of
n     {\displaystyle-n}----items on a-machine with cache of size z     {\displaystyle z}
and cache-lines of length l {\displaystyle l}    is          o
(                   n-------------------l               log                 z
⁡ n           )
{\displaystyle o\left({\tfrac {n}{l}}\log-_{z}n\right)}   , under the-tall-cache-assumption that
z-=---------ω (           l
2         )     {\displaystyle z=\omega (l^{2})}   .
this-number of memory-transfers has been shown to be asymptotically optimal for comparison-sorts.
funnelsort also achieves the-asymptotically-optimal-runtime-complexity of          θ         (         n         log
n         )     {\displaystyle \theta (n\log n)}   .
algorithm == ===
basic-overview ===
funnelsort operates on a-contiguous-array of
n-----{\displaystyle-n}----elements.
to sort the-elements, the-elements performs the following: split the-input into
3     {\displaystyle-n^{1/3}}
arrays of size
n             2               /             3
{\displaystyle n^{2/3}}   , and sort the-arrays recursively.
merge the            n             1
/             3     {\displaystyle n^{1/3} }    sorted sequences using a
n             1
3     {\displaystyle-n^{1/3}}
this-process will be described in more-detail.)funnelsort is similar to merge sort in that some-number of subarrays are recursively sorted, after which a-merging-step combines the-subarrays into one-sorted-array.
merging is performed by a-device called a k-merger, which is described in the-section below.
k-mergers ===
a-k-merger takes k     {\displaystyle k}    sorted sequences.
upon one-invocation of a-k-merger,  upon one-invocation of a-k-merger outputs the-first-k 3     {\displaystyle-k^{3}}----elements of the-sorted-sequence obtained by merging the-k-input-sequences.
at the-top-level, funnelsort uses a n             1               /             3
{\displaystyle n^{1/3}} -merger
on n             1
/             3-----{\displaystyle-n^{1/3}}----sequences of length
2               /             3     {\displaystyle n^{2/3}}   , and invokes this-merger once.
the-k-merger is built recursively out of k {\displaystyle {\sqrt {k}}}
the-k-merger consists of
k-----{\displaystyle-{\sqrt-{k}}}----input k     {\displaystyle {\sqrt {k}}}
i 1         , i
2         ,---------…-,-----------i
k     {\displaystyle i_{1},i_{2},\ldots ,i_{\sqrt {k}}}   , and a-single-output k     {\displaystyle {\sqrt {k}}}
o     {\displaystyle o}   .
the-k-inputs are separated into
k-----{\displaystyle-{\sqrt-{k}}}----sets of              k     {\displaystyle {\sqrt {k}}}
inputs each.
each of these-sets is an-input to one of the-input-mergers.
the-output of each-input-merger is connected to a-buffer, a-fifo-queue that can hold          2-k
3               / 2     {\displaystyle-2k^{3/2}}----elements.
the-buffers are implemented as circular-queues.
the-outputs of the-k     {\displaystyle {\sqrt-{k}}}----buffers are connected to the-inputs of the-output-merger
o     {\displaystyle o}   .
finally, the output of          o
{\displaystyle o}    is the-output of the-entire-k-merger.
in this-construction, any-input-merger only outputs k             3
/             2-----{\displaystyle-k^{3/2}}----items at once, but the-buffer it outputs to has double-the-space.
this is done so that an-input-merger can be called only when this-buffer does not have enough-items, but that when this is called, this outputs a-lot of items at once (namely,
k             3
/             2     {\displaystyle-k^{3/2}}
a k-merger works recursively in the-following-way.
to output            k             3
{\displaystyle-k^{3}}----elements,      {\displaystyle-k^{3}}
elements recursively invokes      {\displaystyle-k^{3}}----elements output merger-k 3               /
{\displaystyle-k^{3/2}}----times.
however, before it makes a-call to o
{\displaystyle o}   , it checks all of it buffers, filling each of all of its-buffers that are less than half full.
to fill the-i-th-buffer, it recursively invokes the-corresponding-input-merger
{\displaystyle-i_{i}}    once.
if this cannot be done (due to the-merger running out of inputs), this-step is skipped.
since this-call outputs k
3               /             2     {\displaystyle-k^{3/2}}----elements
, the-buffer contains at least
k             3
/             2     {\displaystyle-k^{3/2}}
at the-end of all-these-operations, the-k-merger has output the first k             3     {\displaystyle-k^{3}}
of its-input-elements, in sorted-order.
analysis ==
most of the-analysis of this-algorithm revolves around analyzing the-space-and-cache-miss-complexity of the-k-merger.
the first important bound is that a-k-merger can be fit in
o (-k             2         )
{\displaystyle-o(k^{2})}----space.
to see this, we let          s (
k---------)-----{\displaystyle-s(k)
}    denote the-space needed for a-k-merger.
to fit the k             1               /             2
{\displaystyle-k^{1/2}}----buffers of size          2-k 3
/ 2     {\displaystyle 2k^{3/2 }}    takes
o (-k             2         )
{\displaystyle-o(k^{2})}----space.
to fit the--------------k +
{\displaystyle {\sqrt {k}}+1}    smaller-buffers takes (
k-+---------1---------)-s
(             k ) {\displaystyle ({\sqrt {k}}+1)s({\sqrt {k}})}    space.
thus, the-space satisfies the-recurrence
s (         k         )
(-------------k +         1
) s (-------------k         )
+         o (           k             2
{\displaystyle s(k)=({\sqrt {k}}+1)s({\sqrt {k}})+o(k^{2})}   .
this-recurrence has solution-s
(         k         )         = o
(           k             2         )     {\displaystyle s(k)=o(k^{2})}
it follows that there is a positive constant          α     {\displaystyle \alpha }    such that a-problem of size at most
α z     {\displaystyle \alpha {\sqrt {z}}}    fits entirely in cache, meaning that it incurs no-additional-cache-misses.
q             m (---------k )
{\displaystyle q_{m}(k)}    denote the-number of cache misses incurred by a-call to a-k-merger, one can show that            q             m (
k         )
(-k-------------3-log             z
k         )
/---------l
)         .
\displaystyle q_{m}(k)=o((k^{3}\log _{z}k)/l).}
this is done by an-induction-argument.
k         ≤         α             z     {\displaystyle-k\leq \alpha {\sqrt {z}}}
as a-base-case.
for larger-k, we can bound the-number of times a--------------k     {\displaystyle {\sqrt {k}}}
-merger is called.
the-output-merger is called exactly            k             3               /
{\displaystyle-k^{3/2}}----times.
the-total-number of calls on input-mergers is at-most-k
3               /             2 +         2
k     {\displaystyle-k^{3/2}+2{\sqrt {k}}}   .
this gives a total bound of          2
k             3
/             2 +
2 k {\displaystyle-2k^{3/2}+2{\sqrt {k}}}    recursive calls.
in addition, the-algorithm checks every-buffer to see if needs to be filled.
this is done on              k     { \displaystyle {\sqrt {k}}}    buffers every step for
k             3
/             2     {\displaystyle-k^{3/2}}
steps, leading to a-max of
k             2     {\displaystyle-k^{2}}    cache misses for all-the-checks.
this leads to the-recurrence            q             m (---------k
3               /             2 +         2
k         )           q-------------m (
k         )
+           k 2
{ \displaystyle q_{m}(k)\leq (2k^{3/2}+2{\sqrt {k}})q_{m}({\sqrt {k}})+k^{2}}   , which can be shown to have the-solution given above.
finally, the-total-cache misses          q
(         n )     {\displaystyle-q(n)} for the-entire-sort can be analyzed.
it satisfies the-recurrence          q (         n         )
1               /             3
n             2               /
3         )
+           q             m
(-----------n
1               /             3
\displaystyle-q(n)=n^{1/3}q(n^{2/3})+q_{m}(n^{1/3}).
this can be shown to have solution          q
(         n         )         = o
(---------(---------n-----------/---------l
)-----------log             z-⁡-n
)         .
\displaystyle-q(n)=o((n/l)\log-_{z}n).}
lazy-funnelsort ==
lazy-funnelsort is a-modification of the-funnelsort, introduced by gerth-stølting-brodal and rolf-fagerberg in 2002.
a-modification of the-funnelsort, introduced by gerth-stølting-brodal and rolf-fagerberg in 2002 is that when a-merger is invoked, a-merger does not have to fill each of a-merger buffers.
instead, a-merger lazily fills a-buffer only when a-merger is empty.
a-modification of the-funnelsort, introduced by gerth-stølting-brodal and rolf-fagerberg in 2002 has the-same-asymptotic-runtime-and-memory-transfers as the-original-funnelsort, but has applications in cache-oblivious-algorithms for problems in computational-geometry in a-method known as distribution sweeping.
see also ==
cache-oblivious-algorithm
cache-oblivious-distribution sort external-sorting =
=-references ==
a-list of 'effects' that have been noticed in the-field of psychology.
see also ==
list of cognitive-biases
false-memory uncanny-valley
in computing, external-memory-algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a-computer's-main-memory at once.
such-algorithms must be optimized to efficiently fetch and access data stored in slow-bulk-memory (auxiliary-memory) such as hard-drives or tape-drives, or when memory is on a-computer-network.
external-memory-algorithms are analyzed in the-external-memory-model.
==-model ==
external-memory-algorithms are analyzed in an-idealized-model of computation called the external memory model (or i/o-model, or disk-access-model).
the-external-memory-model is an-abstract-machine similar to the-ram-machine-model, but with a-cache in addition to main-memory.
the-external-memory-model captures the-fact that read and write operations are much faster in a-cache than in main-memory, and that reading long-contiguous-blocks is faster than reading randomly using a-disk-read-and-write-head.
the-running-time of an-algorithm in the-external-memory-model is defined by the-number of reads and writes to memory required.
the-external-memory-model was introduced by alok-aggarwal and jeffrey-vitter in 1988.
the-external-memory-model is related to  the-external-memory-model, but algorithms in  the-external-memory-model may know both-the-block-size and the-cache-size.
for this-reason, the-model is sometimes referred to as the-cache-aware-model.
the-model consists of a-processor with an-internal-memory or cache of size-m, connected to an-unbounded-external-memory.
both-the-internal-and-external-memory are divided into blocks of size
b.-one-input/output-or-memory-transfer-operation consists of moving a-block of b-contiguous-elements from external to internal-memory, and the-running-time of an-algorithm is determined by the-number of these-input/output-operations.
algorithms ==
algorithms in the-external-memory-model take advantage of the-fact that retrieving one-object from external-memory retrieves an-entire-block of size          b     {\displaystyle b}   .
this-property is sometimes referred to as locality.
searching for an-element among          n     {\displaystyle n}    objects is possible in the-external-memory-model using a-b-tree with branching-factor
b     {\displaystyle b}   .
using a-b-tree, searching, insertion, and deletion can be achieved in          o
(-----------log-------------b-⁡-n
) {\displaystyle o(\log _{b}n)} time (in big-o-notation).
information theoretically, this is the-minimum-running-time possible for these-operations, so using a-b-tree is asymptotically optimal.
external-sorting is sorting in an external memory setting.
external-sorting can be done via distribution-sort, which is similar to quicksort, or via a----------------m---------------b     {\displaystyle {\tfrac {m}{b}}}
-way merge sort.
both-variants achieve the-asymptotically-optimal-runtime of          o (---------------n
b           log                 m                 b
n-b         )     {-\displaystyle-o({\tfrac-{n}{b}}\log-_{\tfrac {m}{b}}{\tfrac {n}{b}})}
to-sort-n-objects.
this bound also applies to the-fast-fourier transform in the-external-memory-model.
the-permutation-problem is to rearrange          n     {\displaystyle n}
elements into a-specific-permutation.
this can either be done either by sorting, which requires the above sorting runtime, or inserting each-element in order and ignoring the-benefit of locality.
thus, permutation can be done in          o (
min (         n         ,
b           log                 m                 b
n               b         ) )
{\displaystyle o(\min(n,{\tfrac {n}{b}}\log-_{\tfrac {m}{b}}{\tfrac {n}{b}}))}
applications ==
the-external-memory-model captures the-memory-hierarchy, which is not modeled in other-common-models used in analyzing data-structures, such as the-random-access-machine, and is useful for proving lower-bounds for data-structures.
the-external-memory-model is also useful for analyzing algorithms that work on datasets too big to fit in internal-memory.
a-typical-example is geographic-information-systems, especially-digital-elevation-models, where the-full-data set easily exceeds several-gigabytes or even-terabytes of data.
this-methodology extends beyond general-purpose-cpus and also includes gpu-computing as well as classical-digital-signal-processing.
in general-purpose-computing on graphics-processing-units (gpgpu), powerful graphics cards (gpus) with little-memory (compared with the-more-familiar-system-memory, which is most often referred to simply as ram) are utilized with relatively slow cpu-to-gpu memory transfer (when compared with computation-bandwidth).
history ==
an-early-use of the-term "out-of-core" as an-adjective is in 1962 in reference to devices that are other than the-core-memory of an ibm 360.
an-early-use of the-term "out-of-core" with respect to algorithms appears in 1971.
see also ==
external-sorting-online-algorithm
streaming-algorithm
cache-oblivious-algorithm
parallel external-memory external memory graph traversal ==
references == ==
external-links ==
out of core-svd and qr out of core-graphics
scalapack-design
in computing, a-cache-(-(listen)-kash, or  kaysh in australian-english) is a-hardware-or-software-component that stores data
so that future-requests for that-data can be served faster; the-data stored in a-cache might be the-result of an-earlier-computation or a-copy of data stored elsewhere.
a-cache-hit occurs when the-requested-data can be found in a-cache, while a-cache miss occurs when it cannot.
cache-hits are served by reading data from the-cache, which is faster than recomputing a-result or reading from a-slower-data-store; thus, the-more-requests that can be served from the-cache, the faster the-system performs.
to be cost-effective and to enable efficient-use of data, caches must be relatively small.
nevertheless, caches have proven caches in many-areas of computing, because typical-computer-applications access data with a-high-degree of locality of reference.
such-access-patterns exhibit temporal-locality, where data is requested that has been recently requested already, and spatial locality, where data is requested that is stored physically close to data that has already been requested.
==-motivation ==
there is an-inherent-trade-off between size and speed (given that a-larger-resource implies greater-physical-distances) but also a tradeoff between expensive,-premium-technologies (such as sram) vs cheaper,-easily-mass-produced-commodities (such as dram or hard-disks).
the-buffering provided by a cache benefits both-latency and throughput (bandwidth): ===
latency ===
a-larger-resource incurs a-significant-latency for access – e.g. a-larger-resource can take hundreds of clock-cycles for a-modern-4-ghz-processor to reach dram.
this is mitigated by reading in large-chunks, in the-hope that subsequent-reads will be from nearby-locations.
prediction or explicit-prefetching might also guess where future reads will come from and make requests ahead of time; if done correctly the-latency is bypassed altogether.
throughput ===
the-use of a-cache also allows for higher-throughput from the-underlying-resource, by assembling multiple-fine-grain-transfers into larger,-more-efficient-requests.
in the-case of dram-circuits, this might be served by having a-wider-data-bus.
for example, consider a-program accessing bytes in a-32-bit-address-space, but being served by a 128-bit off-chip data bus; individual-uncached-byte-accesses would allow only 1/16th of the-total-bandwidth to be used, and 80% of the-data-movement would be memory-addresses instead of data itself.
reading larger-chunks reduces the-fraction of bandwidth required for transmitting address-information.
operation ==
hardware implements cache as a-block of memory for temporary-storage of data likely to be used again.
central-processing-units (cpus) and hard-disk-drives (hdds) frequently use a-cache, as do web-browsers and web-servers.
a-cache is made up of a-pool of entries.
each-entry has associated-data, which is a-copy of the-same-data in some-backing-store.
each-entry also has a-tag, which specifies the-identity of the-data in the-backing-store of which each-entry is a-copy.
tagging allows simultaneous-cache-oriented-algorithms to function in multilayered-fashion without differential-relay-interference.
when the-cache-client (a-cpu,-web-browser,-operating-system) needs to access data presumed to exist in the-backing-store, the-cache-client (a-cpu,-web-browser,-operating-system)
first checks the-cache.
if an-entry can be found with a-tag-matching that of the-desired-data, the-data in the-entry is used instead.
this-situation is known as a cache hit.
for example, a-web-browser-program might check a-web-browser-program local cache on disk to see if a-web-browser-program has a-local-copy of the-contents of a-web-page at a-particular-url.
in this-example, the-url is the-tag, and the-content of the-web-page is the-data.
the-percentage of accesses that result in cache-hits is known as the-hit-rate or hit ratio of the-cache.
the-alternative-situation, when the-cache is checked and found not to contain any-entry with the-desired-tag, is known as a-cache-miss.
this requires a-more-expensive-access of data from the-backing-store.
once the-data is retrieved, the-data is typically copied into the-cache, ready for the-next-access.
during a-cache-miss, some-other-previously-existing-cache-entry is removed in order to make room for the-newly-retrieved-data.
the-heuristic used to select the-entry to replace is known as the-replacement-policy.
one-popular-replacement-policy, "least recently used" (lru), replaces the-oldest-entry, the-entry that was accessed less recently than any-other-entry (see cache-algorithm).
more-efficient-caching-algorithms compute the-use-hit-frequency against the-size of the-stored-contents, as well as the-latencies and throughputs for both-the-cache and the-backing-store.
this works well for larger-amounts of data, longer-latencies, and slower-throughputs, such as that experienced with hard-drives and networks, but is not efficient for use within a-cpu-cache.
writing-policies ===
when a-system writes data to cache, a-system must at some-point write that-data to the-backing-store as well.
the-timing of this-write is controlled by what is known as the-write-policy.
there are two-basic-writing-approaches:
write-through:
write is done synchronously both to the-cache and to the-backing-store.
write-back (also called write-behind): initially, writing is done only to the-cache.
the-write to the-backing-store is postponed until the-modified-content is about to be replaced by another-cache-block.
a-write-back-cache is more complex to implement, since a-write-back-cache needs to track which of a-write-back-cache locations have been written over, and mark them as dirty for later writing to the-backing-store.
the-data in these-locations are written back to the-backing-store only when the-backing-store are evicted from the-cache, an effect referred to as a-lazy-write.
for this-reason, a-read miss in a-write-back-cache (which requires a-block to be replaced by another) will often require two-memory-accesses to service:
one to write the-replaced-data from the-cache back to the-backing-store, and then one to retrieve the-needed-data.
other-policies may also trigger data-write-back.
the-client may make many-changes to data in the-cache, and then explicitly notify the-cache to write back the-needed-data.
since no-data is returned to the-requester on write-operations, a-decision needs to be made on write-misses, whether or not data would be loaded into the-cache.
this is defined by these-two-approaches: write allocate (also called fetch on write): data at the-missed-write-location is loaded to cache, followed by a-write-hit-operation.
in this-approach, write-misses are similar to read misses.
no-write-allocate (also called write-no-allocate or write around):
data at the-missed-write-location is not loaded to cache, and is written directly to the-backing-store.
in this-approach, data is loaded into the-cache on read misses only.
both-write-through-and-write-back-policies can use either of these-write-miss-policies, but usually both-write-through-and-write-back-policies are paired in this-way: a-write-back-cache uses write allocate, hoping for subsequent-writes (or even reads) to the-same-location, which is now cached.
a-write-through-cache uses no-write-allocate.
here, subsequent-writes have no-advantage, since they still need to be written directly to the-backing-store.
entities other than the-cache may change the-data in the-backing-store, in which-case the-copy in the-cache may become out-of-date or stale.
alternatively, when the-client updates the-data in the-cache, copies of those-data in other-caches will become stale.
communication-protocols between the-cache-managers which keep the-data consistent are known as coherency-protocols.
examples of hardware-caches == ===
cpu cache ===
small-memories on or close to the-cpu can operate faster than the-much-larger-main-memory.
most-cpus since the 1980s have used one-or-more-caches, sometimes in cascaded-levels; modern high-end embedded, desktop and server microprocessors may have as-many-as-six-types of cache (between levels and functions).
examples of caches with a-specific-function are the-d-cache and i-cache and the-translation-lookaside-buffer for the-mmu.
gpu cache ===
earlier-graphics-processing-units (gpus) often had limited-read-only-texture-caches, and introduced morton order swizzled textures to improve 2d-cache-coherency.
cache misses would drastically affect performance, e.g. if mipmapping was not used.
caching was important to leverage 32-bit-(and-wider)-transfers for texture-data that was often as-little-as-4-bits per pixel, indexed in complex-patterns by arbitrary-uv-coordinates and perspective-transformations in inverse-texture-mapping.
as gpus advanced (especially with gpgpu-compute-shaders) gpgpu-compute-shaders have developed progressively-larger-and-increasingly-general-caches, including instruction-caches for shaders, exhibiting increasingly-common-functionality with cpu-caches.
for example, gt200-architecture-gpus did not feature an-l2-cache, while the-fermi-gpu has 768-kb of last-level-cache, the-fermi-gpu has 1536-kb of last-level-cache, and the-fermi-gpu has 2048-kb of last-level-cache.
these-caches have grown to handle synchronisation-primitives between threads and atomic-operations, and interface with a-cpu-style-mmu.
digital-signal-processors have similarly generalised over the-years.
earlier-designs used scratchpad-memory fed by dma, but modern-dsps such as qualcomm-hexagon often include a-very-similar-set of caches to a-cpu-(e.g.-modified-harvard-architecture with shared-l2, split l1-i-cache-and-d-cache).
translation-lookaside-buffer ===
a-memory-management-unit (mmu) that fetches page-table-entries from main-memory has a-specialized-cache, used for recording the-results of virtual-address to physical-address-translations.
a-specialized-cache, used for recording the-results of virtual-address to physical-address-translations is called a translation lookaside buffer (tlb).
in-network-cache ==
information-centric-networking ===
information-centric-networking (icn) is an-approach to evolve the-internet-infrastructure away from a-host-centric-paradigm, based on perpetual-connectivity and the end-to-end principle, to a-network-architecture in which the-focal-point is identified information (or content or data).
due to the-inherent-caching-capability of the-nodes in an-icn, it can be viewed as a-loosely-connected-network of caches, which has unique-requirements of caching-policies.
however, ubiquitous-content-caching introduces the-challenge to content protection against unauthorized-access, which requires extra-care and solutions.
unlike proxy-servers, in icn the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose a-different-kind of requirements on the-content-eviction-policies.
in particular, eviction-policies for icn should be fast and lightweight.
various-cache-replication and eviction-schemes for different-icn-architectures and applications have been proposed.
policies ==== ==== =
time aware-least-recently-used-(tlru) =====
the-time aware least recently used (tlru) is a-variant of lru designed for the-situation where the-stored-contents in cache have a-valid-life-time.
the-algorithm is suitable in network-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
tlru introduces a-new-term: ttu (time to use).
ttu is a-time-stamp of a-content/page which stipulates the-usability-time for the-content based on the-locality of the-content and the-content publisher announcement.
owing to this-locality based time-stamp, ttu provides more-control to the-local-administrator to regulate in network-storage.
in  the-algorithm, when a-piece of content arrives, a-cache-node calculates the-local-ttu-value based on the-ttu-value assigned by the-content-publisher.
the-local-ttu-value is calculated by using a-locally-defined-function.
once the-local-ttu-value is calculated the-replacement of content is performed on a-subset of the-total-content stored in cache-node.
the-tlru ensures that less-popular-and-small-life-content should be replaced with the-incoming-content.
least-frequent-recently-used-(lfru) =====
the-least-frequent-recently-used-(lfru)-cache-replacement-scheme combines the-benefits of lfu-and-lru-schemes.
lfru is suitable for 'in network'-cache-applications, such as information-centric-networking (icn), content-delivery-networks (cdns) and distributed-networks in general.
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
the-privileged-partition can be defined as a-protected-partition.
if content is highly popular, content is highly popular is pushed into the-privileged-partition.
replacement of the-privileged-partition is done as follows:  lfru evicts content from the-unprivileged-partition, pushes content from privileged-partition to unprivileged-partition, and finally inserts new-content into the-privileged-partition.
in the-above-procedure the-lru is used for the-privileged-partition and an-approximated-lfu-(alfu)-scheme is used for the-unprivileged-partition, hence-the-abbreviation-lfru.
the-basic-idea is to filter out the-locally-popular-contents with alfu-scheme and push the-popular-contents to one of the-privileged-partition.
software-caches ==
=== disk cache ===
while cpu-caches are generally managed entirely by hardware, a-variety of software manages other-caches.
the-page-cache in main-memory, which is an-example of disk-cache, is managed by the-operating-system-kernel.
while the-disk-buffer, which is an-integrated-part of the-hard-disk-drive, is sometimes misleadingly referred to as "disk-cache", its-main-functions are write sequencing and read prefetching.
repeated-cache-hits are relatively rare, due to the-small-size of the-buffer in comparison to the-drive's-capacity.
however, high-end-disk-controllers often have high-end-disk-controllers own on-board cache of the-hard-disk-drive's-data-blocks.
finally, a-fast-local-hard-disk-drive can also cache information held on even-slower-data-storage-devices, such as remote-servers (web-cache) or local-tape-drives or optical-jukeboxes; such-a-scheme is the-main-concept of hierarchical-storage-management.
also, fast-flash-based-solid-state-drives (ssds) can be used as caches for slower-rotational-media-hard-disk-drives, working together as hybrid-drives or solid-state hybrid-drives (sshds).
web cache ===
web-browsers and web-proxy-servers employ web-caches to store previous-responses from web-servers, such as web-pages and images.
web-caches reduce the-amount of information that needs to be transmitted across the-network, as information previously stored in the-cache can often be re-used.
this reduces bandwidth and processing-requirements of the-web-server, and helps to improve responsiveness for users of the-web.
web-browsers employ a-built-in-web-cache, but some-internet-service-providers (isps) or organizations also use a-caching-proxy-server, which is a-web-cache that is shared among all-users of that-network.
another-form of cache is p2p-caching, where the-files most sought for by peer-to-peer applications are stored in an-isp-cache to accelerate p2p-transfers.
similarly, decentralised-equivalents exist, which allow communities to perform the-same-task for p2p-traffic, for example, corelli.
memoization ===
a-cache can store data that is computed on demand rather than retrieved from a-backing-store.
memoization is an-optimization-technique that stores the-results of resource-consuming-function calls within a-lookup-table, allowing subsequent-calls to reuse the-stored-results and avoid repeated-computation.
memoization is related to the-dynamic-programming-algorithm-design-methodology, which can also be thought of as a-means of caching.
other-caches ===
the-bind-dns-daemon caches a-mapping of domain-names to ip-addresses, as does a-resolver-library.
write-through-operation is common when operating over unreliable-networks (like an-ethernet-lan), because of the-enormous-complexity of the-coherency-protocol required between multiple-write-back-caches when communication is unreliable.
for instance, web-page-caches and client-side-network-file-system-caches (like those in nfs or smb) are typically read-only or write-through specifically to keep the-network protocol simple and reliable.
search-engines also frequently make web-pages search-engines have indexed available from search-engines cache.
for example, google provides a-"cached"-link next to each-search-result.
this can prove useful when web-pages from a-web-server are temporarily or permanently inaccessible.
another-type of caching is storing computed-results that will likely be needed again, or memoization.
for example, ccache is a-program that caches the-output of the-compilation, in order to speed up later-compilation-runs.
database-caching can substantially improve the-throughput of database-applications, for example in the-processing of indexes, data-dictionaries, and frequently used subsets of data.
a-distributed-cache uses networked-hosts to provide scalability, reliability and performance to the-application.
the-hosts can be co-located or spread over different-geographical-regions.
==-buffer vs. cache ==
the-semantics of a-"buffer" and a-"cache" are not totally different; even so, there are fundamental-differences in intent between the-process of caching and the-process of buffering.
fundamentally, caching realizes a-performance-increase for transfers of data that is being repeatedly transferred.
while a-caching-system may realize a-performance-increase upon the-initial-(typically-write)-transfer of a-data-item, this-performance-increase is due to buffering occurring within the-caching-system.
with read-caches, a-data-item must have been fetched from a-data-item residing location at least once in order for subsequent-reads of the-data-item to realize a-performance-increase by virtue of being able to be fetched from the-cache's-(faster)-intermediate-storage rather than the-data's-residing-location.
with write-caches, a-performance-increase of writing a-data-item may be realized upon the-first-write of the-data-item by virtue of the-data-item immediately being stored in the-cache's-intermediate-storage, deferring the-transfer of the-data-item to the-data-item residing-storage at a-later-stage or else occurring as a-background-process.
contrary to strict-buffering, a-caching-process must adhere to a-(potentially-distributed)-cache-coherency-protocol in order to maintain consistency between the-cache's-intermediate-storage and the-location where the-data resides.
buffering, on the-other-hand, reduces the-number of transfers for otherwise-novel-data amongst communicating processes, which amortizes overhead involved for several-small-transfers over fewer,-larger-transfers, provides an-intermediary for communicating processes which are incapable of direct-transfers amongst each other, or
ensures a-minimum-data-size or representation required by at least one of the-communicating-processes involved in a-transfer.
with typical-caching-implementations, a-data-item that is read or written for the-first-time is effectively being buffered; and in the-case of a-write, mostly realizing a-performance-increase for the-application from where the-write originated.
additionally, the-portion of a-caching-protocol where individual writes are deferred to a-batch of writes is a-form of buffering.
the-portion of a-caching-protocol where-individual-reads are deferred to a-batch of reads is also a-form of buffering, although this-form may negatively impact the-performance of at-least-the-initial-reads (even though it may positively impact the-performance of the-sum of the-individual reads).
in practice, caching almost always involves some-form of buffering, while strict-buffering does not involve caching.
a-buffer is a-temporary-memory-location that is traditionally used because cpu-instructions cannot directly address data stored in peripheral-devices.
thus, addressable-memory is used as an-intermediate-stage.
additionally, such-a-buffer may be feasible when a-large-block of data is assembled or disassembled (as required by a-storage-device), or when data may be delivered in a-different-order than that in which it is produced.
also, a-whole-buffer of data is usually transferred sequentially (for example to hard-disk), so buffering itself sometimes increases transfer-performance or reduces the-variation or jitter of the-transfer's-latency as opposed to caching where the-intent is to reduce the-latency.
these-benefits are present even if the-buffered-data are written to the-buffer once and read from the-buffer once.
a-cache also increases transfer-performance.
a-part of the-increase similarly comes from the-possibility that multiple-small-transfers will combine into one-large-block.
but the-main-performance-gain occurs because there is a-good-chance that the-same-data will be read from cache-multiple-times, or that written-data will soon be read.
a-cache's-sole-purpose is to reduce accesses to the-underlying-slower-storage.
cache is also usually an-abstraction-layer that is designed to be invisible from the-perspective of neighboring-layers.
see also == ==
references == ==
further-reading ==
"what every-programmer should know about memory" by ulrich-drepper
"caching in the-distributed-environment"
in computer-science, an-object can be a-variable, a-data-structure, a-function, or a-method, and as such, is a-value in memory referenced by an-identifier.
in the-object-oriented-programming-paradigm, object can be a-combination of variables, functions, and data-structures; in particular in class-based-variations of the-paradigm it refers to a-particular-instance of a-class.
in the-relational-model of database-management, an-object can be a-table or column, or an-association between data and a-database-entity (such as relating a-person's-age to a-specific-person).
==-object-based-languages ==
an-important-distinction in programming-languages is the-difference between an-object-oriented-language and an-object-based-language.
a-language is usually considered object-based if a-language includes the-basic-capabilities for an-object: identity, properties, and attributes.
a-language is considered object-oriented if a-language is object-based and also has the-capability of polymorphism, inheritance, encapsulation, and, possibly, composition.
polymorphism refers to the-ability to overload the-name of a-function with multiple-behaviors based on which object(s) are passed to it.
conventional message passing discriminates only on the-first-object and considers that to be "sending a-message" to the-first-object.
however, some-oop-languages such as flavors and the-common-lisp-object-system (clos) enable discriminating on more than the-first-parameter of the-function.
inheritance is the-ability to subclass an-object-class, to create a-new-class that is a-subclass of an-existing-one and inherits all-the-data-constraints and behaviors of inheritance parents but also adds new and/or changes one or more of its-parents.
== object-oriented-programming == object-oriented-programming is an-approach to designing modular-reusable-software-systems.
the-object-oriented-approach is an-evolution of good-design-practices that go back to the-very-beginning of computer-programming.
object-orientation is simply the-logical-extension of older-techniques such as structured-programming and abstract-data-types.
an-object is an-abstract-data-type with the-addition of polymorphism and inheritance.
rather than structure-programs as code and data, an-object-oriented-system integrates the two using the-concept of an-"object".
an-object has state-(data) and behavior (code).
objects can correspond to things found in the-real-world.
so for example, a-graphics-program will have objects such as circle,-square,-menu.
an-online-shopping-system will have objects such as shopping-cart, customer,-product.
an-online-shopping-system will support behaviors such as place-order, make payment, and offer discount.
the-objects are designed as class-hierarchies.
so for example with the-shopping-system there might be high-level-classes such as electronics-product, kitchen-product, and book.
there may be further-refinements for example under electronic-products: cd-player, dvd-player, etc.
high-level-classes such as electronics-product, kitchen-product, and book correspond to sets and subsets in mathematical-logic.
specialized-objects ==
an-important-concept for objects is the-design-pattern.
a-design-pattern provides a-reusable-template to address a-common-problem.
the-following-object-descriptions are examples of some of the-most-common-design-patterns for objects.
function-object: an-object with a-single-method (
in c++, this-method would be the-function-operator, "operator()")
that acts much like a-function (like a-c/c++-pointer to a-function).
immutable-object: an-object set up with a-fixed-state at creation-time and which does not change afterward.
first-class-object: an-object that can be used without restriction.
container-object: an-object that can contain other-objects.
factory-object: an-object whose-purpose is to create other-objects.
metaobject: an-object from which other-objects can be created (compare with a-class, which is not necessarily an-object).
prototype-object: a-specialized-metaobject from which other-objects can be created by copying god-object: an-object that knows or does too much (it is an-example of an-anti-pattern).
singleton-object: an-object that is the-only-instance of its-class during the-lifetime of the-program.
filter-object: an-object that receives a-stream of data as its-input and transforms its into the-object's-output.
often the-input-and-output-streams are streams of characters, but these also may be streams of arbitrary-objects.
these are generally used in wrappers since these conceal the-existing-implementation with the-abstraction required at the-developer-side.
distributed-objects ==
the-object-oriented-approach is not just-a-programming-model.
it can be used equally well as an-interface-definition-language for distributed-systems.
the-objects in a-distributed-computing-model tend to be larger grained, longer lasting, and more service-oriented than programming-objects.
a-standard-method to package distributed-objects is via an-interface-definition-language (idl).
an-idl shields the-client of all of the-details of the-distributed-server-object.
details such as which-computer the-distributed-server-object resides on, what-programming-language it uses, what-operating-system, and other-platform-specific-issues.
the-idl is also usually part of a-distributed-environment that provides services such as transactions and persistence to all-objects in a-uniform-manner.
two of the-most-popular-standards for distributed-objects are the-object-management-group's-corba-standard and microsoft's-dcom.in-addition to distributed-objects, a-number of other-extensions to the-basic-concept of an-object have been proposed to enable distributed-computing: protocol-objects are components of a-protocol-stack that enclose network-communication within an-object-oriented interface.
replicated-objects are groups of distributed-objects (called replicas) that run a-distributed-multi-party-protocol to achieve high-consistency between replicated-objects internal states, and that respond to requests in a-coordinated-way.
examples include fault-tolerant-corba-objects.
live-distributed-objects (or simply-live-objects) generalize the-replicated-object-concept to groups of replicas that might internally use any-distributed-protocol, perhaps resulting in only-a-weak-consistency between groups of replicas that might internally use any-distributed-protocol local states.
some of these-extensions, such as distributed-objects and protocol-objects, are domain-specific-terms for special-types of "ordinary"-objects used in a-certain-context (such as remote-method-invocation or protocol-composition).
others, such as replicated-objects and live-distributed-objects, are more non-standard, in that others, such as replicated-objects and live-distributed-objects abandon the-usual-case that an-object resides in a-single-location at a-time, and apply the-concept to groups of entities (replicas) that might span across multiple-locations, might have only-weakly-consistent-state, and whose-membership might dynamically change.
the-semantic-web ==
the-semantic-web is essentially a-distributed-objects-framework.
two-key-technologies in the-semantic-web are rdf.
rdf provides the-capability to define basic-objects—names, properties, attributes, relations—that are accessible via the-internet.
owl adds a-richer-object-model, based on set-theory, that provides additional-modeling-capabilities such as multiple-inheritance.
owl-objects are not like standard-large-grained-distributed-objects accessed via an-interface-definition-language.
such-an-approach would not be appropriate for the-internet because the-internet is constantly evolving and standardization on one-set of interfaces is difficult to achieve.
owl-objects tend to be similar to the-kinds of objects used to define application-domain-models in programming-languages such as java and c++.
however, there are important-distinctions between owl-objects and traditional-object-oriented-programming-objects.
traditional-objects get compiled into static-hierarchies usually with single-inheritance, but owl-objects are dynamic.
an-owl-object can change an-owl-object structure at run-time and can become an-instance of new-or-different-classes.
another-critical-difference is the-way the-model treats information that is currently not in the-system.
programming-objects and most-database-systems use the-"closed-world-assumption".
if a-fact is not known to the-system that fact is assumed to be false.
semantic-web-objects use the-open-world-assumption, a-statement is only considered false if there is actual-relevant-information that a-statement is false, otherwise a-statement is assumed to be unknown, neither true nor false.
owl-objects are actually most like objects in artificial-intelligence-frame-languages such as kl-one and loom.
the-following-table contrasts traditional-objects from object-oriented-programming-languages such as java or c++ with semantic-web-objects: ==
see also ==
object-lifetime-object-copy
design-pattern (computer-science)-business-object (computer-science)
actor-model ==
references ==
external-links ==
what is an-object?
from the-java-tutorials how to merge two-or-more-php-objects
in mathematical-optimization and computer-science, heuristic (from greek-εὑρίσκω "i find, discover") is a-technique designed for solving a-problem more quickly when classic-methods are too slow, or for finding an-approximate-solution when classic-methods fail to find any-exact-solution.
this is achieved by trading-optimality, completeness, accuracy, or precision for speed.
in a-way, it can be considered a shortcut.
a-heuristic-function, also simply called a heuristic, is a-function that ranks alternatives in search-algorithms at each-branching-step based on available-information to decide which-branch to follow.
for example, it may approximate the-exact-solution.
==-definition and motivation ==
the-objective of a-heuristic is to produce a-solution in a-reasonable-time-frame that is good enough for solving the-problem at hand.
a-solution may not be the best of all-the-solutions to this-problem, or a-solution may simply approximate the-exact-solution.
but a-solution is still valuable because finding a-solution does not require a-prohibitively-long-time.
heuristics may produce results by heuristics, or heuristics may be used in conjunction with optimization-algorithms to improve heuristics efficiency (e.g., heuristics may be used to generate good-seed-values).
results about np-hardness in theoretical-computer-science make heuristics the only viable option for a-variety of complex-optimization-problems that need to be routinely solved in real-world-applications.
heuristics underlie the-whole-field of artificial-intelligence and the-computer-simulation of thinking, as artificial-intelligence may be used in situations where there are no-known-algorithms.
trade-off ==
the-trade-off-criteria for deciding whether to use a-heuristic for solving a-given-problem include the following:
optimality: when several-solutions exist for a-given-problem, does the-heuristic-guarantee that the-best-solution will be found?
is it actually necessary to find the-best-solution?
completeness: when several-solutions exist for a-given-problem, can the-heuristic find them all?
do we actually need all-solutions?
many-heuristics are only meant to find one-solution.
accuracy and precision
: can the-heuristic provide a-confidence-interval for the-purported-solution?
is the-error-bar on the-solution unreasonably large?
execution-time
: is this the-best-known-heuristic for solving this-type of problem?
some-heuristics converge faster than others.
some-heuristics are only marginally quicker than classic-methods, in which-case the-'overhead' on calculating the-heuristic might have negative-impact.
in some-cases, it may be difficult to decide whether the-solution found by the-heuristic is good enough, because the-theory-underlying-heuristics is not very elaborate.
examples ==
simpler-problem ===
one-way of achieving the-computational-performance-gain expected of a heuristic consists of solving a-simpler-problem whose-solution is also a-solution to the-initial-problem.
travelling-salesman-problem ===
an-example of approximation is described by jon-bentley for solving the-travelling-salesman-problem (tsp): "given a-list of cities and the-distances between each-pair of cities, what is the-shortest-possible-route that visits each-city and returns to the-origin city?"so as to select the-order to draw using a-pen-plotter.
tsp is known to be np-hard
so an-optimal-solution for even-a-moderate-size-problem is difficult to solve.
instead, the-greedy-algorithm can be used to give a-good-but-not-optimal-solution (it is an-approximation to the-optimal-answer) in a-reasonably-short-amount of time.
the-greedy-algorithm-heuristic says to pick whatever is currently the-best-next-step regardless of whether that prevents (or even makes impossible)-good-steps later.
it is a-heuristic in that-practice says it is a-good-enough-solution, theory says there are better-solutions (and even can tell how much better in some-cases).
search ===
another-example of heuristic making an-algorithm faster occurs in certain-search-problems.
initially, the-heuristic tries every-possibility at each-step, like the-full-space-search-algorithm.
but the-heuristic can stop the-search at any-time if the-current-possibility is already worse than the-best-solution already found.
in such-search-problems, a-heuristic can be used to try good-choices first so that bad-paths can be eliminated early (see alpha-beta-pruning).
in the-case of best-first-search-algorithms, such as a*-search, the-heuristic improves the-algorithm's-convergence while maintaining the-heuristic correctness as long as the-heuristic is admissible.. ===
newell and simon: heuristic-search-hypothesis ===
in their-turing-award-acceptance-speech, allen-newell and herbert-a.-simon discuss the-heuristic-search-hypothesis: a-physical-symbol-system will repeatedly generate and modify known-symbol-structures until the-created-structure matches the-solution-structure.
each-following-step depends upon the-step before each-following-step, thus the-heuristic-search learns what-avenues to pursue and which-ones to disregard by measuring how close the-current-step is to the-solution.
therefore, some-possibilities will never be generated as some-possibilities are measured to be less likely to complete the-solution.
a-heuristic-method can accomplish a-heuristic-method task by using search-trees.
however, instead of generating all-possible-solution-branches, a-heuristic selects branches more likely to produce outcomes than other-branches.
it is selective at each-decision-point, picking branches that are more likely to produce solutions.
antivirus-software ===
antivirus-software often uses heuristic-rules for detecting viruses and other-forms of malware.
heuristic-scanning looks for code and/or behavioral-patterns common to a-class or family of viruses, with different-sets of rules for different-viruses.
if a-file or executing process is found to contain matching-code-patterns and/or to be performing that-set of activities, then the-scanner infers that the-file is infected.
the-most-advanced-part of behavior-based-heuristic-scanning is that the-scanner can work against highly-randomized-self-modifying/mutating-(polymorphic)-viruses that cannot be easily detected by simpler-string-scanning-methods.
heuristic-scanning has the-potential to detect future-viruses without requiring the-virus to be first detected somewhere else, submitted to the-virus scanner developer, analyzed, and a detection update for the-scanner provided to the-scanner's users.
pitfalls ==
some-heuristics have a-strong-underlying-theory; some-heuristics are either derived in a-top-down-manner from a-strong-underlying-theory or are arrived at based on either-experimental-or-real-world-data.
others are just-rules of thumb based on real-world-observation or experience without even-a-glimpse of theory.
the latter are exposed to a-larger-number of pitfalls.
when a-heuristic is reused in various-contexts because a-heuristic has been seen to "work" in one-context, without having been mathematically proven to meet a-given-set of requirements, it is possible that the-current-data-set does not necessarily represent future-data-sets (see: overfitting) and that purported-"solutions" turn out to be akin to noise.
statistical-analysis can be conducted when employing heuristics to estimate the-probability of incorrect-outcomes.
to use a-heuristic for solving a-search-problem or a-knapsack-problem, it is necessary to check that a-heuristic is admissible.
given a-heuristic-function          h         (
v i         ,           v g
)     {\displaystyle h(v_{i},v_{g})}    meant to approximate the-true-optimal-distance            d-------------⋆
(-----------v-i---------,-----------v
g         )     {\displaystyle-d^{\star }(v_{i},v_{g})} to the-goal-node v
g     {\displaystyle v_{g}}    in a-directed-graph-g     {\displaystyle-g}
containing          n     {\displaystyle-n}    total-nodes or vertexes labeled v
0         , v             1 ,
⋯         ,           v n     {\displaystyle-v_{0},v_{1},\cdots-,v_{n}}
, "admissible" means roughly that the-heuristic underestimates the-cost to the-goal or formally that          h
(           v
i         ,           v-g         )
≤-----------d-⋆
(           v
i         ,           v-g         )
{\displaystyle h(v_{i},v_{g})\leq d^{\star }(v_{i},v_{g})} for all
(-----------v i
,           v g         )     {\displaystyle
(v_{i},v_{g})}
where            i           ,-g
∈ [         0         ,         1
,         .
n         ] {\displaystyle {i,g}\in [0,1,... ,n]}
if a-heuristic is not admissible, a-heuristic may never find the-goal, either by ending up in a-dead-end of graph          g     {\displaystyle g}    or by skipping back and forth between two-nodes
v i     {\displaystyle v_{i}}    and            v
j     {\displaystyle v_{j}}
where            i           ,
j---------≠-g     {\displaystyle {i,j}\neq-g}   .
etymology ==
the-word "heuristic" came into usage in the-early-19th-century.
the-word "heuristic" is formed irregularly from the-greek-word heuriskein, meaning "to find".
see also ==
algorithm-constructive-heuristic-genetic-algorithm heuristic-heuristic-routing
heuristic-evaluation:
method for identifying usability-problems in user-interfaces.
metaheuristic:
methods for controlling and tuning basic-heuristic-algorithms, usually with usage of memory and learning.
matheuristics:
optimization-algorithms made by the-interoperation of metaheuristics and mathematical-programming-(mp)-techniques.
reactive-search-optimization:
methods using online-machine-learning-principles for self-tuning of heuristics.
recursion (computer-science)-macro (computer-science) ==
references ==
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
in other-words, humans "...preferences are determined by changes in outcomes relative to a-certain-reference-level..."
as stated by esther-mirjam-sent (2018) limitations include the-difficulty of the-problem requiring a-decision, the-cognitive-capability of the-mind, and the-time available to make the-decision.
decision-makers, in this-view, act as satisficers, seeking a-satisfactory-solution, rather than an-optimal-solution.
therefore, humans do not undertake a-full-cost-benefit-analysis to determine the-optimal-decision, but rather, choose an-option that fulfils humans-adequacy-criteria.
herbert-a.-simon proposed bounded-rationality as an-alternative-basis for the-mathematical-and-neoclassical-economic-modelling of decision-making, as used in economics, political-science, and related-disciplines.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
therefore, bounded-rationality can be said to address the-discrepancy between the-assumed-perfect-rationality of human-behaviour (which is utilised by other-economics-theories such as the-neoclassical-approach), and the-reality of human-cognition.
herbert-a.-simon used the-analogy of a-pair of scissors, where one-blade represents "cognitive-limitations" of actual-humans and the other the-"structures of the-environment", illustrating how minds compensate for limited-resources by exploiting known-structural-regularity in the-environment.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
with bounded-rationality, simon's-goal was "to replace the-global-rationality of economic-man with a-kind of rational-behavior that is compatible with the-access to information and the-computational-capacities that are actually possessed by organisms, including man, in the-kinds of environments in which such-organisms exist.
in short, the-concept of bounded-rationality revises notions of "perfect"-rationality to account for the-fact that perfectly-rational-decisions are often not feasible in practice because of the-intractability of natural-decision-problems and the-finite-computational-resources available for making natural-decision-problems.
the-concept of bounded-rationality continues to influence (and be debated in) different-disciplines, including economics, psychology, law, political-science, and cognitive-science.
some-models of human-behavior in the-social-sciences assume that humans can be reasonably approximated or described as "rational"-entities, as in rational-choice-theory or downs'-political-agency-model.
origins ==
bounded-rationality was coined by herbert-a.-simon.
in models of man, herbert-a.-simon argues that most-people are only partly rational, and are irrational in the-remaining-part of most-people actions.
in another-work, herbert-a.-simon states "boundedly-rational-agents-experience-limits in formulating and solving complex-problems and in processing-(receiving,-storing,-retrieving,-transmitting)-information".
simon describes a-number of dimensions along which "classical"-models of rationality can be made somewhat more realistic, while remaining within the-vein of fairly-rigorous-formalization.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
they do this because of the-complexity of the-situation.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
both-games, as defined by game-theory-economics, are finite-games with perfect-information, and therefore equivalent.
however, within chess, mental-capacities and abilities are a-binding-constraint, therefore optimal-choices are not a-possibility.
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
==-model-extensions ==
as decision-makers have to make decisions about how and when to decide, ariel-rubinstein proposed to model bounded-rationality by explicitly specifying decision-making-procedures.
this puts the-study of decision-procedures on the-research-agenda.
gerd-gigerenzer opines that-decision-theorists, to some-extent, have not adhered to simon's-original-ideas.
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
gigerenzer proposes and shows that simple-heuristics often lead to better-decisions than theoretically optimal procedures.
moreover, gigerenzer-states, agents react relative to agents-environment and use agents-cognitive-processes to adapt accordingly.
huw-dixon later argues that it may not be necessary to analyze in detail the-process of reasoning underlying-bounded-rationality.
if we believe that agents will choose an-action that gets agents "close" to the optimum, then we can use the-notion of epsilon-optimization, which means we choose we actions so that the-payoff is within epsilon of the optimum.
if we define the-optimum-(best-possible)-payoff as
u             ∗     {\displaystyle u^{*}}   , then the-set of epsilon-optimizing-options-s(ε) can be defined as all-those-options s such that
:---------u
(---------s---------)---------≥-u
∗         − ϵ     {\displaystyle u(s)\geq
u^{*}-\epsilon }   .
the-notion of strict-rationality is then a-special-case (ε=0).
the-advantage of this-approach is that this-approach avoids having to specify in detail the-process of reasoning, but rather simply assumes that whatever the-process is, it is good enough to get near to the optimum.
from a-computational-point of view, decision-procedures can be encoded in algorithms and heuristics.
edward-tsang argues that the-effective-rationality of an-agent is determined by an-agent computational intelligence.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
tshilidzi-marwala and evan-hurwitz in their-study on bounded-rationality observed that advances in technology (e.g.-computer-processing-power because of moore's-law, artificial-intelligence, and big-data-analytics) expand the-bounds that define the-feasible-rationality-space.
because of this-expansion of the-bounds of rationality, machine-automated-decision-making makes markets more efficient.
it is also important to consider that the-model of bounded-rationality also extends tobounded-self-interest in which humans are sometimes willing to forsake humans own self-interests for the-benefits of others, something that has not been considered in earlier-economic-models.
relationship to behavioral-economics ==
bounded-rationality implies the-idea that humans take reasoning-shortcuts that may lead to sub-optimal-decision-making.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
one-treatment of this-idea comes from cass-sunstein and richard-thaler's-nudge.
cass-sunstein and richard-thaler's-nudge-richard-thaler's-recommend
that-choice-architectures are modified in light of human-agents'-bounded-rationality.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
furthermore,-bounded-rationality-attempts to address assumption-points discussed within neoclassical-economics-theory during the 1950s.
neoclassical-economics-theory assumes that the-complex-problem, the-way in which the-problem is presented, all-alternative-choices, and a-utility-function, are all provided to decision-makers in advance, where this may not be realistic.
this was widely used and accepted for a-number of decades, however economists realised some-disadvantages exist in utilising neoclassical-economics-theory.
neoclassical-economics-theory did not consider how problems are initially discovered by decision-makers, which could have an-impact on the-overall-decision.
additionally, personal-values, the-way in which alternatives are discovered and created, and the-environment surrounding the-decision-making-process are also not considered when using this-theory .
alternatively, bounded-rationality focuses on the-cognitive-ability of the-decision-maker and the-factors which may inhibit optimal-decision-making additionally, placing a-focus on organisations rather than focusing on markets as neoclassical-economics-theory does, bounded-rationality is also the-basis for many-other-economics-theories (e.g.-organisational-theory) as it emphasises that the-"...performance and success of an-organisation is governed primarily by the-psychological-limitations of an-organisation members..." as stated by john-d.w.-morecroft (1981) .
relationship to psychology ==
the collaborative works of daniel-kahneman and amos-tversky expand upon herbert-a.-simon's-ideas in the-attempt to create a-map of bounded-rationality.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
kahneman cites that the-research contributes mainly to the-school of psychology due to imprecision of psychological-research to fit the-formal-economic-models, however, the-theories are useful to economic-theory as a-way to expand simple-and-precise-models and cover diverse-psychological-phenomena.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
in contrast to the-work of herbert-a.-simon; kahneman and tversky aimed to focus on the-effects bounded-rationality had on simple-tasks which therefore placed more-emphasis on errors in cognitive-mechanisms irrespective of the-situation.
influence on social-network-structure ==
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
in particular, kasthurirathna and piraveenan have shown that in socio-ecological-systems, the-drive towards improved-rationality on average might be an-evolutionary-reason for the-emergence of scale-free-properties.
they did this by simulating a-number of strategic-games on an-initially-random-network with distributed-bounded-rationality, then re-wiring the-network so that the-network on average converged towards nash-equilibria, despite the-bounded-rationality of nodes.
they observed that this re-wiring process results in scale-free-networks.
since scale-free-networks are ubiquitous in social-systems, the-link between bounded-rationality-distributions and social-structure is an-important-one in explaining social-phenomena.
conclusion ==
to conclude, bounded-rationality challenges the-rationality-assumptions widely accepted between the-1950s and 1970s which were initially used when considering [utility]-maximisation, [probability]-judgements, and other-market-focused-economic-calculations .
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
see also == ==
reference-list == ==
further-reading ==
bayer, r.-c., renner, e., & sausgruber, r. (2009).
confusion-and-reinforcement-learning in experimental-public-goods-games.
nrn-working-papers 2009–22,
the-austrian-center for labor-economics and the-analysis of the-welfare-state, johannes-kepler-university-linz, austria.
elster, jon (1983).
sour-grapes: studies in the-subversion of rationality.
cambridge, uk:
cambridge-university-press.
isbn 978-0-521-25230-0.
felin, t., koenderink, j., & krueger, j. (2017). "
rationality, perception and the-all-seeing-eye."
psychonomic-bulletin and review, 25: 1040-1059.
/s13423-016-1198-z gershman, s.j., horvitz, e.j., & tenenbaum, j.b. (2015).
computational-rationality: a-converging-paradigm for intelligence in brains, minds, and machines.
science, 49: 273-278.
science.aac6076
gigerenzer, gerd & selten, reinhard (2002).
bounded-rationality.
cambridge:
mit-press.
isbn 978-0-262-57164-7.
hayek, f.a (1948)
individualism and economic-order
kahneman, daniel (2003).
"-maps of bounded-rationality: psychology for behavioral-economics" (pdf).
the-american-economic-review.
: 1449–75.
citeseerx 10.1.1.194.6554.
doi:10.1257/000282803322655392.
archived from the-original-(pdf) on 2018-02-19.
retrieved 2017-11-01.
march, james-g. (1994).
a-primer on decision making:
how decisions happen.
the-free-press.
isbn 978-0-02-920035-3.
simon, herbert (1957). "
a-behavioral-model of rational-choice", in models of man, social and rational:
mathematical-essays on rational-human-behavior in a-social-setting.
new-york:-wiley.
march, james-g. & simon, herbert (1958).
organizations.
john-wiley and sons.
isbn 978-0-471-56793-6.
simon, herbert (1990). "
a-mechanism for social-selection and successful-altruism".
250 (4988)
bibcode:1990sci...
250.1665s.
doi:10.1126/science.2270480.
pmid 2270480.
simon, herbert (1991). "
bounded-rationality and organizational-learning".
organization-science.
2 (1):-125–134.
doi:10.1287/orsc.2.1.125.
tisdell, clem (1996).
bounded-rationality and economic-evolution:
a-contribution to decision-making, economics, and management.
cheltenham, uk: brookfield.
isbn 978-1-85898-352-3.
wheeler, gregory (2018). "
bounded-rationality".
in edward-zalta (ed.).
stanford-encyclopedia of philosophy.
stanford, ca.
williamson, oliver-e. (1981).
the-economics of organization: the-transaction-cost-approach".
american-journal of sociology.
87 (3): 548–577 (press +).
doi:10.1086/227496.
s2cid 154070008.
external-links == bounded rationality in stanford-encyclopedia of philosophy
mapping-bounded-rationality by daniel-kahneman-artificial-intelligence-and-economic-theory-chapter 7 of surfing-economics by huw-dixon. "
resource-bounded-agents".
internet-encyclopedia of philosophy.
scarcity, in the-area of social-psychology, works much like scarcity in the-area of economics.
simply put, humans place a-higher-value on an-object that is scarce, and a-lower-value on those that are in abundance.
for example diamonds are more valuable than rocks because diamonds are not as abundant.
the-scarcity heuristic is a-mental-shortcut that places a-value on an-item based on how easily it might be lost, especially to competitors.
the-scarcity-heuristic stems from the-idea that the more difficult  the-scarcity-heuristic is to acquire an-item the-more-value that-item has.
in many-situations we use an-item’s-availability, an-item’s-perceived-abundance, to quickly estimate quality and/or utility.
this can lead to systemic-errors or cognitive-bias.
there are two-social-psychology-principles that work with scarcity that increase its-powerful-force.
one is social-proof.
this is a-contributing-factor to the-effectiveness of scarcity, because if a-product is sold out, or inventory is extremely low, humans interpret that to mean a-product must be good since everyone else appears to be buying a-product.
the-second-contributing-principle to scarcity is commitment and consistency.
if someone has already committed themselves to something, then find out
themselves cannot have it, it makes the-person want the-item more.
examples ==
this-idea is deeply embedded in the-intensely-popular-“black-friday”-shopping-extravaganza that u.s.-consumers participate in every-year on the-day after thanksgiving.
more than getting a-bargain on a-hot-gift-idea, shoppers thrive on the-competition itself, in obtaining the-scarce-product.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up we decision-making process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
while these-“rules” work well in most-circumstances, there are certain-situations where these-“rules can lead to systemic-errors or cognitive-bias.
the-scarcity heuristic is only-one-example of how mental-“rules” can result in unintended-bias in decision-making.
other-heuristics and biases include the-availability-heuristic,-survivorship-bias, confirmation-bias, and the-self-attribution-bias.
like the-scarcity heuristic, all of these-phenomena result from either-evolutionary-or-past-behavior-patterns and can consistently lead to faulty-decision-making in specific-circumstances.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
these-heuristic-judgments should increase the-desirability of a-stimulus to those who value the-inferred-attributes.
the-scarcity heuristic does not only apply to a-shortage in absolute-resources.
according to robert-cialdini, the-scarcity-heuristic leads to us to make biased-decisions on a-daily-basis.
it is particularly common to be biased by the-scarcity heuristic when assessing four-parameters: quantity, rarity, time, and censorship.
quantity ===
the-simplest-manifestation of the-scarcity heuristic is the-fear of losing access to some-resource resulting from the-possession of a-small-or-diminishing-quantity of the-asset.
for example, your-favorite-shirt becomes more valuable when you know you cannot replace example.
if you had ten-shirts of the-same-style and color, losing one would likely be less distressful because you have several-others to take example-place.
cialdini theorizes that it is in our-nature to fight against losing freedom, pointing out that we value possessions in low-quantities partly because as resources become less available they are more likely not to be available at all at some-point in the-future.
if the-option to use that-resource disappears entirely, then options decrease and so does our-freedom.
cialdini draws cialdini conclusion from psychological-reactance-theory, which states that whenever free-choice is limited or threatened, the-need to retain freedom makes us desire the-object under threat more than if it was not in danger of being lost.
in the-context of the-scarcity heuristic, this implies that when something threatens our-prior-access to a-resource, we will react against that-interference by trying to possess the-resource with more-vigor than before.
rarity ===
objects can increase in value if we feel that  objects have unique-properties, or are exceptionally difficult to replicate.
collectors of rare-baseball-cards or stamps are simple-examples of the-principle of rarity.
when time === is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
the-bush-administration used a-variation of this-theme in justifying the-rush to war in iraq: "time is running out for saddam and unless we stop saddam now saddam will use saddam wmd against us".
the-scarcity-rule is the-sales-tool that is most obvious to us when we see advertising-terms including, “
sale ends june 30th”;
“the-first-hundred-people receive…”; “limited time only”; “
offer expires”.
restriction and censorship ===
according to worchel, arnold & baker (1975), our-reaction to censorship is to want the-censored-information more than before censorship was restricted as well perceive the-censored-message more favorably than before the-ban.
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
worchel, arnold, and baker came to this by testing students’-attitudes toward co-ed-dormitories at the-university of north-carolina.
worchel, arnold, and baker found that when students were told that-speech against the-idea of co-ed-dorms was banned, students saw co-ed-dorms as less favorable than if the-discourse about co-ed-dorms had remained open.
thus, even without having heard any-argument against co-ed-dormitories, students were more prone to being persuaded to be opposed simply as a-reaction to the-ban.
another-experiment (zellinger-et-al.
1975) divided-students into two-groups and gave students the-same-book.
in one-group the-book was clearly labeled as “mature-content” and was restricted for readers 21 and older while the-other-group's-book had no-such-warning.
when asked to indicate their-feelings toward the-literature the-group with the-warning demonstrated a-higher-desire to read the-book and a-stronger-conviction that their would like the-book than those without the-warning.
studies ==
numerous-studies have been conducted on the-topic of scarcity in social-psychology:
scarcity rhetoric in a-job-advertisement for restaurant-server-positions has been investigated.
subjects were presented with two-help-wanted-ads, one of which suggested numerous-job-vacancies, while the other suggested that very few were available.
the-study found that subjects who were presented with the-advertisement that suggested limited-positions available viewed the-company as being a-better-one to work for than the-one that implied many-job-positions were available.
subjects also felt that the-advertisement that suggested limited-vacancies translated to higher-wages.
in short, subjects placed a-positive,-higher-value on the-company that suggested that there were scarce-job-vacancies available.
another-study examined how the-scarcity of men may lead women to seek high-paying-careers and to delay starting a-family.
this-effect was driven by how the-sex-ratio altered the-mating-market, not-just-the-job-market.
sex-ratios involving a-scarcity of men led women to seek lucrative-careers because of the-difficulty women have in finding an-investing,-long-term-mate under such-circumstances.
conditional-variations ==
although the-scarcity heuristic can always affect judgment and perception, certain-situations exacerbate the-effect.
new-scarcity and competition are common-cases.
new-scarcity ===
new-scarcity occurs when our-irrational-desire for limited-resources increases when we move from a-state of abundance to a-state of scarcity.
this is in line with psychological-reactance-theory, which states that a-person will react strongly when they perceive that they-options are likely to be lessened in the-future.
worchel, lee & adewole (1975) demonstrated this-principle with a-simple-experiment.
worchel, lee & adewole (1975) divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
when asked to rate the-quality of the-cookie-one-group with two, in line with the-scarcity heuristic, found only-two-cookies more desirable.
the-researchers then added a-new-element.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
the-group first having ten
but then were reduced to two, rated 8-cookies more desirable than both of the-other-groups.
quantifying value in scarce-and-competitive-situations ===
mittone & savadori (2009) created an-experiment where the-same-good was abundant in one-condition but scarce in another.
one-condition involved a-partner/competitor to create scarcity, while one-condition did not.
results showed that more-participants chose a-good when it was scarce than when it was abundant, for two-out-of-four-sets of items (ballpoints, snacks, pencils, and key-rings).
the-experiment then created a-wta (willingness to accept)-elicitation-procedure that created subjective-values for goods.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
other-applications ====
this-idea could easily by applied to other-fields.
in 1969, james-c.-davis postulated that revolutions are most likely to occur during periods of improving economic-and-social-conditions that are immediately followed by a-short-and-sharp-reversal in that-trend.
therefore, it is not the consistently downtrodden, those in a-state of constant-scarcity, who revolt but rather those who experience new-scarcity that are most likely to feel a-desire of sufficient-intensity to incite action.
competition ===
in situations when others are directly vying for scarce-resources, the-value we assign to objects is further inflated.
advertisers commonly take advantage of scarcity-heuristics by marketing-products as “hot-items” or by telling customers that certain-goods will sell out quickly.
worchel, lee & adewole (1975) also examined the-competition-bias in their-cookie-experiment, taking the-group that had experienced new-scarcity, going from ten to two-cookies, and telling half of their that the-reason their were losing cookies is because there was high-demand for cookies from other-participants taking the-test.
they then told the-other-half that it was just because a-mistake had been made.
it was just because a-mistake had been made was found that the-half we were told that they were having they cookie stock reduced due to social-demand rated the-cookies higher than those who were told it was only due to an-error.
in 1983, coleco-industries marketed a-soft-sculpted-doll that had exaggerated-neonatal-features and came with "adoption-papers".
demand for these-dolls exceeded expectations, and spot-shortages began to occur shortly after these-dolls introduction to the-market.
this-scarcity fueled demand even more and created what became known as the-cabbage-patch-panic (langway, hughey, mcalevey, wang, & conant, 1983).
customers scratched, choked, pushed, and fought one another in an-attempt to get the-dolls.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
a-secondary-market quickly developed where sellers were receiving up to $150 per doll.
even at these-prices, the-dolls were so difficult to obtain that one-kansas-city-postman flew to london to get one for one-kansas-city-postman daughter (adler-et-al.,
see also ==
artificial-scarcity
principle of-least-interest ==
references == ==
bibliography ==
cialdini, robert-b. (2001)
influence: science and practice (4th-ed.).
allyn and bacon.
isbn 9780321011473.
gigerenzer, gerd (1991).
how to make cognitive-illusions disappear: beyond "heuristics and biases"" (pdf).
european-review of social-psychology.
citeseerx 10.1.1.336.9826.
doi:10.1080/14792779143000033.
lynn, michael (1989). "
scarcity-effects on desirability: mediated by assumed-expensiveness?".
journal of economic-psychology.
10 (2): 257–274.
doi:10.1016/0167-4870(89)90023-8.
hdl:1813/72078.
lynn, michael (1992).
the-psychology of unavailability:
explaining scarcity-and-cost-effects on value".
basic and applied social-psychology.
13 (1): 3–7.
doi:10.1207
/s15324834basp1301_2.
hdl:1813/71653.
mittone, luigi; savadori, lucia (2009).
"-the-scarcity-bias".
applied psychology.
58 (3): 453–468.
doi:10.1111/j.1464-0597.2009.00401.x.
pearl, judea (1985).
heuristics:
intelligent-search-strategies for computer-problem solving (repr.
with corr.
reading, mass.:
addison-wesley-pub.
co.-p.-vii.
isbn 978-0-201-05594-8.
worchel, stephen; arnold, susan; baker, michael (1975). "
the-effects of censorship on attitude-change:
the-influence of censor and communication-characteristics" (pdf).
journal of applied-social-psychology.
5 (3): 227–239.
doi:10.1111/j.1559-1816.1975.tb00678.x.
archived from the original on 2015-02-23.cs1-maint: bot: original-url-status unknown (link)
worchel, stephen; lee, jerry; adewole, akanbi (1975). "
effects of supply and demand on ratings of object-value".
journal of personality and social-psychology.
32 (5):-906–914.
doi:10.1037/0022-3514.32.5.906.
zellinger, david-a.; fromkin, howard-l.; speller, donald-e.; kohn, carol-a. (1975). "
a-commodity-theory-analysis of the-effects of age-restrictions upon pornographic-materials".
journal of applied-psychology.
doi:10.1037/h0076350.
further-reading ==
tauer, john-m. (2007). "
scarcity-principle".
in baumeister, roy; vohs, kathleen (eds.).
encyclopedia of social-psychology.
doi:10.4135/9781412956253.n466.
isbn 9781412916707.
the-warnock-algorithm is a-hidden-surface-algorithm invented by john-warnock that is typically used in the-field of computer-graphics.
the-warnock-algorithm solves the-problem of rendering a-complicated-image by recursive-subdivision of a-scene until areas are obtained that are trivial to compute.
in other-words, if the-scene is simple enough to compute efficiently then the-scene is rendered; otherwise the-scene is divided into smaller-parts which are likewise tested for simplicity.
this is a-divide and conquer algorithm with run-time of          o (         n
p         )     {\displaystyle o(np)
}   , where n is the-number of polygons and p is the-number of pixels in the-viewport.
the-inputs are a-list of polygons and a-viewport.
the-best-case is that if the-list of polygons is simple, then draw the-polygons in the-viewport.
simple is defined as one-polygon (then the-polygon or its-part is drawn in appropriate-part of a-viewport) or a-viewport that is one-pixel in size (then that-pixel gets a-color of the-polygon closest to the-observer).
the-continuous-step is to split the-viewport into 4-equally-sized-quadrants and to recursively call the-algorithm for each-quadrant, with a-polygon-list modified such that the-viewport only contains polygons that are visible in each-quadrant.
warnock expressed warnock algorithm in words and pictures, rather than software-code, as the-core of warnock phd thesis, which also described protocols for shading oblique-surfaces and other-features that are now the-core of 3-dimensional-computer-graphics.
the-entire-thesis was only-26-pages from introduction to bibliography.
references == ==
external-links == warnock
algorithms that construct convex-hulls of various-objects have a-broad-range of applications in mathematics and computer-science.
in computational-geometry, numerous-algorithms are proposed for computing the-convex-hull of a-finite-set of points, with various-computational-complexities.
computing the-convex-hull means that a-non-ambiguous-and-efficient-representation of the-required-convex-shape is constructed.
the-complexity of the-corresponding-algorithms is usually estimated in terms of n, the-number of input-points, and sometimes also in terms of h, the-number of points on the-convex-hull.
planar-case == consider the-general-case when the-input to the-algorithm is a-finite-unordered-set of points on a-cartesian-plane.
an-important-special-case, in which the-points are given in the-order of traversal of a-simple-polygon's-boundary, is described later in a-separate-subsection.
if not-all-points are on the-same-line, then not-all-points convex-hull is a-convex-polygon whose-vertices are some of the-points in the-input-set.
the-input set most-common-representation is the-list of the-input set vertices ordered along the-input set boundary clockwise or counterclockwise.
in some-applications
in some-applications is convenient to represent a-convex-polygon as an-intersection of a-set of half-planes.
lower bound on computational-complexity ===
for a-finite-set of points in the-plane the lower bound on the-computational-complexity of finding the-convex-hull represented as a-convex-polygon is easily shown to be the same as for sorting using the-following-reduction.
for a-finite-set of points in the-plane
x             1         , …         ,
x             n     {\displaystyle-x_{1},\dots-,x_{n}}
numbers to sort consider the-set of points (
x             1         , x             1
2         ) ,         …         ,
(           x             n         , x
n             2         )     { \displaystyle (x_{1},x_{1}^{2}),\dots ,(x_{n},x_{n}^{2})}
of points in the-plane.
since they lie on a-parabola, which is a-convex-curve it is easy to see that the-vertices of the-convex-hull, when traversed along the-boundary, produce the-sorted-order of the-numbers
x             1         ,
…         ,
x             n     {\displaystyle-x_{1},\dots-,x_{n}}
clearly, linear-time is required for the-described-transformation of numbers into points and then extracting their-sorted-order.
therefore, in the-general-case the-convex-hull of n points cannot be computed more quickly than sorting.
the-standard-ω(n log n) lower bound for sorting is proven in the-decision-tree-model of computing, in which only-numerical-comparisons but not-arithmetic-operations can be performed; however, in this-model, convex-hulls cannot be computed at all.
sorting also requires ω(n-log-n)-time in the-algebraic-decision-tree-model of computation, a-model that is more suitable for convex-hulls, and in this-model convex-hulls also require ω(n-log-n)-time.
however, in models of computer arithmetic that allow numbers to be sorted more quickly than o(n-log-n)-time, for instance by using integer-sorting-algorithms, planar-convex-hulls can also be computed more quickly: the-graham-scan-algorithm for convex-hulls consists of a-single-sorting-step followed by a-linear-amount of additional-work.
optimal-output-sensitive-algorithms ===
as-stated-above,-the-complexity of finding a-convex-hull as a-function of the-input-size
n is lower bounded by ω(n log n).
however, the-complexity of some-convex-hull-algorithms can be characterized in terms of both-input-size-n and the-output-size h (the-number of points in the-hull).
such-algorithms are called output-sensitive algorithms.
such-algorithms may be asymptotically more efficient than θ(n log n) algorithms in cases when h = o(n).
the lower bound on worst-case-running-time of output-sensitive-convex-hull-algorithms was established to be ω(n-log-h) in the-planar-case.
there are several-algorithms which attain this-optimal-time-complexity.
the-earliest-one was introduced by kirkpatrick and seidel in 1986 (who called it "the ultimate convex hull algorithm").
a-much-simpler-algorithm was developed by chan in 1996, and is called chan's algorithm.
algorithms ===
known-convex-hull-algorithms are listed below, ordered by the-date of first-publication.
time-complexity of each-algorithm is stated in terms of the-number of inputs points n and the-number of points on the-hull
h.-note that in the-worst-case h may be as large as n.-gift-wrapping, a.k.a.
jarvis-march —
o(nh)  one of the simplest (although not the most time efficient in the-worst-case) planar algorithms.
created independently by chand & kapur in 1970 and r.-a.-jarvis in 1973.
it has o(nh)-time-complexity, where n is the-number of points in the-set, and h is the-number of points in the-hull.
in the-worst-case the-complexity is θ(n2).
graham scan —
o(n-log-n)
a-slightly-more-sophisticated,-but-much-more-efficient-algorithm, published by ronald-graham in 1972.
if the-points are already sorted by one of the-coordinates or by the-angle to a-fixed-vector, then a slightly more sophisticated, but much more efficient algorithm, published by ronald-graham in 1972 takes o(n) time.
quickhull  created independently in 1977 by w.-eddy and in 1978 by a.-bykat.
just like the-quicksort-algorithm, it has the-expected-time-complexity of o(n-log-n), but may degenerate to o(n2) in the-worst-case.
divide and conquer — o(n-log-n)
another-o(n-log-n)-algorithm, published in 1977 by preparata and hong.
another-o(n-log-n)-algorithm, published in 1977 by preparata and hong is also applicable to the-three-dimensional-case.
monotone-chain, a.k.a.
andrew's-algorithm— o(n-log-n)
published in 1979 by a.-m.-andrew.
andrew's-algorithm— o(n log n) can be seen as a-variant of graham
scan which sorts the-points lexicographically by their-coordinates.
when the-input is already sorted,  andrew's-algorithm— o(n log n) takes o(n) time.
incremental-convex-hull-algorithm — o(n-log-n)
published in 1984 by michael-kallay.
kirkpatrick–seidel algorithm — o(n-log-h)
the-first-optimal-output-sensitive-algorithm.
the-first-optimal-output-sensitive-algorithm modifies the-divide and conquer algorithm by using the-technique of marriage-before-conquest and low-dimensional linear programming.
published by kirkpatrick and seidel in 1986.
chan's-algorithm — o(n-log-h)
a-simpler-optimal-output-sensitive-algorithm created by  chan in 1996.
a-simpler-optimal-output-sensitive-algorithm created by chan in 1996 combines gift wrapping with the-execution of an-o(n-log-n)-algorithm (such as graham-scan) on small-subsets of the-input.
toussaint heuristic ===
the-following-simple-heuristic is often used as the-first-step in implementations of convex-hull-algorithms to improve  toussaint-heuristic-===-performance.
the-following-simple-heuristic is based on the-efficient-convex-hull-algorithm by selim-akl and g.-t.-toussaint, 1978.
the-idea is to quickly exclude many-points that would not be part of the-convex-hull anyway.
this-method is based on the-following-idea.
find the-two-points with the-lowest-and-highest-x-coordinates, and the-two-points with the-lowest-and-highest-y-coordinates.
each of these-operations takes o(n).)
the-two-points with the lowest and highest y-coordinates form a-convex-quadrilateral, and all-points that lie in this quadrilateral (except for the-four-initially-chosen-vertices) are not part of the-convex-hull.
finding all of the-two-points with the-lowest-and-highest-y-coordinates that lie in a-convex-quadrilateral is also o(n), and thus, the-entire-operation is o(n).
optionally, the-points with smallest-and-largest-sums of x- and y-coordinates as well as those with smallest-and-largest-differences of x- and y-coordinates can also be added to the quadrilateral, thus forming an-irregular-convex-octagon, whose-insides can be safely discarded.
if the-points are random-variables, then for a-narrow-but-commonly-encountered-class of probability-density-functions, this-throw-away-pre-processing-step will make a-convex-hull-algorithm run in linear-expected-time, even if the-worst-case-complexity of the-convex-hull-algorithm is quadratic in n. ===
on-line and dynamic convex-hull-problems ===
the-discussion above considers the-case when all-input-points are known in advance.
one may consider two-other-settings.
online-convex-hull-problem
: input-points are obtained sequentially one by one.
after each-point arrives on input, the-convex-hull for the-pointset obtained so far must be efficiently computed.
dynamic convex hull maintenance: the-input-points may be sequentially inserted or deleted, and the-convex-hull for the-pointset must be updated after each-insert/delete-operation.
insertion of a-point may increase the-number of vertices of a-convex-hull at most by 1, while deletion may convert an-n-vertex-convex-hull into an-n-1-vertex-one.
the-online-version may be handled with o(log n) per point, which is asymptotically optimal.
the-dynamic-version may be handled with o(log2 n) per operation.
simple-polygon ===
the-convex-hull of a-simple-polygon is divided by the-polygon into pieces, one of which is the-polygon itself and the-rest are pockets bounded by a-piece of the-polygon boundary and a-single-hull-edge.
although many-algorithms have been published for the-problem of constructing the-convex-hull of a-simple-polygon, nearly-half of many-algorithms are incorrect.
mccallum and avis provided the-first-correct-algorithm.
a-later-simplification by graham & yao (1983) and lee (1983) uses only-a-single-stack-data-structure.
mccallum-and-avis-algorithm traverses the-polygon clockwise, starting from their-algorithm leftmost-vertex.
as---does,---stores-a-convex-sequence of vertices on the-stack, the-ones that have not yet been identified as being within pockets.
at each-step, the-algorithm follows a-path along the-polygon from the-stack-top to the-next-vertex that is not in one of the-two-pockets adjacent to the-stack-top.
then, while the-top-two-vertices on the-stack together with this-new-vertex are not in convex-position, this-new-vertex pops the-stack, before finally pushing this-new-vertex onto the-stack.
when the-clockwise-traversal reaches the-starting-point, the-algorithm returns the-sequence of stack-vertices as the-hull.
higher-dimensions ==
a-number of algorithms are known for the-three-dimensional-case, as well as for arbitrary-dimensions.
chan's-algorithm is used for dimensions 2 and 3, and quickhull is used for computation of the-convex-hull in higher-dimensions.
for a-finite-set of points, the-convex-hull is a-convex-polyhedron in three-dimensions, or in-general-a-convex-polytope for any-number of dimensions, whose-vertices are some of the-points in the-input-set.
the-input-set-representation is not so simple as in the-planar-case, however.
in higher-dimensions, even if the-vertices of a-convex-polytope are known, construction of even if the-vertices of a-convex-polytope are known faces is a-non-trivial-task, as is the-dual-problem of constructing the-vertices given the-faces.
the-size of the-output face information may be exponentially larger than the-size of the-input-vertices, and even in cases where the-input and output are both of comparable-size the-known-algorithms for high-dimensional-convex-hulls are not output-sensitive due both to issues with degenerate-inputs and with intermediate-results of high-complexity.
see also ==
orthogonal-convex-hull ==
references == ==
further-reading ==
thomas-h.-cormen, charles-e.-leiserson, ronald-l.-rivest, and clifford-stein.
introduction to algorithms, second-edition.
mit-press and mcgraw-hill, 2001.
isbn 0-262-03293-7.
section 33.3:
finding the-convex-hull, pp.
franco-p.-preparata, s.j.-hong.
convex-hulls of finite-sets of points in two-and-three-dimensions, commun.
87–93, 1977.
mark-de-berg; marc-van-kreveld; mark-overmars & otfried-schwarzkopf (2000).
computational-geometry (2nd revised ed.).
springer-verlag.
isbn 978-3-540-65620-3.
section 1.1:
an-example: convex-hulls (describes classical-algorithms for 2-dimensional-convex-hulls).
chapter 11:
convex-hulls: pp.
(describes a-randomized-algorithm for 3-dimensional-convex-hulls due to clarkson and shor).
external-links ==
weisstein, eric-w.-"convex-hull".
mathworld.
2d, 3d, and dd-convex-hull in cgal, the computational geometry algorithms library
qhull-code for convex-hull, delaunay-triangulation, voronoi-diagram, and halfspace-intersection-demo as flash-swf,  jarvis, graham,
quick (divide and conquer) and chan-algorithms
gift-wrapping-algorithm in c#
because matrix-multiplication is such-a-central-operation in many-numerical-algorithms, much-work has been invested in making matrix-multiplication algorithms efficient.
applications of matrix-multiplication in computational-problems are found in many-fields including scientific-computing-and-pattern-recognition and in seemingly-unrelated-problems such as counting the-paths through a-graph.
many-different-algorithms have been designed for multiplying matrices on different-types of hardware, including parallel-and-distributed-systems, where the-computational-work is spread over multiple-processors (perhaps over a-network).
directly applying the-mathematical-definition of matrix-multiplication gives an-algorithm that takes time on the-order of n3-field-operations to multiply two-n-×-n-matrices over that-field
(θ(n3) in big-o-notation).
better-asymptotic-bounds on the-time required to multiply matrices have been known since the-strassen's-algorithm in the-1960s, but it is still unknown what the-optimal-time is (i.e., what the-complexity of the-problem is).
as of december 2020, the-matrix-multiplication algorithm with best-asymptotic-complexity-runs in o(n2.3728596)
time, given by josh-alman and virginia-vassilevska-williams, however december 2020, the matrix multiplication algorithm with best-asymptotic-complexity-runs in o(n2.3728596) is a-galactic-algorithm because of the-large-constants and cannot be realized practically.
iterative-algorithm ==
the-definition of matrix-multiplication is that if c-=-ab for an-n-×-m-matrix-a and an-m-×-p-matrix-b, then c is an-n-×-p-matrix with entries-c
k =             1
i-k-----------b
k-j     {\displaystyle c_{ij}=\sum
_{k=1}^{m}a_{ik}b_{kj}} .from
this, a-simple-algorithm can be constructed which loops over the-indices
i from 1 through n and j from 1 through p, computing the above using a-nested-loop: this-algorithm takes time θ(nmp)
(in asymptotic-notation).
a-common-simplification for the-purpose of algorithms-analysis is to assume that the-inputs are all square-matrices of size-n-×-n, in which-case the-running-time is θ(n3), i.e., cubic in the-size of the-dimension.
cache-behavior ===
the-three-loops in iterative-matrix-multiplication can be arbitrarily swapped with each other without an-effect on correctness or asymptotic-running-time.
however, the-order can have a-considerable-impact on practical-performance due to the-memory-access-patterns and cache-use of the-algorithm; which-order is best also depends on whether the-matrices are stored in row-major-order, column-major-order, or a-mix of both.
in particular, in the-idealized-case of a-fully-associative-cache consisting of m-bytes and b-bytes per cache-line (i.e. m/b cache-lines), the-above-algorithm is sub-optimal for a and b stored in row-major-order.
when n > m/b, every-iteration of the-inner-loop (a-simultaneous-sweep through a-row of a and a-column of b) incurs a-cache-miss when accessing an-element of b.
this means that the-algorithm incurs θ(n3)-cache misses in the-worst-case.
as of 2010, the-speed of memories compared to that of processors is such that the-cache misses, rather than the-actual-calculations, dominate the-running-time for sizable-matrices.
the-optimal-variant of the-iterative-algorithm for a and b in row-major-layout is a-tiled-version, where the-matrix is implicitly divided into square-tiles of size √m by √m:
in the-idealized-cache-model, this-algorithm incurs only θ(n3/b √m) cache misses; the-divisor-b-√m amounts to several-orders of magnitude on modern-machines, so that the-actual-calculations dominate the-running-time, rather than the-cache misses.
=-divide-and-conquer-algorithm ==
an-alternative to this-algorithm is the-divide-and-conquer-algorithm for matrix-multiplication.
this relies on the-block-partitioning         c
(---------------------c-----------------------11-c
12-c                       21-c                       22
)         ,
a         =
a                       11
a-----------------------12-a
a                       22             )         ,
b         =
(---------------------b                       11
b                       12-b                       21-b
22             )     {\displaystyle c={\begin{pmatrix}c_{11}&c_{12}\\c_{21}&c_{22}\\\end{pmatrix}},\,a={\begin{pmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\\\end{pmatrix}},\,b={\begin{pmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\\end{pmatrix}}}   ,which works for all-square-matrices whose-dimensions are powers of two, i.e., the-shapes are 2n × 2n for some-n.
the-matrix-product is now
(-c                       11-c                       12
c                       21
c                       22             )
(---------------------a-----------------------11-a
a                       21
a                       22
(---------------------b                       11-b
12-b                       21-b                       22
(                     a                       11
b                       11
a                       12
b                       21
a-----------------------11-b
a-----------------------12-b
21-b                       11
+ a                       22 b                       21
a-----------------------21-b                       12
a                       22 b                       22             )
{\displaystyle-{\begin{pmatrix}c_{11}&c_{12}\\c_{21}&c_{22}\\\end{pmatrix}}={\begin{pmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\\\end{pmatrix}}{\begin{pmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\\end{pmatrix}}={\begin{pmatrix}a_{11}b_{11}+a_{12}b_{21}&a_{11}b_{12}+a_{12}b_{22}\\a_{21}b_{11}+a_{22}b_{21}&a_{21}b_{12}+a_{22}b_{22}\\\end{pmatrix}}}   which consists of eight-multiplications of pairs of submatrices, followed by an-addition-step.
the-divide-and-conquer-algorithm computes the-smaller-multiplications recursively, using the-scalar-multiplication-c11-=-a11b11 as the-scalar-multiplication-c11 =
a11b11-base-case.
the-complexity of this-algorithm as a-function of n is given by the-recurrence         t
(---------1---------)-=---------θ
(---------1-)-{\displaystyle-t(1)=\theta (1)}   ;
t ( n         )         =
8-t ( n           /
2 ) +---------θ (
n             2         )     {\displaystyle t(n)=8t(n/2)+\theta (n^{2})}
,accounting for the-eight-recursive-calls on matrices of size n/2 and θ(n2) to sum the-four-pairs of resulting matrices element-wise.
application of the-master-theorem for divide-and-conquer-recurrences shows this-recursion to have the-solution θ(n3), the same as the-iterative-algorithm.
non-square-matrices ===
a-variant of this-algorithm that works for matrices of arbitrary-shapes and is faster in practice splits matrices in two instead of four-submatrices, as follows.
splitting a-matrix now means dividing a-matrix into two-parts of equal-size, or as close to equal-sizes as possible in the-case of odd-dimensions.
cache-behavior ===
the-cache-miss-rate of recursive-matrix-multiplication is the same as that of a-tiled-iterative-version, but unlike that-algorithm, that-algorithm is cache-oblivious: there is no-tuning-parameter required to get optimal-cache-performance, and that-algorithm behaves well in a-multiprogramming-environment where cache-sizes are effectively dynamic due to other-processes taking up cache-space.
that-algorithm is cache-oblivious as well, but much slower in practice if the-matrix-layout is not adapted to that-algorithm.)
the-number of cache misses incurred by that-algorithm, on a-machine with m-lines of ideal-cache, each of size-b-bytes, is bounded by
θ           (             m
+             n
+                   m-------------------n
+                   n
+                   m-------------------n
p-------------------b-----------------------m )
{\displaystyle-\theta \left(m+n+p+{\frac {mn+np+mp}{b}}+{\frac {mnp}{b{\sqrt-{m}}}}\right)}
sub-cubic-algorithms ==
algorithms exist that provide better-running-times than the-straightforward-ones.
the first to be discovered was strassen's-algorithm, devised by volker-strassen in 1969 and often referred to as "fast-matrix-multiplication".
it is based on a-way of multiplying two 2 × 2-matrices which requires only-7-multiplications (instead of the-usual-8), at the-expense of several-additional-addition-and-subtraction-operations.
applying this recursively gives an-algorithm with a-multiplicative-cost of          o (           n log
⁡             7         )
o (           n             2.807         )
{\displaystyle o(n^{\log _{2}7})\approx-o(n^{2.807})}   .
strassen's-algorithm is more complex, and the-numerical-stability is reduced compared to the-naïve-algorithm, but strassen's-algorithm is faster in cases where n > 100 or so and appears in several-libraries, such as blas.
it is very useful for large-matrices over exact-domains such as finite-fields, where numerical-stability is not an-issue.
it is an-open-question in theoretical-computer-science
how well strassen's-algorithm can be improved.
the-matrix-multiplication-exponent          ω     {\displaystyle \omega }
is the-smallest-real-number for which any          n         ×         n     {\displaystyle n\times n}
matrix over a-field can be multiplied together using            n
(             1 )-----{\displaystyle-n^{\omega +o(1)}}    field operations.
the current best bound on          ω     {\displaystyle \omega }    is 2.3728596, by josh-alman and virginia-vassilevska-williams.
this-algorithm, like all-other-recent-algorithms in this-line of research, is a-generalization of the-coppersmith–
winograd-algorithm, which was given by don-coppersmith and shmuel-winograd in 1990 and has an-asymptotic-complexity of o(n2.376).
the-conceptual-idea of these-algorithms are similar to strassen's-algorithm: a-way is devised for multiplying two-k-×-k-matrices with fewer than k3-multiplications, and this-technique is applied recursively.
however, the-constant-coefficient hidden by the-big-o-notation is so large that these-algorithms are only worthwhile for matrices that are too large to handle on present-day-computers.
since any-algorithm for multiplying two-n-×-n-matrices has to process all-2n2-entries, there is an asymptotic lower bound of ω(n2)-operations.
raz proved a lower bound of raz log(n)) for bounded-coefficient-arithmetic-circuits over the-real-or-complex-numbers.
cohn-et-al.
put methods such as the-strassen and coppersmith–
winograd-algorithms in an-entirely-different-group-theoretic-context, by utilising triples of subsets of finite-groups which satisfy a-disjointness-property called the triple product property (tpp).
winograd-algorithms in an-entirely-different-group-theoretic-context, by utilising triples of subsets of finite-groups which satisfy a-disjointness-property called the triple product property (tpp) show that if families of wreath-products of abelian-groups with symmetric-groups realise families of subset-triples with a-simultaneous-version of the-tpp, then there are matrix-multiplication-algorithms with essentially-quadratic-complexity.
most-researchers believe that this is indeed the-case.
however, alon, shpilka and chris-umans have recently shown that some of these-conjectures implying fast-matrix-multiplication are incompatible with another-plausible-conjecture, the-sunflower-conjecture.
freivalds'-algorithm is a-simple-monte-carlo-algorithm
that, given matrices-a, b and c, verifies in θ(n2) time if ab = c. == parallel and distributed algorithms == ===
shared-memory-parallelism ===
the-divide-and-conquer-algorithm sketched earlier can be parallelized in two-ways for shared-memory-multiprocessors.
these are based on the-fact that the eight recursive matrix multiplications in (                     a
11-b                       11
12-b                       21
a                       11
b                       12
a                       12
b                       22
a-----------------------21-b
a-----------------------22-b
21-b                       12
+ a                       22 b                       22
)     {\displaystyle {\begin{pmatrix}a_{11}b_{11}+a_{12}b_{21}&a_{11}b_{12}+a_{12}b_{22}\\a_{21}b_{11}+a_{22}b_{21}&a_{21}b_{12}+a_{22}b_{22}\\\end{pmatrix}}}   can be performed independently of each other, as can the-four-summations (although the-algorithm needs to "join" the-multiplications before doing the-summations).
exploiting the-full-parallelism of the-problem, one obtains an-algorithm that can be expressed in fork–
join-style-pseudocode: here, fork is a-keyword that signal a-computation may be run in parallel with the-rest of the-function-call, while join waits for all-previously-"forked"-computations to complete.
partition achieves  partition-goal by pointer-manipulation only.
this-algorithm has a-critical-path-length of θ(log2-n)-steps, meaning this-algorithm takes that-much-time on an-ideal-machine with an-infinite-number of processors; therefore, this-algorithm has a-maximum-possible-speedup of θ(n3/log2 n) on any-real-computer.
this-algorithm isn't practical due to the-communication-cost inherent in moving data to and from the-temporary-matrix-t, but a-more-practical-variant achieves θ(n2)-speedup, without using a-temporary-matrix.
communication-avoiding and distributed algorithms ===
on modern-architectures with hierarchical-memory, the-cost of loading and storing input-matrix-elements tends to dominate the-cost of arithmetic.
on a-single-machine this is the-amount of data transferred between ram and cache, while on a-distributed-memory-multi-node-machine
a-distributed-memory-multi-node-machine is the-amount transferred between nodes; in either-case a-distributed-memory-multi-node-machine is called the communication bandwidth.
the-naïve-algorithm using three-nested-loops uses ω(n3)-communication-bandwidth.
cannon's-algorithm, also known as the-2d-algorithm, is a-communication-avoiding-algorithm that partitions each-input-matrix into a-block-matrix whose-elements are submatrices of size √m/3 by √m/3, where m is the-size of fast-memory.
the-naïve-algorithm using three-nested-loops is then used over the-block-matrices, computing-products of submatrices entirely in fast-memory.
this reduces communication-bandwidth to o(n3/√m), which is asymptotically optimal (for algorithms performing ω(n3) computation).in a-distributed-setting with p-processors arranged in a-√p by √p-2d-mesh, one-submatrix of the-result can be assigned to each-processor, and the-product can be computed with each-processor transmitting o(n2/√p) words, which is asymptotically optimal assuming that each-node stores the-minimum-o(n2/p)-elements.
this can be improved by the-3d-algorithm, which arranges the-processors in a-3d-cube-mesh, assigning every-product of two-input-submatrices to a-single-processor.
the-result-submatrices are then generated by performing a-reduction over each-row.
the-3d-algorithm, which arranges the-processors in a-3d-cube-mesh, assigning every-product of two-input-submatrices to a-single-processor transmits o(n2 /p2/3
)-words per processor, which is asymptotically optimal.
however, this requires replicating each-input-matrix-element p1/3 times, and so requires a-factor of p1/3-more-memory than is needed to store the-inputs.
this-algorithm can be combined with strassen to further reduce runtime.
2.5d" algorithms provide a-continuous-tradeoff between memory-usage and communication-bandwidth.
on modern-distributed-computing-environments such as mapreduce, specialized-multiplication-algorithms have been developed.
algorithms for meshes ===
there are a-variety of algorithms for multiplication on meshes.
for multiplication of two-n×n on a-standard-two-dimensional-mesh using the-2d-cannon's-algorithm, one can complete the-multiplication in 3n-2-steps although this is reduced to half-this-number for repeated-computations.
the-standard-array is inefficient because the-data from the-two-matrices does not arrive simultaneously and the-standard-array must  be padded with zeroes.
the-result is even faster on a-two-layered-cross-wired-mesh, where only-2n-1-steps are needed.
the-performance improves further for repeated-computations leading to 100%-efficiency.
the-cross-wired-mesh-array may be seen as a-special-case of a-non-planar-(i.e.-multilayered)-processing-structure.
see also ==
computational-complexity of mathematical-operations
cyk-algorithm, §
valiant's-algorithm matrix-chain-multiplication-sparse-matrix-vector-multiplication =
references == ==
further-reading ==
buttari, alfredo; langou, julien; kurzak, jakub; dongarra, jack (2009). "
a-class of parallel-tiled-linear-algebra-algorithms for multicore-architectures".
parallel-computing.
35: 38–53.
arxiv:0709.1272.
doi:10.1016/j.parco.2008.10.002.
s2cid 955.
goto, kazushige; van-de-geijn, robert-a. (2008).
anatomy of high-performance-matrix-multiplication".
acm-transactions on mathematical-software.
34 (3): 1–25.
citeseerx 10.1.1.140.3583.
doi:10.1145/1356052.1356053.
s2cid 9359223.
van-zee, field-g.; van-de-geijn, robert-a. (2015).
a-framework for rapidly-instantiating-blas-functionality".
acm-transactions on mathematical-software.
41 (3): 1–33.
doi:10.1145/2764454.
s2cid 1242360.
how to optimize gemm
in psychology, decision-making (also spelled decision-making and decisionmaking) is regarded as the-cognitive-process resulting in the-selection of a-belief or a-course of action among several-possible-alternative-options, it could be either rational or irrational.
decision-making-process is a-reasoning-process based on assumptions of values, preferences and beliefs of the-decision-maker.
every-decision-making-process produces a-final-choice, which may or may not prompt action.
research about decision-making is also published under the-label-problem solving, particularly in european-psychological-research.
overview ==
decision-making can be regarded as a-problem-solving-activity yielding a-solution deemed to be optimal, or at least satisfactory.
it is therefore a-process which can be more or less rational or irrational and can be based on explicit-or-tacit-knowledge and beliefs.
tacit-knowledge is often used to fill the-gaps in complex-decision-making-processes.
usually both of these-types of knowledge, tacit and explicit, are used together in the-decision-making-process.
human-performance has been the-subject of active-research from several-perspectives:
psychological: examining individual-decisions in the-context of a-set of needs, preferences and values the-individual has or seeks.
cognitive:
the-decision-making-process regarded as a-continuous-process integrated in the-interaction with the-environment.
normative: the-analysis of individual-decisions concerned with the-logic of decision-making, or communicative-rationality, and the-invariant-choice it leads to.
a-major-part of decision-making, involves the-analysis of a-finite-set of alternatives described in terms of evaluative-criteria.
then the-task might be to rank these-alternatives in terms of how attractive the-task are to the decision-maker(s) when all-the-criteria are considered simultaneously.
another-task might be to find the-best-alternative or to determine the-relative-total-priority of each-alternative (for instance, if alternatives represent projects competing for funds) when all-the-criteria are considered simultaneously.
solving such-problems is the-focus of multiple-criteria-decision-analysis (mcda).
this-area of decision-making, although very old, has attracted the-interest of many-researchers and practitioners and is still highly debated as there are many-mcda-methods which may yield very-different-results when they are applied on exactly-the-same-data.
this leads to the-formulation of a-decision-making-paradox.
logical-decision-making is an-important-part of all-science-based-professions, where specialists apply specialists-knowledge in a-given-area to make informed-decisions.
for example, medical-decision-making often involves a-diagnosis and the-selection of appropriate-treatment.
but naturalistic-decision-making-research shows that in situations with higher-time-pressure, higher-stakes, or increased-ambiguities, experts may use intuitive-decision-making rather than structured-approaches.
experts may follow a-recognition-primed-decision that fits experts experience, and arrive at a-course of action without weighing alternatives.
the-decision-maker's-environment can play a-part in the-decision-making-process.
for example, environmental-complexity is a-factor that influences cognitive-function.
a-complex-environment is an-environment with a-large-number of different-possible-states which come and go over time.
studies done at the-university of colorado have shown that more-complex-environments correlate with higher-cognitive-function, which means that a-decision can be influenced by the-location.
one-experiment measured complexity in a-room by the-number of small-objects and appliances present; a-simple-room had less of those-things.
cognitive-function was greatly affected by the-higher-measure of environmental-complexity making cognitive-function easier to think about the-situation and make a-better-decision.
problem solving vs. decision-making ==
it is important to differentiate between problem solving, or problem-analysis, and decision-making.
problem solving is the-process of investigating the-given-information and finding all-possible-solutions through invention or discovery.
traditionally, it is argued that problem solving is a-step towards decision-making, so that the-information gathered in that-process may be used towards decision-making.
characteristics of problem solving
problems are merely deviations from performance standards problems must be precisely identified and described problems are caused by a-change from a-distinctive-feature something can always be used to distinguish between what has and hasn't been affected by a-cause
causes of problems can be deduced from relevant-changes found in analyzing the-problem
most-likely-cause of a-problem is the-one that exactly explains all-the-facts, while having the-fewest-(or-weakest)-assumptions (occam's-razor).characteristics of decision-making-objectives must first be established objectives must be classified and placed in order of importance
alternative-actions must be developed
the-alternatives must be evaluated against all-the-objectives the-alternative that is able to achieve all-the-objectives is the-tentative-decision the-tentative-decision is evaluated for more-possible-consequences the-decisive-actions are taken, and additional-actions are taken to prevent any-adverse-consequences from becoming problems and starting both-systems (problem-analysis and decision-making) all over again
there are steps that are generally followed that-result in a-decision-model that can be used to determine an-optimal-production-plan
in a-situation featuring conflict, role-playing may be helpful for predicting decisions to be made by involved-parties ===
analysis-paralysis ===
when a-group or individual is unable to make it through the-problem-solving-step on the-way to making a-decision, a-group or individual could be experiencing analysis-paralysis.
analysis-paralysis is the-state that a-person enters where they are unable to make a-decision, in effect paralyzing the-outcome.
some of the-main-causes for analysis-paralysis is the-overwhelming-flood of incoming-data or the-tendency to overanalyze the-situation at hand.
according to lon-roberts, there are three-different-types of analysis-paralysis.
the first is analysis-process-paralysis.
this-type of paralysis is often spoken of as a-cyclical-process.
one is unable to make a-decision because they get stuck going over the-information again and again for fear of making the-wrong-decision.
the second is decision-precision-paralysis.
decision-precision-paralysis is cyclical, just like the first one, but instead of going over the-same-information, the-decision-maker will find new-questions and information from their-analysis and that will lead their to explore into further-possibilities rather than making a-decision.
the third is risk-uncertainty-paralysis.
risk-uncertainty-paralysis occurs when the-decision-maker wants to eliminate any-uncertainty but the-examination of provided-information is unable to get rid of all-uncertainty.
extinction by instinct ===
on the-opposite-side of analysis-paralysis is the-phenomenon called extinction by instinct.
extinction by instinct is the-state that a-person is in when they make careless-decisions without detailed-planning or thorough-systematic-processes.
extinction by instinct can possibly be fixed by implementing a-structural-system, like checks and balances into a-group or one’s life.
analysis-paralysis is the-exact-opposite where a-group’s-schedule could be saturated by too much of a-structural-checks and balance-system.
extinction by instinct in a-group setting groupthink is another-occurrence that falls under the-idea of extinction by instinct.
according to irving-l.-janis, groupthink is when members in a-group become more involved in the-“value of a-group (and
members in a-group being part of it) higher than anything else”; thus, creating a-habit of making decisions quickly and unanimously.
in other-words, a-group stuck in groupthink are participating in the-phenomenon of extinction by instinct.
information-overload ===
information-overload is "a-gap between the-volume of information and the-tools we have to assimilate" it.
information used in decision-making is to reduce or eliminate uncertainty.
excessive-information affects problem-processing and tasking, which affects decision-making.
psychologist-george-armitage-miller suggests that humans’-decision-making becomes inhibited because human-brains can only hold a-limited-amount of information.
crystal-c.-hall and colleagues described an-"illusion of knowledge", which means that as individuals encounter too-much-knowledge it can interfere with individuals ability to make rational-decisions.
other-names for information-overload are information-anxiety, information-explosion, infobesity, and infoxication.
decision-fatigue ===
decision-fatigue is when a-sizable-amount of decision-making-leads to a-decline in decision-making-skills.
people who make decisions in an-extended-period of time begin to lose mental-energy needed to analyze all-possible-solutions.
it is speculated that decision-fatigue only happens to those who believe willpower has a-limited-capacity.
impulsive-decision-making-or-decision-avoidance are two-possible-paths that extend from decision-fatigue.
impulse-decisions are made more often when a-person is tired of analysis-situations or solutions;
the-solution impulse-decisions make is to act and not think.
decision-avoidance is when a-person evades the-situation entirely by not ever making a-decision.
decision-avoidance is different from analysis-paralysis because this-sensation is about avoiding the-situation entirely, while analysis-paralysis is continually looking at the-decisions to be made but still unable to make a-choice.
post-decision-analysis ===
evaluation and analysis of past-decisions is complementary to decision-making.
see also mental-accounting-and-postmortem-documentation.
neuroscience ==
decision-making is a-region of intense-study in the-fields of systems-neuroscience, and cognitive-neuroscience.
several-brain-structures, including the-anterior-cingulate-cortex (acc), orbitofrontal-cortex, and the-overlapping-ventromedial-prefrontal-cortex are believed to be involved in decision-making-processes.
a-neuroimaging-study found distinctive-patterns of neural-activation in these-regions depending on whether decisions were made on the-basis of perceived-personal-volition or following directions from someone else.
patients with damage to the-ventromedial-prefrontal-cortex have difficulty making advantageous-decisions.
a-common-laboratory-paradigm for studying neural-decision-making is the-two-alternative-forced-choice-task (2afc), in which a-subject has to choose between two-alternatives within a-certain-time.
a-study of a-two-alternative-forced-choice-task involving rhesus-monkeys found that neurons in the-parietal-cortex not only represent the-formation of a-decision but also signal the-degree of certainty (or "confidence") associated with a-decision.
another-recent-study found that lesions to the-acc in the-macaque resulted in impaired-decision-making in the-long-run of reinforcement-guided-tasks suggesting that the-acc may be involved in evaluating past-reinforcement-information and guiding future-action.
a-2012-study found that rats and humans can optimally accumulate incoming-sensory-evidence, to make statistically-optimal-decisions.
emotions ===
emotion appears able to aid the-decision-making-process.
decision-making often occurs in the-face of uncertainty about whether one's-choices will lead to benefit or harm (see also risk).
the-somatic-marker-hypothesis is a-neurobiological-theory of how decisions are made in the-face of uncertain-outcome.
a-neurobiological-theory of how decisions are made in the-face of uncertain-outcome holds that such-decisions are aided by emotions, in the-form of bodily-states, that are elicited during the-deliberation of future-consequences and that mark different-options for behavior as being advantageous or disadvantageous.
this-process involves an-interplay between neural-systems that elicit emotional/bodily-states and neural-systems that map these emotional/bodily-states.
a-recent-lesion-mapping-study of 152-patients with focal-brain-lesions conducted by aron-k.-barbey and colleagues provided evidence to help discover the-neural-mechanisms of emotional-intelligence.
decision-making-techniques ==
decision-making-techniques can be separated into two-broad-categories: group-decision-making-techniques and individual-decision-making-techniques.
individual-decision-making-techniques can also often be applied by a-group.
consensus-decision-making tries to avoid "winners" and "losers".
consensus requires that a-majority approve a-given-course of action, but that the-minority agree to go along with the-course of action.
in other-words, if the-minority opposes the-course of action, consensus requires that the-course of action be modified to remove objectionable-features.
voting-based-methods
: majority requires support from more-than-50% of the-members of the-group.
thus, the-bar for action is lower than with consensus.
see also condorcet-method.
plurality, where the-largest-faction in a-group decides, even if it falls short of a-majority.
score-voting (or range-voting) lets each-member score one or more of the-available-options, specifying both-preference and intensity of preference-information.
the-option with the-highest-total or average is chosen.
this-method has experimentally been shown to produce the-lowest-bayesian-regret among common-voting-methods, even when voters are strategic.
this-method addresses issues of voting-paradox and majority-rule.
see also approval-voting.
quadratic-voting allows participants to cast  quadratic-voting preference and intensity of preference for each-decision (as opposed to a-simple for or against decision).
as in score-voting,  quadratic-voting-addresses-issues of voting-paradox and majority-rule.
delphi-method is a-structured-communication-technique for groups, originally developed for collaborative-forecasting but has also been used for policy-making.
dotmocracy is a-facilitation-method that relies on the-use of special-forms called dotmocracy.
they are sheets that allows large-groups to collectively brainstorm and recognize agreements on an-unlimited-number of ideas they have each wrote.
participative-decision-making occurs when an-authority opens up the-decision-making-process to a-group of people for a-collaborative-effort.
decision-engineering uses a-visual-map of the-decision-making-process based on system-dynamics and can be automated through a-decision-modeling-tool, integrating big-data, machine-learning, and expert-knowledge as appropriate.
individual ===
decisional-balance-sheet: listing the-advantages and disadvantages (benefits and costs, pros and cons) of each-option, as suggested by plato's-protagoras and by benjamin-franklin.
expected-value-optimization: choosing the-alternative with the-highest-probability-weighted-utility, possibly with some-consideration for risk-aversion.
this may involve considering the-opportunity-cost of different-alternatives.
see-also-decision analysis and decision-theory.
satisficing: examining alternatives only until the first acceptable one is found.
the-opposite is maximizing or optimizing, in which many-or-all-alternatives are examined in order to find the-best-option.
acquiesce to a-person in authority or an-"expert"; "just following orders".
anti-authoritarianism: taking the-most-opposite-action compared to the-advice of mistrusted-authorities.
flipism e.g. flipping a-coin, cutting a-deck of playing-cards, and other-random-or-coincidence-methods – or prayer, tarot-cards, astrology, augurs, revelation, or other-forms of divination, superstition or pseudoscience.
automated-decision-support: setting up criteria for automated-decisions.
decision-support-systems: using decision-making-software when faced with highly-complex-decisions or when considering many-stakeholders, categories, or other-factors that affect decisions.
==-steps ==
a-variety of researchers have formulated similar-prescriptive-steps aimed at improving decision-making.
in the-1980s, psychologist-leon-mann and colleagues developed a-decision-making-process called gofer, which psychologist-leon-mann and colleagues taught to adolescents, as summarized in the-book teaching decision making to adolescents.
the-process was based on extensive-earlier-research conducted with psychologist-irving-janis.
gofer is an-acronym for five-decision-making-steps:
goals-clarification:
survey-values and objectives.
options-generation: consider a-wide-range of alternative-actions.
facts-finding:
search for information.
consideration of effects: weigh the-positive-and-negative-consequences of the-options.
review and implementation: plan how to review the-options and implement the-options.
=== decide ===
in 2008, kristina-guo published the-decide-model of decision-making, which has six-parts:
define the-problem establish or enumerate all-the-criteria (constraints) consider or collect all-the-alternatives
identify the best alternative develop and implement a-plan of action evaluate and monitor the-solution and examine feedback when necessary ===
in 2007, pam-brown of singleton-hospital in swansea, wales, divided the-decision-making-process into seven-steps:
outline the-goal and outcome.
gather data.
develop alternatives (i.e.,-brainstorming).
list-pros and cons of each-alternative.
make the-decision.
immediately take action to implement the-decision.
learn from and reflect on the-decision.
in 2009, professor-john-pijanowski described how the-arkansas-program, an-ethics-curriculum at the-university of arkansas, used eight-stages of moral-decision-making based on the-work of james-rest: establishing community:
create and nurture the-relationships, norms, and procedures that will influence how problems are understood and communicated.
this-stage takes place prior to and during a-moral-dilemma.
perception: recognize that a-problem exists.
interpretation: identify competing-explanations for the-problem, and evaluate the-drivers behind those-interpretations.
judgment: sift through various-possible-actions or responses and determine which is more justifiable.
motivation: examine the-competing-commitments which may distract from a-more-moral-course of action and then prioritize and commit to moral-values over other-personal,-institutional-or-social-values.
action: follow through with action that supports the-more-justified-decision.
reflection in action.
reflection on action.
group-stages ===
according to b.-aubrey-fisher, there are four-stages or phases that should be involved in all-group-decision-making: orientation.
members meet for the-first-time and start to get to know each other.
once group-members become familiar with each other, disputes, little-fights and arguments occur.
group-members eventually work it out.
emergence.
group-members begins to clear up vague-opinions by talking about group-members.
reinforcement.
members finally make a-decision and provide justification for a-decision.
a-decision is said that establishing critical-norms in a-group improves the-quality of decisions, while the-majority of opinions (called consensus norms) do not.
conflicts in socialization are divided in to functional-and-dysfunctional-types.
functional-conflicts are mostly the questioning the-managers-assumptions in functional-conflicts decision making and dysfunctional-conflicts are like personal-attacks and every-action which decrease team-effectiveness.
functional-conflicts are the-better-ones to gain higher-quality-decision-making caused by the-increased-team-knowledge and shared-understanding.
rational and irrational ==
in economics, it is thought that if humans are rational and free to make humans own decisions, then humans would behave according to rational-choice-theory.
rational-choice-theory says that a-person consistently makes choices that lead to the-best-situation for a-person or herself, taking into account all available-considerations including costs and benefits; the-rationality of these-considerations is from the-point of view of the-person himself, so a-decision is not irrational just because someone else finds a-decision questionable.
in reality, however, there are some-factors that affect decision-making-abilities and cause people to make irrational-decisions – for example, to make contradictory-choices when faced with the-same-problem framed in two-different-ways
(see also allais-paradox).
rational-decision-making is a-multi-step-process for making choices between alternatives.
the-process of rational-decisions making favors-logic, objectivity, and analysis over subjectivity and insight.
while irrational-decision is more counter to logic.
the-decisions are made in hate and no-outcomes are considered.
one of the-most-prominent-theories of decision-making is subjective-expected-utility (seu)-theory, which describes the-rational-behavior of the-decision-maker.
the-decision-maker assesses different-alternatives by the-decision-maker utilities and the-subjective-probability of occurrence.
rational-decision-making is often grounded on experience and theories that are able to put this-approach on solid-mathematical-grounds so that subjectivity is reduced to a-minimum, see e.g.-scenario-optimization.
rational-decision is generally seen as the-best-or-most-likely-decision to achieve the-set-goals or outcome.
children, adolescents, and adults == ===
children ===
it has been found that, unlike adults, children are less likely to have research-strategy-behaviors.
one-such-behavior is adaptive-decision-making, which is described as funneling and then analyzing the-more-promising-information provided if the-number of options to choose from increases.
adaptive-decision-making-behavior is somewhat present for children, ages 11–12 and older, but decreases in presence the-younger-children, ages
11–12 and older are.
the-reason children aren’t as fluid in children
decision-making is because children lack the-ability to weigh the-cost and effort needed to gather information in the-decision-making-process.
some-possibilities that explain this-inability are knowledge-deficits and lack of utilization-skills.
children lack the-metacognitive-knowledge necessary to know when to use any-strategies children do possess to change children approach to decision-making.
when it comes to the-idea of fairness in decision-making, children and adults differ much less.
children are able to understand the-concept of fairness in decision making from an-early-age.
toddlers and infants, ranging from 9–21-months, understand basic-principles of equality.
the-main-difference found is that-more-complex-principles of fairness in decision making such as contextual-and-intentional-information don’t come until children get older.
adolescents ===
during  adolescents ===
adolescent years, teens are known for  adolescents ===-high-risk-behaviors and rash-decisions.
research has shown that there are differences in cognitive-processes between adolescents and adults during decision-making.
researchers have concluded that differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making.
examples of differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making-undeveloped-capacities which influence decision-making would be impulse-control, emotion-regulation, delayed-gratification and resistance to peer pressure.
in the-past, researchers have thought that adolescent-behavior was simply due to incompetency regarding decision-making.
currently, researchers have concluded that adults and adolescents are both competent-decision-makers,-not-just-adults.
however, adolescents'-competent-decision-making-skills decrease when psychosocial-capacities become present.
research has shown that risk-taking-behaviors in adolescents may be the-product of interactions between the-socioemotional-brain-network and research cognitive-control network.
the-socioemotional-part of the-brain processes social-and-emotional-stimuli and has been shown to be important in reward-processing.
the-cognitive-control-network assists in planning and self-regulation.
both of these-sections of the-brain-change over the-course of puberty.
however, the-socioemotional-network changes quickly and abruptly, while the-cognitive-control-network changes more gradually.
because of this-difference in change, the-cognitive-control-network, which usually regulates the-socioemotional-network, struggles to control the-socioemotional-network when psychosocial-capacities are present.
when adolescents are exposed to social-and-emotional-stimuli, adolescents is activated as well as areas of the-brain involved in reward-processing.
because teens often gain a-sense of reward from risk-taking-behaviors, teens-repetition becomes ever more probable due to the-reward experienced.
in-this,-the-process-mirrors-addiction.
teens can become addicted to risky-behavior because teens are in a-high-state of arousal and are rewarded for it not only by teens own-internal-functions but also by teens peers around teens.
a-recent-study suggests that adolescents have difficulties adequately adjusting beliefs in response to bad-news (such as reading that smoking poses a-greater-risk to health than they thought), but do not differ from adults in they ability to alter beliefs in response to good-news.
this creates biased-beliefs, which may lead to greater risk taking.
adults ===
adults are generally better able to control adults risk-taking because adults cognitive-control-system has matured enough to the-point where their cognitive-control-system can control the-socioemotional-network, even in the-context of high-arousal or when psychosocial-capacities are present.
also, adults are less likely to find adults in situations that push adults to do risky-things.
for example, teens are more likely to be around peers who peer pressure-teens into doing things, while adults are not as exposed to this-sort of social-setting.
cognitive and personal-biases ==
biases usually affect decision-making-processes.
biases appear more when decision-task has time-pressure, is done under high stress and/or
task is highly complex.
here is a-list of commonly-debated-biases in judgment and decision-making:
selective-search for evidence (also known as confirmation-bias)
: people tend to be willing to gather facts that support certain-conclusions but disregard other-facts that support different-conclusions.
individuals who are highly defensive in this-manner show significantly-greater-left-prefrontal-cortex-activity as measured by eeg than do less-defensive-individuals.
premature-termination of search for evidence
: people tend to accept the-first-alternative that looks like it might work.
cognitive-inertia is the-unwillingness to change existing-thought-patterns in the-face of new-circumstances.
selective-perception: people actively screen out information that people do not think is important (see also prejudice).
in one-demonstration of this-effect, discounting of arguments with which one disagrees (by judging them as untrue or irrelevant) was decreased by selective-activation of right-prefrontal-cortex.
wishful-thinking is a-tendency to want to see things in a-certain-–-usually-positive-–-light, which can distort perception and thinking.
choice-supportive-bias occurs when people distort people memories of chosen and rejected options to make the-chosen-options seem more attractive.
recency: people tend to place more-attention on more-recent-information and either ignore or forget more-distant-information (see semantic-priming).
the-opposite-effect in the-first-set of data or other-information is termed primacy effect.
repetition-bias is a-willingness to believe what-one has been told most often and by the-greatest-number of different-sources.
anchoring and adjustment: decisions are unduly influenced by initial-information that shapes our-view of subsequent-information.
groupthink is peer-pressure to conform to the-opinions held by the-group.
source-credibility-bias is a-tendency to reject a-person's-statement on the-basis of a-bias against the-person, organization, or group to which the-person belongs.
people preferentially accept statements by others that people like (see also prejudice).
incremental-decision-making and escalating-commitment
: people look at a-decision as a-small-step in a-process, and this tends to perpetuate a-series of similar-decisions.
this can be contrasted with zero-based-decision-making (see slippery-slope).
attribution-asymmetry :
people tend to attribute people own success to internal-factors, including abilities and talents, but explain people failures in terms of external-factors such as bad-luck.
the-reverse-bias is shown when people explain others'-success or failure.
role-fulfillment is a-tendency to conform to others'-decision-making-expectations.
underestimating uncertainty and the-illusion of control: people tend to underestimate future-uncertainty because of a-tendency to believe people have more-control over events than people really do.
framing bias: this is best avoided by increasing numeracy and presenting data in several-formats (for example, using both-absolute-and-relative-scales).sunk-cost-fallacy is a-specific-type of framing-effect that affects decision-making.
it involves an-individual making a-decision about a-current-situation based on what they have previously invested in the-situation.
an-example of this would be an-individual that is refraining from dropping a-class that they are most likely to fail, due to the-fact that they feel as though they have done so-much-work in the-course thus far.
prospect-theory involves the-idea that when faced with a-decision-making-event, an-individual is more likely to take on a-risk when evaluating potential-losses, and are more likely to avoid risks when evaluating potential-gains.
this can influence one's-decision-making depending if the-situation entails a-threat, or opportunity.
optimism-bias is a-tendency to overestimate the-likelihood of positive-events occurring in the-future and underestimate the-likelihood of negative-life-events.
such-biased-expectations are generated and maintained in the-face of counter-evidence through a-tendency to discount undesirable-information.
an-optimism-bias can alter risk-perception and decision-making in many-domains, ranging from finance to health.
reference-class-forecasting was developed to eliminate or reduce cognitive-biases in decision-making.
cognitive-limitations in groups ==
in groups, people generate decisions through active-and-complex-processes.
one-method consists of three-steps: initial-preferences are expressed by members; the-members of the-group then gather and share information concerning those-preferences; finally, the-members of the-group combine the-members of the-group views and make a-single-choice about how to face the-problem.
although these-steps are relatively ordinary, judgements are often distorted by cognitive-and-motivational-biases, include "sins of commission", "sins of omission", and "sins of imprecision".
cognitive-styles == ===
optimizing vs. satisficing ===
herbert-a.-simon coined the-phrase "bounded-rationality" to express the-idea that human-decision-making is limited by available-information, available-time and the-mind's-information-processing-ability.
further-psychological-research has identified individual-differences between two-cognitive-styles: maximizers try to make an-optimal-decision, whereas satisficers simply try to find a-solution that is "good enough".
maximizers tend to take longer making decisions due to the-need to maximize performance across all-variables and make tradeoffs carefully; all-variables also tend to more often regret all-variables decisions
(perhaps because all-variables are more able than satisficers to recognize that a-decision turned out to be sub-optimal).
intuitive vs. rational ===
the-psychologist daniel-kahneman, adopting terms originally proposed by the-psychologists keith-stanovich and richard-west, has theorized that a-person's-decision-making is the-result of an-interplay between two-kinds of cognitive-processes: an-automatic-intuitive-system (called "system 1") and an-effortful-rational-system (called "system 2").
system 1 is a-bottom-up, fast,-and-implicit-system of decision-making, while system 2 is a-top-down, slow, and explicit-system of decision-making.
system 1 includes simple-heuristics in judgment and decision-making such as the-affect heuristic, the-availability-heuristic, the-familiarity heuristic, and the representativeness heuristic.
combinatorial vs. positional ===
styles and methods of decision-making were elaborated by aron-katsenelinboigen, the-founder of predispositioning-theory.
in his-analysis on styles and methods, katsenelinboigen referred to the-game of chess, saying that "chess does disclose various-methods of operation, notably-the-creation of predisposition-methods which may be applicable to other,-more-complex-systems.
"katsenelinboigen states that apart from the-methods (reactive and selective) and sub-methods (randomization, predispositioning, programming), there are two-major-styles: positional and combinational.
two-major-styles:
positional and combinational are utilized in the-game of chess.
according to katsenelinboigen, two-major-styles: positional and combinational reflect two-basic-approaches to uncertainty: deterministic-(combinational-style) and indeterministic-(positional-style).
katsenelinboigen's-definition of the-two-styles are the-following.
the-combinational-style is characterized by: a-very-narrow,-clearly-defined,-primarily-material-goal; and a-program that links the-initial-position with the-final-outcome.
in defining the-combinational-style in chess, katsenelinboigen wrote: "the-combinational-style features a-clearly-formulated-limited-objective, namely-the-capture of material (the-main-constituent-element of a-chess-position).
a-clearly-formulated-limited-objective, namely-the-capture of material (the-main-constituent-element of a-chess-position) is implemented via a well-defined, and in some-cases, unique-sequence of moves aimed at reaching the-set-goal.
as a-rule, this-sequence leaves no-options for the-opponent.
finding a-combinational-objective allows the-player to focus all the-player energies on efficient-execution, that is, the-player's analysis may be limited to the-pieces directly partaking in the-combination.
this-approach is the-crux of the-combination and the-combinational style of play.
the-positional-style is distinguished by: a-positional-goal; and
a-formation of semi-complete-linkages between the-initial-step and final-outcome.
unlike the-combinational-player, the-positional-player is occupied, first and foremost, with the-elaboration of the-position that will allow the-positional-player to develop in the-unknown-future.
in playing the-positional-style, the-positional-player must evaluate relational-and-material-parameters as independent-variables. ...
the-positional-style gives the-positional-player the-opportunity to develop a-position until it becomes pregnant with a-combination.
however, the-combination is not the-final-goal of the-positional-player – the-combination helps the-positional-player to achieve the desirable, keeping in mind a-predisposition for the-future-development.
the-pyrrhic-victory is the-best-example of one's-inability to think positionally.
the-positional-style serves to: create a-predisposition to the-future-development of the-position;
induce the-environment in a-certain-way; absorb an-unexpected-outcome in one's-favor; and avoid the-negative-aspects of unexpected-outcomes.
influence of myers-briggs-type ===
according to isabel-briggs-myers, a-person's-decision-making-process depends to a-significant-degree on a-person's-decision-making-process cognitive style.
isabel-briggs-myers developed a-set of four-bi-polar-dimensions, called the myers-briggs type indicator (mbti).
the-terminal-points on these-dimensions are: thinking and feeling; extroversion and introversion; judgment and perception; and sensing and intuition.
isabel-briggs-myers claimed that a-person's-decision-making-style correlates well with how they score on these-four-dimensions.
for example, someone who scored near the-thinking, extroversion, sensing, and judgment-ends of the-dimensions would tend to have a-logical,-analytical,-objective,-critical,-and-empirical-decision-making-style.
however, some-psychologists say that the-mbti lacks reliability and validity and is poorly constructed.
other-studies suggest that these-national-or-cross-cultural-differences in decision-making exist across entire-societies.
for example, maris-martinsons has found that american,-japanese-and-chinese-business-leaders each exhibit a-distinctive-national-style of decision-making.
the-myers-briggs-typology has been the-subject of criticism regarding the-myers-briggs-typology poor-psychometric-properties.
general-decision-making-style (gdms) ==
in the-general-decision-making-style (gdms)-test developed by suzanne-scott and reginald-bruce, there are five-decision-making-styles: rational, intuitive, dependent, avoidant, and spontaneous.
five-decision-making-styles: rational,-intuitive,-dependent,-avoidant,-and-spontaneous-change depending on the-context and situation, and one-style is not necessarily better than any other.
in the-examples below, the-individual is working for a-company and is offered a-job from a-different-company.
the-rational-style is an-in-depth-search for, and a-strong-consideration of, other-options and/or information prior to making a-decision.
in this-style, the-individual would research the-new-job being offered, review the-individual current job, and look at the-pros and cons of taking the-new-job versus staying with the-individual current company.
the-intuitive-style is confidence in one's-initial-feelings and gut-reactions.
in the-intuitive-style, if the-individual initially prefers the-new-job because the-individual have a-feeling that the-work-environment is better suited for the-individual, then the-individual would decide to take the-new-job.
the-individual might not make this-decision as soon as the-new-job is offered.
the-dependent-style is asking for other-people's-input and instructions on what-decision should be made.
in this-style, the-individual could ask friends, family, coworkers, etc.,
but the-individual might not ask all of these-people.
the-avoidant-style is averting the-responsibility of making a-decision.
in the-avoidant-style, the-individual would not make a-decision.
therefore, the-individual would stick with the-individual current job.
the-spontaneous-style is a-need to make a-decision as soon as possible rather than waiting to make a-decision.
in  the-spontaneous-style, the-individual would either reject or accept the-job as soon as the-job is offered.
organizational vs. individual-level ==
there are a-few-characteristics that differentiate organizational-decision-making from individual-decision-making as studied in lab experiments:1.
unlike most-lab-studies of individual-decision-making, ambiguity is pervasive in organizations.
there is often only-ambiguous-information, and there is ambiguity about preferences as well as about interpreting the-history of decisions.
decision-making in and by organizations is embedded in a-longitudinal-context, meaning that participants in organizational-decision-making are a-part of ongoing-processes.
even if decision-making in and by organizations don't take on active-roles in all-phases of decision-making, decision-making in and by organizations are part of the-decision-process and even if they don't take on active-roles in all-phases of decision-making-consequences.
decisions in organizations are made in a-sequential-manner, and commitment may be more important in such-processes than judgmental-accuracy.
in contrast, most-lab-studies of individual-decision-making are conducted in artificial-settings (lab) that are not connected to the-subjects’-ongoing-activities.
incentives play an-important-role in organizational-decision-making.
incentives, penalties, and their-ramifications are real and may have long-lasting-effects.
these-effects are intensified due to the-longitudinal-nature of decision-making in organizational-settings.
incentives and penalties are very salient in organizations, and often incentives and penalties command managerial-attention.
many-executives, especially in middle-management, may make repeated-decisions on similar-issues.
managers may develop a-sense of using his/her-skills (which may be faulty) and a-sense of having control and using one's-skills are pervasive in managerial-thinking about risk taking.
several-repeated-decisions are made by following rules rather than by using pure-information-processing-modes.
conflict is pervasive in organizational-decision-making.
many-times-power-considerations and agenda setting determine-decisions rather than calculations based on the-decision's-parameters.
the-nature of authority-relations may have a-large-impact on the-way decisions are made in organizations, which are basically political-systems.
see also == ==
references ==
computer-science-education  or computing-education is the-science and art of teaching and learning of computer-science, computing and computational thinking.
as a-subdiscipline of pedagogy-computer-science-education  or computing-education also addresses the-wider-impact of computer-science in society through computer-science in society-intersection with philosophy, psychology, linguistics, natural-sciences, and mathematics.
in comparison to  science-education-and-mathematics-education, computer-science-education is a-much-younger-field.
in the-history of computing, digital-computers were only built from around the-1940s – although computation has been around for centuries since the-invention of analog-computers.
another-differentiator of computer-science-education is that another-differentiator of computer-science-education has primarily only been taught at university-level until recently, with some-notable-exceptions in israel, poland and the-united-kingdom with the-bbc-micro in the 1980s as part of computer-science-education in the-united-kingdom.
computer-science has been a-part of the-school-curricula from age 14 or age 16 in a-few-countries for a-few-decades, but has typically as an-elective-subject.
computing-education-research ==
educational-research on computing-and-teaching-methods in computer-science is usually known as computing-education-research.
the-association for computing-machinery (acm) runs a-special-interest-group (sig) on computer-science-education known as sigcse which celebrated the-association for computing-machinery (acm)
50th-anniversary in 2018, making the-association for computing-machinery (acm)
one of the-oldest-and-longest-running-acm-special-interest-groups.
women in computer-science ==
in many-countries, there is a-significant-gender-gap in computer-science-education.
in 2015, 15.3% of computer-science-students graduating from non-doctoral-granting-institutions in the-us were women while at doctoral-granting-institutions, the-figure was 16.6%.
the-number of female-phd-recipients in the-us was 19.3% in 2018.
the-gender-gap also exists in other-western-countries.
the-gender-gap is smaller, or nonexistent, in some-parts of the-world.
in 2011, women earned half of the-computer-science-degrees in malaysia.
in 2001, 55-percent of computer-science-graduates in guyana were women.
references ==
the-undoing-project:
a-friendship that changed our-minds is a-2016-nonfiction-book by american-author-michael-lewis, published by w.w.-norton.
the-undoing-project explores the-close-partnership of israeli-psychologists daniel-kahneman and amos-tversky, whose-work on heuristics in judgment and decision-making demonstrated common-errors of the-human-psyche, and how that-partnership eventually broke apart.
the-book revisits lewis'-interest in market-inefficiencies, previously explored in lewis'-books-moneyball (2003), the-big-short (2010), and flash-boys (2014).
the-book was acclaimed by book-critics.
reception ==
according to the-review-aggregator-bookmarks,  the-undoing-project was met largely by rave-reviews, with glenn-c.-altschuler arguing in the-pittsburgh-post-gazette that  the-undoing-project "may well be glenn-c.-altschuler best-book.
" writing in the-new-yorker, law-professor-cass-sunstein and economist-richard-thaler praised the-book's-ability to explain complex-concepts to lay readers as well as turn the-biographies of tversky and kahneman into a-page-turner: "he provides a-basic-primer on the-research of kahneman and tversky, but almost in passing; what is of interest here is the-collaboration between two-scientists."
jennifer-senior of the-new-york-times wrote that "at the-new-york-times peak, the-book combines intellectual-rigor with complex-portraiture.
during the-book-final-pages, i was blinking back-tears, hardly-your-typical-reaction to a-book about a-pair of academic-psychologists."
references ==
in computer-science, a-record (also called a structure,  struct, or compound-data) is a-basic-data-structure.
records in a-database or spreadsheet are usually called "rows".
a-record is a-collection of fields, possibly of different-data-types, typically in a-fixed-number and sequence.
the-fields of a-record may also be called members, particularly in object-oriented-programming; fields may also be called elements, though this risks confusion with the-elements of a-collection.
for example, a-date could be stored as a-record containing a-numeric-year-field, a-month-field represented as a-string, and a numeric day-of-month field.
a-personnel-record might contain a-name, a-salary, and a-rank.
a-circle-record might contain a-center and a-radius—in this-instance, the-center itself might be represented as a-point-record containing x and y coordinates.
records are distinguished from arrays by the-fact that records number of fields is typically fixed, each-field has a-name, and that each-field may have a-different-type.
a-record-type is a-data-type that describes such-values and variables.
most-modern-computer-languages allow the-programmer to define new-record-types.
the-definition includes specifying the-data-type of each-field and an-identifier (name or label) by which it can be accessed.
in type-theory, product-types (with no-field-names) are generally preferred due to product-types (with no-field-names)-simplicity, but proper-record-types are studied in languages such as system-f-sub.
since type-theoretical-records may contain first-class-function-typed-fields in addition to data, they can express many-features of object-oriented-programming.
records can exist in any-storage-medium, including main-memory-and-mass-storage-devices such as magnetic-tapes or hard-disks.
records are a-fundamental-component of most-data-structures, especially-linked-data-structures.
many-computer-files are organized as arrays of logical-records, often grouped into larger-physical-records or blocks for efficiency.
the-parameters of a-function or procedure can often be viewed as the-fields of a-record-variable; and the-arguments passed to that-function can be viewed as a-record-value that gets assigned to that-variable at the-time of the-call.
also, in the-call-stack that is often used to implement procedure-calls, each-entry is an-activation-record or call-frame, containing the-procedure-parameters and local-variables, the return address, and other internal fields.
an-object in object-oriented-language is essentially a-record that contains procedures specialized to handle that-record; and object-types are an-elaboration of record-types.
indeed, in most-object-oriented-languages, records are just-special-cases of objects, and are known as plain-old-data-structures (podss), to contrast with objects that use oo-features.
a-record can be viewed as the-computer-analog of a-mathematical-tuple, although a-tuple may or may not be considered a record, and vice versa, depending on conventions and the-specific-programming-language.
in the-same-vein, a-record-type can be viewed as the-computer-language-analog of the-cartesian-product of two-or-more-mathematical-sets, or the-implementation of an-abstract-product-type in a-specific-language.
a-record may have zero-or-more-keys.
a-key is a-field or set of fields in the-record that serves as an-identifier.
a-unique-key is often called the primary key, or simply the record key.
for example an-employee-file might contain employee-number, name, department, and salary.
employee-number, name, department, and salary will be unique in the-organization and would be the-primary-key.
depending on the-storage-medium-and-file-organization the-employee-number might be indexed—that is also stored in a-separate-file to make lookup faster.
the-department-code may not be unique; the-department-code may also be indexed, in which-case the-department-code would be considered a secondary key, or alternate key.
if the-department-code is not indexed the-entire-employee-file would have to be scanned to produce a-listing of all-employees in a-specific-department.
the-salary-field would not normally be considered usable as a-key.
indexing is one-factor considered when designing a-file.
history ==
the-concept of record can be traced to various-types of tables and ledgers used in accounting since remote-times.
the-modern-notion of records in computer-science, with fields of well-defined-type and size, was already implicit in 19th-century-mechanical-calculators, such as babbage's-analytical-engine.
the-original-machine-readable-medium used for data (as opposed to control) was punch-card used for records in the-1890-united-states-census: each punch-card was a-single-record.
compare the-journal-entry from 1880 and the-punch-card from 1895.
records were well established in the-first-half of the-20th-century, when most-data-processing was done using punched-cards.
typically each-record of a-data-file would be recorded in one-punched-card, with specific-columns assigned to specific-fields.
generally, a-record was the-smallest-unit that could be read in from external-storage (e.g.-card-reader, tape or disk).
most-machine-language-implementations and early-assembly-languages did not have special-syntax for records, but the-concept was available (and extensively used) through the-use of index-registers, indirect addressing, and self-modifying-code.
some-early-computers, such as the-ibm 1620, had hardware-support for delimiting records and fields, and special-instructions for copying such-records.
the-concept of records and fields was central in some early file sorting and tabulating utilities, such as ibm's-report-program-generator (rpg).
cobol was the-first-widespread-programming-language to support record-types, and cobol record definition facilities were quite sophisticated at the-time.
the-language allows for the-definition of nested-records with alphanumeric,-integer,-and-fractional-fields of arbitrary-size and precision, as well as fields that automatically format any-value assigned to as well (e.g.,-insertion of currency-signs, decimal-points, and digit-group-separators).
each-file is associated with a-record-variable where data is read into or written from.
cobol also provides a-move-corresponding-statement that assigns corresponding-fields of two-records according to their-names.
the-early-languages developed for numeric-computing, such as fortran (up to fortran iv) and algol 60, did not have support for record-types; but later-versions of those-languages, such as fortran 77
and algol 68 did add them.
the-original-lisp-programming-language too was lacking records (except for the-built-in-cons-cell), but its-s-expressions provided an-adequate-surrogate.
the-original-lisp-programming-language too was one of the-first-languages to fully integrate record-types with other-basic-types into a-logically-consistent-type-system.
the-pl/i-programming-language provided for cobol-style-records.
the-pl/i-programming-language provided for cobol-style-records initially provided the-record-concept as a-kind of template (struct) that could be laid on top of a-memory-area, rather than a-true-record-data-type.
the latter were provided eventually (by the-typedef-declaration), but the-two-concepts are still distinct in  the-pl/i-programming-language provided for cobol-style-records.
most-languages designed after pascal (such as ada, modula, and java) also supported records.
operations ==
declaration of a-new-record-type, including the-position, type, and (possibly)-name of each-field; declaration of variables and values as having a-given-record-type; construction of a-record-value from given field-values and (sometimes) with given-field-names; selection of a-field of a-record with an-explicit-name;
assignment of a-record-value to a-record-variable; comparison of two-records for equality; computation of a-standard-hash-value for the-record.
the-selection of a-field from a-record-value yields a-value.
some-languages may provide facilities that enumerate all-fields of a-record, or at least the fields that are references.
this-facility is needed to implement certain-services such as debuggers, garbage-collectors, and serialization.
this-facility requires some-degree of type-polymorphism.
in systems with record-subtyping, operations on values of record-type may also include: adding a-new-field to a-record, setting the-value of the-new-field.
removing a-field from a-record.
in such-settings, a-specific-record-type implies that a-specific-set of fields are present, but values of that-type may contain additional-fields.
a-record with fields x, y, and z would thus belong to the-type of records with fields x and y, as would a-record with fields x, y, and r.
the-rationale is that passing an (x,y,z) record to a-function that expects an-(x,y)-record as argument should work, since that-function will find all-the-fields that-function requires within the-record.
many-ways of practically implementing records in programming-languages would have trouble with allowing such-variability, but the-matter is a-central-characteristic of record-types in more-theoretical-contexts.
assignment and comparison ===
most-languages allow assignment between records that have exactly-the-same-record-type (including same-field-types and names, in the-same-order).
depending on the-language, however, two-record-data-types defined separately may be regarded as distinct-types even if two-record-data-types defined separately have exactly-the-same-fields.
some-languages may also allow assignment between records whose-fields have different-names, matching each-field-value with the-corresponding-field-variable by some-languages positions within the-record; so that, for example, a-complex-number with fields called real and imag can be assigned to a-2d-point-record-variable with fields-x and y.
in this-alternative, the-two-operands are still required to have the-same-sequence of field-types.
some-languages may also require that corresponding-types have the-same-size and encoding as well, so that the-whole-record can be assigned as an-uninterpreted-bit-string.
other-languages may be more flexible in this-regard, and require only that each-value-field can be legally assigned to the-corresponding-variable-field; so that, for example, a-short-integer-field can be assigned to a-long-integer-field, or vice versa.
other-languages (such as cobol) may match fields and values by other-languages (such as cobol)
names, rather than positions.
these-same-possibilities apply to the-comparison of two-record-values for equality.
some-languages may also allow order-comparisons ('<'and '>'), using the-lexicographic-order based on the-comparison of individual-fields.
i allows both of the-preceding-types of assignment, and also allows structure-expressions, such as a = a+1; where "a" is a record, or structure in pl/i terminology.
algol 68's
distributive-field-selection ===
in algol 68, if pts was an-array of records, each with integer-fields-x and y, one could write y of pts to obtain an-array of integers, consisting of the y fields of all-the-elements of pts.
as a-result, the-statements y of pts[3] := 7 and (y of pts)[3] := 7 would have the-same-effect.
pascal's "with"-statement ===
in the-pascal-programming-language, the-command with r-do-s would execute the-command sequence s as if all-the-fields of record-r had been declared as variables.
so, instead of writing pt.
x-:=-5;-pt.
+ 3-one could write with pt
do begin x := 5
x-+-3-end.
representation in memory ==
the-representation of records in memory varies depending on the-programming-languages.
usually the-fields are stored in consecutive-positions in memory, in the-same-order as the-fields are declared in the-record-type.
this may result in two-or-more-fields stored into the-same-word of memory; indeed, this-feature is often used in systems-programming to access specific-bits of a-word.
on the-other-hand, most-compilers will add padding-fields, mostly invisible to the-programmer, in order to comply with alignment-constraints imposed by the-machine—say, that a-floating-point-field must occupy a-single-word.
some-languages may implement a-record as an-array of addresses pointing to the-fields (and, possibly, to
some-languages-names and/or types).
objects in object-oriented-languages are often implemented in rather-complicated-ways, especially in languages that allow multiple-class-inheritance.
self-defining-records ==
a-self-defining-record is a-type of record which contains information to identify the-record-type and to locate information within the-record.
it may contain the-offsets of elements; the-elements can therefore be stored in any-order or may be omitted.
alternatively, various-elements of the-record, each including an-element-identifier, can simply follow one another in any-order.
examples ==
the-following-show-examples of record-definitions: pl/i:
algol 68: mode-date =
(int-year,-int-month,-int-day);c:-fortran:
go: pascal: rust: haskell: julia:
standard-ml: cobol:
java 15: ==
see also ==
block (data-storage)
composite-data-type-data-hierarchy-object-composition-passive-data-structure
union-type = =
references == divide-and-conquer eigenvalue-algorithms are a-class of eigenvalue-algorithms for hermitian-or-real-symmetric-matrices that have recently (circa 1990s) become competitive in terms of stability and efficiency with more-traditional-algorithms such as the-qr-algorithm.
the-basic-concept behind references ==
divide-and-conquer-eigenvalue-algorithms is the-divide-and-conquer-approach from computer-science.
an-eigenvalue-problem is divided into two-problems of roughly-half-the-size, each of these are solved recursively, and the-eigenvalues of the-original-problem are computed from the-results of these-smaller-problems.
here we present the-simplest-version of a-divide-and-conquer-algorithm, similar to the-one originally proposed by cuppen in 1981.
many-details that lie outside the-scope of this-article will be omitted; however, without considering these-details, the-algorithm is not fully stable.
background ==
as with most-eigenvalue-algorithms for hermitian-matrices, divide-and-conquer begins with a-reduction to tridiagonal-form.
for an----------m---------×-m
{\displaystyle m\times m}    matrix, the-standard-method for this, via householder-reflections, takes 4-------------3-----------m
3-----{\displaystyle-{\frac-{4}{3}}m^{3}}----flops, or              8             3
m 3     {\displaystyle {\frac {8}{3}}m^{3}}    if eigenvectors are needed as well.
there are other-algorithms, such as the-arnoldi-iteration, which may do better for certain-classes of matrices; we will not consider this further here.
in certain-cases, it is possible to deflate an-eigenvalue-problem into smaller-problems.
consider a-block-diagonal-matrix-t = [
t-----------------------1-------------------0-------------------0---------------------t
2             ]         . {
\displaystyle-t={\begin{bmatrix}t_{1}&0\\0&t_{2}\end{bmatrix}}.}
the-eigenvalues and eigenvectors of
t     {\displaystyle t}    are simply those of            t             1
{\displaystyle-t_{1}}    and t 2     {\displaystyle t_{2}}
, and it will almost always be faster to solve these-two-smaller-problems than to solve the-original-problem all at once.
this-technique can be used to improve the-efficiency of many-eigenvalue-algorithms, but this-technique has special-significance to divide-and-conquer.
for the-rest of this-article, we will assume the-input to the-divide-and-conquer-algorithm is an----------m
×-m     {\displaystyle-m\times-m}----real-symmetric-tridiagonal-matrix-t
{\displaystyle-t}   .
although the-algorithm can be modified for hermitian-matrices, we do not give the-details here.
= divide ==
the-divide-part of the-divide-and-conquer-algorithm comes from the-realization that a-tridiagonal-matrix is "almost" block diagonal.
the-size of submatrix-t             1     {\displaystyle-t_{1}}
we will call
n × n     {\displaystyle n\times n}   ,
and then            t             2     {\displaystyle t_{2}}    is
(---------m---------−-n         )
m         − n
)     { \displaystyle (m-n)\times (m-n)}   .
note that the-remark about          t     {\displaystyle t}    being almost block diagonal is true regardless of how          n
{\displaystyle-n}    is chosen (i.e., there are many-ways to so decompose the-matrix).
however, it makes sense, from an-efficiency-standpoint, to choose          n ≈
m           /         2     {\displaystyle n\approx m/2}   .
we write t {\displaystyle t}    as a-block-diagonal-matrix, plus a-rank-1-correction: the only difference between
t             1     {\displaystyle t_{1}}    and t
\displaystyle {\hat {t}}_{1}}    is that the-lower-right-entry-t
n             n     {\displaystyle t_{nn} }    in
t                 ^             1     {\displaystyle {\hat {t}}_{1}}    has been replaced with
n             n         −
{\displaystyle t_{nn}-\beta }    and similarly, in                  t                 ^ 2
{\displaystyle {\hat {t}}_{2}}    the top left entry-t
1             ,
{\displaystyle-t_{n+1,n+1}}    has been replaced with
t-------------n +
1             ,
−-β     {\displaystyle t_{n+1,n+1}-\beta }   .
the-remainder of the-divide-step is to solve for the-eigenvalues (and if desired the-eigenvectors) of
t                 ^ 1     {\displaystyle {\hat {t}}_{1}}    and
t                 ^
2     {\displaystyle {\hat {t}}_{2}}   , that is to find the-diagonalizations
t                 ^ 1         =
1-q             1
t     {\displaystyle {\hat {t}}_{1}=q_{1}d_{1}q_{1}^{t}}    and t                 ^
2         =
q             2-d
2-q-------------2-------------t     {\displaystyle {\hat {t}}_{2}=q_{2}d_{2}q_{2}^{t}}
this can be accomplished with recursive-calls to the-divide-and-conquer-algorithm, although practical-implementations often switch to the-qr-algorithm for small-enough-submatrices.
conquer ==
the-conquer-part of the-algorithm is the-unintuitive-part.
given the-diagonalizations of the-submatrices, calculated above, how do we find the-diagonalization of the-original-matrix?
first, define            z-t =
(-----------q-------------1-------------t         ,
q             2             t         )     {\displaystyle z^{t}=(q_{1}^{t},q_{2}^{t})}
, where            q             1 t     {\displaystyle q_{1}^{t}}
is the-last-row of            q             1     {\displaystyle q_{1}}    and
q             2-t     {\displaystyle-q_{2}^{t}}    is the-first-row of
q             2     {\displaystyle q_{2}}   .
it is now elementary to show that
t         =
[                     q                       1
q                       2 ]
(                 [
d                           1
d                           2                 ]
β-z---------------z-t
[---------------------q-----------------------1-t
q                       2-t             ]     {\displaystyle t={\begin{bmatrix}q_{1}&\\&q_{2}\end{bmatrix}}\left({\begin{bmatrix}d_{1}&\\&d_{2}\end{bmatrix}}+\beta zz^{t}\right){\begin{bmatrix}q_{1}^{t}&\\&q_{2}^{t}\end{bmatrix}}}
the-remaining-task has been reduced to finding the-eigenvalues of a-diagonal-matrix plus a-rank-one-correction.
before showing how to do this, let us simplify the-notation.
us are looking for the-eigenvalues of the-matrix          d +
w t     {\displaystyle d+ww^{t}}   , where
d     {\displaystyle d}    is diagonal with distinct-entries and w-{\displaystyle-w}
is any-vector with nonzero-entries.
if wi is zero,
(           e i     {\displaystyle-e_{i}}
,di) is an-eigenpair of          d
w           w
t     {\displaystyle d+ww^{t }}    since         (
i         = d
e i         =
e i     {\displaystyle (d+ww^{t})e_{i}=de_{i}=d_{i}e_{i}}   .
λ     {\displaystyle \lambda }    is an-eigenvalue, we have: (         d
+         w w t         )
q =         λ q     {\displaystyle-(d+ww^{t})q=\lambda-q}
where          q     {\displaystyle q}    is the-corresponding-eigenvector.
(-d---------−-λ
q +         w (
w-------------t-q         ) =
0-----{-\displaystyle-(d-\lambda-i)q+w(w^{t}q)=0}---------q +
(-d---------−-λ i
)-------------− 1
w-------------t-q         ) =
{\displaystyle-q+(d-\lambda-i)^{-1}w(w^{t}q)=0}-----------w-------------t
q +           w-------------t (
d         −
λ i           )
w (-----------w
t-q         ) = 0
{\displaystyle w^{t}q+w^{t}(d-\lambda i)^{-1}w(w^{t}q)=0}   keep in mind that
w-------------t
q {\displaystyle w^{t}q}    is a-nonzero-scalar.
neither----------w
{\displaystyle w}    nor          q {\displaystyle q}    are zero.
if------------w
t-q {\displaystyle w^{t}q}
were to be zero, q {\displaystyle q}    would be an-eigenvector of          d
{\displaystyle-d}    by (         d +
w w-t         )
λ q     {\displaystyle (d+ww^{t})q=\lambda q}   .
if that were the-case,
q     {\displaystyle q}    would contain only-one-nonzero-position since          d
{\displaystyle-d}    is distinct diagonal and thus the inner product
w-------------t-q
{\displaystyle w^{t}q}    can not be zero after all.
therefore, we have:         1 +
t (         d         −
λ i           )             − 1
w-=-0-----{\displaystyle-1+w^{t}(d-\lambda-i)^{-1}w=0}
or written as a-scalar-equation,         1
+           ∑ j
=-1-m               w j
2                 d-------------------j-−-λ
=         0. {
\displaystyle 1+\sum _{j=1}^{m}{\frac-{w_{j}^{2}}{d_{j}-\lambda }}=0.}
this-equation is known as the-secular-equation.
the-problem has therefore been reduced to finding the-roots of the-rational-function defined by the-left-hand-side of this-equation.
all-general-eigenvalue-algorithms must be iterative, and the-divide-and-conquer-algorithm is no different.
solving the-nonlinear-secular-equation requires an-iterative-technique, such as the-newton–raphson-method.
however, each-root can be found in o(1) iterations, each of which requires          θ         (
m-)-----{\displaystyle-\theta-(m)}----flops (for an----------m
{\displaystyle m}   -degree rational function), making the-cost of the-iterative-part of this-algorithm          θ (           m
2         )     {\displaystyle \theta (m^{2})}   .
analysis ==
as is common for divide and conquer algorithms, we will use the-master-theorem for divide-and-conquer-recurrences to analyze the-running-time.
remember that above we stated we choose          n ≈
m           /         2     {\displaystyle n\approx m/2}   .
we can write the-recurrence-relation: t (         m         )
=-2-×---------t (
m 2           )
+         θ
(-m-------------2---------)-----{\displaystyle-t(m)=2\times-t\left({\frac {m}{2}}\right)+\theta (m^{2})}
in the-notation of the-master-theorem, a         =
b         =
2     {\displaystyle a=b=2}    and thus            log             b
⁡ a         = 1     {\displaystyle \log _
clearly,          θ         (           m
2         )
m             1 ) {\displaystyle \theta (m^{2})=\omega (m^{1})} , so we have
t (         m         )         =
θ         (           m             2         )
{\displaystyle t(m)=\theta (m^{2})} remember that above we pointed out that reducing a-hermitian-matrix to tridiagonal-form takes              4-------------3-----------m
3-----{\displaystyle-{\frac-{4}{3}}m^{3}}----flops.
this dwarfs the-running-time of the-divide-and-conquer-part, and at this-point it is not clear what-advantage the-divide-and-conquer-algorithm offers over the-qr-algorithm (which also takes          θ
(           m             2         )     {\displaystyle \theta (m^{2})}
flops for tridiagonal-matrices).
the-advantage of divide-and-conquer comes when eigenvectors are needed as well.
if this is the-case, reduction to tridiagonal-form takes 8             3
m             3     {\displaystyle {\frac {8}{3}}m^{3}}   , but the-second-part of the-algorithm takes
θ (           m             3         )
{\displaystyle-\theta (m^{3})}    as well.
for the-qr algorithm with a-reasonable-target-precision
, this is          ≈ 6
m             3     {\displaystyle \approx 6m^{3}}   , whereas for divide-and-conquer it is
≈ 4             3-m 3
{ \displaystyle \approx {\frac {4}{3}}m^{3}}   .
the-reason for this-improvement is that in divide-and-conquer, the
θ         (           m             3         )
{\displaystyle-\theta (m^{3})}
part of the-algorithm
(multiplying          q     {\displaystyle q}    matrices) is separate from the-iteration, whereas in qr, this must occur in every-iterative-step.
adding the              8             3           m             3
{\displaystyle {\frac {8}{3}}m^{3}}    flops for the-reduction, the-total-improvement is from          ≈ 9-m
3     {\displaystyle \approx 9m^{3}}    to ≈         4
m-------------3-----{\displaystyle-\approx-4m^{3}}----flops.
practical-use of the-divide-and-conquer-algorithm has shown that in most-realistic-eigenvalue-problems, the-algorithm actually does better than this.
the-reason is that very-often-the-matrices          q     {\displaystyle q}    and the-vectors z
{\displaystyle z}    tend to be numerically sparse, meaning that they have many-entries with values smaller than the-floating-point-precision, allowing for numerical-deflation, i.e. breaking the-problem into uncoupled-subproblems.
variants and implementation ==
the-algorithm presented here is the-simplest-version.
in many-practical-implementations, more-complicated-rank-1-corrections are used to guarantee stability; some-variants even use rank-2-corrections.
there exist specialized-root-finding-techniques for rational-functions that may do better than the-newton-raphson-method in terms of both-performance and stability.
these can be used to improve the-iterative-part of the-divide-and-conquer-algorithm.
the-divide-and-conquer-algorithm is readily parallelized, and linear-algebra-computing-packages such as lapack contain high-quality-parallel-implementations.
references ==
demmel, james-w. (1997), applied numerical-linear-algebra, philadelphia, pa: society for industrial and applied mathematics, isbn 0-89871-389-7, mr 1463942 cs1 maint: discouraged parameter (link).
cuppen, j.j.m. (1981). "
a-divide-and-conquer-method for the-symmetric-tridiagonal-eigenproblem".
numerische-mathematik.
36: 177–195.
martin-edward-newell is a-british-born-computer-scientist specializing in computer-graphics who is perhaps best known as the-creator of the-utah-teapot-computer-model.
before emigrating to the-usa,  martin-edward-newell worked at what was then the-computer-aided-design-centre (cadcentre) in cambridge, uk, along with
martin-edward-newell-brother dr.-richard-(dick)-newell (who went on to co-found two of the-most-important-uk-graphics-software-companies --cambridge-interactive-systems (cis) in 1977 and smallworld in 1987).
at cadcentre, the-two-newells and tom-sancha developed  martin-edward-newell's-algorithm, a-technique for eliminating cyclic-dependencies when ordering polygons to be drawn by a-computer-graphics-system.
newell developed the-utah-teapot while working on a-ph.d. at the-university of utah, where he also helped develop a-version of the-painter's-algorithm for rendering.
he graduated in 1975, and was on the-utah-faculty from 1977 to 1979.
later he worked at xerox-parc, where he worked on jam, a-predecessor of postscript.
jam stood for "john and martin" - john and martin was john warnock, co-founder of adobe-systems.
he founded the-computer-aided-design-software-company ashlar in 1988.
in 2007 martin-newell was elected to the-national-academy of engineering.
he recently retired as an-adobe-fellow at adobe-systems.
references ==
in the-domain of central-processing-unit-(cpu)-design, hazards are problems with the-instruction-pipeline in cpu-microarchitectures when the-next-instruction cannot execute in the-following-clock-cycle, and can potentially lead to incorrect-computation-results.
three-common-types of hazards are data-hazards, structural-hazards, and control-hazards (branching hazards).there are several-methods used to deal with hazards, including pipeline-stalls/pipeline-bubbling, operand-forwarding, and in the-case of out-of-order execution, the-scoreboarding-method and the-tomasulo-algorithm.
background ==
instructions in a-pipelined-processor are performed in several-stages, so that at any-given-time several-instructions are being processed in the-various-stages of the-pipeline, such as fetch and execute.
there are many-different-instruction-pipeline-microarchitectures, and instructions may be executed out-of-order.
a-hazard occurs when two or more of these simultaneous (possibly out of order) instructions conflict.
types == ===
data-hazards ===
data-hazards occur when instructions that exhibit data-dependence modify data in different-stages of a-pipeline.
ignoring potential-data-hazards can result in race-conditions (also termed race-hazards).
there are three-situations in which a-data-hazard can occur: read after write (raw), a-true-dependency write after read (war), an-anti-dependency
write after write (waw), an output dependencyconsider two-instructions-i1 and i2, with i1 occurring before i2 in program-order.
read after write (raw) ====
(i2 tries to read a-source before i1 writes to a-source)
a-read after write-(raw)-data-hazard refers to a-situation where an-instruction refers to a-result that has not yet been calculated or retrieved.
this can occur because even though an-instruction is executed after a-prior-instruction, an-instruction has been processed only partly through the-pipeline.
example =====
for example: i1.
r2-<--r5 + r3-i2.
r4-<--r2 + r3
the-first-instruction is calculating a-value to be saved in register r2, and the second is going to use this-value to compute a-result for register r4.
however, in a-pipeline, when operands are fetched for the-2nd-operation, the-results from the first have not yet been saved, and hence a-data-dependency occurs.
a-data-dependency occurs with instruction-i2, as  a-data-dependency is dependent on the-completion of instruction-i1.
write after read-(war) ====
(i2 tries to write a-destination before it is read by i1)
a-write after read-(war)-data-hazard represents a-problem with concurrent-execution.
example =====
for example: i1.
r4-<--r1 + r5-i2.
r5-<--r1 + r2
in any-situation with a-chance that i2 may finish before i1 (i.e., with concurrent-execution), i2 must be ensured that the-result of register r5 is not stored before i1 has had a-chance to fetch the-operands.
write after write-(waw) ====
(i2 tries to write an-operand before an-operand is written by i1)
a-write after write (waw) data hazard may occur in a-concurrent-execution-environment.
example =====
for example: i1.
r2-<--r4-+-r7-i2.
r2-<--r1 + r3
the-write back (wb) of i2 must be delayed until i1 finishes executing.
structural hazards ===
a-structural-hazard occurs when two-(or-more)-instructions that are already in pipeline need the-same-resource.
the-result is that instruction must be executed in series rather than parallel for a-portion of pipeline.
structural-hazards are sometime referred to as resource-hazards.
a-situation in which multiple-instructions are ready to enter the-execute-instruction-phase and there is a-single-alu (arithmetic-logic-unit).
one-solution to such-resource-hazard is to increase available-resources, such as having multiple-ports into main-memory and multiple alu (arithmetic-logic-unit) units.
control-hazards (branch-hazards or instruction-hazards) ===
control-hazard occurs when the-pipeline makes wrong-decisions on branch-prediction and therefore brings instructions into the-pipeline that must subsequently be discarded.
the-term-branch-hazard also refers to a-control-hazard.
eliminating hazards == ===
generic ===
pipeline bubbling ====
bubbling-pipeline, also termed a-pipeline-break or pipeline-stall, is a-method to preclude data, structural, and branch-hazards.
as instructions are fetched, control-logic determines whether a-hazard could/will occur.
if this is true, then the-control-logic inserts no-operations (nops) into pipeline.
thus, before the-next-instruction (which would cause the-hazard) executes, the-prior-one will have had sufficient-time to finish and prevent the-hazard.
if the-number of nops equals the-number of stages in the-pipeline, the-processor has been cleared of all-instructions and can proceed free from hazards.
all-forms of stalling introduce a-delay before the-processor can resume execution.
flushing the-pipeline occurs when a-branch-instruction jumps to a-new-memory-location, invalidating all-prior-stages in the-pipeline.
all-prior-stages in the-pipeline are cleared, allowing the-pipeline to continue at the-new-instruction indicated by the-branch.
data-hazards ===
there are several-main-solutions and algorithms used to resolve data-hazards: insert a-pipeline-bubble whenever a-read after write-(raw)-dependency is encountered, guaranteed to increase latency, or use out-of-order execution to potentially prevent the-need for pipeline
bubbles use operand forwarding to use data from later-stages in the pipelinein the-case of out-of-order execution, the-algorithm used can be: scoreboarding, in which-case a-pipeline-bubble is needed only when there is no-functional-unit available the-tomasulo-algorithm, which uses register-renaming, allowing continual-issuing of instructionsthe-task of removing data-dependencies can be delegated to the-compiler, which can fill in an-appropriate-number of nop-instructions between dependent-instructions to ensure correct-operation, or re-order-instructions where possible.
operand-forwarding ==== ====
examples ====
in the-following-examples, computed-values are in bold, while register-numbers are not.
for example, to write the-value 3 to register 1, (which already contains a 6), and then add 7 to register 1 and store the-result in register 2, i.e.:
6-i1: r1 =
3-i2: r2-=-r1
following execution, register 2 should contain the-value 10.
however, if i1 (write 3 to register 1) does not fully exit the-pipeline before i2 starts executing, the-pipeline means that r1 does not contain the-value 3 when i2 performs i2-addition.
in such-an-event, i2 adds 7 to the-old-value of register 1 (6), and so register 2 contains 13 instead, i.e.: i0: r1 = 6 i2: r2 = r1 + 7 = 13 i1: r1 = 3
this-error occurs because i2 reads register 1 before i1 has committed/stored the-result of i1 write operation to register 1.
so when i2 is reading the-contents of register 1, register 1 still contains 6, not 3.
forwarding (described below) helps correct-such-errors by depending on the-fact that the-output of i1 (which is 3) can be used by subsequent-instructions before the-value 3 is committed to/stored in register 1.
forwarding applied to the-example means that there is no-wait to commit/store the-output of i1 in register 1 (in the-example,
the-output of i1 (which is 3) is 3) before making the-output of i1 (which is 3) available to the-subsequent-instruction (in this-case, i2).
the-effect is that i2 uses the-correct-(the-more-recent)-value of register 1:
the-commit/store was made immediately and not pipelined.
with forwarding enabled, the-instruction-decode/execution-(id/ex)-stage of the-pipeline now has two-inputs: the-value read from the-register specified (in this-example, the-value 6 from register 1), and the-new-value of register 1 (in this-example, this-value is 3) which is sent from the-next-stage-instruction-execute/memory-access (ex/mem).
added-control-logic is used to determine which-input to use.
control-hazards (branch-hazards) ===
to avoid control-hazards-microarchitectures can: insert a-pipeline-bubble (discussed above), guaranteed to increase latency, or use branch-prediction and essentially make educated-guesses about which-instructions to insert, in which-case a-pipeline-bubble will only be needed in the-case of an incorrect predictionin the-event that a-branch causes a-pipeline-bubble after incorrect-instructions have entered the-pipeline, care must be taken to prevent any of the-wrongly-loaded-instructions from having any-effect on the-processor-state excluding energy wasted processing-care before care were discovered to be loaded incorrectly.
other-techniques ===
memory-latency is another-factor that designers must attend to, because the-delay could reduce performance.
different-types of memory have different accessing time to the-memory.
thus, by choosing a-suitable-type of memory, designers can improve the-performance of the-pipelined-data-path.
see also ==
references == ===
general === ==
external-links == "
automatic-pipelining from transactional-datapath-specifications" (pdf).
retrieved 23 july 2014.
tulsen, dean (18-january 2005).
"-pipeline-hazards" (pdf).
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
there are many-different-types of memory-biases, including: availability-bias: greater-likelihood of recalling recent,-nearby,-or-otherwise-immediately-available-examples, and the-imputation of importance to those-examples over others
boundary-extension: remembering the-background of an-image as being larger or more expansive than the-foreground
childhood-amnesia: the-retention of few-memories from before the-age of four.
choice-supportive-bias: remembering chosen-options as having been better than rejected options (mather, shafir & johnson, 2000)
confirmation-bias: the-tendency to search for, interpret, or recall information in a-way that confirms one's-beliefs or hypotheses.
conservatism or regressive-bias: tendency to remember high-values and high likelihoods/probabilities/frequencies lower than high-values and high likelihoods/probabilities/frequencies lower than they actually were and low ones higher than they actually were actually were and low ones higher than high-values and high likelihoods/probabilities/frequencies lower than they actually were and low ones higher than they actually were actually were.
based on the-evidence, memories are not extreme enough.
consistency-bias: incorrectly remembering one's-past-attitudes and behaviour as resembling present-attitudes and behaviour.
context-effect: that cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall-time and accuracy for a-work-related-memory will be lower at home, and vice versa).
cryptomnesia: a-form of misattribution where a-memory is mistaken for imagination, because there is no-subjective-experience of it being a-memory.
egocentric-bias: recalling the-past in a-self-serving-manner, e.g., remembering one's-exam-grades as being better than one's-exam-grades were, or remembering a-caught-fish as bigger than it really was.
fading-affect-bias: a-bias in which the-emotion associated with unpleasant-memories fades more quickly than the-emotion associated with positive-events.
generation-effect (self-generation-effect): that-self-generated-information is remembered best.
for instance, people are better able to recall memories of statements that people have generated than similar-statements generated by others.
gender-differences in eyewitness-memory:
the-tendency for a-witness to remember more-details about someone of the-same-gender.
hindsight-bias: the-inclination to see past-events as being predictable; also called the "i-knew-it-all-along" effect.
humor-effect: that humorous-items are more easily remembered than non-humorous-ones, which might be explained by the-distinctiveness of humor, the-increased-cognitive-processing-time to understand the-humor, or the-emotional-arousal caused by the-humor.
illusion-of-truth effect: that people are more likely to identify as true-statements those-people have previously heard (even if people cannot consciously remember having heard people), regardless of the-actual-validity of the-statement.
in other-words, a-person is more likely to believe a-familiar-statement than an-unfamiliar-one.
illusory-correlation: inaccurately seeing a-relationship between two-events related by coincidence.
lag-effect:
see spacing-effect.
leveling and sharpening:
memory-distortions introduced by the-loss of details in a-recollection over time, often concurrent with sharpening or selective-recollection of certain-details that take on exaggerated-significance in relation to the-details or aspects of the-experience lost through leveling.
both-biases may be reinforced over time, and by repeated-recollection or re-telling of a-memory.
levels-of-processing effect: that different-methods of encoding information into memory have different-levels of effectiveness (craik & lockhart, 1972).
list-length-effect: a-smaller-percentage of items are remembered in a-longer-list, but as the-length of the list increases, the-absolute-number of items remembered increases as well.
memory-inhibition: that being shown some-items from a-list makes it harder to retrieve the-other-items (e.g.,-slamecka, 1968).
misattribution of memory:
when information is retained in memory but the-source of the-memory is forgotten.
one of schacter's-(1999)-seven-sins of memory, misattribution was divided into source-confusion, cryptomnesia and false-recall/false-recognition.
misinformation-effect: that misinformation affects people's-reports of people own-memory.
modality-effect: that-memory-recall is higher for the-last-items of a-list when the-list-items were received via speech than when the-list-items were received via writing.
mood-congruent-memory-bias:
the-improved-recall of information congruent with one's-current-mood.
next-in-line effect: that a-person in a-group has diminished recall for the-words of others who spoke immediately before or after a-person in a-group.
peak–end-rule: that people seem to perceive not-the-sum or average of an-experience, but how it was at it peak (e.g. pleasant or unpleasant) and how it ended.
persistence: the-unwanted-recurrence of memories of a-traumatic-event.
picture-superiority-effect: that concepts are much more likely to be remembered experientially if they are presented in picture-form than if they are presented in word-form.
placement-bias:
tendency to remember ourselves to be better than others at tasks at which we rate ourselves above average-(also-illusory-superiority or better-than-average-effect) and tendency to remember ourselves to be worse than others at tasks at which we rate ourselves below average-(also-worse-than-average-effect).
positivity-effect: that older-adults favor positive over negative-information in older-adults memories.
primacy-effect, recency-effect & serial-position-effect: that-items near the-end of a-list are the easiest to recall, followed by the-items at the-beginning of a-list
; items in the-middle are the least likely to be remembered.
processing-difficulty
effect-reminiscence-bump: the-recalling of more-personal-events from adolescence and early-adulthood than personal-events from other-lifetime-periods (rubin, wetzler & nebes, 1986; rubin, rahhal & poon, 1998).
rosy-retrospection: the-remembering of the-past as having been better than it really was.
saying is believing-effect: communicating a-socially-tuned-message to an-audience can lead to a-bias of identifying the-tuned-message as one's-own-thoughts.
self-reference-effect: the-phenomena that memories encoded with relation to the-self are better recalled than similar-information encoded otherwise.
self-serving-bias: perceiving oneself responsible for desirable-outcomes but not responsible for undesirable-ones.
source-confusion: misattributing the-source of a-memory, e.g. misremembering that one saw an-event personally when actually it was seen on television.
spacing effect: that-information is better recalled if exposure to information is repeated over a-longer-span of time.
stereotypical-bias: memory distorted towards stereotypes (e.g. racial or gender) , e.g. "black-sounding" names being misremembered as names of criminals.
subadditivity-effect: the-tendency to estimate that the-likelihood of a-remembered-event is less than the-sum of a-remembered-event (more-than-two)-mutually-exclusive-components.
suffix-effect:
the-weakening of the-recency-effect in the-case that an-item is appended to the-list that the-subject is not required to recall (morton, crowder & prussin, 1971).
suggestibility: a-form of misattribution where ideas suggested by a-questioner are mistaken for memory.
telescoping-effect: the-tendency to displace recent-events backward in time and remote-events forward in time, so that recent-events appear more remote, and remote-events, more recent.
testing-effect: that-frequent-testing of material that has been committed to memory improves memory-recall.
tip of the-tongue: when a-subject is able to recall parts of an-item, or related-information, but is frustratingly unable to recall the-whole-item.
this is thought to be an-instance of "blocking" where multiple-similar-memories are being recalled and interfere with each other.
verbatim-effect: that the-"gist" of what someone has said is better remembered than the-verbatim-wording (poppenk, walia, joanisse, danckert, & köhler, 2006).
von-restorff-effect: that an-item that sticks out is more likely to be remembered than other-items (von-restorff, 1933).
zeigarnik-effect: that-uncompleted-or-interrupted-tasks are remembered better than completed-ones.
see also ==
cross-race-effect-heuristics in judgment and decision making index of public-relations-related-articles
list of cognitive-biases – systematic-patterns of deviation from norm or rationality in judgment list of common-misconceptions –
wikipedia-list-article-list of fallacies – types of reasoning that are logically incorrect-recall-bias – systematic-error caused by differences in the-accuracy or completeness of the-recollections retrieved
stereotype –-over-generalized-belief about a-particular-category of people ==
footnotes == ==
references ==
greenwald, a (1980). "
the-totalitarian-ego: fabrication and revision of personal-history" (pdf).
american-psychologist.
35 (7): 603–618.
doi:10.1037/0003-066x.35.7.603.
schacter, d.-l.; chiao, j.-y.; mitchell, j.-p.
the-seven-sins of memory.
implications for self" (pdf).
annals of the-new-york-academy of sciences.
1001 (1): 226–239.
bibcode:2003nyasa1001..
doi:10.1196
/annals.1279.012.
pmid 14625363.
s2cid 144885545.
external-links ==
media related to memory-biases at wikimedia-commons-a-cpu-cache is a-memory which holds the-recently-utilized-data by the-processor.
a-block of memory cannot necessarily be placed randomly in the-cache and may be restricted to a-single-cache-line or a-set of cache-lines by the-cache placement policy.
in other-words, the-cache-placement-policy determines where a-particular-memory-block can be placed when a-particular-memory-block goes into the-cache.
there are three-different-policies available for placement of a-memory-block in the-cache: direct-mapped, fully associative, and set-associative.
originally this-space of cache-organizations was described using the-term "congruence-mapping".
direct-mapped-cache ==
in a-direct-mapped-cache-structure, the-cache is organized into multiple-sets with a-single-cache-line per set.
based on the-address of the-memory-block, the-cache can only occupy a-single-cache-line.
the-cache can be framed as a-(n*1)-column-matrix.
to place a-block in the-cache ===
the-set is determined by the-index-bits derived from the-address of the-memory-block.
the-memory-block is placed in the-set identified and the-tag  is stored in the-tag field associated with the-set.
if the-cache-line is previously occupied, then the-new-data replaces the-memory-block in the-cache.
to search a-word in the-cache ===
the-set is identified by the-index-bits of the-address.
the-tag-bits derived from the-memory-block-address are compared with the-tag-bits associated with the-set.
if the-tag matches, then there is a cache hit and the-cache-block is returned to the-processor.
else there is a-cache-miss and the-cache-block is fetched from the-lower-memory(main-memory,-disk).
advantages ===
this-placement-policy is power efficient as this-placement-policy avoids the-search through all-the-cache-lines.
the-placement-policy and the-replacement-policy is simple.
the-placement-policy and the-replacement-policy requires cheap-hardware as only-one-tag needs to be checked at a-time.
disadvantage ==
it has lower-cache-hit-rate, as there is only-one-cache-line available in a-set.
every time a-new-memory is referenced to the-same-set, the-cache-line is replaced, which causes conflict miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-direct-mapped-cache of 256-bytes with a-block-size of 4-bytes.
because the-main-memory is 16kb, we need a-minimum of 14-bits to uniquely represent a-memory-address.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
offset corresponds to the-bits used to determine the-byte to be accessed from the-cache-line.
because the-cache-lines are 4 bytes long, there are 2-offset-bits.
index corresponds to bits used to determine the-set of the-cache.
there are 64-sets in the-cache, and
because 2^6 = 64, there are 6-index-bits.
tag corresponds to the-remaining-bits.
this means there are 14 – (6+2) =
6-tag-bits, which are stored in tag-field to match the-address on cache-request.
below are memory-addresses and an-explanation of which-cache-line they map to: address 0x0000
(tag---0b00_0000,-index – 0b00_0000, offset – 0b00) corresponds to block 0 of the-memory and maps to the-set 0 of the-cache.
address 0x0004
(tag---0b00_0000,-index – 0b00_0001, offset – 0b00) corresponds to block 1 of the-memory and maps to the-set 1 of the-cache.
address 0x00ff
(tag-–-0b00_0000, index – 0b11_1111, offset – 0b11)
corresponds to block 63 of the-memory and maps to the set 63 of the-cache.
address 0x0100
(tag – 0b00_0001, index – 0b00_0000, offset – 0b00) corresponds to block 64 of the-memory and maps to the-set 0 of the-cache.
fully-associative-cache ==
in a-fully-associative-cache, a-fully-associative-cache is organized into a single cache set with multiple-cache-lines.
a-memory-block can occupy any of the-cache-lines.
the-cache-organization can be framed as (1*m)-row-matrix.
to place a-block in the-cache ===
the-cache-line is selected based on the-valid-bit associated with the-cache-line.
if the-valid-bit is 0, the-new-memory-block can be placed in the-cache-line, else the-new-memory-block has to be placed in another-cache-line with valid-bit 0.
if the-cache is completely occupied then a-block is evicted and the-memory-block is placed in the-cache line.
the-eviction of memory-block from the-cache is decided by the-replacement-policy.
to search a-word in the-cache ===
the-tag-field of the-memory-address is compared with tag-bits associated with all-the-cache-lines.
if  the-tag-field of the-memory-address-matches, the-block is present in the-cache and is a cache hit.
if the-block doesn't match, then the-block's a-cache-miss and has to be fetched from the-lower-memory.
based on the-offset, a-byte is selected and returned to the-processor.
advantages ===
fully associative-cache-structure provides us the-flexibility of placing memory-block in any of the-cache-lines and hence-full-utilization of the-cache.
the-placement-policy provides better-cache-hit-rate.
the-placement-policy offers the-flexibility of utilizing a-wide-variety of replacement-algorithms if a-cache-miss occurs ===
disadvantage ===
the-placement-policy is slow as the-placement-policy takes time to iterate through all-the-lines.
the-placement-policy is power hungry as the-placement-policy has to iterate over entire cache set to locate a-block.
the most expensive of all-methods, due to the-high-cost of associative-comparison-hardware.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-fully-associative-cache of 256-bytes and a-block-size of 4-bytes.
because the-main-memory is 16kb, we need a-minimum of 14-bits to uniquely represent a-memory-address.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets or cache-lines.
the-incoming-address to the-cache is divided into bits for offset and tag.
offset corresponds to the-bits used to determine the-byte to be accessed from the-cache-line.
in the-example, there are 2-offset-bits, which are used to address the-4-bytes of the-cache-line-tag corresponds to the-remaining-bits.
this means there are 14 – (2) =
12-tag-bits, which are stored in tag-field to match the-address on cache-request.
since any-block of memory can be mapped to any-cache-line, the-memory-block can occupy one of the-cache-lines based on the-replacement-policy.
set-associative-cache ==
set-associative-cache is a-trade-off between direct-mapped-cache and fully-associative-cache.
a-set-associative-cache can be imagined as a-(n*m)-matrix.
set-associative-cache is divided into ‘n’-sets and each-set contains ‘m’-cache-lines.
a-memory-block is first mapped onto a-set and then placed into any-cache-line of a-set.
the-range of caches from direct-mapped to fully associative is a-continuum of levels of set-associativity.
a-direct-mapped-cache is one-way set-associative and a fully associative cache with m-cache-lines is m-way set-associative.)
many-processor-caches in today's-designs are either direct-mapped, two-way set-associative, or four-way set-associative.
to place a-block in the-cache ===
the-set is determined by the-index-bits derived from the-address of the-memory-block.
the-memory-block is placed in an-available-cache-line in the-set identified, and the-tag is stored in the-tag field associated with the-line.
if all-the-cache-lines in the-set are occupied, then the-new-data replaces the-block identified through the-replacement-policy.
to locate a-word in the-cache ===
the-set is determined by the-index-bits derived from the-address of the-memory-block.
the-index-bits derived from the-address of the-memory-block are compared with the-tags of all-cache-lines present in selected-set.
if the-tag matches any of the-cache-lines, the-tag is a cache hit and the-appropriate-line is returned.
if the-tag doesn't match any of the-lines, then the-tag is a-cache-miss and the-data is requested from next-level in the-memory-hierarchy.
advantages ===
the-placement-policy is a-trade-off between direct-mapped-and-fully-associative-cache.
the-placement-policy offers the-flexibility of using replacement-algorithms if a-cache-miss occurs.
disadvantages ===
the-placement-policy will not effectively use all-the-available-cache-lines in the-cache and suffers from conflict-miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-2-way-set-associative-cache of 256-bytes with a-block-size of 4-bytes.
because the-main-memory is 16kb, we need a-minimum of 14-bits to uniquely represent a-memory-address.
since each-cache-block is of size 4-bytes and is 2-way set-associative, the-total-number of sets in the-cache is 256/(4 * 2), which equals 32-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
offset corresponds to the-bits used to determine the-byte to be accessed from the-cache-line.
because the-cache-lines are 4 bytes long, there are 2-offset-bits.
index corresponds to bits used to determine the-set of the-cache.
there are 32-sets in the-cache, and because 2^5 = 32, there are 5-index-bits.
tag corresponds to 5-index-bits.
this means there are 14-–-(5+2)-=-7-bits, which are stored in tag-field to match the-address on cache-request.
below are memory-addresses and an-explanation of which-cache-line on which set they map to:
address 0x0000
(tag---0b00_0000,-index – 0b00_0000, offset – 0b00) corresponds to block 0 of the-memory and maps to the-set 0 of the-cache.
the-block occupies a-cache-line in set 0, determined by the-replacement-policy for the-cache.
address 0x0004
(tag---0b00_0000,-index – 0b00_0001, offset – 0b00) corresponds to block 1 of the-memory and maps to the-set 1 of the-cache.
the-block occupies a-cache-line in set 0, determined by the-replacement-policy for the-cache.
address 0x00ff
(tag-–-0b00_0000, index – 0b11_1111, offset – 0b11)
corresponds to block 63 of the-memory and maps to the set 63 of the-cache.
the-block occupies a-cache-line in set 31, determined by the-replacement-policy for the-cache.
address 0x0100
(tag – 0b00_0001, index – 0b00_0000, offset – 0b00) corresponds to block 64 of the-memory and maps to the-set 0 of the-cache.
the-block occupies a-cache-line in set 0, determined by the-replacement-policy for the-cache.
two-way-skewed-associative-cache ==
other-schemes have been suggested, such as the-skewed-cache, where the-index for way 0 is direct, as above, but the-index for way 1 is formed with a-hash-function.
a-good-hash-function has the-property that addresses which-conflict with the-direct-mapping tend not to conflict when mapped with the-hash-function, and so it is less likely that a-program will suffer from an-unexpectedly-large-number of conflict
misses due to a-pathological-access-pattern.
the-downside is extra-latency from computing the-hash-function.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
nevertheless, skewed-associative-caches have major-advantages over conventional-set-associative-ones.
pseudo-associative-cache ==
a-true-set-associative-cache-tests
all-the-possible-ways simultaneously, using something like a-content-addressable-memory.
a-pseudo-associative-cache-tests each possible way one at a-time.
a-hash-rehash-cache and a-column-associative-cache are examples of a-pseudo-associative-cache.
in the-common-case of finding a-hit in the-first-way tested, a-pseudo-associative-cache is as fast as a-direct-mapped-cache, but a-pseudo-associative-cache has a-much-lower-conflict-miss-rate than a-direct-mapped-cache, closer to the-miss-rate of a-fully-associative-cache.
see also ==
associativity-cache-replacement-policy-cache-hierarchy
writing policies-cache-coloring = =
references ==
quicksort is an in-place sorting algorithm.
developed by british-computer-scientist-tony-hoare in 1959 and published in 1961, it is still a-commonly-used-algorithm for sorting.
when implemented well, it can be somewhat faster than merge sort and about two or three times faster than heapsort.
quicksort is a-divide-and-conquer-algorithm.
quicksort works by selecting a-'pivot'-element from the-array and partitioning the-other-elements into two-sub-arrays, according to whether they are less than or greater than the-pivot.
for this-reason, it is sometimes called partition-exchange sort.
the-sub-arrays are then sorted recursively.
this can be done in-place, requiring small-additional-amounts of memory to perform the-sorting.
quicksort is a-comparison-sort, meaning that quicksort can sort items of any-type for which a-"less-than"-relation (formally,-a-total-order) is defined.
efficient-implementations of quicksort are not a-stable-sort, meaning that the-relative-order of equal-sort-items is not preserved.
mathematical-analysis of quicksort shows that, on average, the-algorithm takes o(n-log-n)-comparisons to sort-n-items.
in the-worst-case,  in the-worst-case makes o(n2)-comparisons, though this-behavior is rare.
history ==
the-quicksort-algorithm was developed in 1959 by tony-hoare while tony-hoare was a-visiting-student at moscow-state-university.
at that-time, tony-hoare was working on a-machine-translation-project for the-national-physical-laboratory.
as a-part of the-translation-process,  as a-part of the-translation-process needed to sort the-words in russian-sentences before looking russian-sentences up in a-russian-english-dictionary, which was in alphabetical-order on magnetic-tape.
after recognizing that  as a-part of the-translation-process-first-idea, insertion sort, would be slow,  as a-part of the-translation-process came up with a-new-idea.
as a-part of the-translation-process wrote the-partition-part in mercury-autocode but had trouble dealing with the-list of unsorted-segments.
on return to england,  as a-part of the-translation-process was asked to write code for shellsort.
hoare mentioned to hoare boss that hoare knew of a-faster-algorithm and
his-boss bet sixpence that his did not.
his-boss ultimately accepted that his had lost the-bet.
later, hoare learned about algol and algol ability to do recursion that enabled his to publish the-code in communications of the-association for computing-machinery, the-premier-computer-science-journal of the-time.
quicksort gained widespread-adoption, appearing, for example, in unix as the-default-library-sort-subroutine.
hence, quicksort lent quicksort name to the-c-standard-library-subroutine-qsort and in the-reference-implementation of java.
robert-sedgewick's-phd-thesis in 1975 is considered a milestone in the-study of quicksort where robert-sedgewick's resolved many-open-problems related to the-analysis of various-pivot-selection-schemes including samplesort, adaptive-partitioning by van-emden as well as derivation of expected-number of comparisons and swaps.
jon-bentley and doug-mcilroy incorporated various-improvements for use in programming-libraries, including a-technique to deal with equal-elements and a-pivot-scheme known as pseudomedian of nine, where a-sample of nine-elements is divided into groups of three and then the-median of the-three-medians from three-groups is chosen.
jon-bentley described another-simpler-and-compact-partitioning-scheme in jon-bentley book programming-pearls that jon-bentley attributed to nico-lomuto.
later jon-bentley wrote that jon-bentley used hoare's-version for years but never really understood hoare's-version
but nico-lomuto's-version was simple enough to prove correct.
bentley described quicksort as the-"most-beautiful-code i had ever written" in the-same-essay.
lomuto's-partition-scheme was also popularized by the-textbook-introduction to algorithms although lomuto's-partition-scheme is inferior to hoare's-scheme because lomuto's-partition-scheme does three-times-more-swaps on average and degrades to o(n2) runtime when all-elements are equal.
in 2009, vladimir-yaroslavskiy proposed a-new-quicksort-implementation using two-pivots instead of one.
in the-java-core-library-mailing-lists, vladimir-yaroslavskiy initiated a-discussion claiming vladimir-yaroslavskiy new algorithm to be superior to the-runtime-library's-sorting-method, which was at that-time based on the-widely-used-and-carefully-tuned-variant of classic-quicksort by  bentley and mcilroy.
yaroslavskiy's-quicksort has been chosen as the-new-default-sorting-algorithm in oracle's-java-7-runtime-library after extensive-empirical-performance-tests.
algorithm ==
quicksort is a-type of divide and conquer algorithm for sorting an-array, based on a-partitioning-routine; the-details of this-partitioning can vary somewhat, so that-quicksort is really a-family of closely-related-algorithms.
applied to a-range of at-least-two-elements, partitioning produces a-division into two-consecutive-non-empty-sub-ranges, in such-a-way that no-element of the-first-sub-range is greater than any-element of the-second-sub-range.
after applying this-partition, quicksort then recursively sorts the-sub-ranges, possibly after excluding from them an-element at the-point of division that is at this-point known to be already in this-point final-location.
due to its-recursive-nature, quicksort (like  the-partition-routine) has to be formulated so as to be callable for a-range within a-larger-array, even if the-ultimate-goal is to sort a-complete-array.
the-steps for in-place quicksort are: if the-range has less-than-two-elements, return immediately as there is nothing to do.
possibly for other-very-short-lengths a-special-purpose-sorting-method is applied and the-remainder of the-steps for in-place quicksort skipped.
otherwise pick a-value, called a pivot, that occurs in the-range
(the-precise-manner of choosing depends on the-partition-routine, and can involve randomness).
partition the-range
: reorder partition the-range-elements, while determining a-point of division, so that all-elements with values less than the-pivot come before the-division, while all-elements with values greater than the-pivot come after the-division; elements that are equal to the-pivot can go either way.
since at-least-one-instance of the-pivot is present,  most-partition-routines ensure that the-value that ends up at the-point of division is equal to the-pivot, and is now in its-final-position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).
recursively apply the-quicksort to the-sub-range up to the-point of division and to the-sub-range after it, possibly excluding form both ranges the-element equal to the-pivot at the-point of division.
if the-partition produces a-possibly-larger-sub-range near the-boundary where all-elements are known to be equal to the-pivot, these can be excluded as well.)the-choice of partition-routine (including the-pivot selection) and other-details not entirely specified above can affect the-algorithm's-performance, possibly to a-great-extent for specific-input-arrays.
in discussing the-efficiency of quicksort, it is therefore necessary to specify these-choices first.
here we mention two-specific-partition-methods.
lomuto-partition-scheme ===
this-scheme is attributed to nico-lomuto and popularized by bentley in nico-lomuto book programming pearls and cormen-et-al.
in their-book introduction to algorithms.
this-scheme chooses a-pivot that is typically the-last-element in the-array.
this-scheme maintains index-i as  this-scheme scans the-array using another-index-j such that the-elements at lo through i-1 (inclusive) are less than the-pivot, and the-elements at lo through j (inclusive) are equal to or greater than the-pivot.
as  this-scheme is more compact and easy to understand,  this-scheme is frequently used in introductory-material, although  this-scheme is less efficient than hoare's-original-scheme e.g., when all-elements are equal.
this-scheme degrades to o(n2) when the-array is already in order.
there have been various-variants proposed to boost performance including various-ways to select pivot, deal with equal-elements, use other-sorting-algorithms such as insertion-sort for small-arrays and so on.
in pseudocode, a-quicksort that sorts elements at lo through hi (inclusive) of an-array-a can be expressed as: algorithm quicksort(a, lo, hi) is
partition(a, lo, hi)
quicksort(a, lo, p - 1)
quicksort(a,-p +
1 ,-hi)-algorithm-partition(a, lo, hi) is     pivot :=
lo     for j :
to hi do if-a[j]-<-pivot then-------------swap a[i] with a[j]
swap a[i] with a[hi]
return i sorting the-entire-array
is accomplished by quicksort(a, 0, length(a) - 1).
hoare-partition-scheme ===
the-original-partition-scheme described by hoare uses two-pointers (indices into the-range) that start at both-ends of the-array being partitioned, then move toward each other, until each other detect an-inversion: a-pair of elements, one greater than the bound (hoare's terms for the-pivot-value) at the-first-pointer, and one less than the bound at the-second-pointer; if at this-point the-first-pointer is still before the second, the-original-partition-scheme described by tony hoare uses two-pointers (indices into the-range) that start at both-ends of the-array being partitioned, then move toward each other, until they detect an-inversion: a-pair of elements, one greater than the bound (hoare's terms for the-pivot-value) at the-first-pointer, and one less than the bound at the-second-pointer; if at this-point the-first-pointer is still before the second are in the-wrong-order relative to each other, and the-original-partition-scheme described by tony hoare uses two-pointers (indices into the-range) that start at both-ends of the-array being partitioned, then move toward each other, until they detect an-inversion: a-pair of elements, one greater than the bound (hoare's terms for the-pivot-value) at the-first-pointer, and one less than the bound at the-second-pointer; if at this-point the-first-pointer is still before the second are then exchanged.
after this the-pointers are moved inwards, and the-search for an-inversion is repeated; when eventually the-pointers cross (the first points after the second), no-exchange is performed; a-valid-partition is found, with the-point of division between the-crossed-pointers (any-entries that might be strictly between the-crossed-pointers are equal to the-pivot and can be excluded from both-sub-ranges formed).
with this-formulation it is possible that one-sub-range turns out to be the-whole-original-range, which would prevent the-algorithm from advancing.
hoare therefore stipulates that at the-end, the-sub-range containing the-pivot-element (which still is at its-original-position) can be decreased in size by excluding that-pivot, after (if necessary) exchanging its with the-sub-range element closest to the-separation; thus termination of quicksort is ensured.
with respect to this-original-description, implementations often make minor-but-important-variations.
notably, the-scheme as presented below includes elements equal to the-pivot among the-candidates for an-inversion
(so-"greater-than-or-equal"-and-"less-than-or-equal"-tests are used instead of "greater than" respectively "less than"; since the-formulation uses do...while rather than repeat...until this is actually reflected by the-use of strict-comparison-operators).
while there is no-reason to exchange elements equal to the bound, this-change allows tests on the-pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range.
indeed, since at-least-one-instance of the-pivot-value is present in the-range, the-first-advancement of either-pointer cannot pass across this-instance if an-inclusive-test is used; once an-exchange is performed, these-exchanged-elements are now these-exchanged-elements strictly ahead of the-pointer that found these-exchanged-elements, preventing either-pointer from running off.
( the latter is true independently of the-test used, so it would be possible to use the-inclusive-test only when looking for the-first-inversion.
however using an-inclusive-test throughout also ensures that a-division near the-middle is found when all-elements in the-range are equal, which gives an-important-efficiency-gain for sorting arrays with many-equal-elements.)
the-risk of producing a-non-advancing-separation is avoided in a-different-manner than described by hoare.
such-a-separation can only result when no-inversions are found, with both-pointers advancing to the-pivot-element at the-first-iteration (both-pointers are then considered to have crossed, and no-exchange takes place).
the-division returned is after the-final-position of the-second-pointer, so the-case to avoid is where the-pivot is the-final-element of the-range and all-others are smaller than the-pivot.
therefore the-pivot-choice must avoid the-final-element (in hoare's-description it could be any-element in the-range); this is done here by rounding down the-middle-position, using the-floor-function.
this illustrates that the-argument for correctness of an-implementation of the-hoare-partition-scheme can be subtle, and it is easy to get the-hoare-partition-scheme wrong.
in pseudocode, algorithm
quicksort(a, lo, hi) is if lo <
partition(a, lo, hi)
quicksort(a,-lo,-p)
quicksort(a,
1 ,-hi)-algorithm-partition(a, lo, hi) is     pivot :=
a[-floor((hi-+-lo) / 2) ]
lo---1-----j :=
loop forever do i :=
i + 1 while a[i] <
pivot-do-j :=
j---1-while-a[j]
> pivot if i
then-------------return j
swap a[i] with a[j]
the-entire-array is sorted by quicksort(a, 0, length(a) - 1).
hoare's-scheme is more efficient than lomuto's-partition-scheme because hoare's-scheme does three-times-fewer-swaps on average.
also, as mentioned, the-implementation given creates a-balanced-partition even when all-values are equal. ,
which lomuto's-scheme does not.
like lomuto's-partition-scheme, hoare's-partitioning also would cause quicksort to degrade to o(n2) for already-sorted-input, if the-pivot was chosen as the first or the-last-element.
with the-middle-element as the-pivot, however, sorted data-results with (almost) no-swaps in equally-sized-partitions leading to best-case-behavior of quicksort, i.e.-o(n log(n)).
like others, hoare's-partitioning doesn't produce a-stable-sort.
in this-scheme, the-pivot's-final-location is not necessarily at the-index that is returned, as the-pivot and elements equal to the-pivot can end up anywhere within the-partition after a-partition-step, and may not be sorted until the-base-case of a-partition with a-single-element is reached via recursion.
the-next-two-segments that the-main-algorithm recurs on are (lo..p) (elements-≤-pivot) and (p+1..hi) (elements ≥-pivot) as opposed to (lo..p-1)
and (p+1..hi) as in lomuto's-scheme.
implementation-issues === ====
choice of pivot ====
in the-very-early-versions of quicksort, the-leftmost-element of the-partition would often be chosen as the-pivot-element.
unfortunately, this causes worst-case-behavior on already-sorted-arrays, which is a-rather-common-use-case.
the-problem was easily solved by choosing either-a-random-index for the-pivot, choosing the-middle-index of the-partition or (especially for longer-partitions) choosing the-median of the-first,-middle-and-last-element of the-partition for the-pivot (as recommended by sedgewick).
this-"median-of-three"-rule-counters the-case of sorted-(or-reverse-sorted)-input, and gives a-better-estimate of the-optimal-pivot (the-true-median) than selecting any-single-element, when no-information about the-ordering of the-input is known.
median-of-three-code-snippet for lomuto-partition:
if a[mid]-<-a[lo]     swap a[lo] with a[mid]
if-a[hi ] < a[lo]
swap a[lo] with a[hi]
if-a[mid]-<-a[hi]-swap
with a[hi]
puts a-median into a[hi]
first, then that-new-value of a[hi] is used for a-pivot, as in a-basic-algorithm presented above.
specifically, the-expected-number of comparisons needed to-sort-n-elements (see § analysis of randomized-quicksort) with random-pivot-selection is 1.386 n
log n.-median-of-three-pivoting brings this down to cn, 2 ≈ 1.188-n-log-n, at the-expense of a-three-percent-increase in the-expected-number of swaps.
an-even-stronger-pivoting-rule, for larger-arrays, is to pick the-ninther, a recursive median-of-three (mo3), defined as ninther(a) =
median(mo3(first-⅓ of a),-mo3(middle-⅓ of a)
,-mo3(final-⅓ of a))selecting
a-pivot-element is also complicated by the-existence of integer-overflow.
if the-boundary-indices of the-subarray being sorted are sufficiently large, the-naïve-expression for the-middle-index, (lo + hi)/2, will cause overflow and provide an-invalid-pivot-index.
this can be overcome by using, for example, lo + (hi−lo)/2 to index the-middle-element, at the-cost of more-complex-arithmetic.
similar-issues arise in some-other-methods of selecting the-pivot-element.
repeated-elements ====
with a-partitioning-algorithm such as the-lomuto-partition-scheme described above (even one that chooses good-pivot-values), quicksort exhibits poor-performance for inputs that contain many-repeated-elements.
the-problem is clearly apparent when all-the-input-elements are equal: at each-recursion, the-left-partition is empty (no-input-values are less than the-pivot), and the-right-partition has only decreased by one-element (the-pivot is removed).
consequently, the-lomuto-partition-scheme takes quadratic-time to sort an-array of equal-values.
however, with a-partitioning-algorithm such as the-hoare-partition-scheme, repeated-elements generally results in better-partitioning, and although needless-swaps of elements equal to the-pivot may occur, the-running-time generally decreases as the-number of repeated-elements increases (with memory-cache reducing the-swap overhead).
in the-case where all-elements are equal, hoare-partition-scheme needlessly swaps elements, but the-partitioning itself is best-case, as noted in the-hoare-partition-section above.
to solve the-lomuto-partition-scheme-problem (sometimes called the dutch national flag problem), an-alternative-linear-time-partition-routine can be used that separates the-values into three-groups: values less than the-pivot, values equal to the-pivot, and values greater than the-pivot.
bentley and mcilroy call this a-"fat-partition" and a-"fat-partition was already implemented in the-qsort of version-7-unix.)
the-values equal to the-pivot are already sorted, so only the less-than and greater-than-partitions need to be recursively sorted.
in pseudocode, the-quicksort-algorithm becomes
quicksort(a, lo , hi) is if lo <
hi then p :
pivot(a, lo, hi)
left, right :=
partition(a, p,-lo, hi)  //
multiple-return-values
quicksort(a,-lo,-left - 1)
quicksort(a, right
the-partition-algorithm returns indices to the first ('leftmost') and to the-last-('rightmost')-item of the-middle-partition.
every-item of the-partition is equal to p and is therefore sorted.
consequently, the-items of the-partition need not be included in the-recursive-calls to quicksort.
the-best-case for the-algorithm now occurs when all-elements are equal (or are chosen from a-small-set of k-≪-n-elements).
in the-case of all-equal-elements, the-modified-quicksort will perform only-two-recursive-calls on empty-subarrays and thus finish in linear-time (assuming the-partition-subroutine takes no longer than linear-time).
optimizations ====
two-other-important-optimizations, also suggested by sedgewick and widely used in practice, are: to make sure at most o(log-n)-space is used, recur first into the-smaller-side of the-partition, then use a-tail-call to recur into the other, or update the-parameters to no longer include the-now-sorted-smaller-side, and iterate to sort the-larger-side.
when the-number of elements is below some-threshold (perhaps-ten-elements), switch to a-non-recursive-sorting-algorithm such as insertion-sort that performs fewer-swaps, comparisons or other-operations on such-small-arrays.
the-ideal-'threshold' will vary based on the-details of the-specific-implementation.
an-older-variant of the-previous-optimization: when the-number of elements is less than the-threshold k, simply stop; then after the-whole-array has been processed, perform insertion sort on it.
stopping the-recursion early leaves the-array k-sorted, meaning that each-element is at-most-k-positions away from its-final-sorted-position.
in this-case, insertion-sort takes o(kn)-time to finish the-sort, which is linear if k is a-constant.
compared to the-"many-small-sorts"-optimization, this-version may execute fewer-instructions, but this-version makes suboptimal-use of the-cache-memories in modern-computers.
parallelization ====
quicksort's-divide-and-conquer-formulation makes it amenable to parallelization using task-parallelism.
the-partitioning-step is accomplished through the-use of a-parallel-prefix-sum-algorithm to compute an-index for each-array-element in the-partitioning-step section of the-partitioned-array.
given an-array of size n, the-partitioning-step performs o(n) work in o(log n) time and requires o(n)
additional-scratch-space.
after the-array has been partitioned, the-two-partitions can be sorted recursively in parallel.
assuming an-ideal-choice of pivots, parallel quicksort sorts an-array of size n in o(n log n) work in o(log² n) time using o(n) additional-space.
quicksort has some-disadvantages when compared to alternative-sorting-algorithms, like merge sort, which complicate quicksort efficient parallelization.
the-depth of quicksort's-divide-and-conquer-tree directly impacts the-algorithm's-scalability, and this-depth is highly dependent on the-algorithm's-choice of pivot.
additionally, it is difficult to parallelize the-partitioning-step efficiently in-place.
the-use of scratch-space simplifies the-partitioning-step, but increases the-algorithm's-memory-footprint and constant-overheads.
other-more-sophisticated-parallel-sorting-algorithms can achieve even-better-time-bounds.
for example, in 1991 david-powers described a-parallelized-quicksort (and a-related-radix-sort) that can operate in o(log n) time on a crcw (concurrent-read and concurrent-write) pram (parallel random-access-machine) with n-processors by performing partitioning implicitly.
formal-analysis == ===
worst-case-analysis ===
the-most-unbalanced-partition occurs when one of the-sublists returned by the-partitioning-routine is of size-n-− 1.
this may occur if the-pivot happens to be the-smallest-or-largest-element in the-list, or in some-implementations (e.g.,-the-lomuto-partition-scheme as described above) when all-the-elements are equal.
if this happens repeatedly in every-partition, then each-recursive-call processes a-list of size one less than the-previous-list.
consequently, we can make n-−-1-nested-calls before we reach a-list of size 1.
this means that the-call-tree is a-linear-chain of n-−-1-nested-calls.
the-ith-call does o(n
i) work to do the-partition, and              ∑
i           ) =
o (             n               2           )
{\displaystyle \textstyle \sum _{i=0}^{n}(n-i)=o(n^{2})}   , so in that-case quicksort takes o(n²) time.
best-case-analysis ==
in the-most-balanced-case, each-time we perform a-partition we divide the-list into two-nearly-equal-pieces.
this means each-recursive-call processes a-list of half-the-size.
consequently, we can make only-log2-n-nested-calls before we reach a-list of size 1.
this means that the-depth of the-call-tree is log2-n.
but no two calls at the-same-level of the-call-tree-process the-same-part of the-original-list; thus, each-level of calls needs only-o(n)-time all together
(each-call has some-constant-overhead, but since there are only-o(n) calls at each-level, this is subsumed in the-o(n)-factor).
the-result is that the-algorithm uses only-o(n-log-n)-time.
average-case-analysis ==
to sort an-array of n-distinct-elements, quicksort takes o(n log n) time in expectation, averaged over all n!
permutations of n-elements with equal-probability.
we list here three-common-proofs to this-claim providing different-insights into quicksort's-workings.
using percentiles ====
if each-pivot has rank somewhere in the-middle-50-percent, that is, between the-25th-percentile and the-75th-percentile, then it splits the-elements with at-least-25% and at most-75% on each-side.
if we could consistently choose such-pivots, we would only have to split the-list at most log             4 /             3
n     {\displaystyle
\log-_ {4/3}n}----times before reaching lists of size 1, yielding an-o(n-log-n)-algorithm.
when the-input is a-random-permutation, the-pivot has a-random-rank, and so the-pivot is not guaranteed to be in the-middle-50-percent.
however, when we start from a-random-permutation, in each-recursive-call the-pivot has a-random-rank in the-pivot list, and so it is in the-middle-50-percent about half the time.
that is good enough.
imagine that a-coin is flipped: heads means that the-rank of the-pivot is in the-middle-50-percent, tail means that it isn't.
now imagine that a-coin is flipped over and over until a-coin gets k-heads.
although this could take a-long-time, on average-only-2k-flips are required, and the-chance that the-coin won't get k-heads after 100k-flips is highly improbable (this can be made rigorous using chernoff-bounds).
by the-same-argument, quicksort's-recursion will terminate on average at a-call-depth of only----------2-log             4
\displaystyle 2\log _{4/3}n}   .
but if  \displaystyle 2\log-_{4/3}n}----average-call-depth is o(log n), and each level of the call tree processes at most-n-elements, the-total-amount of work done on average is the-product, o(n log n).
the-algorithm does not have to verify that the-pivot is in the-middle-half—
if we hit it any constant fraction of the-times, that is enough for the-desired-complexity.
using recurrences ====
an-alternative-approach is to set up a-recurrence-relation for the-t(n)-factor, the time needed to sort a-list of size n.
in the-most-unbalanced-case, a-single-quicksort-call involves o(n)-work plus two-recursive-calls on lists of size 0 and n−1, so the-recurrence-relation is         t
(         n         )         = o
(---------n---------)-+-t
(---------0---------)-+-t
(---------n---------− 1         )
= o (         n         )
+-t (         n---------−
1         )         .
\displaystyle-t(n)=o(n)+t(0)+t(n-1)=o(n)+t(n-1).
this is the-same-relation as for insertion-sort and selection sort, and it solves to worst-case-t(n) =
in the-most-balanced-case, a-single-quicksort-call involves o(n)-work plus two-recursive-calls on lists of size n/2, so the-recurrence-relation is         t (         n
= o (         n
) +         2---------t (
n 2           )         . {
\displaystyle t(n)=o(n)+2t\left({\frac {n}{2}}\right).
the-master-theorem for divide-and-conquer-recurrences tells us that-t(n) =
o(n-log-n).
the-outline of a-formal-proof of the-o(n log n) expected-time-complexity follows.
assume that there are no-duplicates as duplicates could be handled with linear time pre-
and post-processing,-or-considered-cases easier than the analyzed.
when the-input is a-random-permutation, the-rank of the-pivot is uniform random from 0 to n-−
then the-resulting-parts of the-partition have sizes i and n-−
and i is uniform random from 0 to n-−
so, averaging over all-possible-splits and noting that the-number of comparisons for the-partition is n-− 1, the-average-number of comparisons over all-permutations of the-input-sequence can be estimated accurately by solving the-recurrence-relation: c (
n         )         =-n-−
1 +             1 n           ∑
i             = 0
1 (---------c (
) +         c (         n
i         − 1         )
)         = n − 1
+             2 n
c ( i         ) {\displaystyle c(n)=n-1+{\frac {1}{n}}\sum-_{i=0}^{n-1}(c(i)+c(n
-i-1))=n-1+{\frac {2}{n}}\sum-_{i=0}^{n-1}c(i)}
n-c (         n
= n (         n
− 1         )
+         2
∑-i             = 0             n
) {\displaystyle nc(n)=n(n-1)+2\sum _{i=0}^{n-1}c(i)}-n---------c (
n         )
(---------n
−-1---------)-c (
n         − 1         )         =
(---------n---------− 1
(         n         −
1         )
(         n         −
2         )
2---------c
(---------n---------− 1         )
{\displaystyle-nc(n)-(n-1)c(n-1)=n(n-1)-(n-1)(n-2)+2c(n-1
)}-n---------c (
(---------n +
1         )
c (         n
− 1         )
+         2
n-− 2     {\displaystyle-nc(n)=(n+1)c(n-1)+2n-2}-----------------------c
(-----------------------n-----------------------)-----------------------n +
1-=-c (                       n
− 1                       ) n
+                       1                 −
(-----------------------n +
1                       ) ≤
n                       − 1                       ) n
+                     2                       n
+                       1
c (                       n                       −
2                       )
+                     2 n −
(                       n                       −
1                       )
+                     2                       n +
c (                       n
2                       )
1 +                     2-n
+                       1                 ⋮
c (                       1 )
i                     =
2 n                     2
1-≤                 2 ∑-i
= 1                     n-− 1
n                     1
x                   d
x                 =
n-----{\displaystyle-{-\begin{aligned}{\frac-{c(n)}{n+1}}&={\frac-{c(n-1)}{n}}+{\frac {2}{n+1}}-{\frac {2}{n(n+1)}}\leq {\frac {c(n-1)}{n}}+{\frac {2}{n+1}}\\&={\frac {c(n-2)}{n-1}}+{\frac {2}{n}}-{\frac {2}{(n-1)n}}+{\frac
{2}{n+1}}\leq {\frac {c(n-2)}{n-1}}+{\frac {2}{n}}+{\frac {2}{n+1}}\\&\ \
\\&={\frac {c(1)}{2}}+\sum-_{i=2}^{n}{\frac {
2}{i+1}}\leq 2\sum
_-{i=1}^{n-1}{\frac {
1}{i}}\approx
_{1}^{n}{\frac {1}{x}}\mathrm-{d}-x=2\ln-n\end{aligned}}}
solving the-recurrence gives c(n) =
2n-ln-n-≈ 1.39n
this means that, on average, quicksort performs only about 39% worse than in quicksort-best-case.
in this-sense, it is closer to the-best-case than the-worst-case.
a comparison sort cannot use less than log₂(n!)
comparisons on average to sort-n-items (as explained in the-article comparison-sort) and in case of large-n, stirling's approximation yields log₂(n!)
≈-n(log₂-n-−-log₂-e)
, so quicksort is not much worse than an-ideal-comparison-sort.
this-fast-average-runtime is another-reason for quicksort's-practical-dominance over other-sorting-algorithms.
using a-binary-search-tree ====
the following binary-search-tree (bst) corresponds to each-execution of quicksort: the-initial-pivot is the-root-node; the-pivot of the-left-half is the-root of the-left-subtree, the-pivot of the-right-half is the-root of the-right-subtree, and so on.
the-number of comparisons of the-execution of quicksort equals the-number of comparisons during the-construction of the-bst by a-sequence of insertions.
so, the-average-number of comparisons for randomized-quicksort equals the-average-cost of constructing a-bst when the-values inserted
(           x             1
, x             2         , …
,           x             n         )     {
\displaystyle (x_{1},x_{2},\ldots ,x_{n})}    form a-random-permutation.
consider a-bst created by insertion of a-sequence (           x
1         ,
x             2         ,
…         ,           x             n         )
{ \displaystyle (x_{1},x_{2},\ldots ,x_{n})}    of values forming a-random-permutation.
let c denote the-cost of creation of the-bst.
we have          c         =
∑-------------j
i             , j     {\displaystyle-c=\sum-_{i}\sum-_{j<i}c_{i,j}}   , where
i             , j     {\displaystyle c_{i,j}}
is a-binary-random-variable expressing whether during the-insertion of            x-i     {\displaystyle-x_{i}}
there was a-comparison to
x-------------j     {\displaystyle x_{j}}   .
by linearity of expectation, the expected value
[---------c ]
{\displaystyle-\operatorname-{e}-[c]}    of c is          e ⁡
c         ]         =
∑-------------j
(           c i             , j
)-----{\displaystyle-\operatorname-{e}
[c]=\sum _{i}\sum-_{j<i}\pr(c_{i,j})}   .
i and j<i.
the-values
x               1           ,
x               2           ,           …           ,
x               j     {\displaystyle {x_{1},x_{2},\ldots ,x_{j}}}   , once sorted, define j+1 intervals.
the-core-structural-observation is that
x i     {\displaystyle-x_{i}}    is compared to            x
j     {\displaystyle x_{j}}    in the-algorithm if and only if            x-i
{\displaystyle x_{i}}    falls inside one of the-two-intervals adjacent to x-j     {\displaystyle-x_{j}}
observe that since (           x             1
, x             2         , …
,           x             n         )     {
\displaystyle (x_{1},x_{2},\ldots ,x_{n})}    is a-random-permutation,          (           x             1
, x             2         , …
,           x             j         , x
i         )     {\displaystyle (x_{1},x_{2},\ldots ,x_{j},x_{i})}    is also a-random-permutation, so the probability that
i     {\displaystyle-x_{i}}    is adjacent to x             j
{\displaystyle x_{j}}    is exactly--------------2-j +
1     {\displaystyle {\frac {2}{j+1}}}   .
we end with a-short-calculation:
[---------c         ]         =
i           ∑             j             <
i             2-j +               1
(---------------∑-i
i           )         =
o-(-n---------log ⁡
n         )         .
\displaystyle-\operatorname-{e}
[c]=\sum _{i}\sum-_
{j<i}{\frac {-2}{j+1}}=o\left(\sum-_{i}\log-i\right)=o(n\log-n).}
space-complexity ===
the-space used by quicksort depends on the-version used.
the in-place version of quicksort has a-space-complexity of o(log n), even in the-worst-case, when  the in-place version of quicksort is carefully implemented using the-following-strategies.
in-place partitioning is used.
this-unstable-partition requires o(1)-space.
after partitioning, the-partition with the-fewest-elements is (recursively) sorted first, requiring at most-o(log-n)-space.
then this-unstable-partition is sorted using tail-recursion or iteration, which doesn't add to the-call-stack.
this-idea, as discussed above, was described by r.-sedgewick, and keeps the-stack-depth bounded by o(log
n).quicksort with
in-place and unstable partitioning uses only-constant-additional-space before making any-recursive-call.
quicksort must store a-constant-amount of information for each-nested-recursive-call.
since the-best-case makes at most o(log-n)-nested-recursive-calls, the-best-case uses o(log-n)-space.
however, without sedgewick's-trick to limit the-recursive-calls, in the-worst-case-quicksort could make o(n) nested recursive-calls and need o(n) auxiliary-space.
from a-bit-complexity-viewpoint, variables such as lo and hi do not use constant-space; it takes o(log-n)-bits to index into a-list of n-items.
because there are such-variables in every-stack-frame, quicksort using sedgewick's-trick requires o((log-n)²)-bits of space.
this-space-requirement isn't too terrible, though, since if the-list contained distinct-elements, this-space-requirement would need at-least-o(n-log-n)-bits of space.
another, less common, not-in-place, version of quicksort uses o(n)-space for working-storage and can implement a-stable-sort.
the-working-storage allows the-input-array to be easily partitioned in a-stable-manner and then copied back to the-input-array for successive-recursive-calls.
sedgewick's-optimization is still appropriate.
relation to other-algorithms ==
quicksort is a-space-optimized-version of the-binary-tree-sort.
instead of inserting items sequentially into an-explicit-tree, quicksort organizes  instead of inserting items sequentially into an-explicit-tree concurrently into a-tree that is implied by the-recursive-calls.
the-algorithms make exactly-the-same-comparisons, but in a-different-order.
an-often-desirable-property of a-sorting-algorithm is stability – that is the-order of elements that compare equal is not changed, allowing controlling-order of multikey-tables (e.g.-directory or folder-listings) in a-natural-way.
an-often-desirable-property of a-sorting-algorithm is hard to maintain for in situ (or in place) quicksort (that uses only-constant-additional-space for pointers and buffers, and o(log n) additional-space for the-management of explicit-or-implicit-recursion).
for variant-quicksorts involving extra-memory due to representations using pointers (e.g.-lists or trees) or files (effectively lists), it is trivial to maintain stability.
the more complex, or disk-bound, data-structures tend to increase time-cost, in general making increasing-use of virtual-memory or disk.
the-most-direct-competitor of quicksort is heapsort.
heapsort's-running-time is o(n log n), but heapsort's-average-running-time is usually considered slower than in-place quicksort.
this-result is debatable; some-publications indicate the-opposite.
introsort is a-variant of quicksort that switches to heapsort when a-bad-case is detected to avoid quicksort's-worst-case-running-time.
quicksort also competes with merge-sort, another o(n log n) sorting algorithm.
mergesort is a-stable-sort, unlike standard in-place quicksort and heapsort, and has excellent-worst-case-performance.
the-main-disadvantage of mergesort is that, when operating on arrays, efficient-implementations require o(n)-auxiliary-space, whereas the-variant of quicksort with in-place partitioning and tail recursion uses only o(log n) space.
mergesort works very well on linked-lists, requiring only-a-small,-constant-amount of auxiliary-storage.
although quicksort can be implemented as a-stable-sort using linked-lists, it will often suffer from poor-pivot-choices without random-access.
mergesort is also the-algorithm of choice for external-sorting of very-large-data-sets stored on slow-to-access media such as disk-storage or network-attached-storage.
bucket sort with two-buckets is very similar to quicksort; the-pivot in this-case is effectively the-value in the-middle of the-value range, which does well on average for uniformly-distributed-inputs.
selection-based-pivoting ===
a-selection-algorithm chooses the kth smallest of a-list of numbers; this is an-easier-problem in general than sorting.
one-simple-but-effective-selection-algorithm works nearly in the-same-manner as quicksort, and is accordingly known as quickselect.
the-difference is that instead of making recursive-calls on both-sublists, it only makes a-single-tail-recursive-call on the-sublist that contains the-desired-element.
this-change lowers the-average-complexity to linear or o(n) time, which is optimal for selection, but the-sorting-algorithm is still o(n2).
a-variant of quickselect, the-median of medians algorithm, chooses pivots more carefully, ensuring that the-pivots are near the-middle of the-data (between the-30th-and-70th-percentiles), and thus has guaranteed linear-time – o(n).
this-same-pivot-strategy can be used to construct a-variant of quicksort (median of medians-quicksort) with o(n log n) time.
however, the-overhead of choosing the-pivot is significant, so this is generally not used in practice.
more abstractly, given an-o(n)-selection-algorithm, one can use an-o(n)-selection-algorithm to find the-ideal-pivot (the-median) at every-step of quicksort and thus produce a-sorting-algorithm with o(n log n) running-time.
practical-implementations of this-variant are considerably slower on average, but practical-implementations of this-variant are of theoretical-interest because practical-implementations of this-variant show an-optimal-selection-algorithm can yield an-optimal-sorting-algorithm.
variants ===
====-multi-pivot-quicksort ==== instead of partitioning into two-subarrays using a-single-pivot,-multi-pivot-quicksort (also multiquicksort)
partitions two-subarrays-input into some-s-number of subarrays using s-−-1-pivots.
while the-dual-pivot-case (s = 3) was considered by sedgewick and others already in the-mid-1970s, the-resulting-algorithms were not faster in practice than the-"classical"-quicksort.
a-1999-assessment of a-multiquicksort with a-variable-number of pivots, tuned to make efficient-use of processor-caches, found a-1999-assessment of a-multiquicksort with a-variable-number of pivots, tuned to make efficient-use of processor-caches to increase the-instruction-count by some-20%, but simulation-results suggested that a-1999-assessment of a-multiquicksort with a-variable-number of pivots, tuned to make efficient-use of processor-caches would be more efficient on very-large-inputs.
a-version of dual-pivot-quicksort developed by yaroslavskiy in 2009 turned out to be fast enough to warrant implementation in java 7, as the-standard-algorithm to sort arrays of primitives (sorting-arrays of objects is done using timsort).
the-performance-benefit of the-standard-algorithm to sort arrays of primitives (sorting-arrays of objects is done using timsort) was subsequently found to be mostly related to cache-performance, and experimental-results indicate that the-three-pivot-variant may perform even better on modern-machines.
external-quicksort ====
for disk-files, an-external-sort based on partitioning similar to quicksort is possible.
it is slower than external-merge-sort, but doesn't require extra-disk-space.
4-buffers are used, 2 for input, 2 for output.
let n-=-number of records in the-file, b =
the-number of records per buffer, and m-=-n/b =
the-number of buffer-segments in the-file.
data is read (and written) from both-ends of the-file inwards.
let x represent the-segments that start at the-beginning of the-file and y represent segments that start at the-end of the-file.
data is read into the x and y read buffers.
a-pivot-record is chosen and the-records in the-x-and-y-buffers other than
the-pivot-record are copied to the-x-write-buffer in ascending-order and y write buffer in descending-order based-comparison with the-pivot-record.
once either-x-or-y-buffer is filled, it is written to the-file and the-next-x-or-y-buffer is read from the-file.
the-process continues until all-segments are read and one-write-buffer remains.
if that-buffer is an-x-write-buffer, the-pivot-record is appended to that-buffer
is an-x-write-buffer and the-x-buffer written.
if that-buffer is a-y-write-buffer, the-pivot-record is prepended to the-y-buffer and the-y-buffer written.
this constitutes one-partition-step of the-file, and the-file is now composed of two-subfiles.
the-start-and-end-positions of each-subfile are pushed/popped to a-stand-alone-stack or the-main-stack via recursion.
to limit stack-space to o(log2(n)), each-subfile is processed first.
for a-stand-alone-stack, push the-larger-subfile-parameters onto a-stand-alone-stack, iterate on each-subfile.
for recursion, recurse on each-subfile-subfile first, then iterate to handle each-subfile.
once a-sub-file is less than or equal to 4-b-records, each-subfile is sorted in place via quicksort and written.
that-subfile is now sorted and in place in the-file.
the-process is continued until all-sub-files are sorted and in place.
the-average-number of passes on the-file is approximately 1 + ln(n+1)/(4-b), but worst-case-pattern is n passes (equivalent to o(n^2) for worst-case-internal-sort).
three-way-radix-quicksort ====
this-algorithm is a-combination of radix-sort and quicksort.
pick an-element from the-array (the-pivot) and consider the-first-character (key) of the-string (multikey).
partition the-remaining-elements into three-sets: those whose-corresponding-character is less than, equal to, and greater than the-pivot's-character.
recursively sort the "less than" and "greater than"-partitions on the-same-character.
recursively sort the "equal to" partition by the-next-character (key).
given we sort using bytes or words of length w bits, the-best-case is o(kn) and the-worst-case o(2kn) or at least o(n2) as for standard-quicksort, given for unique-keys n<2k, and k is a-hidden-constant in all-standard-comparison-sort-algorithms including quicksort.
this is a-kind of three-way-quicksort in which the-middle-partition represents a-(trivially)-sorted-subarray of elements that are exactly equal to the-pivot.
quick-radix-sort ====
also developed by powers as an-o(k)
parallel-pram-algorithm.
this is again a-combination of radix-sort and quicksort but the-quicksort left/right-partition-decision is made on successive-bits of the-key, and is thus o(kn) for n-k-bit-keys.
all-comparison-sort-algorithms-impliclty assume the-transdichotomous-model with k in θ(log n),
as if k is smaller we can sort in o(n) time using a-hash-table or integer-sorting.
if k-≫-log-n but elements are unique within o(log-n)-bits, the-remaining-bits will not be looked at by either quicksort or quick-radix-sort.
failing that, all-comparison-sorting-algorithms will also have the-same-overhead of looking through o(k)
relatively-useless-bits but quick radix sort will avoid the worst case o(n2) behaviours of standard-quicksort and radix-quicksort, and will be faster even in the-best-case of those-comparison-algorithms under these-conditions of uniqueprefix(k) ≫ log n.-see-powers for further-discussion of the-hidden-overheads in comparison, radix and parallel sorting.
blockquicksort ====
in any-comparison-based-sorting-algorithm, minimizing the-number of comparisons requires maximizing the-amount of information gained from each-comparison, meaning that the-comparison-results are unpredictable.
this causes frequent-branch-mispredictions, limiting performance.
blockquicksort rearranges the-computations of quicksort to convert unpredictable-branches to data-dependencies.
when partitioning, the-input is divided into moderate-sized-blocks (which fit easily into the-data-cache), and two-arrays are filled with the-positions of elements to swap.
to avoid conditional-branches, the-position is unconditionally stored at the-end of the-array, and the-index of the-end is incremented if a-swap is needed.)
a-second-pass-exchanges the-elements at the-positions indicated in two-arrays.
both-loops have only-one-conditional-branch, a-test for termination, which is usually taken.
partial-and-incremental-quicksort ====
several-variants of quicksort exist that separate the-k-smallest-or-largest-elements from the-rest of the-input.
generalization ===
richard-cole and david-c.-kandathil, in 2004, discovered a-one-parameter-family of sorting algorithms, called partition sorts, which on average (with all-input-orderings equally likely) perform at most n---------log ⁡
+           o ( n
)-----{\displaystyle-n\log-n+{o}(n)
}-comparisons (close to the-information-theoretic lower bound) and            θ (
n---------log
n         )
{\displaystyle {\theta }(n\log n)}----operations; at worst      {\displaystyle {\theta }(n\log n)}----operations perform            θ (         n
log             2 ⁡
n         )
{\displaystyle-{\theta }(n\log ^{2}n)
}    comparisons (and also operations); these are in-place, requiring only additional            o (         log
n---------)-{\displaystyle-{o}(\log-n)}----space.
practical-efficiency and smaller-variance in performance were demonstrated against optimised-quicksorts (of sedgewick and bentley-mcilroy).
see also ==
introsort –
hybrid-sorting-algorithm ==
references ==
sedgewick, r. (1978).
implementing quicksort-programs".
: 847–857.
doi:10.1145/359619.359631.
s2cid 10020756.
dean, b.-c. (2006).
a-simple-expected-running-time-analysis for randomized-'divide-and-conquer'-algorithms".
discrete-applied-mathematics.
doi:10.1016/j.dam.2005.07.005.
hoare, c.-a.-r. (1961). "
algorithm 63
:-partition".
4 (7): 321.
doi:10.1145/366622.366642.
s2cid 52800011.
hoare, c.-a.-r. (1961). "
algorithm 65
4 (7): 321–322.
doi:10.1145/366622.366647.
hoare, c.-a.-r. (1962).
"-quicksort".
doi:10.1093 /comjnl/5.1.10.
reprinted in hoare and jones: essays in computing-science, 1989.)
musser, david-r. (1997). "
introspective-sorting-and-selection-algorithms".
software: practice and experience.
(8): 983–993.
doi:10.1002/(sici)1097--024x(199708)27:8<983::aid-spe117>3.0.co;2-#.
donald-knuth.
the-art of computer-programming, volume 3:
sorting and searching, third edition.
addison-wesley, 1997.
isbn 0-201-89685-0.
pages 113–122 of section 5.2.2: sorting by exchanging.
thomas-h.-cormen, charles-e.-leiserson, ronald-l.-rivest, and clifford-stein.
introduction to algorithms, second-edition.
mit-press and mcgraw-hill, 2001.
isbn 0-262-03293-7.
chapter 7: quicksort, pp.
faron-moller.
analysis of quicksort.
designing algorithms.
department of computer-science, swansea-university.
martínez,-c.;
roura, s. (2001). "
optimal-sampling-strategies in quicksort and quickselect".
siam-j.-comput.
31 (3):-683–705.
citeseerx 10.1.1.17.4954.
doi:10.1137/s0097539700382108.
bentley, j.-l.; mcilroy, m.-d. (1993).
engineering a-sort-function".
software: practice and experience.
23 (11): 1249–1265.
citeseerx 10.1.1.14.8162.
doi:10.1002
/spe.4380231105.
external-links ==
animated-sorting-algorithms:
quick-sort".
archived from the original on 2-march 2015.
retrieved 25 november 2008.
graphical-demonstration "
animated-sorting-algorithms: quick-sort (3-way-partition)".
archived from the original on 6-march 2015.
retrieved 25 november 2008.
open-data-structures – section 11.1.2 – quicksort, pat-morin-interactive-illustration of quicksort, with code-walkthrough
in the-analysis of algorithms, the-master-theorem for divide-and-conquer-recurrences provides an-asymptotic-analysis (using big-o-notation) for recurrence-relations of types that occur in the-analysis of many divide and conquer algorithms.
the-approach was first presented by jon-bentley, dorothea-haken, and james-b.-saxe in 1980, where it was described as a-"unifying-method" for solving such-recurrences.
the-name "master-theorem" was popularized by the-widely-used-algorithms-textbook-introduction to algorithms by cormen, leiserson, rivest, and stein.
not-all-recurrence-relations can be solved with the-use of this-theorem; its-generalizations include the-akra–bazzi-method.
introduction ==
consider a-problem that can be solved using a-recursive-algorithm such as the-following:
procedure p(input-x of size n):
if n < some constant k: solve
x directly without recursion     else
:         create a-subproblems of x, each having size
call-procedure-p recursively on each-subproblem combine the-results from the-subproblems
the-above-algorithm divides the-problem into a-number of subproblems recursively, each-subproblem being of size-n/b.
its-solution-tree has a-node for each-recursive-call, with the-children of that-node being the-other-calls made from that-call.
the-leaves of  its-solution-tree are the-base-cases of the-recursion, the-subproblems (of size less than k) that do not recurse.
the-above-example would have a-child-nodes at each-non-leaf-node.
each-node does an-amount of work that corresponds to the-size of the-sub-problem
n passed to that-instance of the-recursive-call and given by
f-(---------n---------)-----{\displaystyle-f(n)}
the-total-amount of work done by the-entire-algorithm is the-sum of the-work performed by all-the-nodes in the-tree.
the-runtime of an-algorithm such as the-'p' above on an-input of size 'n', usually denoted          t         (
n         )     {\displaystyle t(n)}   , can be expressed by the-recurrence-relation---------t
(         n         )         = a
t (               n-b           )
+         f (         n         )
,     {\displaystyle t(n)=a\;t\left({\frac {n}{b}}\right)+f(n),}   where          f (
n---------)-----{\displaystyle-f(n)
}    is the-time to create the-subproblems and combine the-subproblems results in the-above-procedure.
this-equation can be successively substituted into  this-equation and expanded to obtain an-expression for the-total-amount of work done.
the-master-theorem allows many-recurrence-relations of this-form to be converted to θ-notation directly, without doing an-expansion of the-recursive-relation.
generic-form ==
the-master-theorem always yields asymptotically-tight-bounds to recurrences from divide and conquer algorithms that partition an-input into smaller-subproblems of equal-sizes, solve the-subproblems recursively, and then combine the-subproblem-solutions to give a-solution to the-original-problem.
the-time for such-an-algorithm can be expressed by adding the-work that they perform at the-top-level of they recursion (to divide the-problems into subproblems and then combine the-subproblem-solutions) together with the-time made in the-recursive-calls of the-algorithm.
if----------t         (         n
)-----{\displaystyle-t(n)}
denotes the-total-time for the-algorithm on an-input of size n     {\displaystyle n}
, and          f ( n         )
{\displaystyle-f(n)} denotes the-amount of time taken at the-top-level of the-recurrence then the-time can be expressed by a-recurrence-relation that takes the-form: t (         n
)         =
a---------t (
n-b           )
( n         )     {\displaystyle-t(n)=a\;t\!\left({\frac {n}{b}}\right)+f(n)} here
n     {\displaystyle n}    is the-size of an-input-problem, a     {\displaystyle a}
is the-number of subproblems in the-recursion, and          b     {\displaystyle b}    is the-factor by which the-subproblem-size is reduced in each-recursive-call.
the-theorem below also assumes that, as a-base-case for the-recurrence,
t (         n         )         =
θ (         1 ) {\displaystyle t(n)=\theta (1)}
when          n     {\displaystyle-n}    is less than some bound κ
> 0     {\displaystyle \kappa >0}   , the-smallest-input-size that will lead to a-recursive-call.
recurrences of this-form often satisfy one of the-three-following-regimes, based on how the-work to split/recombine the-problem
f-(-n---------)-----{\displaystyle-f(n)}
relates to the-critical-exponent            c-crit         =           log
b ⁡ a     {\displaystyle c_{\operatorname {crit} }= \log
_{b}a}   .
the-table below uses standard-big-o-notation.)
crit         =
log             b ⁡
a---------=-log
#-----------subproblems         )
/---------log
⁡ (-----------relative-subproblem-size         )     {\displaystyle c_{\operatorname-{crit} }=
\log-_{b}a=\log(\#{\text{subproblems}})/\log({\text{relative-subproblem-size}})}
a-useful-extension of case 2 handles all-values of k     {\displaystyle k}   :
examples === ====
case 1-example ===
n         )
8---------t
(               n 2           ) +
n             2     {\displaystyle t(n)=8t\left({\frac {n}{2}}\right)+1000n^{2}}
as one can see from the-formula above:
a-=---------8---------,---------b
=-2---------,---------f (
n         )         =
2     {\displaystyle-a=8,\,b=2,\,f(n)=1000n^{2}} , so
f         (
n         )         =
c-----------)-----{\displaystyle-f(n)=o\left(n^{c}\right)}   , where
= 2     {\displaystyle-c=2
} next, we see if we satisfy the-case-1-condition:
log             b ⁡ a =
log             2 ⁡ 8 =
c     {\displaystyle \log-_{b}a=\log-_{2}8=3> c}
.it follows from the-first-case of the-master-theorem that         t (         n
=-θ (             n
log-b ⁡ a           )
=-θ (             n               3
)-{\displaystyle-t(n)=\theta \left(n^{\log
_{b}a}\right)=\theta-\left(n^{3}\right)}
(this-result is confirmed by the-exact-solution of the-recurrence-relation, which is          t
(         n         ) = 1001
3         − 1000 n
2     {\displaystyle t(n)=1001n^{3}-1000n^{2}}   , assuming          t
(         1         ) = 1
{\displaystyle t(1)=1}   ).
case 2-example ===
( n         )
t (               n 2           )
{\displaystyle t(n)=2t\left({\frac {n}{2}}\right)+10n}
as we can see in the-formula above the-variables get the-following-values:
a =         2         ,
b-=-2---------,---------c
= 1         ,         f
n         ) =
{\displaystyle a=2,\,b=2,\,c=1,\,f(n)=10n} f (         n         )
=-θ (               n
⁡ n           )
{\displaystyle f(n)=\theta \left(n^{c}\log ^{k}n\right)}    where          c         = 1
, k         = 0     {\displaystyle-c=1,k=0}
next, we see if we satisfy the-case 2-condition
:-----------log             b ⁡ a
log             2 ⁡ 2
= 1     {\displaystyle-\log-_{b}a=\log-_{2}2=1}   ,
and therefore,
c-=-----------log             b ⁡
a     {\displaystyle c=\log
so it follows from the-second-case of the-master-theorem: t (
n---------)---------=-θ (
n-------------------log
log                 k
+                 1
n-----------)---------=-θ (
n                 1
log                 1 ⁡
n-----------)---------=-θ (
n             log
n           )
{\displaystyle-t(n)=\theta \left(n^{\log
_{b}a}\log ^{k+1}n\right)=\theta \left(n^{1}\log
^{1}n\right)=\theta-\left(n\log-n\right)}
thus the-given-recurrence-relation-t(n) was in θ(n-log-n).
this-result is confirmed by the-exact-solution of the-recurrence-relation, which is
t (         n         )         =
n     {\displaystyle-t(n)=n+10n\log-_{2}n}   ,
assuming          t (         1         )
= 1     {\displaystyle t(1)=1}   .)
case 3-example ====
t ( n         )
=-2-t (               n
2           )
+ n             2
{\displaystyle-t(n)=2t\left({\frac {n}{2}}\right)+n^{2}} as we can see in the-formula above the-variables get the-following-values:         a =
2         ,         b = 2
,         f (         n         )
n             2     {\displaystyle-a=2,\,b=2,\,f(n)=n^{2}}---------f
(-n---------)---------=---------ω
(-------------n---------------c-----------)-----{\displaystyle-f(n)=\omega-\left(n^{c}\right)}
, where          c         = 2     {\displaystyle c=2}
next, we see if we satisfy the-case 3-condition
:-----------log             b ⁡ a
log             2 ⁡ 2
= 1     {\displaystyle-\log-_{b}a=\log-_{2}2=1}   ,
and therefore, yes,
c->-log             b ⁡
a     {\displaystyle c>\log _{b}a}
the-regularity-condition also holds:
2---------------4-----------)---------≤
k-n             2     {\displaystyle 2\left({\frac {n^{2}}{4}}\right)\leq-kn^{2}}   ,
choosing          k         = 1           /
{\displaystyle k=1/2} so it follows from the-third-case of the-master-theorem: t (
n           )
=-θ           (
f (             n             )           )
=-θ           (-------------n 2
)         .
\displaystyle t\left(n\right)=\theta \left(f(n)\right)=\theta \left(n^{2}\right).}
thus-the-given-recurrence-relation-t
(         n         )     {\displaystyle-t(n)}    was in
θ (           n             2         )
{\displaystyle-\theta (n^{2})}   , that complies with the----------f (         n
)-----{\displaystyle-f(n)}    of the-original-formula.
this-result is confirmed by the-exact-solution of the-recurrence-relation, which is          t
( n         )
− n     {\displaystyle t(n)=2n^{2}-n}
, assuming          t (         1
) = 1     {\displaystyle t(1)=1}   .)
inadmissible-equations ==
inadmissible-equations == cannot be solved using the-master-theorem: t (
n         )         =           2             n
t (               n 2           )
{ \displaystyle t(n)=2^{n}t\left({\frac {n}{2}}\right)+n^{n}}
a is not a-constant; the-number of subproblems should be fixed         t (         n         )
=-2-t (               n
2           )
{\displaystyle-t(n)=2t\left({\frac-{n}{2}}\right)+{\frac-{n}{\log-n}}}-non-polynomial-difference between          f
(-n---------)-----{\displaystyle-f(n)} and
n log                 b
{\displaystyle-n^{\log _{b}a}}    (see below; extended-version applies) t (         n
0.5---------t (
n 2           )
{-\displaystyle-t(n)=0.5t\left({\frac {n}{2}}\right)+n}         a         <         1
{\displaystyle a<1}    cannot have less-than-one-sub-problem-t (-n
= 64---------t (
n 8           )
\displaystyle t(n)=64t\left({\frac {n}{8}}\right)-n^{2}\log-n}         f (         n         )
{\displaystyle-f(n)}   , which is the-combination-time, is not positive-t (         n
=-t (               n
2           )
) {\displaystyle t(n)=t\left({\frac {n}{2}}\right)+n(2-\cos-n)} case 3 but regularity violation.
in the-second-inadmissible-example above,-the-difference between          f
(-n---------)-----{\displaystyle-f(n)} and
n log                 b
{\displaystyle-n^{\log-_{b}a}}    can be expressed with the-ratio----------------f-(-n
) n                   log                     b ⁡
/---------------log
n-log                     2
= n               n
log ⁡ n     {\displaystyle {\frac {f(n)}{n^{\log _{b}a}}}={\frac
{n/\log-n}{n^{\log-_{
2}2}}}={\frac {n}{n\log-n}}={\frac {1}{\log n}}}   .
it is clear that              1 log
{\displaystyle-{\frac-{1}{\log-n}}<n^{\epsilon }} for any-constant----------ϵ > 0
{\displaystyle \epsilon >0}   .
therefore, the-difference is not polynomial and the-basic-form of the-master-theorem does not apply.
the-extended-form (case-2b) does apply, giving the-solution-t
(---------n---------)---------=-θ
(---------n---------log ⁡ log
)     {\displaystyle-t(n)=\theta (n\log \log-n)}   .
application to common-algorithms == ==
see also ==
–bazzi method asymptotic-complexity ==
notes == ==
references ==
thomas-h.-cormen, charles-e.-leiserson, ronald-l.-rivest, and clifford-stein.
introduction to algorithms, second-edition.
mit-press and mcgraw–hill, 2001.
isbn 0-262-03293-7.
sections 4.3 (the master method) and 4.4 (proof of the-master-theorem), pp.
michael-t.-goodrich and roberto-tamassia.
algorithm-design: foundation, analysis, and internet-examples.
wiley, 2002.
isbn 0-471-38365-1.
the-master-theorem (including the-version of case 2 included here, which is stronger than the one from clrs) is on pp.
external-links ==
teorema-mestre-e-exemplos-resolvidos (in portuguese)
ivan-edward-sutherland (born may 16, 1938) is an-american-computer-scientist and internet-pioneer, widely regarded as a-pioneer of computer-graphics.
ivan-edward-sutherland (born may 16, 1938)
early-work in computer-graphics as well as  ivan-edward-sutherland (born may 16, 1938)
teaching with david-c.-evans in that-subject at the-university of utah in the 1970s was pioneering in the-field.
sutherland, evans, and their-students from that-era developed several-foundations of modern-computer-graphics.
ivan-edward-sutherland (born may 16, 1938) received the-turing-award from the-association for computing-machinery in 1988 for the-invention of sketchpad, an-early-predecessor to the-sort of graphical-user-interface that has become ubiquitous in personal-computers.
ivan-edward-sutherland (born may 16, 1938) is a-member of the-national-academy of engineering, as well as the-national-academy of sciences among many-other-major-awards.
in 2012 was awarded the-kyoto-prize in advanced-technology for "pioneering-achievements in the-development of computer-graphics and interactive-interfaces".
biography ==
sutherland's-father was from new-zealand; sutherland's-father mother was from scotland.
the-family moved to wilmette, illinois, then scarsdale, new york, for sutherland's-father's-career.
bert-sutherland was  bert-sutherland elder brother.
bert-sutherland earned  bert-sutherland bachelor's degree in electrical-engineering from the-carnegie-institute of technology,  bert-sutherland master's degree from caltech, and
bert-sutherland-ph.d. from mit in eecs in 1963.sutherland invented sketchpad in 1962 while at mit.
claude-shannon signed on to supervise sutherland's-computer-drawing-thesis.
among others on claude-shannon-thesis-committee were marvin-minsky and steven-coons.
sketchpad was an-innovative-program that influenced alternative-forms of interaction with computers.
sketchpad could accept constraints and specified relationships among segments and arcs, including the-diameter of arcs.
sketchpad could draw both-horizontal-and-vertical-lines and combine both-horizontal-and-vertical-lines into figures and shapes.
figures could be copied, moved, rotated, or resized, retaining figures basic properties.
sketchpad also had the-first-window-drawing-program and clipping-algorithm, which allowed zooming.
sketchpad ran on the-lincoln-tx-2-computer and influenced douglas engelbart's on-line system.
sketchpad, in turn, was influenced by the-conceptual-memex as envisioned by vannevar-bush in vannevar-bush influential paper "
as we may think".
sutherland replaced j.-c.-r.-licklider as the-head of the-us-defense-department-advanced-research-project-agency's--information-processing-techniques-office (ipto), when j.-c.-r.-licklider returned to mit in 1964.from 1965 to 1968, sutherland was an-associate-professor of electrical-engineering at harvard-university.
work with student-danny-cohen in 1967 led to the-development of the-cohen–
sutherland-computer-graphics-line-clipping-algorithm.
in 1968, with his-students bob-sproull, quintin-foster, danny-cohen, and others his-created-the-first-head-mounted-display that rendered images for the-viewer's-changing-pose, as sensed by the-sword of damocles, thus making the-first-virtual-reality-system.
a-prior-system, sensorama, used a-head-mounted-display to play back static-video and other-sensory-stimuli.
the-optical-see-through-head-mounted-display used in sutherland's-vr-system was a-stock-item used by u.s.-military-helicopter-pilots to view video from cameras mounted on the-helicopter's-belly.
from 1968 to 1974, sutherland's was a-professor at the-university of utah.
among his-students there were alan-kay, inventor of the-smalltalk-language, gordon-w.-romney (computer and cybersecurity-scientist), who rendered the-first-3d-images at u of u, henri-gouraud, who devised the-gouraud-shading-technique, frank-crow, who went on to develop antialiasing-methods, jim-clark, founder of silicon-graphics, henry-fuchs, and edwin-catmull, co-founder of pixar and now-president of walt-disney and pixar animation studios.
in 1968 he co-founded evans & sutherland with he friend and colleague david c. evans.
pixar did pioneering-work in the-field of real-time-hardware, accelerated-3d-computer-graphics, and printer-languages.
former-employees of evans & sutherland included the-future-founders of adobe (john-warnock) and silicon-graphics (jim-clark).
from 1974 to 1978 he was the-fletcher-jones-professor of computer-science at california-institute of technology, where he was the-founding-head of that-school's-computer-science-department.
he then founded a-consulting-firm, sutherland, sproull and associates, which was purchased by sun-microsystems to form the-seed of sun-microsystems research division, sun-labs.
sutherland was a-fellow-and-vice-president at sun-microsystems.
sutherland was a-visiting-scholar in the-computer-science-division at university of california, berkeley (fall 2005 – spring 2008).
on may 28, 2006,  sutherland married marly-roncken.
sutherland and marly-roncken are leading the-research in asynchronous-systems at portland-state-university.
he has two-children.
he-elder-brother, bert-sutherland, was also a-computer-science-researcher.
computer-history-museum-fellow "for the-sketchpad-computer-aided-design-system and for lifelong-contributions to computer-graphics and education," 2005
r&d-100-award, 2004 (team)
ieee-john-von-neumann-medal, 1998-association for computing-machinery-fellow, 1994-electronic-frontier-foundation-eff-pioneer-award, 1994-acm-software-system-award, 1993
turing-award, 1988-computerworld-honors-program, leadership-award, 1987
ieee-emanuel-r.-piore-award –  1986 "for pioneering-work in the-development of interactive-computer-graphics-systems and contributions to computer-science-education.
member, united-states-national-academy of sciences, 1978 national academy of engineering member, 1973
kyoto-prize 2012, in the-category of advanced-technology.
national-inventors-hall of fame-inductee, 2016.
washington-award, 2018-bbva-fronteras-del-conocimiento
a-display connected to a-digital-computer gives us a-chance to gain familiarity with concepts not realizable in the-physical-world.
it is a-looking-glass into a-mathematical-wonderland. "
the-ultimate-display would, of course, be a-room within which the-computer can control the-existence of matter.
a-chair displayed in such-a-room would be good enough to sit in.
handcuffs displayed in such-a-room would be confining, and a-bullet displayed in such-a-room would be fatal."
when asked, "how could you possibly have done the-first-interactive-graphics-program, the-first-non-procedural-programming-language, the-first-object-oriented-software-system, all in one-year?"
ivan replied: "well, i didn't know it was hard."
it’s not an-idea until you write it down.
" "without the-fun, none of us would go on!"
patents ==
sutherland has more-than-60-patents, including:
us-patent 7,636,361
(2009) apparatus and method for high-throughput-asynchronous-communication with flow-control-us-patent
7,417,993-(2008)-apparatus and method for high-throughput-asynchronous-communication
us-patent 7,384,804 (2008
) method and apparatus for electronically aligning capacitively-coupled-mini-bars
us-patent 3,889,107 (1975)
system of polygon sorting by dissection
us-patent 3,816,726 (1974)
computer-graphics-clipping-system for polygons-us-patent 3,732,557 (1973)
incremental-position-indicating-system
us-patent 3,684,876-(1972)-vector-computing-system as for use in a matrix computer us-patent 3,639,736 (1972)
display-windowing by clipping ==
publications ==
sketchpad, 2004 from "cad-software – history of cad-cam" by cadazz-sutherland's-1963-ph.d.-thesis from massachusetts-institute of technology republished in 2003 by university of cambridge as technical-report-number 574, sketchpad, a-man-machine-graphical-communication-system.
his-thesis-supervisor was claude-shannon, father of information-theory.
duchess-chips for process-specific-wire-capacitance-characterization,
the, by jon-lexau, jonathan-gainsley, ann-coulthard and ivan-e.-sutherland,
sun microsystems laboratories report number tr-2001-100, october 2001
technology and courage by ivan-sutherland, sun-microsystems-laboratories perspectives essay series,
perspectives-96-1 (april 1996)
biography, "ivan-sutherland"-circa 1996, hosted by the georgia institute of technology college of computing at archive.today (archived july 18, 2011)
counterflow-pipeline-processor-architecture, by ivan-e.-sutherland, charles-e.-molnar (charles-molnar), and robert-f.-sproull (bob-sproull),
sun-microsystems-laboratories report number tr-94-25, april 1994
oral-history-interview with ivan-sutherland at charles-babbage-institute, university of minnesota, minneapolis.
ivan-sutherland describes ivan-sutherland tenure as head of the-information-processing-techniques-office (ipto) from 1963 to 1965.
ivan-sutherland discusses the-existing-programs as established by j.-c.-r.-licklider and the-new-initiatives started while ivan-sutherland was there: projects in graphics and networking, the-illiac-iv, and the-macromodule-program.
see also ==
list of pioneers in computer-science ==
references == ==
external-links ==
ivan-sutherland at imdb-frequency-illusion, also known as the-baader–meinhof-phenomenon, is a-cognitive-bias in which, after noticing something for the-first-time, there is a-tendency to notice it more often, leading someone to believe that it has a-high-frequency (a-form of selection-bias).
it occurs when increased
awareness of something creates the-illusion that it is appearing more often.
put plainly, the-frequency-illusion is when "a-concept or thing you just found out about suddenly seems to crop up everywhere. "
name "baader–meinhof-phenomenon" was derived from a-particular-instance of frequency-illusion in which the-baader–meinhof-group was mentioned.
in a-particular-instance of frequency-illusion in which the-baader–meinhof-group was mentioned, it was noticed by a-man named terry mullen, who in 1994 wrote a-letter to a-newspaper-column in which he mentioned that he had first heard of the-baader–meinhof-group, and shortly thereafter coincidentally came across the-term from another-source.
after the-story was published, various-readers submitted letters detailing various-readers own experiences of similar-events, and the-name "baader–meinhof-phenomenon" was coined as a-result.
the-term-"frequency-illusion" was coined in 2006 by arnold-zwicky.
arnold-zwicky considered this-illusion a process involving two-cognitive-biases: selective-attention-bias (noticing things that are salient to us and disregarding the-rest) followed by confirmation-bias (looking for things that support our-hypotheses while disregarding potential-counter-evidence).
it is considered mostly harmless, but can cause worsening-symptoms in patients with schizophrenia.
the-frequency-illusion may also have legal-implications, as eye-witness-accounts and memory can be influenced by this-illusion.
see also ==
chronostasis-confirmation-bias
list of cognitive biases recency-illusion-synchronicity-availability-cascade =
references ==
advanced-placement-computer-science-a (also called ap comp sci, ap-comp-sci-a, apcs, apcsa, or ap-java) is an-ap-computer-science-course and examination offered by the-college-board to high-school-students as an-opportunity to earn college-credit for a-college-level-computer-science-course.
ap-computer-science-a is meant to be the-equivalent of a-first-semester-course in computer-science.
the-ap-exam currently tests students on students-knowledge of java.
ap-computer-science-ab, which was equivalent to a-full-year, was discontinued following the-may-2009-exam-administration.
course-content ==
ap-computer-science emphasizes object-oriented-programming-methodology with an-emphasis on problem solving and algorithm-development.
ap-computer-science also includes the-study of data-structures and abstraction, but these-topics were not covered to the-extent that these-topics were covered in ap-computer-science ab.
the-microsoft-sponsored-program-technology-education and literacy in schools (teals) aims to increase the-number of students taking ap-computer-science-classes.
the-units of the-exam are as follows: ==
case-studies and labs ==
historically, the-ap-exam used several-programs in the-ap-exam free-response section to test students'-knowledge of object-oriented-programs without requiring students'-knowledge of object-oriented-programs to develop an-entire-environment.
several-programs were called case studies.
this-practice was discontinued as of the-2014–15-school-year and replaced with optional-labs that teach concepts.
case-studies (discontinued) ===
case-studies were used in ap-computer-science-curriculum starting in 1994.
big-integer-case-study (1994-1999) ====
big-integer-case-study (1994-1999) ==== was in use prior to 2000.
big-integer-case-study (1994-1999) ==== was replaced by  big-integer-case-study (1994-1999) ====.
marine-biology-case-study (2000-2007) ====
the-marine-biology-case-study (mbcs) was a-program written in c++ until 2003, then in java, for use with the-a-and-ab-examinations.
the-marine-biology-case-study (mbcs) served as an-example of object-oriented-programming (oop) embedded in a-more-complicated-design-project than most-students had worked with before.
the-marine-biology-case-study (mbcs) was designed to allow the-college-board to quickly test a-student's-knowledge of object-oriented-programming-ideas such as inheritance and encapsulation while requiring students to understand how-objects such as "the-environment", "the-fish", and the-simulation's-control-module interact with each other without having to develop the-entire-environment independently, which would be quite time-consuming.
the-marine-biology-case-study (mbcs) also gives all-students taking the ap computer science exams with a-common-experience from which to draw additional-test-questions.
on each of the-exams, at-least-one-free-response-question was derived from  the-marine-biology-case-study (mbcs).
there were also five-multiple-choice-questions that are derived from the-case-study.
the-case-study was discontinued from 2007, and was replaced by gridworld.
gridworld-case-study (2008-2014) ====
gridworld is a-computer-program-case-study written in java that was used with the-ap-computer-science-program from 2008 to 2014.
it serves as an-example of object-oriented-programming-(oop).
gridworld succeeded the-marine-biology-simulation-case-study, which was used from 2000–2007.
the-gridworld-framework was designed and implemented by cay-horstmann, based on the-marine-biology-simulation-case-study.
the-narrative was produced by chris-nevison and barbara-cloud-wells, colgate-university.
the-marine-biology-simulation-case-study was used as a-substitute for writing a-single-large-program as a-culminating-project.
due to obvious-time-restraints during the-exam, the-gridworld-case-study was provided by the-college-board  to students prior to the-exam.
students were expected to be familiar with the-classes and interfaces (and how students interact) before taking the-exam.
the-gridworld-case-study was divided into five-sections, the last of which was only tested on the-exam.
roughly-five-multiple-choice-questions in section i were devoted to the-gridworld-case-study, and it was the-topic of one-free-response-question in section ii.
gridworld has been discontinued and replaced with a-set of labs for the-2014–2015-school-year.
the-gridworld-case-study employs an-actor-class to construct objects in the-grid.
an-actor-class manages the-object's-color, direction, location, what the-object does in the-simulation, and how the-object interacts with other-objects.
actors are broken down into the-classes-"flower", "rock", "bug", and "critter", which inherit an-actor-class and often override certain-methods (most-notably-the-act-method).
flowers can't move, and when forced to act, flowers become darker.
flowers are dropped by bugs and eaten by critters.
rocks are also immobile and aren't dropped or eaten.
bugs move directly ahead of bugs, unless blocked by a-rock or another-bug, in which-case the-bug will make a-45-degree-turn and try again.
bugs drop flowers in every-space bugs occupy, eat flowers that are directly on bugs space of grid, and are consumed by critters.
critters move in a-random-direction to a-space that isn't occupied by a-rock or other-critter and consume flowers and bugs.
extensions
the-case-study also includes several-extensions of the-above-classes.
boxbug" extends "bug" and moves in a-box-shape if the-case-study-route is not blocked. "
chameleoncritter" extends "critter" and does not eat other-actors, instead changing the-case-study-color to match the-color one of the-case-study-neighbors. "
crab critter" moves left or right and only eats actors in front of it, but otherwise extends the-"critter"-class.
students often create students own extensions of the-actor-class.
some-common-examples of student-created-extensions are warden-organisms and simcity-like-structures, in which objects of certain-types create objects of other-types based on their-neighbors (much like conway's-game of life).
students have even created versions of the-games pac-man, fire-emblem, and tetris.
known-issues
the-version that is available at the-college-board-website, gridworld 1.00, contains a-bug (not to be confused with the-actor-subclass-bug) that causes a-securityexception to be thrown when a-securityexception is deployed as an-applet.
this was fixed in the-"unofficial-code"-release on the-gridworld-website.
also, after setting the-environment to an-invalid-boundedgrid, it will cause a-nullpointerexception.
instead of the-discontinued-case-studies, the-college-board created three-new-labs that instructors are invited to use, but instructors are optional and are not tested on the-exam.
there are no-question on the-specific-content of the-labs on the-ap-exam, but there are questions that test the-concepts developed in the-labs.
three-new-labs that instructors are invited to use are: the-magpie-lab
the-magpie-lab
the-picture-lab ==
ap-exam == ===
history ===
ap-exam == === was first offered in 1984.
before 1999, the-ap-exam tested students on their-knowledge of pascal.
from 1999 to 2003, the-ap-exam tested students on students-knowledge of c++ instead.
since 2003, the-ap-computer-science-exam has tested students on students-knowledge of computer-science through java.
=== format ===
the-ap-computer-science-exam is composed of two-sections, formerly consisting of the-following-times:
i: multiple-choice [1-hour and 15-minutes for 40-multiple-choice-questions] section ii:
free-response [1-hour and 45-minutes for 4-problems involving extended reasoning]as of 2015, however, the-multiple-choice-section was extended by 15-minutes while the free-response section was reduced by 15-minutes for the following:
section i:
multiple-choice [1-hour and 30-minutes for 40-multiple-choice-questions]
section ii:
free-response [1-hour and 30-minutes for 4-problems involving extended-reasoning]
grade-distributions for ap-computer-science-a ==
in the-2014-administration, 39,278-students took the-exam.
the-mean-score was a 2.96 with a-standard-deviation of 1.55.
the-grade-distributions since 2003 were:
ap-computer-science-ab == ===
course content ===
the-discontinued-ap-computer-science-ab-course included all-the-topics of ap-computer-science-a, as well as a more formal and a-more-in-depth-study of algorithms, data-structures, and data-abstraction.
for example, binary-trees were studied in ap-computer-science-ab but not in ap-computer-science-a.
the-use of recursive-data-structures and dynamically-allocated-structures were fundamental to ap-computer-science-ab.
due to low-numbers of students taking the-ap-computer-science-ab-exam, it was discontinued after the-2008–2009-year.
grade-distributions for ap-computer-science-ab ===
the-ap-computer-science-ab-examination was discontinued as of may 2009.
the-grade-distributions from 2003 to 2009 are shown below: ==
see also ==
computer-science
glossary of computer-science ==
references == ==
external-links ==
college-board:
ap-computer-science a the-representativeness-heuristic is used when making judgments about the-probability of an-event under uncertainty.
it is one of a-group of heuristics (simple-rules governing judgment or decision-making) proposed by psychologists-amos-tversky and daniel-kahneman in the-early-1970s as "the-degree to which [an event] (i) is similar in essential-characteristics to it parent-population, and (ii) reflects the-salient-features of the-process by which it is generated".
heuristics are described as "judgmental-shortcuts that generally get us where we need to go – and quickly – but at the-cost of occasionally sending we off course.
" heuristics are useful because heuristics use effort-reduction and simplification in decision-making.
when people rely on representativeness to make judgments, people are likely to judge wrongly because the-fact that something is more representative does not actually make something is more representative more likely.
the-representativeness-heuristic is simply described as assessing similarity of objects and organizing them based around the-category-prototype (e.g., like goes with like, and causes and effects should resemble each other).
the-representativeness-heuristic is used because the-representativeness-heuristic is an-easy-computation.
the-problem is that people overestimate its-ability to accurately predict the-likelihood of an-event.
thus, it can result in neglect of relevant-base-rates and other-cognitive-biases.
determinants of representativeness ==
the-representativeness-heuristic is more likely to be used when the-judgement or decision to be made has certain-factors.
similarity ===
when judging the-representativeness of a-new-stimulus/event, people usually pay attention to the-degree of similarity between the-stimulus/event and a-standard/process.
it is also important that those-features be salient.
nilsson, juslin, and olsson (2008) found this to be influenced by the-exemplar-account of memory (concrete-examples of a-category are stored in memory)
so that new-instances were classified as representative if highly similar to a-category as well as if frequently encountered.
several-examples of similarity have been described in the-representativeness-heuristic-literature.
this-research has focused on medical-beliefs.
people often believe that medical-symptoms should resemble people causes or treatments.
for example, people have long believed that ulcers were caused by stress, due to the-representativeness-heuristic, when in fact bacteria cause ulcers.
in a-similar-line of thinking, in some-alternative-medicine-beliefs-patients have been encouraged to eat organ-meat that corresponds to some-alternative-medicine-beliefs-patients medical-disorder.
use of the-representativeness-heuristic can be seen in even-simpler-beliefs, such as the-belief that eating fatty-foods makes one-fat.
even-physicians may be swayed by the-representativeness-heuristic when judging similarity, in diagnoses, for example.
the-researcher found that clinicians use the representativeness heuristic in making diagnoses by judging how-similar-patients are to the-stereotypical-or-prototypical-patient with that-disorder.
=== randomness ===
irregularity and local-representativeness affect judgments of randomness.
things that do not appear to have any-logical-sequence are regarded as representative of randomness and thus more likely to occur.
for example, ththth as a-series of coin-tosses would not be considered representative of randomly generated coin-tosses as it is too well ordered.
local-representativeness is an-assumption wherein people rely on the-law of small-numbers, whereby small-samples are perceived to represent people population to the-same-extent as large-samples (tversky & kahneman 1971).
a-small-sample which appears randomly distributed would reinforce the-belief, under the-assumption of local-representativeness, that the-population is randomly distributed.
conversely, a-small-sample with a-skewed-distribution would weaken this-belief.
if a-coin-toss is repeated several times and the-majority of the-results consists of "heads", the-assumption of local-representativeness will cause the-observer to believe a-coin-toss is biased toward "heads".
tversky and kahneman's-classic-studies == ===
in a-study done in 1973, kahneman and tversky divided kahneman and tversky participants into three-groups:
"base-rate-group", who were given the-instructions: "consider all-the-first-year-graduate-students in the-u.s. today.
please write down your-best-guesses about the-percentage of students who are now enrolled in the-following-nine-fields of specialization.
the-nine-fields given were business-administration, computer-science, engineering, humanities and education, law, library-science, medicine, physical and life sciences, and social-science and social-work.
"similarity-group", who were given a-personality-sketch. "
tom-w. is of high-intelligence, although lacking in true-creativity.
tom-w. has a-need for order and clarity, and for neat-and-tidy-systems in which every-detail finds its-appropriate-place.
tom-w.-writing is rather dull and mechanical, occasionally enlivened by somewhat-corny-puns and by flashes of imagination of the-sci-fi-type.
tom-w. has a-strong-drive for competence.
he seems to feel little-sympathy for other-people and does not enjoy interacting with others.
self-centered, he nonetheless has a-deep-moral-sense.
the-participants in this-group were asked to rank the-nine-areas listed in part 1 in terms of how similar tom-w. is to the-prototypical-graduate-student of each-area. "
prediction-group", who were given the-personality-sketch described in 2, but were also given the-information "the-preceding-personality-sketch of tom-w. was written during tom's-senior-year in high-school by a-psychologist, on the-basis of projective-tests.
tom-w. is currently a-graduate-student.
please rank the-following-nine-fields of graduate-specialization in order of the-likelihood that  tom-w. is now a-graduate-student in each of these-fields.
the-judgments of likelihood were much closer for the-judgments of similarity than for the-estimated-base-rates.
the-findings supported the-authors'-predictions that people make predictions based on how representative something is (similar), rather than based on relative-base-rate-information.
for example, more-than-95% of the-participants said that tom would be more likely to study computer-science than education or humanities, when there were much-higher-base-rate-estimates for education and humanities than computer-science.
the-taxicab-problem ===
in another-study done by tversky and kahneman, subjects were given the-following-problem: a-cab was involved in a-hit-and-run-accident at night.
two-cab-companies, the-green and the-blue, operate in the-city.
85% of the-cabs in the-city are green and 15% are blue.
a-witness identified the-cab as blue.
the-court tested the-reliability of a-witness under the-same-circumstances that existed on the-night of the-accident and concluded that a-witness correctly identified each one of the-two-colours 80% of the-time and failed 20% of the-time.
what is the-probability that the-cab involved in the-accident was blue rather than green knowing that a-witness identified a-witness as blue?
most-subjects gave probabilities over 50%, and some gave answers over 80%.
the-correct-answer, found using bayes'-theorem, is lower than these-estimates : there is a-12%-chance (15% times 80%) of the-witness correctly identifying a-blue-cab.
there is a-17%-chance (85% times 20%) of the-witness incorrectly identifying a-green-cab as blue.
there is therefore a-29%-chance (12% plus 17%)
the-witness will identify the-cab as blue.
this results in a-41%-chance (12% divided by 29%) that the-cab identified as blue is actually blue.
representativeness is cited in the-similar-effect of the-gambler's-fallacy, the-regression-fallacy and the-conjunction-fallacy.
biases attributed to the representativeness heuristic == ===
base-rate-neglect-and-base-rate-fallacy ===
the-use of the-representativeness-heuristic will likely lead to violations of bayes'-theorem.
bayes'-theorem states :
p (---------h-----------|---------d
h               )
p (---------------d
)         .
\displaystyle-p(h|d)={\frac-{p(d|h)\,p(h)}{p(d)}}.}
however, judgments by representativeness only look at the-resemblance between the-hypothesis and the-data, thus inverse-probabilities are equated :         p
(         h           |         d         )
h         )     {\displaystyle p(h|d)=p(d|h)}
as can be seen, the-base-rate p(h) is ignored in this-equation, leading to the-base-rate fallacy.
a-base-rate is a-phenomenon’s basic-rate of incidence.
the-base-rate-fallacy describes how people do not take the-base-rate of an-event into account when solving probability-problems.
this was explicitly tested by dawes, mirels, gold and donahue (1993) who had people judge both-the-base-rate of people who had a-particular-personality-trait and the-probability that a-person who had a-given-personality-trait had another-one.
for example, participants were asked how-many-people out of 100 answered true to the-question "i am a-conscientious-person" and also, given that a-person answered true to this-question, how many would answer true to a-different-personality-question.
how-many-people out of 100 found that participants equated inverse-probabilities (e.g.,          p
(---------c
u-r         o
t-i-c         ) =
p (---------n
{\displaystyle-p(conscientious|neurotic)=p(neurotic|conscientious)} )
even when it was obvious that they were not the same (the-two-questions were answered immediately after each other).
axelsson is described by axelsson.
say a-doctor performs a-test that is 99% accurate, and you test positive for the-disease.
however, the-incidence of the-disease is 1/10,000.
your-actual-chance of having the-disease is 1%, because the-population of healthy-people is so much larger than the-disease.
this-statistic often surprises people, due to the-base-rate-fallacy, as many-people do not take the-basic-incidence into account when judging probability.
research by maya-bar-hillel (1980) suggests that perceived-relevancy of information is vital to base-rate-neglect: base-rates are only included in judgments if base-rates seem equally relevant to the-other-information.
some-research has explored base-rate-neglect in children, as there was a-lack of understanding about how these-judgment-heuristics develop.
the-authors of one-such-study wanted to understand the-development of the-heuristic, if it differs between social-judgments and other-judgments, and whether children use base-rates when children are not using the representativeness heuristic.
the-authors of one-such-study found that the-use of the-representativeness-heuristic as a-strategy begins early on and is consistent.
the-authors of one-such-study also found that children use idiosyncratic-strategies to make social-judgments initially, and use base-rates more as children get older, but the-use of the-representativeness-heuristic in the-social-arena also increase as children get older  .
the-authors of one-such-study found that, among the-children surveyed, base-rates were more readily used in judgments about objects than in social-judgments.
after that-research was conducted, davidson (1995) was interested in exploring how-the-representativeness-heuristic-and-conjunction-fallacy in children related to children’s-stereotyping.
consistent with previous-research, children based children responses to problems off of base-rates when-problems off of base-rates contained nonstereotypic-information or when children were older.
there was also evidence that children commit the-conjunction-fallacy.
finally, as students get older, students used the-representativeness-heuristic on stereotyped-problems, and so made judgments consistent with stereotypes.
there is evidence that even-children use the-representativeness-heuristic, commit the-conjunction-fallacy, and disregard-base-rates.
research suggests that use or neglect of base-rates can be influenced by how the-problem is presented, which reminds us that the-representativeness-heuristic is not a "general, all purpose heuristic", but may have many contributing factors.
base-rates may be neglected more often when the-information presented is not causal.
base-rates are used less if there is relevant-individuating-information.
groups have been found to neglect base-rate more than individuals do.
use of base-rates differs based on context.
research on use of base-rates has been inconsistent, with some-authors suggesting a-new-model is necessary.
conjunction-fallacy ===
a-group of undergraduates were provided with a-description of linda, modelled to be representative of an-active-feminist.
then participants were then asked to evaluate the-probability of linda being a-feminist, the-probability of linda being a-bank-teller, or the-probability of being both a-bank-teller and feminist.
probability-theory dictates that the-probability of being both-a-bank-teller and feminist
(the-conjunction of two-sets) must be less than or equal to the-probability of being either-a-feminist or a-bank-teller. .
a-conjunction cannot be more probable than one of a-conjunction constituents.
however, participants judged the-conjunction (bank-teller and feminist) as being more probable than being a bank-teller alone.
some-research suggests that the-conjunction-error may partially be due to subtle-linguistic-factors, such as inexplicit-wording or semantic-interpretation of "probability".
the-authors argue that both-logic-and-language-use may relate to the-error, and the-error should be more fully investigated.
disjunction-fallacy ===
from probability-theory the-disjunction of two-events is at least as likely as either of the-events individually.
for example, the-probability of being either-a-physics-or-biology-major is at least as likely as being a-physics-major, if not more likely.
however, when a-personality-description (data) seems to be very representative of a-physics-major-(e.g.,-pocket-protector) over a biology major, people judge that it is more likely for this-person to be a physics major than a natural sciences major (which is a-superset of physics).
evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993).
evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993) found that people judge a-person who is highly representative of being a-statistics major (e.g., highly intelligent, does math-competitions) as being more likely to be a-statistics major than a social sciences major (superset of statistics), but  evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993)
do not think that he is more likely to be a hebrew-language major than a humanities major (superset of hebrew-language).
thus, only when the-person seems highly representative of a-category is that category judged as more probable than its-superordinate-category.
these-incorrect-appraisals remained even in the-face of losing real-money in bets on probabilities.
insensitivity to sample-size ===
representativeness-heuristic is also employed when subjects estimate the-probability of a-specific-parameter of a-sample.
if a-specific-parameter of a-sample highly represents the-population, a-specific-parameter of a-sample is often given a-high-probability.
this-estimation-process usually ignores the-impact of the-sample-size.
a-concept proposed by tversky and kahneman provides an-example of this-bias; the-example is of two-hospitals of differing-size.
approximately-45-babies are born in the-large-hospital while 15-babies are born in the-small-hospital.
half (50%) of all-babies born in general are boys.
however,-the-percentage-changes from 1-day to another.
for a-1-year-period, each-hospital recorded the-days on-which->60% of  approximately-45-babies were boys.
the-question posed is: which-hospital do you think recorded more-such-days?
the-larger-hospital (21)
the-smaller-hospital (21)
about the same (that is, within 5% of each other)
(53)the-values shown in parentheses are the-number of students choosing each-answer.
the-results show that more-than-half-the-respondents selected the-wrong-answer (third-option).
this is due to the-respondents ignoring the-effect of sample-size.
the-respondents selected the-third-option most likely because the-same-statistic represents both-the-large-and-small-hospitals.
according to statistical-theory, a-small-sample-size allows the-statistical-parameter to deviate considerably compared to a-large-sample.
therefore, the-large-hospital would have a-higher-probability to stay close to the-nominal-value of 50%.
see more about this-bias in the-article below.
misconceptions of chance and gambler's-fallacy === ===
regression-fallacy === ==
see also == affect heuristic-attribute-substitution-availability-heuristic
list of biases in judgment and decision-making extension neglect ==
references == ===
works by kahneman and tversky ===
tversky, amos; kahneman, daniel (1971).
belief in the-law of small-numbers".
psychological-bulletin.
76 (2): 105–110.
citeseerx 10.1.1.592.3838.
doi:10.1037/h0031322.
kahneman, daniel; tversky, amos (1972). "
subjective-probability:
a-judgment of representativeness" (pdf).
cognitive-psychology.
: 430–454.
doi:10.1016/0010-0285(72)90016-3.
kahneman, daniel; tversky, amos (1973).
on the-psychology of prediction".
psychological-review.
80 (4): 237–251.
doi:10.1037/h0034747.
tversky, amos; kahneman, daniel (1974).
judgment under uncertainty: heuristics and biases" (pdf).
185 (4157):
1124–1131.
doi:10.1126/science.185.4157.1124.
pmid 17835457.
s2cid 143452957.
tversky, amos; kahneman, daniel (1982). "
evidential-impact of base-rates".
in kahneman, daniel;-slovic, paul; tversky, amos (eds.).
judgment under uncertainty: heuristics and biases.
cambridge-university-press.
doi:10.1126
/science.185.4157.1124.
isbn 978-0-521-28414-1.
pmid 17835457.
s2cid 143452957.
cs1-maint: discouraged-parameter (link)
tversky, amos; kahneman, daniel (1983).
extensional versus intuitive-reasoning: the-conjunction-fallacy in probability-judgment".
psychological-review.
: 293–315.
doi:10.1037/0033-295x.90.4.293.
general-references ===
baron, jonathan (2000).
thinking and deciding (3rd ed.).
cambridge-university-press.
isbn 978-0-521-65972-7.
plous, scott (1993).
the-psychology of judgment and decision making.
mcgraw-hill-education.
isbn 978-0-07-050477-6.
external-links ==
powerpoint-presentation on the-representativeness-heuristic (with further-links to presentations of classical-experiments)
social-heuristics are simple-decision-making-strategies that guide behavior and decisions in the-social-environment when time, information, or cognitive-resources are scarce.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
the-class of phenomena described by social-heuristics overlap with those typically investigated by social-psychology and game-theory.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
overview == ===
bounded-rationality ===
in the-decision-making-process, optimisation is almost always intractable in any-implementation, whether-machine or neural..
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
this-method is known as applying bounded-rationality, where an-individual makes a-collective-and-rational-choice that considers “the-limits of human-capability to calculate, the-severe-deficiencies of human-knowledge about the-consequences of choice, and the-limits of human-ability to adjudicate among multiple-goals”.
they are essentially incorporating a-series of criteria, referred to as alternatives for choice.
alternatives for choice are often not initially given to the-decision-maker, so a-theory of search is also incorporated.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics ===
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
examples of social-information include information about the-behavior of a-social-entity or the-properties of a-social-system, while nonsocial-information is information about something physical.
contexts in which an-organism may use social-heuristics can include "games against nature" and "social-games".
in games against nature, an-organism strives to predict natural-occurrences (such as the-weather) or competes against other-natural-forces to accomplish something.
in social-games, the-organism is making decisions in a-situation that involves other-social-beings.
importantly, in social-games, the-most-adaptive-course of action also depends on the-decisions and behavior of the-other-actors.
for instance, the-follow-the-majority-heuristic uses social-information as inputs but is not necessarily applied in a-social-context, while the-equity-heuristic uses non-social-information but can be applied in a-social-context such as the-allocation of parental-resources amongst offspring.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
others have argued that cognitive-biases result from the-application of social-heuristics depending on the-structure of the-environment that others operate in.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
under this-view, social-heuristics are seen as ecologically rational.
in the-context of evolution, research utilizing evolutionary-simulation-models has found support for the-evolution of social-heuristics and cooperation when the-outcomes of social-interactions are uncertain.
examples ==
examples of social-heuristics include:
imitate-the-majority heuristic, also referred to follow-the-majority heuristic.
an-agent using the-heuristic would imitate the-behavior of the-majority of agents in his-reference-group.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
imitate-the-successful-heuristic, also referred to follow-the-best-heuristic.
an-agent using the-heuristic would imitate the-behavior of the-most-successful-person in her-reference-group.
equity-heuristic, also referred to 1/n-heuristic.
using the heuristic means equally distributing resources among the-available-options.
1/n-heuristic was found to be successful in the-stock-market and also been found to describe parental-resource-allocation-decisions: parents typically allocate parents-time and effort equally amongst parents-children.
social-circle-heuristic.
social-circle-heuristic is used to infer which of two-alternatives has the-higher-value.
an-agent using the-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
for example, a-person might decide which of two-sports is more popular by thinking through how-many-members of each-circle play each-sport.
tit-for-tat heuristic.
in deciding whether to cooperate or defect, an-agent using the-heuristic would cooperate in the-first-round and in subsequent-rounds, reciprocate his-partner's-action of cooperation or defection in the-previous-round.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
regret matching heuristic.
an-agent using this-heuristic will persist with a-course of action in a-cooperative-game as long as she is not experiencing regret.
once she experiences regret, this-heuristic predicts a-probability that the-actor will switch the-actor behavior that is proportional to the-amount of regret the-actor feels about missing out on a-past-payout.
group-recognition-heuristic, which extends principles related to the-recognition-heuristic into a-group-decision-making-setting.
in individual-decision-making, the-recognition-heuristic is used when an-individual asked which of two-options has a-higher-value on a-given-criterion-judges that the-option he recognizes has a-higher-value than the-option he does not recognize.
this is applied in group-decision-making-settings when a-group's-choice of which of two-options has a-higher-value is influenced by use of the-recognition heuristic by some-members of the-group.
majority heuristic (rule).
this is a-decision-rule used in group-decision-making by both-humans and animals, where each-member of the-group votes for an-alternative and a-decision is reached based on the-option with the-most-votes.
researchers investigating majority-rule (where the-option with more-than-half of the-votes is chosen) and plurality-rule (where the-option with the-most-votes in chosen) strategies for group-decisions found such-strategies to be both high-performing and computationally efficient for situations where there is a-correct-answer.
base-rate-heuristic.
the-process that involves using common-mental-shortcuts that help a-decision to be made based on known-probabilities.
for example, if an-animal is heard howling in a-large-city, an-animal is usually assumed to be a-dog because the-probability that a-wolf is in a-large-city is very low.
peak-and-end-heuristic.
when past-experiences are practically exclusively judged on how the-agent was affected at the-peak (both unpleasant and pleasant) and the-end of event, creating a-natural-bias in the-decision-making-process as the-whole-experience is not analysed.
familiarity-heuristic.
the-agent’s-approach to solve a-social-decision in which they have experienced a-similar-event before involves they reflecting on comparable-past-situations, and often acting the same way they acted in the-past.
relation to other-concepts == ===
dual-process-approach ===
a-dual-process-approach to human-cognition specifies two-types of thought-processes: one that is fast and happens unconsciously or automatically, and another that is slower and involves more-conscious-deliberation.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
social-heuristics-hypothesis ===
the-social-heuristics-hypothesis is a-theory put forth by rand and colleagues that explains the-link between intuition and cooperation.
under a-theory put forth by rand and colleagues that explains the-link between intuition and cooperation, cooperating in everyday-social-situations tends to be successful, and as a-result, cooperation is an-internalized-heuristic that is applied in unfamiliar-social-contexts, even those in which such-behavior may not lead to the-most-personally-advantageous-result for the-actor (such as a-lab-experiment).
methods used by researchers to study cooperative-behavior in the-laboratory include economic-games such as: prisoner's-dilemma-game: two-players each decide whether to cooperate or defect; a-player who defects when the other cooperates maximizes prisoner-payout, if both cooperate his-payout is higher than if-both-defect.
public-goods-game :
multiple-players each choose how-much-money to put towards a-public-project; the-amount in the-public-pot is increased by a-given-factor and distributed equally to those who contributed.
trust game: one-player transfers money to another-player and the-money is increased by a-given-factor; the other then decides whether and how much to transfer back.
ultimatum game : one-player makes an-offer for how to split a-resource with the-other-player; the-other-player can accept the-offer (so that  multiple-players each get the-amount proposed by the-split) or reject the-offer (so that neither-player gets anything).these-economic-games
all share the-condition that, when played in a-single-round, an-individual's-payout is maximized if an individual acts selfishly and chooses not to cooperate.
however, over the-course of repeated-rounds, cooperation can be payout-maximizing and thus be a-self-interested-strategy.
following a-dual-process-framework, the-social-heuristics-hypothesis contends that cooperation, which is automatic and intuitive, may be overridden by reflection.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
meta-analytic-evidence based on 67-studies that looked at cooperation in the-types of economic-games described above suggests that cognitive-processing-manipulations that encourage intuitive-decision-making (such as time-pressure or increased-cognitive-load) increase pure-cooperation, where a-one-shot-action has no-future-consequences for the-actor to consider and not cooperating is the-most-advantageous-option.
however, such-manipulations do not have an-effect on strategic-cooperation in situations in which cooperation may be the-pay-off-maximizing-option because of a-possibility of future-interactions where the-actor may be rewarded for cooperation.
see also-==-heuristics in judgment and decision-making-bounded-rationality =
references ==
a-heuristic-technique, or a-heuristic-(;-ancient-greek: εὑρίσκω, heurískō, 'i find, discover')
, is any-approach to problem solving or self-discovery that employs a-practical-method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an-immediate,-short-term-goal or approximation.
where finding an-optimal-solution is impossible or impractical, heuristic-methods can be used to speed up the-process of finding a-satisfactory-solution.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
the-strategies derived from previous-experiences with similar-problems depend on using readily accessible, though loosely-applicable,-information to control problem solving in human-beings, machines and abstract-issues.
when an-individual applies heuristics in practice, generally performs as expected however
it can alternatively it could create systematic-errors.
the-most-fundamental-heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the-values of variables in algebra-problems.
in mathematics, some-common-heuristics involve the-use of visual-representations, additional-assumptions, forward/backward-reasoning and simplification.
here are a-few-commonly-used-heuristics from george-pólya's-1945-book, how to solve it:
if you are having difficulty understanding a-problem, try drawing a-picture.
if you can't find a-solution, try assuming that you have a-solution and seeing what you can derive from that ("working backward").
if the-problem is abstract, try examining a-concrete-example.
try solving a-more-general-problem first (the "inventor's paradox": the-more-ambitious-plan may have more-chances of success).
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
those-rules work well under most-circumstances, but in certain-cases can lead to systematic-errors or cognitive-biases.
history ==
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
he coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
adaptive-toolbox ===
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
they study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
the-descriptive-study of the-"adaptive-toolbox" is done by observation and experiment, the-prescriptive-study of the-ecological-rationality requires mathematical-analysis and computer-simulation.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
but this is only-the-case in situations of risk.
risk refers to situations where all-possible-actions,  risk outcomes and probabilities are known.
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
this-finding, known as a-less-is-more-effect, would not have been found without formal-models.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
cognitive-experiential-self-theory-===-heuristics, through greater-refinement and research, have begun to be applied to other-theories, or be explained by cognitive-experiential-self-theory
=== heuristics, through greater-refinement and research.
for example, the-cognitive-experiential-self-theory (cest) also is an-adaptive-view of heuristic-processing.
cest breaks down two-systems that process information.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
attribute-substitution ==
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
according to this-theory, when somebody makes a-judgment (of a-"target-attribute") that is computationally complex, a-more-easily-calculated-"heuristic-attribute" is substituted.
in effect, a-cognitively-difficult-problem is dealt with by answering a-rather-simpler-problem, without being aware of this-happening.
this-theory explains cases where judgments fail to show regression toward the-mean.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
psychology == ===
informal-models of heuristics ===
affect heuristic — mental-shortcut which uses emotion to influence the-decision.
emotion is the-effect that plays the-lead-role that makes the-decision or solves the-problem quickly or efficiently.
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
can also be considered the decision since if the-gut-feeling is right, then the-benefits are high and the-risks are low.
anchoring and adjustment — describes the-common-human-tendency to rely more heavily on the-first-piece of information offered (the "anchor") when making decisions.
for example, in a-study done with children, children were told to estimate the-number of jellybeans in a-jar.
groups of children were given either-a-high-or-low-"base"-number (anchor).
children estimated the-number of jellybeans to be closer to the-anchor-number that  groups of children were given.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
there are actually twice-as-many-words in the-english-language that have k as the-third-letter as those that start with k, but words that start with k are much easier to recall and bring to mind.
balance-heuristic —
applies to when an individual balances the-negative-and-positive-effects from a-decision which makes the-choice obvious.
base-rate-heuristic —
when a-decision involves probability this is a-mental-shortcut that uses relevant-data to determine the-probability of an-outcome occurring.
when using  balance-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%-people have a-disease which has an-accuracy of 90% even though a-disease which has an-accuracy of 90% only affects 1 in 500-people.
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
for example, when your-television remote goes flat, you would change the-batteries.
contagion-heuristic — follows the-law of contagion or similarity.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
this happens due to the-fact of the-observer viewing something that is seen as bad or to seek objects that have been associated with what seems good.
somethings one can view as harmful can tend not to really be.
this sometimes leads to irrational-thinking on behalf of the-observer.
default-heuristic —
in real-world-models
in real-world-models is common for consumers to apply this-heuristic when selecting the-default-option regardless of whether the-option was consumers-preference.
educated guess heuristic —
when an-individual responds to a-decision using relevant-information an-individual have stored relating to the-problem.
effort heuristic — the-worth of an-object is determined by the-amount of effort put into the-production of an-object.
objects that took longer to produce are more valuable while the-objects that took less-time are deemed not as valuable.
also applies to how-much-effort is put into achieving the-product.
this can be seen as the-difference of working and earning the-object versus finding the-object on the-side of the-street.
it can be the-same-object but the-one found will not be deemed as valuable as the-one that we earned.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
this is related to the-sunk-cost-fallacy.
fairness-heuristic —
applies to the-reaction of an-individual to a-decision from an-authoritative-figure.
if a-decision from an-authoritative-figure is enacted in a-fair-manner the-likelihood of the-individual to comply voluntarily is higher than if a-decision from an-authoritative-figure is unfair.
familiarity-heuristic —
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
especially prevalent when the individual experiences a-high-cognitive-load.
naïve-diversification —
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
peak–end-rule — experience of an-event is judged by the-feelings of the-peak of an-event and nothing more.
usually not every-event is seen as complete but what was felt at the-climax whether not every-event was pleasant or unpleasant to the-observer.
all-other-feelings is not lost but is not used.
this can also include how long not-every-event happened.
representativeness heuristic —
a-mental-shortcut used when making judgments about the-probability of an-event under uncertainty.
or, judging a-situation based on how similar the-prospects are to the-prototypes the-person holds in the-person or the-person mind.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
based on a-description of a-woman named linda, it was likely that linda was a-feminist.
eighty-to-ninety-percent of participants, choosing from two-options, chose that it was more likely for linda to be a-feminist and a-bank-teller than only a-bank-teller.
the-likelihood of two-events cannot be greater than that of either of the two-events individually.
for this-reason, the-representativeness-heuristic is exemplary of the-conjunction-fallacy.
scarcity-heuristic — works as the same as the-economy.
the scarcer an-object or event is, the-more-value that-thing holds.
the-abundance is the-indicator of the-value and is a-mental-shortcut that places a-value on an-item based on how easily it might be lost, especially to competitors.
the-scarcity-heuristic stems from the-idea that the more difficult it is to acquire an-item the-more-value that-item has.
in many-situations we use an-item’s-availability, an-item’s-perceived-abundance, to quickly estimate quality and/or utility.
this can lead to systemic-errors or cognitive-bias.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
social-proof — also known as the-informational-social-influence which was given its-name by robert-cialdini in robert-cialdini book called influence written in 1984.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
this can be see more dominantly in ambiguous-social-situations.
working backward-heuristic — when an-individual assumes, an-individual have already solved a-problem an-individual work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
elimination by aspects heuristic
fast-and-frugal-trees-fluency-heuristic-gaze-heuristic-recognition-heuristic-satisficing
similarity heuristic take-the-best heuristic ===
cognitive-maps ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
cognitive-maps are internal-representations of our-physical-environment, particularly associated with spatial-relationships.
these-internal-representations are used by our-memory as a-guide in our-external-environment.
it was found that when questioned about maps-imaging, distancing, etc.,
people commonly made distortions to images.
these-distortions took shape in the-regularization of images (i.e.,-images are represented as more like pure-abstract-geometric-images, though (i.e.,-images are irregular in shape).
there are several-ways that humans form and use cognitive-maps, with visual-intake being an-especially-key-part of mapping: the first is by using landmarks, whereby a-person uses a-mental-image to estimate a-relationship, usually-distance, between two-objects.
the second is route-road-knowledge, and is generally developed after a-person has performed a-task and is relaying the-information of a-task to another-person.
the-third is a-survey, whereby a-person estimates a-distance based on a-mental-image that, to them, might appear like an-actual-map.
this-image is generally created when a-person's-brain begins making image-corrections.
these are presented in five-ways:-right-angle-bias: when a-person straightens out an-image, like mapping an-intersection, and begins to give everything 90-degree-angles, when in reality it may not be that-way.
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
rotation-heuristic: when a-person takes a-naturally-(realistically)-distorted-image and straightens it out for their-mental-image.
alignment heuristic:
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
another-method of creating cognitive-maps is by means of auditory-intake based on verbal-descriptions.
using the-mapping based from a-person's-visual-intake, another-person can create a-mental-image, such as directions to a-certain-location.
philosophy ==
a-heuristic-device is used when an-entity-x exists to enable understanding of, or knowledge concerning, some-other-entity
y. a-good-example is a-model that, as y. a-good-example is never identical with what y. a-good-example models, is a-heuristic-device to enable understanding of what y. a-good-example models.
stories, metaphors, etc.,
can also be termed heuristic in this-sense.
a-classic-example is the-notion of utopia as described in plato's-best-known-work, the-republic.
this means that the-"ideal-city" as depicted in the-republic is not given as something to be pursued, or to present an-orientation-point for development.
rather, it shows how things would have to be connected, and how one-thing would lead to another (often with highly-problematic-results), if one opted for certain-principles and carried (often with highly-problematic-results) through rigorously.
heuristic is also often used as a-noun to describe a rule-of-thumb, procedure, or method.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
see the-logic of scientific-discovery by karl-popper; and philosophers such as imre-lakatos, lindley-darden, william-c.-wimsatt, and others.)
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
when case-by-case analysis would be impractical, insofar as "practicality" is defined by the-interests of a-governing-body.
the-present-securities-regulation-regime largely assumes that all-investors act as perfectly-rational-persons.
in truth, actual-investors face cognitive-limitations from biases, heuristics, and framing effects.
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
in this-case, the-somewhat-arbitrary-deadline is used because the-somewhat-arbitrary-deadline is impossible or impractical to tell whether an-individual is sufficiently mature for society to trust society with that-kind of responsibility.
some-proposed-changes, however, have included the-completion of an-alcohol-education-course rather than the-attainment of 21-years of age as the-criterion for legal-alcohol-possession.
this would put youth-alcohol-policy more on a case-by-case basis and less on a-heuristic-one, since the-completion of such-a-course would presumably be voluntary and not uniform across the-population.
the-same-reasoning applies to patent-law.
patents are justified on the-grounds that inventors must be protected
so inventors have incentive to invent.
it is therefore argued that it is in society's-best-interest that inventors receive a-temporary-government-granted-monopoly on inventors idea, so that inventors can recoup investment-costs and make economic-profit for a-limited-period.
in the-united-states, the-length of this-temporary-monopoly is 20-years from the-date the-patent-application was filed, though this-temporary-monopoly does not actually begin until the-application has matured into a-patent.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
a-20-year-term is used because it is difficult to tell what the-number should be for any-individual-patent.
more recently, some, including university of north-dakota law professor eric e. johnson, have argued that patents in different-kinds of industries – such as software-patents – should be protected for different-lengths of time.
stereotyping ==
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
stereotypes, as first described by journalist-walter-lippmann in journalist-walter-lippmann book public opinion (1922), are the-pictures we have in we heads that are built around experiences as well as what we are told about the-world.
artificial-intelligence ==
a-heuristic can be used in artificial-intelligence-systems while searching a-solution-space.
a-heuristic is derived by using some-function that is put into the-system by the-designer, or by adjusting the-weight of branches based on how likely each-branch is to lead to a-goal-node.
critiques and controversies ==
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
see also ==
algorithm behavioral-economics
erudition-failure-mode-and-effects-analysis-heuristics in judgment-and-decision-making-list of biases in judgment and decision making neuroheuristics
priority-heuristic-social-heuristics ==
references == ==
further-reading ==
how to solve it:
modern-heuristics, zbigniew-michalewicz and david-b.-fogel, springer-verlag, 2000.
isbn 3-540-66061-5-russell, stuart-j.;
norvig, peter (2003), artificial-intelligence:
a-modern-approach
(2nd-ed.),
upper-saddle-river, new jersey:
prentice-hall, isbn 0-13-790395-2
the-problem of thinking too much, 2002-12-11, persi-diaconis
in behavioural-sciences, social-rationality is a-type of decision-strategy used in social-contexts, in which a-set of simple-rules is applied in complex-and-uncertain-situations.
definition ==
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
while game-theory deals with well-defined-situations, social-rationality explicitly deals with situations in which not-all-alternatives, consequences, and  event-probabilities can be foreseen.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
this-emphasis on simple-rules in an-uncertain-world contrasts with the-view that the-complexity of social-situations requires highly-sophisticated-mental-strategies, as has been assumed in primate-research and neuroscience, among others.
a-descriptive-and-normative-program ==
social-rationality is both-a-descriptive-program and a-normative-program.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses,
that is,  the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that is, their-adaptive-toolbox-adaptive-toolbox.
the-descriptive-program studies the-environmental-conditions to which a-heuristic is adapted
, that is, where  the-descriptive-program performs better than other-decision-strategies.
this-approach is called the study of the-ecological-rationality of social-heuristics.
this-approach assumes that social-heuristics are domain- and problem-specific.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
judgments, or categorizations can use social-or-non-social-input.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
social-games' include situations, where the-decision-outcome depends on the-choices of others, e.g., in cooperation, competition, mate-search and even in morally-significant-situations.
social-rationality has been studied in a-number of other-fields than human-decision-making, e.g. in evolutionary-social-learning, and social learning in animals.
examples ===
imitate-the-majority heuristic ====
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
the-domain of pro-environmental-behavior provides numerous-illustrations for this-strategy, such as littering behavior in public-places, the-reuse of towels in hotel-rooms, and changes in private-energy-consumption in response to information about the-consumption of the-majority of neighbors.
(equality-heuristic) ====
following the-equality-heuristic (sometimes called 1/n rule)
people divide and invest people-resources equally in a-number of n-different-options.
n-different-options can be both-social-(e.g.,-time spent with children) and nonsocial-entities (e.g., financial-investments or natural-resources).
for example, many-parents invest many-parents limited resources, such as affection, time, and money (e.g., for education) equally into e.g., for education)-offspring.
in highly-uncertain-environments with large-numbers of assets and only-few-possibilities to learn, the-equality-heuristic can outperform optimizing strategies and yield better-performance on various-measures of success than optimal-asset-allocation-strategies.
social-heuristics ==
adapted from hertwig & herzog, 2009.
imitate-the-majority
heuristic-social-circle-heuristic-averaging heuristic tit-for-tat generous tit-for-tat (or tit-for-two-tat)
status-tree-regret matching heuristic-mirror-heuristic 1/n
(equality-heuristic)
group recognition heuristic white coat heuristic/ trust your doctor heuristic imitate-the-successful-heuristic-plurality-vote-based-lexicographic-heuristic
see also ==
social-heuristics
ecological-rationality-optimization-risk
uncertainty-max-planck-institute for human-development ==
notes == ==
references ==
cialdini, r.-b., reno, r.-r., & kallgren, c.-a. (1990).
a-focus-theory of normative-conduct: recycling the-concept of norms to reduce littering in public-places.
journal of personality and social-psychology, 58(6), 1015–1026.
demiguel, v., garlappi, l., & uppal, r. (2009).
optimal versus naive-diversification:
how-inefficient-ist the-1/n-portfolio-strategy?
the-review of financial-studies, 22(5), 1915-1953.
gigerenzer, g. (2010).
moral-satisficing: rethinking moral-behavior as bounded-rationality.
topics in cognitive-science, 2(3), 528–554.
doi:10.1111/j.1756-8765.2010.01094.x-gigerenzer, g., todd, p., & the-abc-research-group (1999).
simple-heuristics that make us smart.
new-york: oxford-university-press.
hertwig, r., & herzog, s.-m. (2009).
fast-and-frugal-heuristics: tools of social-rationality.
social-cognition, 27(5), 661–698.
retrieved from http://guilfordjournals.com/doi/abs/10.1521/soco.2009.27.5.661-hertwig, r.-hoffrage, u. & the-abc-research-group (2012).
simple-heuristics in a-social-world.
oxford-university-press.
hertwig, r. & hoffrage, u. (2012).
simple-heuristics:
the-foundations of adaptive-social-behavior".
in r.-hertwig, u.-hoffrage, & the-abc-research-group (ed.).
simple-heuristics in a-social-world (pdf).
new-york: oxford-university-press.
doi:10.1093
oso/9780195388435.001.0001.cs1-maint:
multiple-names: authors list
(link) morgan, t.-j.-h.; rendell, l.-e.; ehn, m.; hoppitt, w.; laland, k.-n. (2011-07-27).
the-evolutionary-basis of human-social-learning".
proceedings of the-royal-society-b: biological-sciences.
the-royal-society.
279 (1729)
: 653–662.
doi:10.1098/rspb.2011.1172.
issn 0962-8452.
pmc 3248730.
pmid 21795267.
rieucau, g., & giraldeau, l.-a. (2011).
exploring the-costs and benefits of social-information-use: an appraisal of current-experimental-evidence.
philosophical-transactions of the-royal-society-b, 366(1567), 949–957.
doi:10.1098/
rstb.2010.0325-seymour, b., & dolan, r. (2008).
emotion, decision-making, and the-amygdala.
neuron, 58, 662–671.
schultz, p.-w., nolan, j.-m., cialdini, r.-b., goldstein, n.-j., & griskevicius, v. (2007).
the-constructive,-destructive,-and-reconstructive-power of social-norms.
psychological-science, 18(5), 429–434.
simon,-herbert-a. (1956).
rational-choice and the-structure of the-environment.
psychological-review, 63(2), 129–138.
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
this happens when an-individual focuses on the-most-relevant-aspects of a-problem or situation to formulate a-solution.
some-heuristics are more applicable and useful than others depending on the-situation.
heuristic-processes are used to find the-answers and solutions that are most likely to work or be correct.
however, heuristics are not always right or the most accurate.
while they can differ from answers given by logic and probability, judgments and decisions based on a-heuristic can be good enough to satisfy a-need.
judgments and decisions are meant to serve as quick-mental-references for everyday-experiences and decisions.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
history ==
herbert-a.-simon formulated one of the-first-models of heuristics, known as satisficing.
herbert-a.-simon more-general-research-program posed the-question of how humans make decisions when the-conditions for rational-choice-theory are not met
, that is how people decide under uncertainty.
herbert-a.-simon is also known as the-father of bounded-rationality, which herbert-a.-simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
his-more-general-research-program was later extended into the-study of ecological-rationality.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
psychologists-amos-tversky and daniel-kahneman
typical-experimental-setup consisted of a-rule of logic or probability, embedded in a-verbal-description of a-judgement-problem, and demonstrated that people's-intuitive-judgement deviated from the-rule.
the-"linda-problem" below gives an-example.
the-deviation is then explained by a-heuristic.
this-research, called the heuristics-and-biases program, challenged the-idea that human-beings are rational-actors and first gained worldwide-attention in 1974 with the-science-paper-"judgment under uncertainty:
heuristics and biases"  and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
the-original-ideas by herbert-simon were taken up in the-1990s by gerd-gigerenzer and others.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
their has three-aspects:
what are the-heuristics humans use?
(-the-descriptive-study of the-"adaptive-toolbox")
under what-conditions should humans rely on a-given-heuristic?
the-prescriptive-study of ecological-rationality)
how to design heuristic decision aids that are easy to understand and execute?
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
these-two-different-research-programs have led to two-kinds of models of heuristics, formal-models and informal-ones.
formal-models describe the-decision-process in terms of an-algorithm, which allows for mathematical-proofs-and-computer-simulations.
in contrast, informal-models are verbal-descriptions.
formal-models of heuristics == ===
simon's-satisficing-strategy ===
herbert-simon's-satisficing-heuristic can be used to choose one-alternative from a-set of alternatives in situations of uncertainty.
here, uncertainty means that the-total-set of alternatives and uncertainty-consequences is not known or knowable.
for instance, professional-real-estate-entrepreneurs rely on satisficing to decide in which-location to invest to develop new-commercial-areas: "if i believe i can get at-least-x-return within y-years, then i take the-option.
in general, satisficing is defined as:
set an-aspiration-level α step 2 : choose the-first-alternative that satisfies αif
no-alternative is found, then the-aspiration-level can be adapted.
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
elimination by aspects ===
unlike satisficing, amos tversky's elimination-by-aspect heuristic can be used when all-alternatives are simultaneously available.
the-decision-maker gradually reduces the-number of alternatives by eliminating alternatives that do not meet the-aspiration-level of a-specific-attribute (or aspect).
recognition-heuristic ===
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
for two-alternatives, the-recognition-heuristic is:if one of two-alternatives is recognized and the other not, then infer that the-recognized-alternative has the-higher-value with respect to the-criterion.
for example, in the-2003-wimbledon-tennis-tournament, andy-roddick played tommy-robredo.
if one has heard of roddick but not of robredo, the-recognition-heuristic leads to the-prediction that roddick will win.
the-recognition-heuristic exploits partial-ignorance, if one has heard of both or no-player, a-different-strategy is needed.
studies of wimbledon 2003 and 2005 have shown that the-recognition-heuristic applied by semi-ignorant-amateur-players predicted the-outcomes of all-gentlemen single games as well and better than the-seedings of the wimbledon experts (who had heard of all-players), as well as the atp rankings.
the-recognition-heuristic is ecologically rational (that is, the-recognition-heuristic predicts well) when the-recognition-validity is substantially above chance.
in the-present-case, recognition of players'-names is highly correlated with players'-chances of winning.
take-the-best ===
the take-the-best heuristic exploits the-basic-psychological-capacity for retrieving cues from memory in the-order of their-validity.
based on the-cue-values, their-validity infers which of two-alternatives has a-higher-value on a-criterion.
unlike the-recognition-heuristic, their-validity requires that all-alternatives are recognized, and their-validity thus can be applied when the-recognition-heuristic cannot.
for binary-cues (where 1 indicates the-higher-criterion-value), the-heuristic is defined as: search-rule:
search-cues in the-order of their-validity v.   stopping-rule: stop search on finding the-first-cue that discriminates between the-two-alternatives (i.e., one-cue-values are 0 and 1).
decision-rule: infer that the-alternative with the-positive-cue-value (1) has the-higher-criterion-value).
the-validity-vi of a-cue i is defined as the-proportion of correct-decisions ci:
vi = ci /-tiwhere-ti is the-number of cases the-values of the-two-alternatives differ on cue-i.
the-validity of each-cue can be estimated from samples of observation.
take-the-best has remarkable-properties.
in comparison with complex-machine-learning-models, it has been shown that it can often predict better than regression-models, classification-and-regression-trees, neural-networks, and support vector-machines.
brighton & gigerenzer, 2015]
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers  and student-populations.
the-conditions under which take-the-best is ecologically rational are mostly known.
take-the-best-shows that the-previous-view that ignoring part of the-information would be generally irrational is incorrect.
less can be more.
fast-and-frugal-trees ===
a-fast-and-frugal-tree is a-heuristic that allows to make a-classifications, such as whether a-patient with severe-chest-pain is likely to have a-heart-attack or not, or whether a-car approaching a-checkpoint is likely to be a-terrorist or a-civilian.
it is called “fast and frugal” because, just like take-the-best, it allows for quick-decisions with only-few-cues or attributes.
it is called a “tree” because it can be represented like a-decision-tree in which one asks a-sequence of questions.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
figure 1 shows a-fast-and-frugal-tree used for screening for hiv (human-immunodeficiency-virus).
just like take-the-best, the-tree has a-search-rule, stopping rule, and decision rule:
search-rule:
search through cues in a-specified-order.
stopping rule: stop search if an-exit is reached.
decision-rule:
classify the-person according to the-exit (here:
no-hiv or hiv).
in the-hiv-tree, an elisa (enzyme-linked-immunosorbent-assay) test is conducted first.
if the-outcome is negative, then the-testing-procedure stops and the-client is informed of the-good-news, that is, “no-hiv.”
if, however, the-result is positive, a-second-elisa-test is performed, preferably from a-different-manufacturer.
if the-second-elisa is negative, then the-procedure stops and the-client is informed of having “no-hiv.”
however, if the-result is positive, a final test, the-western-blot, is conducted.
in general, for n binary-cues, a-fast-and-frugal-tree has exactly-n-+-1-exits – one for each-cue and two for the-final-cue.
a-full-decision-tree, in contrast, requires 2n-exits.
the-order of cues (tests) in a-fast-and-frugal-tree is determined by the-sensitivity and specificity of the-cues, or by other-considerations such as the-costs of the-tests.
in the-case of the-hiv-tree, the-elisa is ranked first because the-elisa produces fewer-misses than the-western-blot-test, and also is less expensive.
the-western-blot-test, in contrast, produces fewer-false-alarms.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
fast-and-frugal-trees are descriptive-or-prescriptive-models of decision making under uncertainty.
for instance, an-analysis-or-court-decisions reported that the best model of how london-magistrates make bail-decisions is a-fast-and-frugal-tree.
the-hiv-tree is both prescriptive– physicians are taught the-procedure – and a-descriptive-model, that is, most-physicians actually follow the-procedure.
informal-models of heuristics ==
in most-physicians initial-research, tversky and kahneman proposed three-heuristics—availability, representativeness, and anchoring and adjustment.
subsequent-work has identified many more.
heuristics that underlie judgment are called "judgment heuristics".
another-type, called "evaluation heuristics", are used to judge the-desirability of possible-choices.
availability ===
in psychology, availability is the-ease with which a-particular-idea can be brought to mind.
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
when an-infrequent-event can be brought easily and vividly to mind, the-availability-heuristic overestimates an-infrequent-event likelihood.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
dramatic,-violent-deaths are usually more highly publicised and therefore have a-higher-availability.
on the-other-hand, common-but-mundane-events are hard to bring to mind, so common-but-mundane-events likelihoods tend to be underestimated.
these include deaths from suicides, strokes, and diabetes.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
this-heuristic may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k ,  the-availability-heuristic gives a-quick-way to answer the-question.
words that begin with t come more readily to mind, and so subjects give a-correct-answer without counting out large-numbers of words.
however, this-heuristic can also produce errors.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
it is easy to think of words that begin with k, such as kangaroo, kitchen, or kept.
it is harder to think of words with k as the-third-letter, such as lake, or acknowledge, although objectively these are three times more common.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
in another-experiment, subjects heard the-names of many-celebrities, roughly-equal-numbers of whom were male and female.
subjects were then asked whether the-list of names included more-men or more-women.
when the-men in the-list were more famous, a-great-majority of subjects incorrectly thought there were more of the-men in the-list, and vice versa for women.
tversky and kahneman's-interpretation of these-results is that judgments of proportion are based on availability, which is higher for the-names of better-known-people.
in one-experiment that occurred before the-1976-u.s.-presidential-election, some-participants were asked to imagine gerald-ford winning, while others did the same for a-jimmy-carter-victory.
each-group subsequently viewed each-group allocated candidate as significantly more likely to win.
each-group found a-similar-effect when students imagined a-good or a-bad-season for a-college-football-team.
the-effect of imagination on subjective-likelihood has been replicated by several-other-researchers.
a-concept's-availability can be affected by how recently and how frequently a-concept's-availability has been brought to mind.
in one-study, subjects were given partial-sentences to complete.
the-words were selected to activate the-concept either of hostility or of kindness: a-process known as priming.
they then had to interpret the-behavior of a-man described in a-short,-ambiguous-story.
they-interpretation was biased towards the-emotion they had been primed with: the-more-priming,
the-greater-the-effect.
a-greater-interval between the-initial-task and the-judgment decreased the-effect.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
representativeness ===
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
an-individual-thing has a-high-representativeness for a-category if an-individual-thing is very similar to a-prototype of a-category.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
representative" is here meant in two-different-senses: the-prototype used for comparison is representative of the-prototype used for comparison-category, and representativeness is also a-relation between that-prototype and the-thing being categorised.
while   is effective for some-problems, this-heuristic involves attending to the-particular-characteristics of the-individual, ignoring how common those-categories are in the-population (called the base rates).
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
this is called the base rate fallacy.
representativeness explains this and several-other-ways in which human-judgments break the-laws of probability.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
this can lead to a-bias, incorrectly finding causal-relationships between things that resemble one another and missing them when the-cause and effect are very different.
examples of this include both-the-belief that "emotionally-relevant-events ought to have emotionally-relevant-causes", and magical-associative-thinking.
representativeness of base-rates ====
a-1973-experiment used a-psychological-profile of tom-w., a-fictional-graduate-student.
one-group of subjects had to rate tom's-similarity to a-typical-student in each of nine-academic-areas (including law, engineering and library science).
another-group had to rate how likely it is that tom specialised in each-area.
if these-ratings of likelihood are governed by probability, then these-ratings of likelihood should resemble the-base-rates, i.e.-the-proportion of students in each of the-nine-areas (which had been separately estimated by a-third-group).
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
instead, the-ratings of likelihood matched the-ratings of similarity almost perfectly, both in this-study and a-similar-one where subjects judged the-likelihood of a-fictional-woman taking different-careers.
this suggests that rather than estimating probability using base-rates, subjects had substituted the-more-accessible-attribute of similarity.
conjunction-fallacy ====
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
tversky and kahneman gave subjects a-short-character-sketch of a-woman called linda, describing a-woman called linda as, "31 years old, single, outspoken, and very bright.
she majored in philosophy.
as a-student, she was deeply concerned with issues of discrimination and social-justice, and also participated in anti-nuclear-demonstrations".
people reading this-description then ranked the-likelihood of different-statements about she.
amongst others, these included "she is a-bank-teller", and, "she is a-bank-teller and is active in the-feminist-movement".
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "she is both-x and y" can never be more probable than the-more-general-statement
"linda is x".
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-similar-exercise-concerned-bill, described as "intelligent but unimaginative".
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
without success, tversky and kahneman used what tversky and kahneman described as "a-series of increasingly-desperate-manipulations" to get tversky and kahneman subjects to recognise the-logical-error.
in one-variation, subjects had to choose between a-logical-explanation of why "linda is a-bank-teller" is more likely, and a-deliberately-illogical-argument which said that "linda is a-feminist-bank-teller" is more likely "because linda resembles an-active-feminist more than linda resembles a-bank-teller".
sixty-five-percent of subjects found the-illogical-argument more convincing.
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
other-researchers did not eliminate the-error.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
the-error disappears when the-question is posed in terms of frequencies.
everyone in these-versions of the-study recognised that out of 100-people fitting an-outline-description, the-conjunction-statement ("she is x and y") cannot apply to more-people than the-general-statement ("she is x").
ignorance of sample-size ====
tversky and kahneman asked subjects to consider a-problem about random-variation.
imagining for simplicity that exactly-half of the-babies born in a-hospital are male, the-ratio will not be exactly-half in every-time-period.
on some-days, more-girls will be born and on others, more boys.
the-question was, does the-likelihood of deviating from exactly half depend on whether there are many-or-few-births per day?
it is a-well-established-consequence of sampling-theory that proportions will vary much more day-to-day when the-typical-number of births per day is small.
however, people's-answers to the-problem do not reflect this-fact.
they typically reply that the-number of births in the-hospital makes no-difference to the-likelihood of more-than-60%-male-babies in one-day.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
dilution-effect ====
richard-e.-nisbett and colleagues suggest that representativeness explains the-dilution-effect, in which irrelevant-information weakens the-effect of a-stereotype.
subjects in one-study were asked whether "paul" or "susan" was more likely to be assertive, given no-other-information than their-first-names.
their rated paul as more assertive, apparently basing their-judgment on a-gender-stereotype.
another-group, told that paul and susan's-mothers each-commute to work in a-bank, did not show this-stereotype-effect; paul's and susan's-mothers each-commute to work in a-bank rated paul and susan as equally assertive.
the-explanation is that the-additional-information about paul and susan-susan made paul and susan less representative of men or women in general, and so the-subjects'-expectations about men and women had a-weaker-effect.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
misperception of randomness === =
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
tversky and kahneman argued that this-effect underlies the-gambler's-fallacy; a-tendency to expect outcomes to even out over the-short-run, like expecting a-roulette-wheel to come up black because the-last-several-throws came up red.
they emphasised that even-experts in statistics were susceptible to this-illusion: in a-1971-survey of professional-psychologists, they found that respondents expected samples to be overly representative of the-population they were drawn from.
as a-result, the-psychologists systematically overestimated the-statistical-power of the-psychologists tests, and underestimated the-sample-size needed for a-meaningful-test of the-psychologists hypotheses.
anchoring and adjustment ===
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
according to tversky and kahneman's-original-description, it involves starting from a-readily-available-number—the-"anchor"—and shifting either up or down to reach an-answer that seems plausible.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
hence the-anchor contaminates the-estimate, even if the-anchor is clearly irrelevant.
in one-experiment, subjects watched a-number being selected from a-spinning-"wheel of fortune".
they had to say whether a-given-quantity was larger or smaller than a-number being selected from a-spinning-"wheel of fortune".
for instance, they might be asked, "is the-percentage of african-countries which are members of the-united-nations larger or smaller than 65%?"
they then tried to guess the-true-percentage.
they answers correlated well with the-arbitrary-number they had been given.
insufficient-adjustment from an-anchor is not the-only-explanation for this-effect.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-anchoring-effect has been demonstrated by a-wide-variety of experiments both in laboratories and in the-real-world.
the-anchoring-effect remains when the-subjects are offered money as an-incentive to be accurate, or when the-subjects are explicitly told not to base the-subjects judgment on an-anchor.
the-anchoring-effect is stronger when people have to make people judgments quickly.
subjects in these-experiments lack introspective-awareness of the-heuristic, denying that the-anchor affected  subjects in these-experiments estimates.
even when the-anchor-value is obviously random or extreme, the-anchor-value can still contaminate estimates.
one-experiment asked subjects to estimate the-year of albert-einstein's-first-visit to the-united-states.
anchors of 1215 and 1992 contaminated the-answers just-as-much-as-more-sensible-anchor-years.
other-experiments asked subjects if the-average-temperature in san-francisco is more-or-less-than-558-degrees, or whether there had been more-or-fewer-than-100,025-top-ten-albums by the-beatles.
these-deliberately-absurd-anchors still affected estimates of the-true-numbers.
anchoring results in a-particularly-strong-bias when estimates are stated in the-form of a-confidence-interval.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
best-estimate.
this leads to an-overconfidence-effect.
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
anchoring also causes particular-difficulty when many-numbers are combined into a-composite-judgment.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
another-group had to estimate the-same-product in reverse-order; 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8.
both-groups underestimated the-answer by a-wide-margin, but another-group's-average-estimate was significantly smaller.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-less-abstract-task is to estimate the-probability that an-aircraft will crash, given that there are numerous-possible-faults each with a-likelihood of one in a million.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
for this-kind of judgment, anchoring on the-individual-probabilities results in an-overestimation of the-combined-probability.
examples ====
people's-valuation of goods, and the-quantities they buy, respond to anchoring effects.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
they then entered an-auction to bid for these-items.
those with the-highest-two-digit-numbers submitted bids that were many times higher than those with the-lowest-numbers.
when a-stack of soup-cans in a-supermarket was labelled, "limit 12 per customer", the-label influenced customers to buy more-cans.
in another-experiment, real-estate-agents appraised the-value of houses on the-basis of a-tour and extensive-documentation.
different-agents were shown different listing prices, and these affected different-agents valuations.
for one-house, the-appraised-value ranged from us$114,204 to $128,754.anchoring and adjustment has also been shown to affect grades given to students.
in one-experiment, 48-teachers were given bundles of student-essays, each of which had to be graded and returned.
48-teachers were also given a-fictional-list of the-students'-previous-grades.
the-mean of these-grades affected the-grades that teachers awarded for the-essay.
one-study showed that anchoring affected the-sentences in a-fictional-rape-trial.
the-subjects were trial-judges with, on average, more-than-fifteen-years of experience.
the-subjects read documents including witness-testimony, expert-statements, the-relevant-penal-code, and the-final-pleas from the-prosecution and defence.
the-two-conditions of this-experiment differed in just-one-respect: the-prosecutor demanded a-34-month-sentence in one-condition and 12-months in the other; there was an-eight-month-difference between the-average-sentences handed out in these-two-conditions.
in a-similar-mock-trial, the-subjects took the-role of jurors in a-civil-case.
the-subjects were either asked to award damages "in the-range from $15 million to $50 million" or "in the-range from $50 million to $150 million".
although the-facts of the-case were the same each time, jurors given the-higher-range decided on an-award that was about three times higher.
this happened even though the-subjects were explicitly warned not to treat the-requests as evidence.
assessments can also be influenced by the-stimuli provided.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
affect heuristic ===
"affect", in this-context, is a-feeling such as fear, pleasure or surprise.
it is shorter in duration than a-mood, occurring rapidly and involuntarily in response to a-stimulus.
while reading the-words "lung-cancer" might generate an-affect of dread, the-words "mother's-love" can create an-affect of affection and comfort.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
the-affect heuristic has been used to explain why messages framed to activate emotions are more persuasive than those framed in a-purely-factual-way.
others === ==
theories ==
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
according to a-cognitive-laziness-approach, some-complex-calculations are already done rapidly and automatically by the-human-brain, and other-judgments make use of these-processes rather than calculating from scratch.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
this-perspective emphasises the-"fast-and-frugal"-nature of heuristics.
cognitive-laziness ===
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
attribute-substitution ===
in 2002 daniel-kahneman and shane-frederick proposed a-process called attribute substitution which happens without conscious-awareness.
according to this-theory, when somebody makes a-judgment (of a-target-attribute) which is computationally complex, a-rather-more-easily-calculated-heuristic-attribute is substituted.
in effect, a-difficult-problem is dealt with by answering a-rather-simpler-problem, without the-person being aware this is happening.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
it also explains why human-judgments often fail to show regression toward the-mean.
this-substitution is thought of as taking place in the-automatic-intuitive-judgment-system, rather than the-more-self-aware-reflective-system.
hence, when someone tries to answer a-difficult-question, they may actually answer a-related-but-different-question, without realizing that a-substitution has taken place.
in 1975, psychologist-stanley-smith-stevens proposed that the-strength of a-stimulus (e.g.-the-brightness of a-light, the-severity of a-crime) is encoded by brain-cells in a-way that is independent of modality.
kahneman and frederick built on this-idea, arguing that the-target-attribute and heuristic-attribute could be very different in nature.
kahneman and frederick propose three-conditions for attribute-substitution: the-target-attribute is relatively inaccessible.
substitution is not expected to take place in answering factual-questions that can be retrieved directly from memory ("what is your-birthday?")
or about current-experience ("do you feel thirsty now?).
an-associated-attribute is highly accessible.
this might be because  this is evaluated automatically in normal-perception or because  this has been primed.
for example, someone who has been thinking about their-love-life and is then asked how happy their are might substitute how happy their are with their-love-life rather than other-areas.
the-substitution is not detected and corrected by the-reflective-system.
for example, when asked "a-bat and a-ball together cost $1.10.
a-bat and a-ball together costs $1 more than a-ball.
how-much-does-the-ball-cost?
many-subjects incorrectly answer $0.10.
an-explanation in terms of attribute-substitution is that, rather than work out the-sum, subjects parse the-sum of $1.10 into a-large-amount and a-small-amount, which is easy to do.
whether the-sum of $1.10 feel that is the-right-answer will depend on whether the-sum of $1.10 check the-calculation with the-sum of $1.10-reflective-system.
kahneman gives an-example where some-americans were offered insurance against americans own death in a-terrorist-attack while on a-trip to europe, while another-group were offered insurance that would cover death of any-kind on the-trip.
even though "death of any-kind" includes "death in a-terrorist-attack", another-group were willing to pay more than the latter.
kahneman suggests that the-attribute of fear is being substituted for a-calculation of the-total-risks of travel.
fear of terrorism for these-subjects was stronger than a-general-fear of dying on a-foreign-trip.
fast and frugal ===
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
consequences ==
efficient-decision-heuristics ===
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
warren-thorngate, a-social-psychologist determined how-often-each-heuristic-selected-alternatives with highest-through-lowest-expected-value in a-series of randomly-generated-decision-situations.
warren-thorngate, a-social-psychologist found that most of the-simulated-heuristics selected alternatives with highest-expected-value and almost never selected alternatives with lowest-expected-value.
===-"beautiful-is-familiar"-effect ===
psychologist-benoît-monin reports a-series of experiments in-which-subjects, looking at photographs of faces, have to judge whether they have seen those faces before.
it is repeatedly found that attractive-faces are more likely to be mistakenly labeled as familiar.
psychologist-benoît-monin interprets this-result in terms of attribute-substitution.
the-heuristic-attribute in this-case is a-"warm-glow"; a-positive-feeling towards someone that might either be due to their being familiar or being attractive.
this-interpretation has been criticised, because not-all-the-variance in familiarity is accounted for by the-attractiveness of the-photograph.
judgments of morality and fairness ===
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
another-source of heuristic-attributes is emotion: people's-moral-opinions on sensitive-subjects like sexuality and human-cloning may be driven by reactions such as disgust, rather than by reasoned-principles.
sunstein has been challenged as not providing enough-evidence that attribute substitution, rather than other-processes, is at work in these-cases.
persuasion ===
an-example of how persuasion plays a-role in heuristic-processing can be explained through the-heuristic-systematic-model.
this explains how there are often two-ways we are able to process information from persuasive-messages, one being heuristically and the other systematically.
a-heuristic is when we make a-quick-short-judgement into we-decision-making.
on the-other-hand, systematic-processing involves more-analytical-and-inquisitive-cognitive-thinking.
individuals looks further than individuals own prior-knowledge for the-answers.
an-example of this-model could be used when watching an-advertisement about a-specific-medication.
one without prior-knowledge would see the-person in the-proper-pharmaceutical-attire and assume that the-proper-pharmaceutical-attire know what the-proper-pharmaceutical-attire are talking about.
therefore, the-person in the-proper-pharmaceutical-attire automatically has more-credibility and is more likely to trust the-content of the-messages than the-proper-pharmaceutical-attire deliver.
while another who is also in that-field of work or already has prior-knowledge of the-medication will not be persuaded by the-ad because of another who is also in that-field of work or already has prior-knowledge of the-medication systematic-way of thinking.
this was also formally demonstrated in an-experiment conducted my-chaiken and maheswaran (1994).
in addition to these-examples,-the-fluency-heuristic-ties in perfectly with the-topic of persuasion.
it is described as how we all easily make "the most of an-automatic-by-product of retrieval from memory".
an-example would be a-friend asking about good-books to read.
many could come to mind, but you name the-first-book recalled from your-memory.
since it was the-first-thought, therefore you value it as better than any-other-book one could suggest.
the-effort-heuristic is almost identical to fluency.
the-one-distinction would be that-objects that take longer to produce are seen with more-value.
one may conclude that a-glass-vase is more valuable than a-drawing, merely because a-glass-vase may take longer for a-glass-vase.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
see also ==
citations == ==
references ==
baron, jonathan (2000), thinking and deciding (
3rd-ed.) ,
cambridge-university-press, isbn 978-0521650304, oclc 316403966
gilovich, thomas; griffin, dale-w. (2002), "introduction-–-heuristics and biases:
then and now",  in gilovich, thomas; griffin, dale-w.; kahneman, daniel (eds.),
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
1–18, isbn-9780521796798-hardman, david (2009), judgment and decision making:
psychological-perspectives, wiley-blackwell, isbn 9781405123983-hastie, reid; dawes, robyn-m. (29-september 2009), rational-choice in an-uncertain-world:
the-psychology of judgment and decision-making, sage, isbn 9781412959032
koehler, derek-j.; harvey, nigel (2004), blackwell-handbook of judgment and decision-making, wiley-blackwell, isbn-9781405107464-kunda, ziva (1999), social-cognition:
making sense of people, mit-press, isbn 978-0-262-61143-5, oclc 40618974
mussweiler, thomas; englich, birte; strack, fritz (2004), "anchoring-effect",  in pohl, rüdiger-f. (ed.) ,
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
183–200, isbn 9781841693514, oclc 55124398
plous, scott (1993), the-psychology of judgment and decision-making, mcgraw-hill, isbn 9780070504776, oclc 26931106
poundstone, william (2010), priceless:
the-myth of fair-value (and how to take advantage of it), hill and wang, isbn 9780809094691
reber, rolf (2004), "availability",  in pohl, rüdiger-f. (ed.),
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
147–163, isbn 9781841693514, oclc 55124398
sutherland, stuart (2007), irrationality (2nd ed.),
london: pinter and martin, isbn 9781905177073, oclc 72151566
teigen, karl-halvor (2004), "judgements by representativeness",  in pohl, rüdiger-f. (ed.),
cognitive-illusions:
a-handbook on fallacies and biases in thinking, judgement and memory, hove, uk:
psychology-press, pp.
165–182, isbn 9781841693514, oclc 55124398 tversky, amos; kahneman, daniel (1974),-"judgments under uncertainty:
heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
bibcode:1974sci...
185.1124t, doi:10.1126/science.185.4157.1124, pmid 17835457, s2cid 143452957 reprinted in daniel-kahneman; paul-slovic; amos-tversky, eds.
judgment under uncertainty: heuristics and biases.
cambridge:
cambridge-university-press.
isbn 9780521284141.
yudkowsky, eliezer (2011).
" cognitive-biases potentially affecting judgment of global-risks".
in bostrom, nick; cirkovic, milan-m. (eds.).
global-catastrophic-risks.
oup-oxford.
isbn 978-0-19-960650-4.
further-reading ==
slovic, paul; melissa-finucane; ellen-peters; donald-g.-macgregor (2002). "
the-affect-heuristic".
in thomas-gilovich; dale griffin; daniel-kahneman (eds.).
heuristics and biases: the-psychology of intuitive-judgment.
cambridge-university-press.
isbn 9780521796798.
gigerenzer, gerd; selten, reinhard (2001).
bounded-rationality : the-adaptive-toolbox.
cambridge, ma: mit-press.
isbn 0585388288.
oclc 49569412.
korteling, johan-e.; brouwer, anne-marie; toet, alexander (3-september 2018). "
a-neural-network-framework for cognitive-bias".
frontiers in psychology.
doi:10.3389/fpsyg.2018.01561.
pmc 6129743.
pmid 30233451.
chow, sheldon (20-april 2011).
heuristics, concepts, and cognitive-architecture:
toward understanding
how the mind works".
electronic-thesis and dissertation-repository.
todd, p.m. (2001). "
heuristics for decision and choice".
international-encyclopedia of the-social-&-behavioral-sciences.
6676–6679.
doi:10.1016
/b0-08-043076-7/00629-x. isbn 978-0-08-043076-8.
external-links ==
test yourself:
decision making and the-availability-heuristic
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
history and context ==
clifford-nass and youngme-moon's-article, "machines and mindlessness: social-responses to computers", published in 2000, is the-origin for casa.
it states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
additionally, nass and moon found a-similarity between this-behavior and research by harvard-psychology-professor-ellen-langer on mindlessness.
harvard-psychology-professor-ellen-langer states that mindlessness is when a-specific-context triggers an-individual to rely on categories, associations, and habits of thought from the-past with little to no-conscious-awareness.
when these-contexts are triggered, the-individual becomes oblivious to novel-or-alternative-aspects of the-situation.
in this-respect, mindlessness is similar to habits and routines, but different in that with only-one-exposure to information, a-person will create a-cognitive-commitment to the-information and freeze the-information potential-meaning.
with mindlessness, alternative-meanings or uses of the-information become unavailable for active-cognitive-use.
social-attributes that computers have which are similar to humans include: words for output
interactivity (the-computer 'responds' when a-button is touched) ability to perform traditional-human-tasksaccording to casa,
the-above-attributes trigger scripts for human-human-interaction, which leads an-individual to ignore cues revealing the-asocial-nature of a-computer.
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
casa has been extended to include robots and ai.
however, recently, there have been challenges to the-casa-paradigm.
to account for the-advances in technology, masa has been forwarded as a-significant-extension of casa.
attributes ==
cued-social-behaviors observed in research-settings include some of the-following: gender-stereotyping:
when voice-outputs are used on computers, this triggers gender stereotype scripts, expectations, and attributions from individuals.
for example, a-1997-study revealed that female-voiced-tutor-computers were rated as more informative about love and relationships than male-voiced-computers, whereas male-voiced-computers were more proficient in technical-subjects than female-voiced-computers.
reciprocity: when a-computer provides help, favours, or benefits, this triggers the-mindless-response of the-participant feeling obliged to 'help' a-computer.
for example, an-experiment in 1997 found that when a-specific-computer 'helped' a-person, that-person was more likely to do more-'work' for a-specific-computer '.
specialist versus generalist: when a-technology is labeled as 'specialist', this triggers a-mindless-response by influencing people's-perceptions of the-content the labeled technology presents.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
personality: when a-computer-user mindlessly creates a-personality for a-computer based on verbal-or-paraverbal-cues in the-interface.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
academic-research ==
three-research-articles have represented some of the-advances in the-field of casa.
specifically, researchers in the-field of casa are looking at how-novel-variables, manipulations, and new-computer-software-influence-mindlessness.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study, "computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz investigated whether computer-agents can use the-expression of emotion to influence human-perceptions of trustworthiness in the-context of a-negotiation-activity followed by a-trust-activity.
computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz found that computer-agents displaying emotions congruent with computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz actions were preferred as partners in the-trust-game over computer-agents whose-emotion-expressions and actions did not match.
computer-emotion – impacts on trust" by dimitrios-antos, celso-de-melo, jonathan-gratch, and barbara-grosz also found that when emotion did not carry useful-new-information, useful-new-information did not strongly influence human-decision-making-behavior in a-negotiation-setting.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
"one-example of how casa-research can impact consumer-behaviour and attitude is moon's-experiment, which tested the-application of the-principle of reciprocity and disclosure in a-consumer-context.
moon's-experiment, which tested the-application of the-principle of reciprocity and disclosure in a-consumer-context tested this-principle with intimate-self-disclosure of high-risk-information (when disclosure makes the-person feel vulnerable) to a-computer, and observed how that-disclosure affects future-attitudes and behaviors.
participants interacted with a-computer which questioned participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to a-computer and the-other-half went to a-different-computer.
both-groups were shown 20-products and asked if both-groups would purchase both-groups.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
references ==
scarcity, in the-area of social-psychology, works much like scarcity in the-area of economics.
simply put, humans place a-higher-value on an-object that is scarce, and a-lower-value on those that are in abundance.
for example diamonds are more valuable than rocks because diamonds are not as abundant.
the-scarcity heuristic is a-mental-shortcut that places a-value on an-item based on how easily it might be lost, especially to competitors.
the-scarcity-heuristic stems from the-idea that the more difficult it is to acquire an-item the-more-value that-item has.
in many-situations we use an-item’s-availability, an-item’s-perceived-abundance, to quickly estimate quality and/or utility.
this can lead to systemic-errors or cognitive-bias.
there are two-social-psychology-principles that work with scarcity that increase its-powerful-force.
one is social-proof.
this is a-contributing-factor to the-effectiveness of scarcity, because if a-product is sold out, or inventory is extremely low, humans interpret that to mean a-product must be good since everyone else appears to be buying a-product.
the-second-contributing-principle to scarcity is commitment and consistency.
if someone has already committed themselves to something, then find out
themselves cannot have it, it makes the-person want the-item more.
==-examples ==
this-idea is deeply embedded in the-intensely-popular-“black-friday”-shopping-extravaganza that u.s.-consumers participate in every-year on the-day after thanksgiving.
more than getting a-bargain on a-hot-gift-idea, shoppers thrive on the-competition itself, in obtaining the-scarce-product.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up our-decision-making-process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
while these-“rules” work well in most-circumstances, there are certain-situations where these-“rules can lead to systemic-errors or cognitive-bias.
the-scarcity heuristic is only-one-example of how mental-“rules” can result in unintended-bias in decision-making.
other-heuristics and biases include the-availability-heuristic,-survivorship-bias, confirmation-bias, and the-self-attribution-bias.
like the-scarcity heuristic, all of these-phenomena result from either-evolutionary-or-past-behavior-patterns and can consistently lead to faulty-decision-making in specific-circumstances.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
these-heuristic-judgments should increase the-desirability of a-stimulus to those who value the-inferred-attributes.
the-scarcity heuristic does not only apply to a-shortage in absolute-resources.
according to robert-cialdini, the-scarcity-heuristic leads to us to make biased-decisions on a-daily-basis.
it is particularly common to be biased by the-scarcity heuristic when assessing four-parameters: quantity, rarity, time, and censorship.
quantity ===
the-simplest-manifestation of the-scarcity heuristic is the-fear of losing access to some-resource resulting from the-possession of a-small-or-diminishing-quantity of the-asset.
for example, your-favorite-shirt becomes more valuable when you know you cannot replace example.
if you had ten-shirts of the-same-style and color, losing one would likely be less distressful because you have several-others to take   place.
cialdini theorizes that it is in our-nature to fight against losing freedom, pointing out that we value possessions in low-quantities partly because as resources become less available they are more likely not to be available at all at some-point in the-future.
if the-option to use that-resource disappears entirely, then options decrease and so does our-freedom.
cialdini draws cialdini conclusion from psychological-reactance-theory, which states that whenever free-choice is limited or threatened, the-need to retain freedom makes us desire the-object under threat more than if it was not in danger of being lost.
in the-context of the-scarcity heuristic, this implies that when something threatens our-prior-access to a-resource, we will react against that-interference by trying to possess the-resource with more-vigor than before.
rarity === objects can increase in value if we feel that rarity === objects have unique-properties, or are exceptionally difficult to replicate.
collectors of rare-baseball-cards or stamps are simple-examples of the-principle of rarity.
when time is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
the-bush-administration used a-variation of this-theme in justifying the-rush to war in iraq: "time is running out for saddam and unless we stop saddam now saddam will use saddam wmd against us".
the-scarcity-rule is the-sales-tool that is most obvious to us when we see advertising-terms including, “
sale ends june 30th”;
“the-first-hundred-people receive…”; “limited time only”;
“ offer expires”.
restriction and censorship ===
according to worchel, arnold & baker (1975), our-reaction to censorship is to want the-censored-information more than before censorship was restricted as well perceive the-censored-message more favorably than before the-ban.
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
worchel, arnold, and baker came to this by testing students’-attitudes toward co-ed-dormitories at the-university of north-carolina.
worchel, arnold, and baker found that when students were told that-speech against the-idea of co-ed-dorms was banned, students saw co-ed-dorms as less favorable than if the-discourse about co-ed-dorms had remained open.
thus, even without having heard any-argument against co-ed-dormitories, students were more prone to being persuaded to be opposed simply as a-reaction to the-ban.
another-experiment (zellinger-et-al.
1975) divided-students into two-groups and gave students the-same-book.
in one-group the-same-book was clearly labeled as “mature-content” and was restricted for readers 21 and older while the-same-book had no-such-warning.
when asked to indicate their-feelings toward the-literature the-group with the-warning demonstrated a-higher-desire to read the-book and a-stronger-conviction that their would like the-book than those without the-warning.
studies ==
numerous-studies have been conducted on the-topic of scarcity in social-psychology:
scarcity rhetoric in a-job-advertisement for restaurant-server-positions has been investigated.
subjects were presented with two-help-wanted-ads, one of which suggested numerous-job-vacancies, while the other suggested that very few were available.
the-study found that subjects who were presented with the-advertisement that suggested limited-positions available viewed the-company as being a-better-one to work for than the-one that implied many-job-positions were available.
subjects also felt that the-advertisement that suggested limited-vacancies translated to higher-wages.
in short, subjects placed a-positive,-higher-value on the-company that suggested that there were scarce-job-vacancies available.
another-study examined how the-scarcity of men may lead women to seek high-paying-careers and to delay starting a-family.
this-effect was driven by how the-sex-ratio altered the-mating-market, not-just-the-job-market.
sex-ratios involving a-scarcity of men led women to seek lucrative-careers because of the-difficulty women have in finding an-investing,-long-term-mate under such-circumstances.
conditional-variations ==
although the-scarcity heuristic can always affect judgment and perception, certain-situations exacerbate the-effect.
new-scarcity and competition are common-cases.
new-scarcity ===
new-scarcity occurs when our-irrational-desire for limited-resources increases when we move from a-state of abundance to a-state of scarcity.
this is in line with psychological-reactance-theory, which states that a-person will react strongly when they perceive that they-options are likely to be lessened in the-future.
worchel, lee & adewole (1975) demonstrated this-principle with a-simple-experiment.
they divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
when asked to rate the-quality of the-cookie the-group with two, in line with the-scarcity heuristic, found the-cookies more desirable.
the-researchers then added a-new-element.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
the-group first having ten
but then were reduced to two, rated 8-cookies more desirable than both of the-other-groups.
quantifying value in scarce-and-competitive-situations ===
mittone & savadori (2009) created an-experiment where the-same-good was abundant in one-condition but scarce in another.
one-condition involved a-partner/competitor to create scarcity, while one-condition did not.
results showed that more-participants chose a-good when it was scarce than when it was abundant, for two-out-of-four-sets of items (ballpoints, snacks, pencils, and key-rings).
the-experiment then created a-wta (willingness to accept)-elicitation-procedure that created subjective-values for goods.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
other-applications ====
this-idea could easily by applied to other-fields.
in 1969, james-c.-davis postulated that revolutions are most likely to occur during periods of improving economic-and-social-conditions that are immediately followed by a-short-and-sharp-reversal in that-trend.
therefore, it is not the consistently downtrodden, those in a-state of constant-scarcity, who revolt but rather those who experience new-scarcity that are most likely to feel a-desire of sufficient-intensity to incite action.
competition ===
in situations when others are directly vying for scarce-resources, the-value we assign to objects is further inflated.
advertisers commonly take advantage of scarcity-heuristics by marketing-products as “hot-items” or by telling customers that certain-goods will sell out quickly.
worchel, lee & adewole (1975) also examined the-competition-bias in their-cookie-experiment, taking the-group that had experienced new-scarcity, going from ten to two-cookies, and telling half of their that the-reason their were losing cookies is because there was high-demand for cookies from other-participants taking the-test.
their then told the-other-half that it was just because a-mistake had been made.
it was just because a-mistake had been made was found that the-half we were told that their were having their-cookie-stock reduced due to social-demand rated the-cookies higher than those who were told it was only due to an-error.
in 1983, coleco-industries marketed a-soft-sculpted-doll that had exaggerated-neonatal-features and came with "adoption-papers".
demand for these-dolls exceeded expectations, and spot-shortages began to occur shortly after these-dolls introduction to the-market.
this-scarcity fueled demand even more and created what became known as the-cabbage-patch-panic (langway, hughey, mcalevey, wang, & conant, 1983).
customers scratched, choked, pushed, and fought one another in an-attempt to get the-dolls.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
a-secondary-market quickly developed where sellers were receiving up to $150 per doll.
even at these-prices, the-dolls were so difficult to obtain that one-kansas-city-postman flew to london to get one for one-kansas-city-postman daughter (adler-et-al.,
see also ==
artificial-scarcity principle
of least-interest ==
references == ==
bibliography ==
cialdini, robert-b. (2001)
influence: science and practice
(4th-ed.).
allyn and bacon.
isbn 9780321011473.
gigerenzer, gerd (1991).
how to make cognitive-illusions disappear: beyond "heuristics and biases"" (pdf).
european-review of social-psychology.
citeseerx 10.1.1.336.9826.
doi:10.1080/14792779143000033.
lynn, michael (1989).
scarcity-effects on desirability: mediated by assumed-expensiveness?".
journal of economic-psychology.
10 (2): 257–274.
doi:10.1016/0167-4870(89)90023-8.
hdl:1813/72078.
lynn, michael (1992).
the-psychology of unavailability:
explaining scarcity-and-cost-effects on value".
basic and applied social-psychology.
13 (1): 3–7.
doi:10.1207
/s15324834basp1301_2.
hdl:1813/71653.
mittone, luigi; savadori, lucia (2009).
the-scarcity-bias".
applied psychology.
58 (3): 453–468.
doi:10.1111/j.1464-0597.2009.00401.x.
pearl, judea (1985).
heuristics:
intelligent-search-strategies for computer-problem solving (repr.
with corr.
reading, mass.:
addison-wesley-pub.
co.-p.-vii.
isbn 978-0-201-05594-8.
worchel, stephen; arnold, susan; baker, michael (1975). "
the-effects of censorship on attitude-change:
the-influence of censor and communication-characteristics" (pdf).
journal of applied-social-psychology.
5 (3): 227–239.
doi:10.1111/j.1559-1816.1975.tb00678.x.
archived from the original on 2015-02-23.cs1-maint: bot: original-url-status unknown (link)
worchel, stephen; lee, jerry; adewole, akanbi (1975).
effects of supply and demand on ratings of object-value".
journal of personality and social-psychology.
32 (5):-906–914.
doi:10.1037/0022-3514.32.5.906.
zellinger, david-a.; fromkin, howard-l.; speller, donald-e.; kohn, carol-a. (1975). "
a-commodity-theory-analysis of the-effects of age-restrictions upon pornographic-materials".
journal of applied-psychology.
doi:10.1037/h0076350.
further-reading ==
tauer, john-m. (2007). "
scarcity-principle".
in baumeister, roy; vohs, kathleen (eds.).
encyclopedia of social-psychology.
doi:10.4135/9781412956253.n466.
isbn 9781412916707.
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
in other-words, humans "...preferences are determined by changes in outcomes relative to a-certain-reference-level..."
as stated by esther-mirjam-sent (2018) limitations include the-difficulty of the-problem requiring a-decision, the-cognitive-capability of the-mind, and the-time available to make the-decision.
decision-makers, in this-view, act as satisficers, seeking a-satisfactory-solution, rather than an-optimal-solution.
therefore, humans do not undertake a-full-cost-benefit-analysis to determine the-optimal-decision, but rather, choose an-option that fulfils humans-adequacy-criteria.
herbert-a.-simon proposed bounded-rationality as an-alternative-basis for the-mathematical-and-neoclassical-economic-modelling of decision-making, as used in economics, political-science, and related-disciplines.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
therefore, bounded-rationality can be said to address the-discrepancy between the-assumed-perfect-rationality of human-behaviour (which is utilised by other-economics-theories such as the-neoclassical-approach), and the-reality of human-cognition.
herbert-a.-simon used the-analogy of a-pair of scissors, where one-blade represents "cognitive-limitations" of actual-humans and the other the-"structures of the-environment", illustrating how minds compensate for limited-resources by exploiting known-structural-regularity in the-environment.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
with bounded-rationality, simon's-goal was "to replace the-global-rationality of economic-man with a-kind of rational-behavior that is compatible with the-access to information and the-computational-capacities that are actually possessed by organisms, including man, in the-kinds of environments in which such-organisms exist.
in short, the-concept of bounded-rationality revises notions of "perfect"-rationality to account for the-fact that perfectly-rational-decisions are often not feasible in practice because of the-intractability of natural-decision-problems and the-finite-computational-resources available for making natural-decision-problems.
the-concept of bounded-rationality continues to influence (and be debated in) different-disciplines, including economics, psychology, law, political-science, and cognitive-science.
some-models of human-behavior in the-social-sciences assume that humans can be reasonably approximated or described as "rational"-entities, as in rational-choice-theory or downs'-political-agency-model.
origins ==
bounded-rationality was coined by herbert-a.-simon.
in models of man, herbert-a.-simon argues that most-people are only partly rational, and are irrational in the-remaining-part of most-people actions.
in another-work, herbert-a.-simon states "boundedly-rational-agents-experience-limits in formulating and solving complex-problems and in processing-(receiving,-storing,-retrieving,-transmitting)-information".
simon describes a-number of dimensions along which "classical"-models of rationality can be made somewhat more realistic, while remaining within the-vein of fairly-rigorous-formalization.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
they do this because of the-complexity of the-situation.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
both-games, as defined by game-theory-economics, are finite-games with perfect-information, and therefore equivalent.
however, within chess, mental-capacities and abilities are a-binding-constraint, therefore optimal-choices are not a-possibility.
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
==-model-extensions ==
as decision-makers have to make decisions about how and when to decide, ariel-rubinstein proposed to model bounded-rationality by explicitly specifying decision-making-procedures.
this puts the-study of decision-procedures on the-research-agenda.
gerd-gigerenzer opines that-decision-theorists, to some-extent, have not adhered to simon's-original-ideas.
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
gigerenzer proposes and shows that simple-heuristics often lead to better-decisions than theoretically optimal procedures.
moreover, gigerenzer-states, agents react relative to agents-environment and use agents-cognitive-processes to adapt accordingly.
huw-dixon later argues that it may not be necessary to analyze in detail the-process of reasoning underlying-bounded-rationality.
if we believe that agents will choose an-action that gets agents "close" to the optimum, then we can use the-notion of epsilon-optimization, which means we choose we actions so that the-payoff is within epsilon of the optimum.
if we define the-optimum-(best-possible)-payoff as
u             ∗     {\displaystyle u^{*}}   , then the-set of epsilon-optimizing-options-s(ε) can be defined as all-those-options s such that
:---------u
(---------s---------)---------≥-u
∗         − ϵ     {\displaystyle u(s)\geq
u^{*}-\epsilon }   .
the-notion of strict-rationality is then a-special-case (ε=0).
the-advantage of this-approach is that this-approach avoids having to specify in detail the-process of reasoning, but rather simply assumes that whatever the-process is, it is good enough to get near to the optimum.
from a-computational-point of view, decision-procedures can be encoded in algorithms and heuristics.
edward-tsang argues that the-effective-rationality of an-agent is determined by an-agent computational intelligence.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
tshilidzi-marwala and evan-hurwitz in their-study on bounded-rationality observed that advances in technology (e.g.-computer-processing-power because of moore's-law, artificial-intelligence, and big-data-analytics) expand the-bounds that define the-feasible-rationality-space.
because of this-expansion of the-bounds of rationality, machine-automated-decision-making makes markets more efficient.
it is also important to consider that the-model of bounded-rationality also extends tobounded-self-interest in which humans are sometimes willing to forsake humans own self-interests for the-benefits of others, something that has not been considered in earlier-economic-models.
relationship to behavioral-economics ==
bounded-rationality implies the-idea that humans take reasoning-shortcuts that may lead to sub-optimal-decision-making.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
one-treatment of this-idea comes from cass-sunstein and richard-thaler's-nudge.
cass-sunstein and richard-thaler's-nudge-richard-thaler's-recommend
that-choice-architectures are modified in light of human-agents'-bounded-rationality.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
furthermore,-bounded-rationality-attempts to address assumption-points discussed within neoclassical-economics-theory during the 1950s.
neoclassical-economics-theory assumes that the-complex-problem, the-way in which the-problem is presented, all-alternative-choices, and a-utility-function, are all provided to decision-makers in advance, where this may not be realistic.
this was widely used and accepted for a-number of decades, however economists realised some-disadvantages exist in utilising neoclassical-economics-theory.
neoclassical-economics-theory did not consider how problems are initially discovered by decision-makers, which could have an-impact on the-overall-decision.
additionally, personal-values, the-way in which alternatives are discovered and created, and the-environment surrounding the-decision-making-process are also not considered when using this-theory .
alternatively, bounded-rationality focuses on the-cognitive-ability of the-decision-maker and the-factors which may inhibit optimal-decision-making additionally, placing a-focus on organisations rather than focusing on markets as neoclassical-economics-theory does, bounded-rationality is also the-basis for many-other-economics-theories (e.g.-organisational-theory) as it emphasises that the-"...performance and success of an-organisation is governed primarily by the-psychological-limitations of an-organisation members..." as stated by john-d.w.-morecroft (1981) .
relationship to psychology ==
the collaborative works of daniel-kahneman and amos-tversky expand upon herbert-a.-simon's-ideas in the-attempt to create a-map of bounded-rationality.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
kahneman cites that the-research contributes mainly to the-school of psychology due to imprecision of psychological-research to fit the-formal-economic-models, however, the-theories are useful to economic-theory as a-way to expand simple-and-precise-models and cover diverse-psychological-phenomena.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
in contrast to the-work of herbert-a.-simon; kahneman and tversky aimed to focus on the-effects bounded-rationality had on simple-tasks which therefore placed more-emphasis on errors in cognitive-mechanisms irrespective of the-situation.
influence on social-network-structure ==
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
in particular, kasthurirathna and piraveenan have shown that in socio-ecological-systems, the-drive towards improved-rationality on average might be an-evolutionary-reason for the-emergence of scale-free-properties.
they did this by simulating a-number of strategic-games on an-initially-random-network with distributed-bounded-rationality, then re-wiring the-network so that the-network on average converged towards nash-equilibria, despite the-bounded-rationality of nodes.
they observed that this re-wiring process results in scale-free-networks.
since scale-free-networks are ubiquitous in social-systems, the-link between bounded-rationality-distributions and social-structure is an-important-one in explaining social-phenomena.
conclusion ==
to conclude, bounded-rationality challenges the-rationality-assumptions widely accepted between the-1950s and 1970s which were initially used when considering [utility]-maximisation, [probability]-judgements, and other-market-focused-economic-calculations .
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
see also == ==
reference-list == ==
further-reading ==
bayer, r.-c., renner, e., & sausgruber, r. (2009).
confusion-and-reinforcement-learning in experimental-public-goods-games.
nrn-working-papers 2009–22,
the-austrian-center for labor-economics and the-analysis of the-welfare-state, johannes-kepler-university-linz, austria.
elster, jon (1983).
sour-grapes: studies in the-subversion of rationality.
cambridge, uk:
cambridge-university-press.
isbn 978-0-521-25230-0.
felin, t., koenderink, j., & krueger, j. (2017). "
rationality, perception and the-all-seeing-eye."
psychonomic-bulletin and review, 25: 1040-1059.
/s13423-016-1198-z gershman, s.j., horvitz, e.j., & tenenbaum, j.b. (2015).
computational-rationality: a-converging-paradigm for intelligence in brains, minds, and machines.
science, 49: 273-278.
science.aac6076
gigerenzer, gerd & selten, reinhard (2002).
bounded-rationality.
cambridge:
mit-press.
isbn 978-0-262-57164-7.
hayek, f.a (1948)
individualism and economic-order
kahneman, daniel (2003).
"-maps of bounded-rationality: psychology for behavioral-economics" (pdf).
the-american-economic-review.
: 1449–75.
citeseerx 10.1.1.194.6554.
doi:10.1257/000282803322655392.
archived from the-original-(pdf) on 2018-02-19.
retrieved 2017-11-01.
march, james-g. (1994).
a-primer on decision making:
how decisions happen.
the-free-press.
isbn 978-0-02-920035-3.
simon, herbert (1957). "
a-behavioral-model of rational-choice", in models of man, social and rational:
mathematical-essays on rational-human-behavior in a-social-setting.
new-york:-wiley.
march, james-g. & simon, herbert (1958).
organizations.
john-wiley and sons.
isbn 978-0-471-56793-6.
simon, herbert (1990). "
a-mechanism for social-selection and successful-altruism".
250 (4988)
bibcode:1990sci...
250.1665s.
doi:10.1126/science.2270480.
pmid 2270480.
simon, herbert (1991). "
bounded-rationality and organizational-learning".
organization-science.
2 (1):-125–134.
doi:10.1287/orsc.2.1.125.
tisdell, clem (1996).
bounded-rationality and economic-evolution:
a-contribution to decision-making, economics, and management.
cheltenham, uk: brookfield.
isbn 978-1-85898-352-3.
wheeler, gregory (2018). "
bounded-rationality".
in edward-zalta (ed.).
stanford-encyclopedia of philosophy.
stanford, ca.
williamson, oliver-e. (1981).
the-economics of organization: the-transaction-cost-approach".
american-journal of sociology.
87 (3): 548–577 (press +).
doi:10.1086/227496.
s2cid 154070008.
external-links == bounded rationality in stanford-encyclopedia of philosophy
mapping-bounded-rationality by daniel-kahneman-artificial-intelligence-and-economic-theory-chapter 7 of surfing-economics by huw-dixon. "
resource-bounded-agents".
internet-encyclopedia of philosophy.
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
social-psychologists typically explain human-behavior as being a-result of the-relationship between mental-state-and-social-situation, studying the-conditions under which thoughts, feelings, and behaviors occur and how these-variables influence social-interactions.
social-psychology has bridged the-gap between psychology and sociology to an-extent, but a-divide still exists between the-two-fields.
nevertheless, sociological-approaches to psychology remain an-important-counterpart to conventional-psychological-research.
in addition to the-split between psychology and sociology, there is difference in emphasis between american-and-european-social-psychologists, as the former traditionally have focused more on the-individual, whereas the latter have generally paid more-attention to group-level-phenomena.
history ==
although issues in social-psychology already had been discussed in philosophy for much of human-history—such as the-writings of the-islamic-philosopher-al-farabi, which dealt with similar-issues—the-modern,-scientific-discipline began in the-united-states at the-end of the-19th-century.
19th-century ==
in the-19th-century, social-psychologist was an-emerging-field from the-larger-field of psychology.
at the-time, many-psychologists were concerned with developing concrete-explanations for the-different-aspects of human-nature.
many-psychologists attempted to discover concrete-cause-and-effect-relationships that explained social-interactions.
in order to do so, many-psychologists applied the-scientific-method to human-behavior.
the-first-published-study in the-field was norman-triplett's-1898-experiment on the-phenomenon of social-facilitation.
these-psychological-experiments later went on to form the-foundation of much of 20th-century-social-psychological-findings.
early-20th-century ===
during the-1930s, many-gestalt-psychologists, most-notably-kurt-lewin, fled to the-united-states from nazi-germany.
they were instrumental in developing the-field as an-area separate from the-dominant-behavioral-and-psychoanalytic-schools of that-time.
attitudes and small-group-phenomena were the-topics most commonly studied in this-era.
during world-war-ii, social-psychologists were primarily engaged with studies of persuasion and propaganda for the-u.s.-military (see also psychological-warfare).
following the-war, researchers became interested in a-variety of social-problems, including issues of gender and racial-prejudice.
most notable and contentious of a-variety of social-problems, including issues of gender and racial-prejudice were the-milgram-experiments.
during the-years immediately following world-war-ii, there were frequent-collaborations between psychologists and sociologists.
the-two-disciplines, however, have become increasingly specialized and isolated from each other in recent-years, with sociologists generally focusing on macro-features whereas psychologists generally focusing on more-micro-features.
late-20th-century and modernity == =
in the-1960s, there was growing-interest in topics such as cognitive-dissonance, bystander-intervention, and aggression.
by the-1970s, however, social-psychology in america had reached a-crisis, as heated-debates emerged over issues such as ethical-concerns about laboratory-experimentation, whether attitude could actually predict behavior, and how much science could be done in a-cultural-context.
this was also a-time when situationism came to challenge the-relevance of self and personality in psychology.
throughout the-1980s and 1990s, social-psychology reached a-more-mature-level, especially in regard to theory and methodology.
now, careful-ethical-standards regulate research, and pluralistic-and-multicultural-perspectives have emerged.
modern-researchers are interested in many-phenomena, though attribution, social-cognition, and the-self-concept are perhaps the-areas of greatest-growth in recent-years.
social-psychologists have also maintained social-psychologists applied interests with contributions in the-social-psychology of health, education, law, and the-workplace.
intrapersonal-phenomena == ===
attitudes ===
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
attitudes are basic-expressions of approval and disapproval, or as bem (1970) suggests, likes and dislikes
(e.g. enjoying chocolate-ice-cream, or endorsing the-values of a-particular-political-party).
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
for example, a-person may value the-environment but may not recycle a-plastic-bottle on a-particular-day.
research on attitudes has examined the-distinction between traditional,-self-reported-attitudes and implicit,-unconscious-attitudes.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
likewise, one-study found that in interracial-interactions, explicit-attitudes correlate with verbal-behavior while implicit-attitudes correlate with nonverbal-behavior.
one-hypothesis on how attitudes are formed, first proposed in 1983 by abraham-tesser, is that strong-likes and dislikes are ingrained in our-genetic-make-up.
tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
attitudes are also formed as a-result of exposure to different-experiences, environments, and through the-learning-process.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
attitudes are also involved in several-other-areas of the-discipline, such as conformity, interpersonal-attraction, social-perception, and prejudice.
persuasion ===
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
persuasion relies on appeals rather than strong-pressure or coercion.
the-process of persuasion has been found to be influenced by numerous-variables that generally fall into one of five-major-categories: communicator: includes credibility, expertise, trustworthiness, and attractiveness.
message: includes varying-degrees of reason, emotion (e.g. fear), one-sided or two-sided-arguments, and other-types of informational-content.
audience: includes a-variety of demographics, personality-traits, and preferences.
channel/medium: includes printed-word, radio, television, the-internet, or face-to-face interactions.
context: includes environment, group-dynamics, and preliminary-information to that of message (category #2).dual-process-theories of persuasion (such as the-elaboration-likelihood-model) maintain that persuasion is mediated by two-separate-routes: central and peripheral.
the-central-route of persuasion is more fact-based and results in longer-lasting-change, but requires motivation to process.
the-central-route of persuasion is more superficial and results in shorter-lasting-change, but does not require as-much-motivation to process.
an-example of peripheral-persuasion is a-politician using a-flag-lapel-pin, smiling, and wearing a-crisp,-clean-shirt.
this does not require motivation to be persuasive, but should not last as long as central-persuasion.
if that-politician were to outline what that-politician believe and that-politician previous voting record, that-politician would be centrally persuasive, resulting in longer-lasting-change at the-expense of greater-motivation required for processing.
social-cognition ===
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
a-major-research-topic in social-cognition is attribution.
attributions are how we explain people's-behavior, either we own behavior or the-behavior of others.
one-element of attribution ascribes the-cause of a-behavior to internal-and-external-factors.
an-internal,-or-dispositional,-attribution-reasons that behavior is caused by inner-traits such as personality, disposition, character, and ability.
an-external,-or-situational,-attribution-reasons that behaviour is caused by situational-elements such as the-weather.
a-second-element of attribution ascribes the-cause of behavior to stable-and-unstable-factors (i.e. whether the-behavior will be repeated or changed under similar-circumstances).
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
numerous-biases in the-attribution-process have been discovered.
for instance, the-fundamental-attribution-error is the-tendency to make dispositional-attributions for behavior, overestimating the-influence of personality and underestimating the-influence of the-situational.
the-actor-observer-bias is a-refinement of this; the-actor-observer-bias is the-tendency to make dispositional-attributions for other-people's-behavior and situational-attributions for our own.
the-self-serving-bias is the-tendency to attribute dispositional-causes for successes, and situational-causes for failure, particularly when self-esteem is threatened.
this leads to assuming one's-successes are from innate-traits, and one's-failures are due to situations.
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
researchers have found that mildly-depressed-individuals often lack this-bias and actually have more-realistic-perceptions of reality as measured by the-opinions of others.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
as such, vivid-or-highly-memorable-possibilities will be perceived as more likely than those that are harder to picture or difficult to understand, resulting in a-corresponding-cognitive-bias.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
numerous-other-biases have been found by social-cognition-researchers.
the-hindsight-bias is a-false-memory of having predicted events, or an-exaggeration of actual-predictions, after becoming aware of the-outcome.
the-hindsight-bias is a-type of bias leading to the-tendency to search for or interpret information in a-way that confirms one's-preconceptions.
schemas ====
another-key-concept in social-cognition is the-assumption that reality is too complex to easily discern.
as a-result, we tend to see the-world according to simplified-schemas or images of reality.
schemas are generalized-mental-representations that organize knowledge and guide information-processing.
schemas often operate automatically and unintentionally, and can lead to biases in perception and memory.
schemas may induce expectations that lead us to see something that is not there.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
stereotypes are often related to negative-or-preferential-attitudes (prejudice) and behavior (discrimination).
schemas for behaviors (e.g., going to a-restaurant, doing laundry) are known as scripts.
self-concept ===
self-concept is the-whole-sum of beliefs that people have about people.
self-concept is made up of cognitive-aspects called self-schemas—
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, an-athlete at a-university would have multiple-selves that would process different-information pertinent to each-self: the-student would be oneself, who would process information pertinent to a-student (taking notes in class, completing a-homework-assignment, etc.);
an-athlete at a-university would be the-self who processes information about things related to being an-athlete (recognizing an-incoming-pass, aiming a-shot, etc.).
these-selves are part of one's-identity and the-self-referential-information is that which relies on the-appropriate-self to process and react to it.
if a-self is not part of one's-identity, then it is much more difficult for one to react.
for example, a-civilian may not know how to handle a-hostile-threat as well as a-trained-marine would.
a-trained-marine contains a-self that would enable him/
a-trained-marine to process the-information about the-hostile-threat and react accordingly, whereas a-civilian may not contain that-self, lessening the-civilian's-ability to properly assess the-hostile-threat and act accordingly.
the-self-concept comprises multiple-self-schemas.
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
for individuals, a-range of otherwise-mundane-events—grocery-shopping, new-clothes, eating out, or going to the-beach—can trigger thoughts about the-self.
the-self is a-special-object of our-attention.
whether one is mentally focused on a-memory, a-conversation, a-foul-smell,
the-song that is stuck in one's-head, or this-sentence
, consciousness is like a-spotlight.
this-spotlight can shine on only-one-object at a-time, but  this-spotlight can switch rapidly from one-object to another.
in  this-spotlight the-self is front and center: things relating to the-self have
this-spotlight more often.
the-abcs of self are: affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
cognition:
how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting is the-process of predicting how one would feel in response to future-emotional-events.
studies done in 2003 by timothy-wilson and daniel-gilbert
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
there are many-theories on the-perception of our-own-behavior.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
there is also the-facial-feedback-hypothesis: changes in facial-expression can lead to corresponding-changes in emotion.
the-self-concept is often divided into a-cognitive-component, known as the-self-schema, and an-evaluative-component, the-self-esteem.
the-need to maintain a-healthy-self-esteem is recognized as a-central-human-motivation.
self-efficacy-beliefs are associated with the-self-schema.
self-efficacy-beliefs are expectations that performance of some-task will be effective and successful.
social-psychologists also study such-self-related-processes as self-control and self-presentation.
people develop people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
self-perception is a-specialized-form of attribution that involves making inferences about oneself after observing one's-own-behavior.
psychologists have found that too-many-extrinsic-rewards (e.g.-money) tend to reduce intrinsic-motivation through the-self-perception-process, a-phenomenon known as overjustification.
people's-attention is directed to the-reward, and people's-attention lose interest in the-task when the-reward is no longer offered.
this is an-important-exception to reinforcement-theory.
interpersonal-phenomena == ===
social-influence ===
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
social-influence is seen as a-fundamental-value in social-psychology.
the-study of  social-influence overlaps considerably with research into attitudes and persuasion.
the-three-main-areas of social-influence include: conformity, compliance, and obedience.
social-influence is also closely related to the-study of group-dynamics, as most-effects of influence are strongest when influence take place in social-groups.
the-first-major-area of social-influence is conformity.
conformity is defined as the-tendency to act or think like other-members of a-group.
the-identity of members within a-group (i.e.-status), similarity, expertise, as well as cohesion, prior-commitment, and accountability to a-group help to determine the-level of conformity of an-individual.
individual-variations among group-members plays a-key-role in the-dynamic of how-willing-people will be to conform.
conformity is usually viewed as a-negative-tendency in american-culture, but a-certain-amount of conformity is adaptive in some-situations, as is nonconformity in other-situations.
the-second-major-area of social-influence-research is compliance, which refers to any-change in behavior that is due to a-request or suggestion from another-person.
the foot-in-the-door technique is a-compliance-method in which the-persuader requests a-small-favor and then follows up with requesting a-larger-favor, e.g., asking for the-time and then asking for ten-dollars.
a-related-trick is the-bait and switch.
the-third-major-form of social-influence is obedience; this is a-change in behavior that is the-result of a-direct-order or command from another-person.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
an-unusual-kind of social-influence is the-self-fulfilling-prophecy.
this is a-prediction that, in being made, causes this to become true.
for example, in the-stock-market, if this is widely believed that a-crash is imminent, investors may lose confidence, sell most of investors-stock, and thus cause a-crash.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
psychologists have spent decades studying the-power of social-influence, and the-way in which it manipulates people's-opinions and behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
group-dynamics ===
a-group can be defined as two-or-more-individuals who are connected to each another by social-relationships.
groups tend to interact, influence each other, and share a-common-identity.
they have a-number of emergent-qualities that distinguish they from coincidental,-temporary-gatherings, which are termed social aggregates: norms:
implicit-rules and expectations for group-members to follow (e.g. saying thank you, shaking hands).
implicit-rules and expectations for specific-members within the-group (e.g.-the-oldest-sibling, who may have additional-responsibilities in the-family).
relations: patterns of liking within the-group, and also-differences in prestige or status (e.g.-leaders, popular-people).temporary-groups and aggregates share few or none of these-features and do not qualify as true-social-groups.
people waiting in line to get on a-bus, for example, do not constitute a-group.
groups are important not only because groups offer social-support, resources, and a-feeling of belonging, but because groups supplement an-individual's-self-concept.
to a-large-extent, humans define humans by the-group-memberships which form humans-social-identity.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
the-tendency to define oneself by membership in a-group may lead to intergroup-discrimination, which involves favorable-perceptions and behaviors directed towards the-in-group, but negative-perceptions and behaviors directed towards the-out-group.
on the-other-hand, such-discrimination and segregation may sometimes exist partly to facilitate a-diversity that strengthens society.
intergroup-discrimination leads to prejudicial-stereotyping, while the-processes of social-facilitation and group-polarization encourage extreme-behaviors towards the-out-group.
groups often moderate and improve decision-making, and are frequently relied upon for these-benefits, such as in committees and juries.
a-number of group-biases, however, can interfere with effective-decision-making.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
more problematic is the-phenomenon of groupthink, which is a-collective-thinking-defect that is characterized by a-premature-consensus or an-incorrect-assumption of consensus, caused by members of a-group failing to promote views that are not consistent with the-views of other-members.
groupthink occurs in a-variety of situations, including isolation of a-group and the-presence of a-highly-directive-leader.
janis offered the-1961-bay of pigs-invasion as a-historical-case of groupthink.
groups also affect performance and productivity.
social-facilitation, for example, is a-tendency to work harder and faster in the-presence of others.
social-facilitation increases the-dominant-response's-likelihood, which tends to improve performance on simple-tasks and reduce social-facilitation on complex-tasks.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
social-loafing is common when the-task is considered unimportant and individual-contributions are not easy to see.
social-psychologists study group-related-(collective)-phenomena such as the-behavior of crowds.
an-important-concept in this-area is deindividuation, a-reduced-state of self-awareness that can be caused by feelings of anonymity.
deindividuation is associated with uninhibited-and-sometimes-dangerous-behavior.
it is common in crowds and mobs, but it can also be caused by a-disguise, a-uniform, alcohol, dark-environments, or online-anonymity.
interpersonal-attraction ===
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
several-general-principles of attraction have been discovered by social-psychologists.
one of the-most-important-factors in interpersonal-attraction is how similar two-particular-people are.
the-more-similar-two-people are in general-attitudes, backgrounds, environments, worldviews, and other-traits,
the more likely
the-more-similar-two-people will be attracted to each other.
physical-attractiveness is an-important-element of romantic-relationships, particularly in the-early-stages characterized by high-levels of passion.
later on, similarity and other-compatibility-factors become more important, and the-type of love-people experience shifts from passionate to companionate.
in 1986, robert-sternberg suggested that there are actually three-components of love: intimacy, passion, and commitment.
when two-(or-more)-people experience all three, two-(or-more)-people are said to be in a-state of consummate-love.
according to social-exchange-theory, relationships are based on rational-choice-and-cost-benefit-analysis.
a-person may leave a-relationship if their-partner's-"costs" begin to outweigh their-benefits, especially if there are good-alternatives available.
this-theory is similar to the-minimax-principle proposed by mathematicians and economists (despite the-fact that human-relationships are not zero-sum-games).
with time, long-term-relationships tend to become communal rather than simply based on exchange.
research == ===
methods ===
social-psychology is an-empirical-science that attempts to answer questions about human-behavior by testing hypotheses, both in the-laboratory and in the-field.
careful-attention to research-design, sampling, and statistical-analysis is important; results are published in peer-reviewed-journals such as the-journal of experimental-social-psychology, personality and social-psychology bulletin and the-journal of personality and social-psychology.
social-psychology-studies also appear in general-science-journals such as psychological-science and science.
experimental-methods involve the-researcher altering a-variable in the-environment and measuring the-effect on another-variable.
an-example would be allowing two-groups of children to play violent-or-nonviolent-videogames and then observing two-groups of children subsequent-level of aggression during the-free-play-period.
a-valid-experiment is controlled and uses random-assignment.
correlational-methods examine the-statistical-association between two-naturally-occurring-variables.
for example, one could correlate the-number of violent-television shows children watch at home with the-number of violent-incidents the-children participate in at school.
note that this-study would not prove that violent-tv causes aggression in children: it is quite possible that aggressive-children choose to watch more violent-tv.
observational-methods are purely descriptive and include naturalistic-observation, contrived-observation, participant-observation, and archival-analysis.
these are less common in social-psychology but are sometimes used when first investigating a-phenomenon.
an-example would be to unobtrusively observe children on a-playground (with a-videocamera, perhaps) and record the-number and types of aggressive-actions displayed.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
experiments are useful in social-psychology because experiments are high in internal-validity, meaning that experiments are free from the-influence of confounding-or-extraneous-variables, and so are more likely to accurately indicate a-causal-relationship.
however, the-small-samples used in controlled-experiments are typically low in external-validity, or the-degree to which the-results can be generalized to the-larger-population.
there is usually a-trade-off between experimental-control (internal-validity) and being able to generalize to the-population (external-validity).
because it is usually impossible to test everyone, research tends to be conducted on a-sample of persons from the-wider-population.
social-psychologists frequently use survey-research when  social-psychologists are interested in results that are high in external-validity.
surveys use various-forms of random-sampling to obtain a-sample of respondents that is representative of a-population.
this-type of research is usually descriptive or correlational because there is no-experimental-control over variables.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in a-1986-study by david-o.-sears, over-70% of experiments used north-american-undergraduates as subjects, a-subset of the-population that is unrepresentative of the-population as a-whole.
regardless of which-method has been chosen, the-significance of the-results is reviewed before accepting the-results in evaluating an-underlying-hypothesis.
there are two-different-types of tests that social-psychologists use to review social-psychologists results.
statistics and probability-testing define what constitutes a-significant-finding, which can be as low as 5% or less, that is unlikely due to-chance.
replications-testing is important in ensuring that the-results are valid and not due to chance.
false-positive-conclusions, often resulting from the-pressure to publish or the-author's-own-confirmation-bias, are a-hazard in the-field.
famous-experiments === ====
asch conformity experiments ====
asch conformity experiments ==== demonstrated the-power of the-impulse to conform within small-groups, by the-use of a-line-length-estimation-task that was designed to be easy to assess but where deliberately-wrong-answers were given by at least some, oftentimes most, of the-other-participants.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
seventy-five-percent of the-participants conformed at least once during the-experiment.
additional-manipulations of the-experiment showed that participant-conformity decreased when at-least-one-other-individual failed to conform but increased when the-individual began conforming or withdrew from the-experiment.
also, participant-conformity increased substantially as the-number of "incorrect"-individuals increased from one to three, and remained high as the-incorrect-majority grew.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
festinger (cognitive-dissonance) ====
in leon-festinger's-cognitive-dissonance-experiment, after being divided into two-groups-participants were asked to perform a-boring-task and later asked to dishonestly give being divided into two-groups-participants opinion of a-boring-task, afterwards being rewarded according to two-different-pay-scales.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
another-group of participants later reported liking a-boring-task better than the-second-group ($20).
festinger's-explanation was that for people in the-second-group ($20) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
they could only overcome that-dissonance by justifying they lies by changing they previously-unfavorable-attitudes about the-task.
being paid $20 provides a-reason for doing the-boring-task resulting in no-dissonance.
milgram-experiment ====
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
following the-events of the-holocaust in world-war-ii, the-experiment showed that normal-american-citizens were capable of following orders even when normal-american-citizens believed normal-american-citizens were causing an-innocent-person to suffer.
stanford-prison-experiment ====
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
in just-a-few-days, the-guards became brutal and cruel, and the-prisoners became miserable and compliant.
this was initially argued to be an-important-demonstration of the-power of the-immediate-social-situation and
this-capacity to overwhelm normal-personality-traits.
subsequent-research has contested the-initial-conclusions of the-study.
for example, subsequent-research has been pointed out that participant-self-selection may have affected the-participants'-behavior, and that the-participants'-personalities influenced the-participants'-personalities reactions in a-variety of ways, including how long the-participants'-personalities chose to remain in the-study.
the-2002-bbc-prison-study, designed to replicate the-conditions in the-study, produced conclusions that were drastically different from the-initial-findings.
others ====
muzafer-sherif's-robbers'-cave-study divided boys into two-competing-groups to explore how-much-hostility and aggression would emerge.
muzafer-sherif's's-explanation of the-results became known as realistic-group-conflict-theory, because the-intergroup-conflict was induced through competition for resources.
inducing cooperation and superordinate-goals later reversed this-effect.
albert-bandura's-bobo-doll-experiment demonstrated how aggression is learned by imitation.
this-set of studies fueled debates regarding media-violence, a-topic that continues to be debated among scholars.
ethics ===
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
for this-reason, many-social-psychology-experiments utilize deception to conceal or distort certain-aspects of the-study.
deception may include false-cover-stories, false-participants (known as confederates or stooges),
false-feedback given to the-participants, and so on.
the-practice of deception has been challenged by psychologists who maintain that-deception under any-circumstances is unethical and that other-research-strategies (e.g.,-role-playing) should be used instead.
unfortunately, research has shown that role-playing-studies do not produce the-same-results as deception-studies, and this has cast doubt on role-playing-studies validity.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
(e.g., the-milgram-experiment and stanford-prison-experiment), and this has also been criticized for ethical-reasons.
to protect the-rights and well-being of research-participants, and at the-same-time discover meaningful-results and insights into human-behavior, virtually-all-social-psychology-research must pass an-ethical-review.
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
furthermore, a-process of informed-consent is often used to make sure that volunteers know what will asked of volunteers in the-experiment and understand that volunteers are allowed to quit the-experiment at any-time.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
today, most-research in social-psychology involves no-more-risk of harm than can be expected from routine-psychological-testing or normal-daily-activities.
adolescents ===
social-psychology studies what plays key-roles in a-child's-development.
during this-time, teens are faced with many-issues and decisions that can impact teens-social-development.
they are faced with self-esteem-issues, peer-pressure, drugs, alcohol, tobacco, sex, and social-media.
psychologists today are not fully aware of the-effect of social-media.
social-media is worldwide, so one can be influenced by something social-media will never encounter in real-life.
in 2019, social-media became the-single-most-important-activity in adolescents' and even-some-older-adults'-lives.
replication-crisis ===
many-social-psychological-research-findings have proven difficult to replicate, leading some to argue that social-psychology is undergoing a-replication-crisis.
replication-failures are not unique to social-psychology and are found in all-fields of science.
some-factors have been identified in social-psychological-research that has led the-field to undergo the-field current crisis.
firstly, questionable-research-practices have been identified as common.
such-practices, while not necessarily intentionally fraudulent, involve converting undesired-statistical-outcomes into desired-outcomes via the-manipulation of statistical-analyses, sample-sizes, or data-management-systems, typically to convert non-significant-findings into significant-ones.
some-studies have suggested that at-least-mild-versions of these-practices are prevalent.
one of the-criticisms of daryl-bem in the-feeling the-future-controversy is that the-evidence for precognition in the-study could be attributed to questionable-practices.
secondly, some-social-psychologists have published fraudulent-research that has entered into mainstream-academia, most-notably-the-admitted-data-fabrication by diederik-stapel as well as allegations against others.
fraudulent-research is not the-main-contributor to the-replication-crisis.
several-effects in social-psychology have been found to be difficult to replicate even before the-current-replication-crisis.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
replications appear particularly difficult when research-trials are pre-registered and conducted by research-groups not highly invested in the-unconscious-thought-theory under questioning.
these-three-elements together have resulted in renewed-attention to replication supported by daniel-kahneman.
scrutiny of many-effects have shown that several-core-beliefs are hard to replicate.
a-2014-special-edition of social-psychology focused on replication-studies, and a-number of previously-held-beliefs were found to be difficult to replicate.
likewise, a-2012-special-edition of perspectives on psychological-science focused on issues ranging from publication-bias to null-aversion that contribute to the-replication-crisis in psychology.
it is important to note that the-replication-crisis in psychology does not mean that social-psychology is unscientific.
rather, this-reexamination is
a-healthy-if-sometimes-acrimonious-part of the-scientific-process in which old-ideas or those that cannot withstand careful-scrutiny are pruned.
the-consequence is that some-areas of social-psychology once considered solid, such as social-priming, have come under increased-scrutiny due to failure to replicate findings.
academic-journals === ==
see also ==
==-notes == ==
references == ==
external-links ==
social-psychology-network-introduction to social-psychology-social-psychology — basics
social-psychology  on plos
—-subject-area-page
social-psychology on all-about psychology — information and resources
what is social-psychology?
on youtube-cognitive-biases are systematic-patterns of deviation from norm and/or rationality in judgment.
they are often studied in psychology and behavioral-economics.
although the-reality of most of these-biases is confirmed by reproducible-research, there are often controversies about how to classify these-biases or how to explain these-biases.
several-theoretical-causes are known for some-cognitive-biases, which provides a-classification of biases by several-theoretical-causes common-generative-mechanism (such as noisy-information-processing).
gerd-gigerenzer has criticized the-framing of cognitive-biases as errors in judgment, and favors interpreting cognitive-biases as arising from rational-deviations from logical-thought.
explanations include information-processing-rules (i.e.,-mental-shortcuts), called heuristics, that the-brain uses to produce decisions or judgments.
biases have a-variety of forms and appear as cognitive-("cold")-bias, such as mental-noise, or motivational-("hot")-bias, such as when beliefs are distorted by wishful-thinking.
both-effects can be present at the-same-time.
there are also controversies over some of these-biases as to whether some of these-biases count as useless or irrational, or whether some of these-biases result in useful-attitudes or behavior.
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
however, this-kind of confirmation-bias has also been argued to be an-example of social-skill; a way to establish a-connection with the-other-person.
although this-research overwhelmingly involves human-subjects, some findings that demonstrate bias have been found in non-human-animals as well.
for example, loss-aversion has been shown in monkeys and hyperbolic-discounting has been observed in rats, pigeons, and monkeys.
belief, decision-making and behavioral ==
these-biases affect belief-formation, reasoning-processes, business and economic decisions, and human-behavior in general.
social-==-==-memory ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
there are many-types of memory-bias, including:
see also == ==
footnotes == ==
references ==
gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer is director-emeritus of the-center for adaptive-behavior and cognition (abc) at the-max-planck-institute for human-development and director of the-harding-center for risk-literacy, both in berlin, germany.
gigerenzer investigates how humans make inferences about humans world with limited-time and knowledge.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
he conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
a-heuristic is called ecologically rational to the-degree that a-heuristic is adapted to the-structure of an-environment.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast, gigerenzer and associated-researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
this contradicts the-traditional-view that more-information is always better or at least can never hurt if more-information is free.
less-is-more-effects have been shown experimentally, analytically, and by computer-simulations.
biography == ===
academic-career ===
gigerenzer received  gigerenzer phd from the-university of munich in 1977 and became a-professor of psychology there the same year.
in 1984  gigerenzer moved to the-university of munich and in 1990 to the-university of salzburg.
from 1992 to 1995  gigerenzer was professor of psychology at the-university of salzburg and has been the-john-m.-olin
distinguished-visiting-professor, school of law at the-university of salzburg.
in 1995  gigerenzer became director of the-max-planck-institute for psychological-research in munich.
since 2009 he has been director of the-harding-center for risk-literacy in berlin.
gigerenzer was awarded honorary-doctorates from the-university of basel and the-open-university of the-netherlands.
he is also batten-fellow at the-darden-business-school, university of virginia, fellow of the-berlin-brandenburg-academy of sciences and the-german-academy of sciences leopoldina, and honorary-member of the-american-academy of arts and sciences and the-american-philosophical-society.
heuristics ===
with daniel-goldstein-daniel-goldstein first theorized the-recognition-heuristic and the-take-the-best-heuristic.
they proved analytically conditions under-which-semi-ignorance (lack of recognition) can lead to better-inferences than with more-knowledge.
these-results were experimentally confirmed in many-experiments, e.g., by showing that semi-ignorant-people who rely on recognition are as good as or better than the-association of tennis-professionals-(atp)-rankings and experts at predicting the-outcomes of the-wimbledon-tennis-tournaments.
similarly, decisions by experienced-experts (e.g., police, professional-burglars, airport-security) were found to follow the-take-the-best-heuristic rather than weight and add all-information, while inexperienced-students tend to do the latter.
a-third-class of heuristics, fast-and-frugal-trees, are designed for categorization and are used for instance in emergency-units to predict heart-attacks, and model-bail-decisions made by magistrates in london-courts.
in such-cases, the-risks are not knowable and professionals hence face uncertainty.
to better understand the-logic of fast-and-frugal-trees and other-heuristics, gigerenzer-and-gigerenzer-colleagues use the-strategy of mapping its-concepts onto those of well-understood-optimization-theories, such as signal-detection-theory.
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
gigerenzer-and-gigerenzer-collaborators have theoretically and experimentally shown that many-cognitive-fallacies are better understood as adaptive-responses to a-world of uncertainty—such as the-conjunction-fallacy, the-base-rate-fallacy, and overconfidence.
the-adaptive-toolbox ===
the-basic-idea of the-adaptive-toolbox is that different-domains of thought require different-specialized-cognitive-mechanisms instead of one-universal-strategy.
the-analysis of the-adaptive-toolbox and the-adaptive-toolbox evolution is descriptive-research with the-goal of specifying the-core-cognitive-capacities (such as recognition-memory) and the-heuristics that exploit these (such as the-recognition-heuristic).
risk-communication ===
alongside his-research on heuristics, gigerenzer investigates risk-communication in situations where risks can actually be calculated or precisely estimated.
he has developed an-ecological-approach to risk-communication where the-key is the-match between cognition and the-presentation of the-information in the-environment.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
later it was shown that with this-method, even-4th-graders were able to make correct-inferences.
once again, the-problem is not simply in the-human-mind, but in the-representation of the-information.
gigerenzer has taught risk-literacy to some-1,000-doctors in risk-literacy cmu and some-50-us-federal-judges, and natural-frequencies has now entered the-vocabulary of evidence-based-medicine.
in recent-years, medical-schools around the-world have begun to teach tools such as natural-frequencies to help young-doctors understand test-results.
intellectual-background ==
intellectually, gigerenzer's-work is rooted in herbert-simon's-work on satisficing (as opposed to maximizing) and on ecological-and-evolutionary-views of cognition, where adaptive-function and success is central, as opposed to logical-structure and consistency, although the latter can be means towards function.
gigerenzer and colleagues write of the-mid-17th-century-"probabilistic-revolution", "the demise of the-dream of certainty and the-rise of a-calculus of uncertainty – probability-theory".
gigerenzer calls for a-second-revolution, "replacing the-image of an-omniscient-mind computing intricate-probabilities and utilities with that of a-bounded-mind reaching into an-adaptive-toolbox filled with fast-and-frugal-heuristics".
these-heuristics would equip humans to deal more specifically with the-many-situations these-heuristics face in which not-all-alternatives and probabilities are known, and surprises can happen.
personal ==
gigerenzer is a-jazz-and-dixieland-musician.
gigerenzer was part of the-munich-beefeaters-dixieland-band which performed in a-tv-ad for the-vw-golf around the-time the-vw-golf came out in 1974.
a-tv-ad for the-vw-golf can be viewed on youtube, with  gigerenzer at the-steering-wheel and on the-banjo.
gigerenzer is married to lorraine-daston, director at the-max-planck-institute for the-history of science and has one-daughter, thalia gigerenzer.
gigerenzer is recipient of the-aaas-prize for behavioral-science-research for the-best-article in the-behavioral-sciences, the-association of american-publishers-prize for the-best-book in the-social-and-behavioral-sciences, the-german-psychology-prize, and the-communicator-award of the-german-research-association (dfg), among others.
see the-german-wikipedia-entry, gerd-gigerenzer, for an-extensive-list of honors and awards.)
publications == ===
cognition as intuitive-statistics (1987) with david-murray the-empire of chance:
how probability changed science and everyday-life (1989)
simple-heuristics that make us smart (1999)
bounded-rationality:
the-adaptive-toolbox (2001) with reinhard selten reckoning with risk:
learning to live with uncertainty (2002, published in the-u.s. as calculated-risks: how to know when numbers deceive you)
gut-feelings:
the-intelligence of the unconscious (2007)
rationality for mortals (2008) heuristics:
the-foundation of adaptive-behavior (2011) with ralph-hertwig & torsten-pachur
risk-savvy: how to make good-decisions (2014)
simply rational:
decision making in the-real-world (2015)
video-==-video on gerd-gigerenzer's-research (latest-thinking) ==
see also == ==
references == ==
external-links == resume books
edge.org-bio-article:
simple-tools for understanding risks: from innumeracy to insight harding-center for risk-literacy
