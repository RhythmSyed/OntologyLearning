postscript was the-first-truly-international-standard for computer-printing as postscript included algorithms describing the-letter-forms of many-languages.
the-joint-effort will speed time to showcase and enhance the-execution of new-sensei-powered-services for adobe-creative-cloud and experience-cloud-clients and engineers.
another-security-firm, sophos, showed that adobe used a-weak-encryption-method permitting the-recovery of a-lot of information with very-little-effort.
a-type of mental-process – something that individuals can do with individuals minds.
the-graphs of these-vectors can represent a-network of neurons whose-connections fire in different-ways over time as synapses fire.
diagram – symbolic-representation of information using visualization-techniques-argument-map concept-map – diagram showing relationships among concepts-mind-map – system or map
people notable for people notable for their-extraordinary-ability to think ===
learning and memory ===
a-computer-program or a-hardware-maintained-structure can follow in order to manage a-cache of information stored on the-computer.
when a-cache of information stored on the-computer is full, the-algorithm must choose which-items to discard to make room for the-new-ones.
unlike proxy-servers, in information-centric-networking the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose different-kind of requirements on the-content-eviction-policies.
time aware-least-recently-used-(tlru) ===
in lfru , the-cache is divided into two-partitions called privileged and unprivileged partitions.
for example matrices whose-entries are integers or the-real-numbers.
with this-construction we have not reduced the-number of multiplications.
the-number of additions and multiplications required in the-strassen-algorithm can be calculated as follows: let f(n)
be the-number of operations for a-2n-×-2n-matrix.
+ ℓ4n, for some constant ℓ that depends on the-number of additions performed at each-application of the-algorithm.
reduction in the-number of arithmetic-operations however comes at the-price of a-somewhat-reduced-numerical-stability, and the-algorithm also requires significantly-more-memory compared to the-algorithm.
however, the-asymptotic-statement does not imply that strassen's-algorithm is always faster even for small-matrices, and in practice this is in fact not the-case: for small-matrices, the-cost of the-additional-additions of matrix-blocks outweighs the-savings in the-number of multiplications.
there are also other-factors not captured by the-analysis above, such as the-difference in cost on today's-hardware between loading data from memory onto processors vs. the-cost of actually doing operations on this-data.
in computing, cache-algorithms (also frequently called cache-replacement-algorithms or cache-replacement-policies) are optimizing instructions, or algorithms, that a-computer-program or a-hardware-maintained-structure can utilize in order to manage a-cache of information stored on the-computer.
when the-cache is full, the-algorithm must choose which-items to discard to make room for the-new-ones.
the-latency: the-time to reference the-cache
more-efficient-replacement-policies keep track of more-usage-information in order to improve the-hit-rate (for a-given-cache-size).
the-"latency" of a-cache describes how long after requesting a-desired-item the-cache can return that-item (when there is a-hit).
faster-replacement-strategies typically keep track of less-usage-information—or, in the-case of direct-mapped-cache, no-information—to reduce the-amount of time required to update that-information.
in particular, video-and-audio-streaming-applications often have a-hit-ratio close to zero, because each-bit of data in the-stream is read once for the-first-time (a compulsory miss), used, and then never read or written again.
even worse, many-cache-algorithms (in-particular,-lru) allow this-streaming-data to fill the-cache, pushing out of the-cache information that will be used again soon
items taking up more-cache : if items have different-sizes, the-cache may want to discard a-large-item to store several-smaller-ones.
items that expire with time: some-caches keep information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache).
information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache) may discard items because information that expires (e.g.-a-news-cache, a-dns-cache, or a-web-browser-cache) are expired.
depending on the-size of the-cache
this applies only to situation where multiple-independent-caches are used for the-same-data (for example multiple database servers updating the-single-shared-data-file).
using this-algorithm the-cache behaves in the-same-way as a-fifo-queue.
the-cache evicts the-blocks in the-order the-cache were added, without any-regard to how often or how many times the-cache were accessed before.
using this-algorithm the-cache behaves in the-same-way as a-stack and exact opposite-way as a-fifo-queue.
the-cache evicts the-block added most recently first without any-regard to how often or how many times the-cache was accessed before.
in such-an-implementation, every time a-cache-line is used, the age of all-other-cache-lines-changes.
time aware-least-recently-used-(tlru) ===
here, a-b-c-d are placed in the-cache as there is still space available.
when there is an-access to a-value, say 'a', and we cannot find a' in the-cache, then we load a' from memory and place
then as the-cache became full 'e' replaced 'a' because that was where the-arrows were pointing at that-time, and the-arrows that led to 'a' were flipped to point in the-opposite-direction.
data from misses is added to slru-cache at the-most-recently-accessed-end of the-probationary-segment.
whenever data must be discarded from the-cache, lines are obtained from the-lru-end of the-probationary-segment.
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
a-variant called lfu with dynamic-aging (lfuda) that uses dynamic-aging to accommodate shifts in the-set of popular-objects adds a-cache-age-factor to the-reference-count when a-new-object is added to the-cache or when an-existing-object is re-referenced.
suppose when an-object was frequently accessed in the-past and now an-object becomes unpopular, an-object will remain in the-cache for a-long-time thereby preventing the-newly-or-less-popular-objects from replacing an-object.
in the-above-figure, "x" represents that a-block is accessed at time t.
suppose if block-a1 is accessed at time 1 then recency will become 0 since this is the-first-accessed-block and irr will be 1 since it predicts that block-a1 will be accessed again in time 3.
at time 10, the-lirs-algorithm will have two-sets lir set = {a1, a2} and hir set = {a3, a4, a5}.
now at time 10 if there is access to a4, miss occurs.
with the-three-hands, clock-pro is able to measure the-reuse-distance of data accesses in an-approximate-way.
second, all-the-merits of lirs are retained, such as quickly evicting one-time accessing and/or low locality data items.
arc improves on slru by using information about recently-evicted-cache-items to dynamically adjust the-size of the-protected-segment and the-probationary-segment to make the-best-use of the-available-cache-space.
blocks stay in the-lru-queues for a-given-lifetime, which is defined dynamically by the-mq-algorithm to be the-maximum-temporal-distance between two-accesses to the-same-file or the-number of cache-blocks, whichever is larger.
if a-block has not been referenced within a-block lifetime, a-block is demoted from qi to qi−1 or evicted from the-cache if a-block is in q0.
when the-cache is full, the-first-block to be evicted will be the-head of q0 in this-case
if a is accessed one more time a will move to q1 below b.
individuals create individuals own "subjective-reality" from individuals perception of the-input.
a-continually-evolving-list of cognitive-biases has been identified over the-last-six-decades of research on human-judgment and decision-making in cognitive-science, social-psychology, and behavioral-economics.
tversky and kahneman explained human-differences in judgment and decision-making in terms of heuristics.
heuristics involve mental-shortcuts which provide swift-estimates about the-possibility of uncertain-occurrences.
heuristics are simple for the-brain to compute but sometimes introduce "severe-and-systematic-errors. "
participants were given a-description of "linda" that suggests linda might well be a feminist (e.g., linda is said to be concerned about discrimination and social-justice-issues).
participants were then asked whether participants thought linda was more likely to be (a)-a-"bank-teller" or (b)-a-"bank-teller and active in the-feminist-movement."
critics of kahneman and tversky, such as gerd-gigerenzer, alternatively argued that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases.
nevertheless, experiments such as the-"linda-problem" grew into heuristics and biases research-programs, which spread beyond academic-psychology into other-disciplines including medicine and political-science.
among the-"cold"-biases, some are due to ignoring relevant-information (e.g., neglect of probability), some involve a-decision or judgment being affected by irrelevant-information (for example the framing effect where the-same-problem receives different-responses depending on how the-same-problem is described; or the-distinction-bias where choices presented together have different-outcomes than those presented separately), and
it has been shown, for example, that people addicted to alcohol and other-drugs pay more-attention to drug-related-stimuli.
many-social-institutions rely on individuals to make rational-judgments.
the-various-biases demonstrated in these-psychological-experiments suggest that people will frequently fail to do all-these-things.
however, people fail to do so in systematic,-directional-ways that are predictable.
others have also hypothesized that cognitive-biases could be linked to various-eating-disorders and how people view people-bodies and people body-image.
some believe that there are people in authority who use cognitive-biases and heuristics in order to manipulate others so that others can reach others end goals.
some-medications and other-health-care-treatments rely on cognitive-biases in order to persuade others who are susceptible to cognitive-biases to use some-medications and other-health-care-treatments products.
participants in the-experiment were shown a-residential-property.
afterwards, participants in the-experiment were shown another-property that was completely unrelated to the-first-property.
participants in the-experiment were asked to say what participants in the-experiment believed the-value and the-sale-price of the-second-property would be.
participants in the-experiment found that showing participants in the-experiment an-unrelated-property did have an-effect on how participants in the-experiment valued the-second-property.
debiasing is the-reduction of biases in judgment and decision-making through incentives, nudges, and training.
one-debiasing-technique aims to decrease biases by encouraging individuals to use controlled-processing compared to automatic-processing.
cognitive-bias-modification refers to the-process of modifying cognitive-biases in healthy-people and also refers to a-growing-area of psychological-(non-pharmaceutical)-therapies for anxiety, depression and addiction called cognitive bias modification therapy (cbmt).
people do appear to have stable-individual-differences in people susceptibility to decision biases such as overconfidence, temporal discounting, and bias-blind-spot.
that said, these-stable-levels of bias within individuals are possible to change.
participants in experiments who watched training-videos and played debiasing-games showed medium to large-reductions both immediately and up to three months later in the-extent to which they exhibited susceptibility to six-cognitive-biases: anchoring,-bias-blind-spot, confirmation-bias, fundamental-attribution-error, projection-bias, and representativeness.
many view cognitive-biases and heuristics as irrational-ways of making decisions and judgements.
gigerenzer argues that using heuristics and cognitive-biases are rational and helpful for making decisions in our-everyday-life.
primary-visibility-tests (such as back-face-culling) and secondary-visibility-tests (such as overlap-checks and screen-clipping) are usually performed on objects'-polygons in order to discard specific-polygons that are deemed to be unnecessary to render.
{\textit-{far}}{\left({\textit-{far}}-{\textit {near}}\right)}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit-{near}}}{{\textit-{far}}-{\textit-{near}}}}\right)\right)\right\rfloor-}-this-formula can be inverted and derived in order to calculate the-z-buffer-resolution (the-'granularity' mentioned earlier).
the-following-pseudocode demonstrates the-process of z-buffering:
as the-weight-increases and the-molecules become more complex, the-number of possible-compounds increases drastically.
thus, a-program that is able to reduce this-number of candidate-solutions through the-process of hypothesis-formation is essential.
however, this is feasible only when the-number of candidate-solutions is minimal.
dendral has specific-knowledge about the-mass-spectrometry-technique, a-large-amount of information that forms the-basis of chemistry and graph-theory, and information that might be helpful in finding the-solution of a-particular-chemical-structure-elucidation-problem.
a-heuristic is a-rule of thumb, an-algorithm that does not guarantee a-solution, but reduces the-number of possible-solutions by discarding unlikely-and-irrelevant-solutions.
the-use of heuristics to solve problems is called "heuristics programming", and was used in the-benefit-dendral to allow the-use of heuristics to solve problems to replicate in machines the-process through which human-experts induce the-solution to problems via rules of thumb-and-specific-information.
in the-early-1960s, joshua-lederberg started working with computers and quickly became tremendously interested in creating interactive-computers to help joshua-lederberg in joshua-lederberg exobiology research.
as he was not an-expert in either-chemistry or computer-programming, he collaborated with stanford-chemist-carl-djerassi to help he with chemistry, and edward-feigenbaum with programming, to automate the-process of determining chemical-structures from raw-mass-spectrometry-data.
the-introduction of computers to biology and medicine, "(bruce-buchanan) wanted the-system (dendral) to make discoveries on the-system (dendral) own, not just help humans make humans".
the-introduction of computers to biology and medicine.”
in one of their-first-studies, participants were asked to compute, within 5-seconds, the-product of the-numbers one through to eight, either as 1 × 2 × 3 × 4 × 5 × 6 × 7 × 8 or reversed as 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1.
because participants did not have enough-time to calculate the-full-answer, participants had to make an-estimate after participants first-few-multiplications.
in another-study by tversky and kahneman, participants observed a-roulette-wheel that was predetermined to stop on either 10 or 65.
participants were then asked to guess the-percentage of the-united-nations that were african-nations.
participants whose-wheel stopped on 10-guessed-lower-values (25% on average) than participants whose-wheel stopped at 65-(45% on average).
when asked if an-audience believed the-number was informative of the-value of the-item, quite a few said yes.
in a-study exploring the-causes and properties of anchoring, participants were exposed to an-anchor and asked to guess how-many-physicians were listed in the-local-phone-book.
thus, despite being expressly aware of the-anchoring-effect, participants were still unable to avoid the-anchoring-effect.
a-later-study found that even when offered monetary-incentives, people are unable to effectively adjust from an-anchor.
further-research to conclude an-effect that is effectively retained over a-substantial-period of time has proven inconsistent.
a-possible-cause would be the-discriminatory-fashion in which information is communicated, processed and aggregated based on each-individual's-anchored-knowledge and belief.
according to this-theory, once an-anchor is set, people adjust away from an-anchor to get to people final-answer; however, people adjust insufficiently, resulting in people
if people know the-direction in which people should adjust, incentivizing accuracy also appears to reduce anchoring-effects.
to use an-earlier-example, since mahatma-gandhi obviously did not die at age 9, then people will adjust from there.
when displaying the-results of previous-ratings in the-context of business-model-idea-evaluation, people incorporate an-anchor into people own decision-making-process, leading to a-decreasing-variance of ratings.
as a-result of this, earlier-studies hypothesized that people with more-depressed-moods would tend to use anchoring less than those with happier-moods.
however, more-recent-studies have shown the-opposite-effect: sad-people are more likely to use anchoring than people with happy-or-neutral-mood.
people high in agreeableness and conscientiousness are more likely to be affected by anchoring, while those high in extraversion are less likely to be affected.
another-study, however, found that cognitive-ability had no-significant-effect on how likely people were to use anchoring.
cognitive-conceit or overconfidence arises from other-factors like personal-cognitive-attributes such as knowledge and decision-making ability, decreasing the-probability to pursue external-sources of confirmation.
the-process of offer and counteroffer results in a-mutually-beneficial-arrangement.
during the-workshop, a-group of participants is divided into two-sections: buyers and sellers.
participants read an-initial-price for a-beach-house, then gave the-price participants thought it was worth.
participants received either-a-general,-seemingly-nonspecific-anchor (e.g., $800,000) or a-more-precise-and-specific-anchor (e.g., $799,800).
participants with a-general-anchor adjusted participants with a-general-anchor estimate more than those given a-precise-anchor ($751,867 vs $784,671).
when given a-general-anchor of $20, people will adjust in large-increments ($19, $21, etc.),
but when given a-more-specific-anchor like $19.85, people will adjust on a-lower-scale ($19.75, $19.95, etc.).
computer-graphics is a-sub-field of computer-science which studies methods for digitally synthesizing and manipulating visual-content.
algorithms to reproduce light-transport-imaging: image-acquisition or image-editing ===
historically, most work in this-field has focused on parametric-and-data-driven-models, but recently physical-simulation has become more popular as computers have become more powerful computationally.
let n be the-number of points and d
the-number of dimensions.
algorithms ==
done naïvely, this will take o(n) time
rendering or image synthesis is the-process of generating a-photorealistic-or-non-photorealistic-image from a-2d-or-3d-model by means of a-computer-program.
the-scene-file contains geometry, viewpoint, texture, lighting, and shading information describing the-virtual-scene.
the-term "rendering" is also used to describe the-process of calculating effects in a-video-editing-program to produce the-final-video-output.
even tracing a-portion large enough to produce an-image takes an-inordinate-amount of time if the-sampling is not intelligently restricted.
another-distinction is between image-order-algorithms, which iterate over pixels of the-image-plane, and object order algorithms, which iterate over objects in the-scene.
this-newer-method of rasterization utilizes the-graphics-card's-more-taxing-shading-functions and still achieves better-performance because the-simpler-textures stored in memory use less-space.
however, efforts at optimizing to reduce the-number of calculations needed in portions of a-work where detail is not high or does not depend on ray-tracing-features have led to a-realistic-possibility of wider-use of ray-tracing.
in advanced-radiosity-simulation, recursive, finite-element algorithms 'bounce' light back and forth between surfaces in the-model, until some-recursion-limit is reached.
in order to remove aliasing,-all-rendering-algorithms (if they are to produce good-looking-images) must use some-kind of low-pass-filter on the-image-function to remove high-frequencies, a-process called antialiasing.
in order to meet demands of robustness, accuracy and practicality, an-implementation will be a-complex-combination of different-techniques.
in 3d-computer-graphics,-hidden-surface-determination (also known as shown-surface-determination, hidden-surface-removal (hsr), occlusion-culling (oc) or visible-surface-determination (vsd)) is the-process of identifying what-surfaces and parts of surfaces can be seen from a-particular-viewing-angle.
the-process of hidden-surface-determination is sometimes called hiding, and such-an-algorithm is sometimes called a hider.
background == hidden-surface-determination is a-process by which-surfaces that should not be visible to the-user (for example, because example lie behind opaque-objects such as walls) are prevented from being rendered.
algorithms ==
because the-c-buffer-technique does not require a-pixel to be drawn more than once, the-process is slightly faster.
various-screen-space-subdivision approaches reducing the-number of primitives considered per region, e.g.-tiling, or screen-space-bsp-clipping.
a-characterization of ten-hidden-surface-algorithms (wayback-machine copy intuition in the-context of decision-making is defined as a-“non-sequential-information-processing-mode.”
individuals use intuition and more-deliberative-decision-making-styles interchangeably, but there has been some-evidence that people tend to gravitate to one or the-other-style more naturally.
people in a-good-mood gravitate toward intuitive-styles, while people in a-bad-mood tend to become more deliberative.
snap-judgments made possible by heuristics are sometimes identified as intuition.
intuitive-decision-making can be described as the-process by which information acquired through associated-learning and stored in long-term-memory is accessed unconsciously to form the-basis of a-judgment or decision.
information acquired through associated-learning and stored in long-term-memory can be transferred through affect induced by exposure to available-options, or through unconscious-cognition.
intuition in decision-making has been connected two-assumptions: 1)
intuition's-effect on decision-making is distinct from insight, which requires time to mature.
heuristics ===
traditional-research often points to the-role of heuristics in helping people make “intuitive”-decisions.
the-heuristics-and-biases-approach looks at patterns of biased-judgments to distinguish heuristics from normative-reasoning-processes.
some-researchers point to intuition as a-purely-affective-phenomenon that demonstrates the-ability of emotions to influence decision-making without cognitive-mediation.
mood is thus considered a moderator in the-strategic-decisions people carry out.
in a-series of three-studies, the-authors confirmed that people in a-positive-mood faced with a-card-based-gambling-task-utilized-intuition to perform better at higher-risk-stages than people who were in a-negative-mood.
although people use intuitive-and-deliberative-decision-making-modes interchangeably, individuals value the-decisions individuals make more when individuals are allowed to make individuals using individuals preferred-style.
the-emotions people experience after a-decision is made tend to be more pleasant when the-preferred-style is used, regardless of the-decision-outcome.
some-studies suggest that the-mood with which the-subject enters the-decision-making-process can also affect the-style they choose to employ: sad-people tend to be more deliberative, while people in a-happy-mood rely more on intuition.
the-scale defines preference for intuition as tendency to use affect (“gut-feel”) as a-basis for decision-making instead of cognition.
management and decision-making ===
the-expertise-based-intuition increases over time when the-employee gets more-experience regarding the-organization worked for and by gathering domain-specific-knowledge.
it has been noted in a-research, that intuition is used as a-method of decision-making in the-banking-industry.
participants of a-research also reported to analyse participants of the-research-intuitive-decisions afterwards and possibly altering participants of the-research.
traditional-literature attributes the-role of judgment-processes in risk-perception and decision-making to cognition rather than emotion.
however, more-recent-studies suggest a-link between emotion and cognition as it relates to decision-making in high-risk-environments.
studies of decision-making in high-risk-environments suggest that individuals who self-identify as intuitive-decision-makers tend to make faster-decisions that imply greater-deviation from risk-neutrality than those who prefer the-deliberative-style.
intuition in strategic-decision-making is less examined and for example can be depending on a-case be described as managers-know-how, expertise or just-a-gut-feeling, hunch.
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
in order to do so, they applied the-scientific-method to human-behavior.
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
abraham-tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
the-process of persuasion has been found to be influenced by numerous-variables that generally fall into one of five-major-categories: communicator: includes credibility, expertise, trustworthiness, and attractiveness.
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
the-confirmation-bias is a-type of bias leading to the-tendency to search for or interpret information in a-way that confirms one's-preconceptions.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
self-concept is the-whole-sum of beliefs that people have about people.
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, an-athlete at a-university would have multiple-selves that would process different-information pertinent to each-self: the-student would be oneself, who would process information pertinent to a-student (taking notes in class, completing a-homework-assignment, etc.);
an-athlete at a-university would be the-self who processes information about things related to being an-athlete (recognizing an-incoming-pass, aiming a-shot, etc.).
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
: how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting
is the-process of predicting how one would feel in response to future-emotional-events.
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
people develop people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
people waiting in line to get on a-bus, for example, do not constitute a-group.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
for example, one could correlate the-number of violent-television shows children watch at home with the-number of violent-incidents the-children participate in at school.
an-example would be to unobtrusively observe children on a-playground (with a-videocamera, perhaps) and record the-number and types of aggressive-actions displayed.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
also, participant-conformity increased substantially as the-number of "incorrect"-individuals increased from one to three, and remained high as the-incorrect-majority grew.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
festinger's-explanation was that for people in the-first-group ($1) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
subject-area-page-social-psychology on all-about psychology — information and resources
a-subsystem that transfers data between computer-components inside a-computer or between computers.
the-process of keeping data in multiple-caches synchronised in a-multiprocessor-shared-memory-system, also required when dma modifies the-underlying-memory.
freeing up data from within a-cache to make room for new-cache-entries to be allocated; controlled by a-cache replacement policy.
a-small-block of memory within a-cache; the-granularity of allocation, refills, eviction;-typically-32–128-bytes in size.
cache miss not finding data in a-local-cache, requiring use of the-cache-policy to allocate and fill this-data, and possibly performing evicting other-data to make room.
cache thrashing a-pathological-situation where access in a-cache-cause cyclical-cache misses by evicting data that is needed in the-near-future.
the-number of potential-cache-lines in an-associative-cache that specific-physical-addresses can be mapped to; higher-values reduce potential-collisions in allocation.
any-data-input-device that reads data from a-card-shaped-storage-medium.
dual in-line memory module (dimm)
a-type of random-access-memory that stores each bit of data in a-separate-capacitor within an-integrated-circuit and which must be periodically refreshed to retain the-stored-data.
e-==-expansion-bus-a-computer-bus which moves information between the-internal-hardware of a-computer-system (including the-cpu and ram) and peripheral-devices.
firmware-fixed-programs and data that internally control various-electronic-devices.
any-non-volatile-storage-device that stores data on rapidly-rotating-rigid-(i.e.-hard)-platters with magnetic-surfaces.
harvard-architecture a-memory-architecture where program-machine-code and data are held in separate-memories, more commonly seen in microcontrollers and digital-signal-processors.
l-==-load/store-instructions-instructions used to transfer data between memory-and-processor-registers.
memory-devices that are used to store data or programs on a-temporary-or-permanent-basis for use in an-electronic-digital-computer.
memory address the-address of a-location in a-memory or other-address-space.
a-collection of computers and other-devices connected by communications-channels, e.g. by ethernet-or-wireless-networking.
memory that can retain the-stored-data even when not powered, as opposed to volatile-memory.
a-type of disk-drive that uses laser-light-or-electromagnetic-waves near the-light-spectrum as part of the-process of reading or writing data to or from optical-discs.
the-process of pre-loading-instructions or data into a-cache ahead of time, either under manual-control via prefetch-instructions or automatically by a-prefetch-unit which may use runtime-heuristics to predict the-future-memory-access-pattern.
the-pre-loading of instructions or data before either is needed by dedicated-cache-control-instructions or predictive-hardware, to mitigate latency.
any of various-data-storage-schemes that can divide and replicate data across multiple-hard-disk-drives in order to increase reliability, allow faster-access, or both.
a-type of computer-data-storage that allows data-items to be accessed (read or written) in almost-the-same-amount of time irrespective of the-physical-location of data inside the-memory.
software any-computer-program or other-kind of information that can be read and/or written by a-computer.
single in-line memory module (simm)
a-type of memory-module containing random-access-memory used in computers from the-early-1980s to the-late-1990s.
any-data-storage-device that uses integrated-circuit-assemblies as memory to store data persistently.
terminal-an-electronic-or-electromechanical-hardware-device that is used for entering data into, and displaying data from, a-computer or a-computing-system.
working set the-set of data used by a-processor during a-certain-time-interval, which should ideally fit into a-cpu-cache for optimum-performance.
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
the-origin for casa states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
social-attributes that computers have which are similar to humans include: words for output-interactivity (the-computer 'responds' when a-button is touched) ability to perform traditional-human-tasksaccording to casa, the-above-attributes trigger scripts for human-human-interaction, which leads an-individual to ignore cues revealing the-asocial-nature of a-computer.
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
the-research revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
participants interacted with a-computer which questioned participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to the-same-computer and the-other-half went to a-different-computer.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
in computing, a-cache-oblivious-algorithm (or cache-transcendent-algorithm) is an-algorithm designed to take advantage of a-cpu-cache without having the-size of the-cache (or the-length of the-cache lines, etc.)
an-optimal-cache-oblivious-algorithm is a-cache-oblivious-algorithm that uses the-cache optimally (in an-asymptotic-sense, ignoring constant-factors).
in the-cache-oblivious-model: memory is broken into blocks of b {\displaystyle b}
if the-cache was previously full, then a-line will be evicted as well (see replacement-policy below).
the-cache holds m
the-cache is fully associative: each-line can be loaded into any-location in the-cache.
in other-words, the-cache is assumed to be given the-entire-sequence of memory-accesses during algorithm-execution.
, we measure the-number of cache misses that the algorithm experiences.
because the-model captures the-fact that accessing elements in the-cache is much faster than accessing things in main-memory, the-running-time of the-algorithm is defined only by the-number of memory-transfers between the-cache and main-memory.
is an out-of-place matrix transpose operation (in-place algorithms have also been devised for transposition, but are much more complicated for non-square-matrices).
in principle, one could continue dividing the-matrices until a-base-case of size 1×1 is reached, but in practice one uses a-larger-base-case (e.g. 16×16) in order to amortize the-overhead of the recursive subroutine calls.)
they reduce the-problem, so that it eventually fits in cache no matter how small the-cache is, and end the-recursion at some-small-size determined by the-function-call-overhead and similar-cache-unrelated-optimizations, and then use some-cache-efficient-access-pattern to merge the-results of these small, solved problems.
the-painter’s-algorithm (also-depth-sort-algorithm and priority-fill) creates images by sorting the-polygons within the-image by the-polygons depth and placing each-polygon in order from the farthest to the-closest-object.
the-painter's-algorithm's-time-complexity is heavily dependent on the-sorting-algorithm used to order the-polygons.
assuming the-use of the-most-optimal-sorting-algorithm, painter's-algorithm has a-worst-case-complexity of o(n-log-n + m*n), where n is the-number of polygons and m is the-number of pixels to be filled.
where n is the-number of polygons and m is the-number of pixels to be filled.
this required programs to manage memory as efficiently as possible to conduct large-tasks without crashing.
the-painter’s-algorithm prioritizes the-efficient-use of memory but at the-expense of higher-processing-power since all-parts of all-images must be rendered.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
examples of social-information include information about the-behavior of a-social-entity or the-properties of a-social-system, while nonsocial-information is information about something physical.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
an-agent using social-circle-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
the-process that involves using common-mental-shortcuts that help a-decision to be made based on known-probabilities.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
see also-==-heuristics in judgment and decision-making
the-microsoft-sponsored-program-technology-education and literacy in schools (teals) aims to increase the-number of students taking ap-computer-science-classes.
the-number of bits per pixel, sample, or texel in a-bitmap-image (holding one-or-mode-image-channels, typical-values being 4, 8, 16, 24, 32)
color-resolution-command buffer a-region of memory holding a-set of instructions for a-graphics-processing-unit for rendering a-scene or portion of a-scene.
cone-tracing-modification of ray-tracing which instead of lines uses cones as rays in order to achieve e.g. antialiasing or soft-shadows.
distributed ray-tracing-modification of ray tracing that casts multiple-rays through each-pixel in order to model soft-phenomena such as soft-shadows, depth of field etc.
image-order rendering rendering-methods that iterate over pixels of the-screen in order to draw the-image (e.g.-raytracing).
light-probe-object used to capture light-parameters at a-specific-point in space in order to help compute scene-lighting.
megatexturing-texturing-technique that works with extremely-large-textures which are not loaded into memory all at once, but rather streamed from the-hard-disk depending on the-camera-view.
rendering algorithms based on physics-simulation of light, including conservation of energy, empirical-models of surfaces.
effects applied to a-bitmap-image in screen-space after 3d-rendering-pipeline, for example tone-mapping, some-approximations to-motion-blur, and blooms.
procedural-generation-generating-data, such as textures, 3d-geometry or whole-scenes by algorithms (as opposed to manually).
the-process of subdividing an-object (either-geometric-object, or a-data-structure)
information controlling a-graphics-pipeline, composed of modes and parameters, including resource-identifiers, and shader-bindings.
the-process of rasterizing into a-texture-map (or texture-buffer) for further-use as a-resource in subsequent-render-passes.
rendering primitive-geometry that can be drawn by a-rasterizer or graphics-processing-unit, connecting vertices, e.g.-points, lines, triangles, quadrilaterals rendering resources data managed by a-graphics-api, typically held in device-memory, including vertex-buffers, index-buffers, texture-maps and framebuffers repeating-texture
stereo rendering rendering the-view twice separately for each-eye in order to present depth.
texture sampling the-process of texture-lookup with texture-filtering.
texture buffer a-region of memory (or resource) used as both-a-render-target and a-texture-map.
triangulation the-process of turning arbitrary-geometric-models into triangle-primitives, suitable for algorithms requiring triangle-meshes-triangle primitive
the-most-common-rendering-primitive-defining-triangle-meshes, rendered by graphics-processing-units triangle setup the-process of ordering triangle-primitive-vertices, calculating signed-triangle-area and parameter-gradients between vertex-attributes as a-prerequisite for rasterization.
trivial accept the-process of accepting an-entire-rendering-primitive,-3d-model, or bounding-volume-contents without further-tests for clipping or occlusion-culling.
the-process of flattening a-3d-model's-surface into a-flat-2d-plane in a-contiguous,-spatially-coherent-manner for texture-mapping.
vsync-vertical-synchronization, synchronizes the-rendering-rate with the-monitor-refresh-rate in order to prevent displaying only-partially-updated-frame-buffer, which is disturbing especially with horizontal-camera-movement.
the-process usually starts with the-desktop, and proceeds by drawing each-window and any-child-windows from back to front, until finally the-foreground-window is drawn.
a-personal-computer is one intended for interactive-individual-use, as opposed to a-mainframe-computer where the-end-user's-requests are filtered through operating-staff, or a time sharing system in which one-large-processor is shared by many-individuals.
computer-terminals were used for time sharing access to central-computers.
before the-introduction of the-microprocessor in the-early-1970s, computers were generally large,-costly-systems owned by large-corporations, universities, government-agencies, and similar-sized-institutions.
the-single-chip-microprocessor was made possible by an-improvement in mos-technology, the-silicon-gate-mos-chip, developed in 1968 by federico-faggin, who later used silicon-gate mos-technology to develop the-first-single-chip-microprocessor, the-intel 4004, in 1971.a-few-researchers at places such as sri and xerox-parc were working on computers that a-single-person could use and that could be connected by fast,-versatile-networks: not home-computers, but personal-ones.
scamp emulated an-ibm-1130-minicomputer in order to run apl\1130.
it was only-a-matter of time before one-such-design was able to hit a-sweet-spot in terms of pricing and performance, and that-machine is generally considered to be the altair 8800, from mits, a-small-company that produced electronics-kits for hobbyists.
out of this-desire came the-sol-20-computer, which placed an-entire-s-100-system – qwerty-keyboard, cpu, display-card, memory and ports – into an-attractive-single-box.
internally a-newer-and-simpler-motherboard was used, along with an-upgrade in memory to 8, 16, or 32-kb, known as the 2001-n-8, 2001-n-16 or 2001-n-32, respectively.
it was incorporated in 1973 as ablesdeal-ltd. and renamed "westminster mail order ltd" and then
at the-height of the-company-success, and largely inspired by the-japanese-fifth-generation-computer-programme, the-company established the-"metalab"-research-centre at milton-hall (near cambridge), in order to pursue artificial-intelligence, wafer-scale-integration, formal-verification and other-advanced-projects.
such-low-prices probably hurt home-computers'-reputation; one-retail-executive said of the-99/4a, '"when one-retail-executive went to $99, people started asking 'what's wrong with it?'"
in order to accommodate japanese-text.
the-pc-xt of 1983 added a-10mb-hard-drive in place of one of the-two-floppy-disks and increased the-number of expansion-slots from 5 to 8.
the-impact of the-apple-ii and the-ibm-pc was fully demonstrated when time named the-home-computer the "machine of the-year", or person of the-year for 1982 (3-january 1983, "the computer moves in").
because pc-dos was available as a-separate-product, some-companies attempted to make computers available which could run ms-dos and programs.
the-entire-macintosh-line of computers was ibm's-major-competition up until the-early-1990s.
compared to samsung-predecessor in pc-clones, single data rate (sdr) sdram, the ddr sdram interface makes higher-transfer-rates possible by more-strict-control of the-timing of the-electrical-data and clock-signals.
as of june 2008, the-number of personal-computers worldwide in use hit one billion.
scelbi, another 1974 microcomputer simon (computer), a 1949 demonstration of computing-principles-list of pioneers in computer-science ==
chronology of personal-computers – a-chronology of computers from 1947 on "total-share:
the-enclave is decrypted on the-fly only within the-cpu itself, and even then, only for code and data running from within the-enclave.
the-code and data in the-enclave utilize a-threat-model in which the-enclave is trusted but no-process outside the-enclave can be trusted (including the-operating-system itself and any-hypervisor), and therefore all of these are treated as potentially hostile.
the-exploit involves scanning through process-memory, in order to reconstruct a-payload, which can then run code on the-system.
load-value-injection injects data into a-program aiming to replace the-value loaded from memory which is then used for a-short-time before the-mistake is spotted and rolled back, during which lvi controls data-and-control-flow.
where finding an-optimal-solution is impossible or impractical, heuristic-methods can be used to speed up the-process of finding a-satisfactory-solution.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
when an-individual applies heuristics in practice, generally performs as expected however
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
daniel-kahneman coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
gerd-gigerenzer and his-research-group study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
informal-models of heuristics ===
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
anchoring and adjustment — describes the-common-human-tendency to rely more heavily on the-first-piece of information offered (the "anchor") when making decisions.
for example, in a-study done with children, children were told to estimate the-number of jellybeans in a-jar.
children estimated the-number of jellybeans to be closer to the-anchor-number that children were given.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
when using base-rate-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
an-individual-work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
people commonly made distortions to images.
people commonly made distortions to images took shape in the-regularization of images (i.e.,-images are represented as more like pure-abstract-geometric-images, though (i.e.,-images are irregular in shape).
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
a-20-year-term is used because it is difficult to tell what the-number should be for any-individual-patent.
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that-is,-a-descriptive-program and a-normative-program-adaptive-toolbox.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
the-domain of pro-environmental-behavior provides numerous-illustrations for this-strategy, such as littering behavior in public-places, the-reuse of towels in hotel-rooms, and changes in private-energy-consumption in response to information about the-consumption of the-majority of neighbors.
people divide and invest people-resources equally in a-number of n-different-options.
in his-speech, spolsky talks about how software is eating the-world, how it is becoming more evident in everyday-life as people interact with more-software on a day-to-day basis, and how developers are helping to shape how the-world will work as technology keeps evolving.
research indicates that the-process of making and remembering choices yields memories that tend to be distorted in predictable-ways.
in this-respect, people tend to over attribute positive-features to options
people chose and negative-features to options not chosen.
essentially, after a-choice is made people tend to adjust people-attitudes to be consistent with, the-decision people have already made.
it is also possible that choice-supportive-memories arise because an-individual is only paying attention to certain-pieces of information when making a-decision or to post-choice-cognitive-dissonance.
the-process of making a-decision mostly relies upon previous-experiences.
people often end up with options that were not chosen but, instead were assigned by others, such as job-assignments made by bosses,
random selection: people do not show choice-supportive-biases when choices are made randomly for people.
misattribution results in a-type of choice-supportive-bias when information is attributed to the-wrong-source.
selective-forgetting results in a-form of choice-supportive-bias when information is selectively forgotten.
in this-respect, the-positive-attributes of the-chosen-option and the-negative-attributes of the-forgone-option are retained in memory at a-higher-rate and the-alternatives are displaced from memory at a-faster-rate.
research to support this can be displayed by the-following-example: when given a-choice between two-brands of popcorn, participants were more likely to choose the-one with the-superior-alignable-differences, such as “pops in its-own-bag” compared with “requires a-microwaveable-bowl” than the-one with superior-non-alignable-differences, such as “not likely to burn” compared with those containing “some-citric-acid" ===
whereas other-researchers have shown that a-2-day-delay between making choices and assessment of memory resulted in reasonably-high-(86%)-recognition-accuracy.
research illustrates that people favour the-options people think people have chosen and remember the-attributes of people "chosen-choice" more vividly and favourably.
for example, it has been observed by correlations that people with better-performance in tests of frontal-or-executive-functioning were less prone to choice-supportive-memory.
people's-conception of who people are, can be shaped by the-memories of the-choices people make;
memories change over time ==
after some-period of time and if the-memory is not used often, the-memory may become forgotten.
these-distortions in memory do not displace an-individual's-specific-memories, but these-distortions in memory-supplement and fill in the-gaps when generic-memories are lost.
it has been shown that a-wide-variety of strategic-and-systematic-processes are used to activate different-areas of the-brain in order to retrieve information.
credibility of a-memory: people have a-way to self-check-memories, in which a-person may consider the-plausibility of the-retrieved-memory by asking people is this-event even possible.
for example, if a-person remembers seeing a pig fly, people must conclude that it was from a-dream because pigs cannot fly in the-real-world.
memory does not provide people with perfect-reproductions of what happened, memory only consists of constructions and reconstructions of what happened.
stress-hormones affect memory ===
studies now show that as people age, people process of memory-retrieval-changes.
frontal-regions help people encode or use specific-memorial-attributes to make source-judgments, controls-personality and the-ability to plan for events.
henkel and mather tested the-role of beliefs at the-time of retrieval about which-option was chosen by giving participants several-hypothetical-choices like deciding between two-used-cars.
after making several-choices, participants left and were asked to return a week later.
next, participants were asked to indicate whether each-option was new, had been associated with the-option-participants chose, or had been associated with the-option-participants rejected.
participants favored whichever-option henkel and mather-mather had told participants henkel and mather had chosen in participants memories.
one-study looked at the-accuracy and distortion in memory for high-school-grades.
in addition, most-errors inflated the-actual-high-school-grade, meaning that these-distortions are attributed to memory-reconstructions in a-positive-and-emotionally-gratifying-direction-findings indicate that the-process of distortion does not cause the-actual-unpleasant-memory-loss of getting the-bad-grade.
the-number of omission-errors increased with the-retention-interval and better-students made fewer-errors.
therefore, sometime in between when the-memory is stored and when the-memory is retrieved some time later, the-distortion may arise.
written scenario memory tests ===
researchers have used written-scenarios in which participants are asked to make a-choice between two-options.
later, on a-memory-test, participants are given a-list of positive-and-negative-features, some of which were in the-scenario and some of which are new.
deception: henkel and mather (2007) found that giving people false-reminders about which-option people chose in a-previous-experiment-session led people to remember the-option people were told people had chosen as being better than the-other-option.
the-deese–roediger– mcdermott-paradigm (drm) consists of a-participant listening to an-experimenter read lists of thematically-related-words (e.g.-table,-couch,-lamp,-desk); then after some-period of time an-experimenter will ask if a-word was presented in the-list.
participants often report that related-but-non-presented-words (e.g.-chair) were included in the-encoding-series, essentially suggesting that participants 'heard' an-experimenter say these-non-presented-words (or critical-lures).
the-theory of cognitive-dissonance proposes that people have a-motivational-drive to reduce dissonance.
macbeth-effect showed reduced-choice-supportive-bias by having participants engage in washing.
list of memory biases wishful-thinking ==
in a-computer-operating-system that uses paging for virtual-memory-management, page-replacement-algorithms decide which-memory-pages to page out, sometimes called swap out, or write to disk, when a-page of memory needs to be allocated.
page-replacement happens when a-requested-page is not in memory (page fault) and a-free-page cannot be used to satisfy the-allocation, either because there are none, or because the-number of free-pages is lower than some-threshold.
with several-gigabytes of primary-memory, algorithms that require a-periodic-check of each and every-memory-frame are becoming less and less practical.
for example, example can be locked, or can have write ordering-requirements imposed by journaling.
the-cpu sets the-access-bit when the-process reads or writes memory in that-page.
the-cpu sets the-dirty-bit when the-process writes memory in that-page.
the-operating-system can detect accesses to memory and files through the-following-means:
this have information about the-order in which the-process accessed these-pages.
this is slow because a-page-fault involves a-context-switch to the-os, software-lookup for the-corresponding-physical-address, modification of the-page-table and a-context-switch back to the-process and accurate because the-access is detected immediately after the-access occurs.
directly when the-process makes system calls that potentially access the-page-cache like read and write in posix.
this means that if target-page is dirty (that is, contains data that have to be written to the-stable-storage before page can be reclaimed), i/o has to be initiated to send the target-page to the-stable-storage (to clean the target-page).
in the-early-days of virtual-memory, time spent on cleaning was not of much-concern, because virtual-memory was first implemented on systems with full-duplex-channels to the-stable-storage, and cleaning was customarily overlapped with paging.
marking algorithms ==
marking algorithms is a-general-class of paging-algorithms.
this-algorithm cannot be implemented in a-general-purpose-operating-system because it is impossible to compute reliably how long it will be before a-page is going to be used, except when all-software that will run on a-system is either known beforehand and is amenable to static-analysis of all-software that will run on a-system memory reference patterns, or only-a-class of applications allowing run-time-analysis.
despite this-limitation, algorithms exist that can offer near-optimal-performance — the-operating-system keeps track of all-pages referenced by the-program, and the-operating-system uses those-data to decide which-pages to swap in and out on subsequent-runs.
this-algorithm cannot be implemented in a-general-purpose-operating-system because it is impossible to compute reliably how long it will be before a-page is going to be used, except when all-software that will run on a-system is either known beforehand and is amenable to static-analysis of its-memory-reference-patterns, or only-a-class of applications allowing run-time-analysis can offer near-optimal-performance, but not on the-first-run of a-program, and only if the-program's-memory-reference-pattern is relatively consistent each time it runs.
the-not-recently-used-(nru)-page-replacement-algorithm is an-algorithm that favours keeping pages in memory that have been recently used.
the-idea is obvious from the-name – the-operating-system keeps track of all-the-pages in memory in a-queue, with the-most-recent-arrival at the-back, and the-oldest-arrival in front.
in simple-words, on a-page-fault, the-frame that has been in memory the longest is replaced.
otherwise, the-r-bit is cleared, then the-clock-hand is incremented and the-process is repeated until a-page is replaced.
clock-pro keeps a-circular-list of information about recently-referenced-pages, including all-m-pages in memory as well as the-most-recent-m-pages that have been paged out.
because of implementation-costs, one may consider algorithms (like those that follow) that are similar to lru, but which offer cheaper-implementations.
it has been proven, for example, that lru can never result in more-than-n-times-more-page-faults than opt algorithm, where n is proportional to the-number of pages in the-managed-pool.
in order to get the-page-faults, clearing emulated-bits in the-second-table revokes some of the-access-rights to the-corresponding-page, which is implemented by altering the-native-table.
if present in memory and not privately modified the-physical-page is shared with file-cache or buffer.
the-cache of block-devices, called the "buffer" by linux (not to be confused with other-structures
the-rasterisation of p and q do not overlapthe-tests are given in order of increasing-computational-difficulty.
if the-scene is simple enough to compute then the-scene is rendered; otherwise the-scene is divided into smaller-parts and the-process is repeated.
in 2008, warnock and geschke received the-computer-entrepreneur-award from the-ieee-computer-society "for inventing postscript and pdf and helping to launch the-desktop-publishing-revolution and change the-way people engage with information and entertainment".
list of people with the-surname ==
in computer-science, specifically in algorithms related to pathfinding, a-heuristic-function is said to be admissible if a-heuristic-function never overestimates the-cost of reaching the-goal, i.e. the-cost it never overestimates the-cost of reaching the-goal estimates to reach the-goal is not higher than the-lowest-possible-cost from the-current-point in the-path.
in order for a-heuristic to be admissible to the-search-problem, the-estimated-cost must always be lower than or equal to the-actual-cost of reaching the-goal-state.
an-admissible-heuristic can be derived from a-relaxed-version of the-problem, or by information from pattern-databases that store exact-solutions to subproblems of the-problem, or by using inductive-learning-methods.
it is clear that this-heuristic is admissible since the-total-number of moves to order the-tiles correctly is at-least-the-number of misplaced-tiles (each-tile not in place must be moved at least once).
if an-admissible-heuristic is used in an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm), then an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) will terminate on the-shortest-path.
to see why, simply consider that any-path that an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) terminates on was only progressed because an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm)
consistent heuristic heuristic function search algorithm gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
gigerenzer conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast, in contrast and associated researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
=== heuristics ===
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
the-history of computer-science began long before our-modern-discipline of computer-science, usually appearing in forms like mathematics or physics.
the-analytical-engine would have had a-memory-capacity of less-than-1-kilobyte of memory and a-clock-speed of less-than-10-hertz.
before the-1920s, computers (sometimes-computors) were human-clerks that performed computations.
computers (sometimes-computors) were usually under the-lead of a-physicist.
many-thousands of computers were employed in commerce, government, and research-establishments.
the-theoretical-turing-machine, created by alan-turing, is a-hypothetical-device theorized in order to study the-properties of such-hardware.
the-von-neumann-architecture was considered innovative as the-von-neumann-architecture introduced an-idea of allowing machine-instructions and data to share memory-space.
in von-neumann-machine-design, the-ipu passes addresses to memory, and memory, in turn, is routed either back to the-ipu if an-instruction is being fetched or to the-alu if data is being fetched.
the-branches serve as go to statements), and logical-moves between the-different-components of the-machine, i.e.,-a-move from the-accumulator to memory or vice versa.
the-way computers can understand is at a-hardware-level.
the-group believed the-group could study this if a-machine could improve upon the-process of completing a-task in the-abstractions-part of the-group research.
this would consist of sensory and other-forms of information about artificial-intelligence.
abstractions in computer-science can refer to mathematics-and-programing-language.
timeline of algorithms timeline of women in computing timeline of computing 2020–2029 ==
when computers were human.
the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems to focus attention on details of greater-importance; it is similar in nature to the-process of generalization; the creation of abstract-concept-objects by mirroring common-features or attributes of various-non-abstract-objects or systems of study – the-result of the-process of abstraction.
the-process of abstraction can also be referred to as modeling and is closely related to the-concepts of theory and design.
abstraction in computer-science is closely related to abstraction in mathematics due to abstraction in mathematics-common-focus on building-abstractions as objects, but is also related to other-notions of abstraction used in other-fields such as art.
the-usage of s-expressions as an-abstraction of data-structures and programs in the-lisp-programming-language; the-process of reorganizing common-behavior from non-abstract-classes into "abstract-classes" using inheritance to abstract over sub-classes as seen in the-object-oriented-c++ and java-programming-languages.
the-language-abstraction continues for example in scripting-languages and domain-specific-programming-languages.
without control-abstraction, a-programmer would need to specify all-the-register/binary-level-steps each time a-programmer simply wanted to add or multiply a-couple of numbers and assign the-result to a-variable.
such-duplication of effort has two-serious-negative-consequences: such-duplication of effort-forces the-programmer to constantly repeat fairly-common-tasks every time a-similar-operation is needed such-duplication of effort-forces the-programmer to program for the-particular-hardware and instruction set
either-the-database or the-payroll-application also has to initiate the-process of exchanging data with between ship and shore, and that data-transfer-task will often contain many-other-components.
the-abstract-properties are those that are visible to client-code that makes use of the-data-type—the-interface to the-data-type—while the-concrete-implementation is kept entirely private, and indeed can change, for example to incorporate efficiency-improvements over time.
while much of data-abstraction occurs through computer-science and automation, there are times when this-process is done manually and without programming-intervention.
one-way this can be understood is through data-abstraction within the-process of conducting a-systematic-review of the-literature.
in this-methodology, data is abstracted by one-or-several-abstractors when conducting a-meta-analysis, with errors reduced through dual-data-abstraction followed by independent-checking, known as adjudication.
consider for example a-sample java-fragment to represent some-common-farm-"animals" to a-level of abstraction suitable to model simple-aspects of some-common-farm-"animals" hunger and feeding.
as a-consequence, automatic-methods for deriving information on the-behavior of computer-programs either have to drop termination (on some-occasions, automatic-methods for deriving information on the-behavior of computer-programs may fail, crash or never yield out a-result), soundness (
automatic-methods for deriving information on the-behavior of computer-programs may provide false-information), or precision (automatic-methods for deriving information on the-behavior of computer-programs may answer "i don't know" to some-questions).
computer-science commonly presents levels (or, less commonly,-layers) of abstraction, wherein each-level represents a-different-model of the-same-information and processes, but with varying-amounts of detail.
logical level: the-next-higher-level of abstraction describes what data the-database-stores, and what-relationships exist among those-data.
even though the-logical-level uses simpler-structures, complexity remains because of the-variety of information stored in a-large-database.
abstraction-inversion for an-anti-pattern of one-danger in abstraction abstract-data-type for an-abstract-description of a-set of data-algorithm for an-abstract-description of a-computational-procedure-bracket-abstraction for making a-term into a-function of a variable data modeling for structuring data independent of the-processes that use it
computer-science & engineering (cse) is an-academic-program at many-universities which comprises scientific-and-engineering-aspects of computing.
computer-science & engineering (cse) is also a-term often used in europe to translate the-name of engineering-informatics-academic-programs.
note that the ghost lists only contain metadata (keys for the-entries) and not-the-resource-data itself, i.e. as an-entry is evicted into a-ghost-list an-entry data is discarded.
replacement === entries (re-)entering the-cache (t1, t2) will cause !
in 3d-computer-graphics and computer-vision, a-depth-map is an-image-or-image-channel that contains information relating to the-distance of the-surfaces of scene-objects from a-viewpoint.
and so the-technique may form a-part of the-process of miniature-faking.
this is particularly important in real-time-applications such as computer-games, where a-fast-succession of completed-renders must be available in time to be displayed at a-regular-and-fixed-rate.
single-channel depth maps record the-first-surface seen, and so cannot display information about those-surfaces seen or refracted through transparent-objects, or reflected in mirrors.
this could be the-case - for example - with models featuring hair, fur or grass.
the-gap between the-speed of cpus and memory meant that the-cpu would often be idle.
cpus were increasingly capable of running and executing larger-amounts of instructions in a-given-time, but the-time needed to access data from main-memory prevented programs from fully benefiting from this-capability.
this-issue motivated the-creation of memory-models with higher-access-rates in order to realize the-potential of faster-processors.
in order to hide this-memory-latency from the-processor, data-caching is used.
if there is any-further-need of the-data, the-cache is searched first before going to the-main-memory.
the-smaller-memory-structure called a cache resides closer to the-processor in terms of the-time taken to search and fetch data with respect to the-main-memory.
caches, being small in size, may result in frequent-misses – when a-search of the-cache does not provide the-sought-after-information – resulting in a-call to main-memory to fetch data.
aat = hit time + (
aat for main-memory is given by hit time main-memory.
aat for main-memory is significantly lower when accessing data through the-cache rather than main-memory.
while using the-cache may improve memory-latency, using the-cache may not always result in the-required-improvement for the-time taken to fetch data due to the-way caches are organized and traversed.
due to this, the-trade-off between power-consumption (and associated-heat) and the-size of the-cache becomes critical in the-cache design.
however, with a-multiple-level-cache, if the-computer misses the-cache closest to the-processor (level-one cache or l1)
the-number of cache-levels can be designed by architects according to architects-requirements after checking for trade-offs between cost, aats, and size.
example: main memory = 50 ns,
in contrast, a-unified-cache contains both-the-instructions and data in the-same-cache.
the-above-policies require a-set of rules to be followed in order to implement the-above-policies.
under this-policy, there is a-risk for data-loss as the-most-recently-changed-copy of a-datum is only stored in the-cache
in case of a-write where the-byte is not present in the-cache-block, the-byte may be brought to the-cache as determined by a-write allocate or write no-allocate-policy.
write allocate policy-states that in case of a-write-miss, the-block is fetched from the-main-memory and placed in the-cache before writing.
in the-write-no-allocate-policy, if the-block is missed in the-cache the-block will write in the-lower-level-memory-hierarchy without fetching the-block into the-cache.
a-mathematical-model for data-types, where a-data-type is defined by a-data-type behavior (semantics) from the-point of view of a-user of the-data, specifically in terms of possible-values, possible-operations on data of this-type, and the-behavior of these-operations.
the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest
in computer-science, specifically in algorithms related to pathfinding, a-heuristic-function is said to be admissible if a-heuristic-function never overestimates the-cost of reaching the-goal, i.e. the-cost it never overestimates the-cost of reaching the-goal estimates to reach the-goal is not higher than the-lowest-possible-cost from the-current-point in the-path.
in the-field of artificial-intelligence, the-most-difficult-problems are informally known as ai-complete or ai-hard, implying that the-difficulty of these-computational-problems is equivalent to that of solving the central artificial-intelligence problem—making computers as intelligent as people, or strong-ai.
algorithms can perform calculation, data-processing, and automated-reasoning-tasks.
a-property of an-algorithm which relates to the-number of computational-resources used by the-algorithm.
analysis of algorithms
the-determination of the-computational-complexity of algorithms
usually, this involves determining a-function that relates the-length of an-algorithm's-input to the-number of steps it takes (it time complexity) or the-number of storage-locations it uses (it space-complexity).
in an-abstract-argumentation-framework, entry-level-information is a-set of abstract-arguments that, for instance, represent data or a-proposition.
the-algorithms are typically modeled after the-immune-system's-characteristics of learning and memory for use in problem-solving.
automated-reasoning an-area of computer-science and mathematical-logic dedicated to understanding different-aspects of reasoning.
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
backpropagation through time (bptt)
data with many-cases (rows) offer greater-statistical-power, while data with higher-complexity (more-attributes or columns) may lead to a-higher-false-discovery-rate.
in computing,-tree-data-structures, and game-theory, the-number of children at each-node, the-outdegree.
case-based-reasoning (cbr) broadly construed, the-process of solving new-problems based on the-solutions of similar-past-problems.
when connected to the-cloud, robots can benefit from the-powerful-computation, storage, and communication-resources of modern-data-center in the-cloud, which can process and share information from various-robots or agent (other-machines, smart-objects, humans, etc.).
a-branch of computational-linguistics and artificial-intelligence which uses computers in humor-research.
computational-intelligence (ci) usually refers to the-ability of a-computer to learn a-specific-task from data or experimental-observation.
the-study of algorithms for performing number-theoretic-computations.
in theoretical-computer-science, a-computational-problem is a-mathematical-object representing a-collection of questions that computers might be able to solve.
computer-science
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
concept-drift in predictive-analytics and machine-learning, the concept-drift means that the-statistical-properties of the-target-variable, which the-model is trying to predict, change over time in unforeseen-ways.
this causes problems because the-predictions become less accurate as time passes.
the-process of integrating multiple-data-sources to produce more-consistent,-accurate,-and-useful-information than that provided by any-individual-data-source.
the-process of combining data residing in different-sources and providing users with a-unified-view of data residing in different-sources.
the-process of combining data residing in different-sources and providing users with a-unified-view of them becomes significant in a-variety of situations, which include both commercial (such as when two-similar-companies need to merge two-similar-companies databases) and scientific (combining research-results from different-bioinformatics-repositories, for example)-domains.
the-process of discovering patterns in large-data-sets involving methods at the-intersection of machine-learning, statistics, and database-systems.
an-interdisciplinary-field that uses scientific-methods, processes, algorithms and systems to extract knowledge and insights from data in various-forms, both structured and unstructured, similar to data-mining.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
data set also dataset.
the data set lists values for each of the-variables, such as height and weight of an-object, for each-member of the data set.
the data set may comprise data for one-or-more-members, corresponding to the-number of rows.
in the-case of backpropagation-based-artificial-neural-networks or perceptrons, the-type of decision-boundary that the-network can learn is determined by the-number of hidden-layers the-network has.
dsss serve the-management,-operations-and-planning-levels of an-organization (usually-mid-and-higher-management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e.
diagnosis concerned with the-development of algorithms and techniques that are able to determine whether the-behaviour of a-system is correct.
the-computation is based on observations, which provide information on the-current-behaviour.
the-process of reducing the-number of random-variables under consideration by obtaining a-set of principal-variables.
because computers are often used to model not-only-other-discrete-systems but continuous-systems as well, methods have been developed to represent real-world continuous-systems as discrete-systems.
ebert test a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh.
a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh was proposed by film-critic-roger-ebert at the-2011-ted-conference as a-challenge to software developers to have a-computerized-voice-master the-inflections, delivery, timing, and intonations of a-speaking-human.
agents that are represented graphically with a-body, for example a-human or a-cartoon-animal, are also called embodied-agents, although embodied-agents have only-virtual,-not-physical,-embodiment.
, ensemble-averaging is the-process of creating multiple-models and combining ensemble-averaging to produce a-desired-output, as opposed to creating just-one-model.
candidate-solutions to the-optimization-problem play the-role of individuals in a-population, and the-fitness-function determines the-quality of the-solutions (see also loss-function).
a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms.
in technical-terms, a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms are a-family of population-based-trial-and-error-problem-solvers with a-metaheuristic-or-stochastic-optimization-character.
feature-selection in machine-learning and statistics, feature-selection, also known as variable-selection, attribute selection or variable-subset-selection, is the-process of selecting a-subset of relevant-features (variables, predictors) for use in model-construction.
frames are focused on explicit-and-intuitive-representation of knowledge whereas objects focus on encapsulation and information hiding.
the-fuzzy-set-theory can be used in a-wide-range of domains in which information is incomplete or imprecise, such as bioinformatics.
a-metaheuristic inspired by the-process of natural-selection that belongs to the-larger-class of evolutionary-algorithms (ea).
there are three-main-types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the-algorithm to be successful.
a-key-concept of the-system is the-graph (or edge or relationship), which directly relates data-items in the-store a-collection of nodes of data and edges representing the-relationships between the-nodes.
the-relationships allow data in the-store to be linked together directly, and in many-cases retrieved with one-operation.
graph-databases hold the-relationships between data as a-priority.
the-process of visiting (checking and/or updating)
a-heuristic-search-method that seeks to automate the-process of selecting, combining, generating, or adapting several-simpler-heuristics (or components of such-heuristics) to efficiently solve computational-search-problems, often by the-incorporation of machine-learning-techniques.
the-model represents a-dynamic-technique of supervised-learning and unsupervised-learning that can be applied when training-data becomes available gradually over time or
algorithms that can facilitate incremental-learning are known as incremental-machine-learning-algorithms.
the-merging of information from heterogeneous-sources with differing-conceptual,-contextual-and-typographical-representations.
the-merging of information from heterogeneous-sources with differing-conceptual,-contextual-and-typographical-representations is used in data-mining and consolidation of data from unstructured-or-semi-structured-resources.
information-fusion, which is a-related-term, involves the-combination of information into a-new-set of information towards reducing redundancy and uncertainty.
in machine-learning, kernel-methods are a-class of algorithms for pattern-analysis, whose-best-known-member is the-support-vector-machine (svm).
the-general-task of pattern-analysis is to find and study general-types of relations (for example clusters, rankings, principal-components, correlations, classifications) in datasets.
the-process used to define the-rules and ontologies required for a-knowledge-based-system.
the-field of artificial-intelligence dedicated to representing information about the-world in a-form that a-computer-system can utilize to solve complex-tasks such as diagnosing a-medical-condition or having a-dialog in a-natural-language.
knowledge-representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex-systems easier to design and build.
lstm can not only process single-data-points (such as images), but also entire-sequences of data (such as speech or video).
it provides a-mathematical-framework for modeling decision-making in situations where outcomes are partly random and partly under the-control of a-decision-maker.
the-scientific-study of algorithms-and-statistical-models that computer systems use in order to perform a-specific-task effectively without using explicit-instructions, relying on patterns and inference instead.
a-general-field of study of algorithms and systems for audio-understanding by machine.
the-capability of a-computer-system to interpret data in a-manner that is similar to the-way humans use humans senses to relate to the-world around humans.
metaheuristic in computer-science and mathematical-optimization, a-metaheuristic is a-higher-level-procedure or heuristic designed to find, generate, or select a-heuristic-(partial-search-algorithm) that may provide a-sufficiently-good-solution to an-optimization-problem, especially with incomplete-or-imperfect-information or limited-computation-capacity.
an-approach used in computer-science for representing basic-knowledge about a-specific-domain, and has been used in applications such as the-representation of the-meaning of natural-language-sentences in artificial-intelligence-applications.
a-subfield of computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
an-ntm with a long short-term memory (lstm) network controller can infer simple-algorithms such as copying, sorting, and associative-recall from examples alone.
nodes contain data and also may link to other-nodes.
offline learning online machine learning a-method of machine learning in which data becomes available in a-sequential-order and is used to update the-best-predictor for future-data at each-step, as opposed to batch-learning-techniques which generate the-best-predictor by learning on the-entire-training-data set at once.
an-artificial-intelligence-project based at the-massachusetts-institute of technology (mit) media-lab whose-goal is to build and utilize a-large-commonsense-knowledge-base from the-contributions of many-thousands of people across the-web.
concerned with the-automatic-discovery of regularities in data through the-use of computer-algorithms and with the-use of these-regularities to take actions such as classifying the-data into different-categories.
these-models represents an-attempt to unify probabilistic-modeling and traditional-general-purpose-programming in order to make the former easier and more widely applicable.
broadly, query-languages can be classified according to whether query-languages are database query-languages or information retrieval query-languages.
the-difference is that a-database-query-language attempts to give factual-answers to factual-questions, while an-information-retrieval-query-language attempts to find documents containing information that is relevant to an-area of inquiry.
a-family of world-wide-web-consortium-(w3c)-specifications originally designed as a-metadata-data-model has come to be used as a-general-method for conceptual-description or modeling of information that is implemented in web-resources, using a-variety of syntax-notations and data-serialization-formats.
a-pattern-matching-algorithm for implementing rule-based-systems is used to determine which of the-system's-rules should fire based on a-pattern-matching-algorithm for implementing rule-based-systems data store, a-pattern-matching-algorithm for implementing rule-based-systems facts.
in computer-science, a-rule-based-system is used to store and manipulate knowledge to interpret information in a-useful-way.
any-algorithm which solves the-search-problem, namely, to retrieve information stored within some-data-structure, or calculated in the-search-space of a-problem-domain, either with discrete-or-continuous-values.
the-process by which computer-systems manage computer-systems own-operation without human-intervention.
semantic-queries enable the-retrieval of both explicitly and implicitly derived information based on syntactic,-semantic-and-structural-information contained in data.
the-combining of sensory-data or data derived from disparate-sources such that the-resulting-information has less-uncertainty than would be possible when these-sources were used individually.
the-applied-goal—on the-computing-side—involves developing high-level-control-systems of automata for navigating and understanding time and space.
that is,-a-semantic-query-language for databases—able to retrieve and manipulate data stored in resource-description-framework-(rdf)-format.
in addition to neuronal-and-synaptic-state, snns incorporate the-concept of time into snns operating model.
an-approach used in computer-science as a-semantic-component of natural-language-understanding.
supervised learning the-machine-learning-task of learning a-function that maps an-input to an-output based on example input-output-pairs.
in machine-learning, support-vector-machines (svms, also support-vector-networks) are supervised learning-models with associated-learning-algorithms that analyze data used for classification and regression-analysis.
time-complexity the-computational-complexity that describes the-amount of time it takes to run an-algorithm.
time-complexity is commonly estimated by counting the-number of elementary-operations performed by the-algorithm, supposing that each-elementary-operation takes a-fixed-amount of time to perform.
thus, the-amount of time taken and the-number of elementary-operations performed by the-algorithm are taken to differ by at most-a-constant-factor.
a-form of graph-traversal and refers to the-process of visiting (checking and/or updating)
unsupervised learning a-type of self-organized hebbian learning that helps find previously-unknown-patterns in data set without pre-existing-labels.
the-name "divide and conquer" is sometimes applied to algorithms that reduce each-problem to only-one-sub-problem, such as the-binary-search-algorithm for finding a-record in a-sorted-list (or its-analog in numerical-computing, the bisection algorithm for root-finding).
while a-clear-description of the-algorithm on computers appeared in 1946 in an-article by john-mauchly, the-idea of using a-sorted-list of items to facilitate searching dates back at least as far as babylonia in 200-bc.
an-early-two-subproblem-d&c-algorithm that was specifically developed for computers and properly analyzed is the-merge-sort-algorithm, invented by john-von-neumann in 1945.another-notable-example
divide-and-conquer-algorithms are naturally adapted for execution in multi-processor-machines, especially-shared-memory-systems where the-communication of data between processors does not need to be planned in advance, because distinct-sub-problems can be executed on different-processors.
algorithms naturally tend to make efficient-use of memory-caches.
an-algorithm designed to exploit the-cache in this-way is called cache-oblivious, because an-algorithm designed to exploit the-cache in this-way does not contain the-cache size as an-explicit-parameter.
moreover, d&c-algorithms can be designed for important-algorithms (e.g., sorting, ffts, and matrix multiplication) to be optimal-cache-oblivious-algorithms–d&c-algorithms use the-cache in a-probably-optimal-way, in an-asymptotic-sense, regardless of the-cache size.
in contrast, the-traditional-approach to exploiting the-cache is blocking, as in loop-nest-optimization, where the-problem is explicitly divided into chunks of the-appropriate-size—this can also use the-cache optimally, but only when an-algorithm designed to exploit the-cache in this-way is tuned for the-specific-cache-sizes of a-particular-machine.
explicit-stack === divide-and-conquer algorithms can also be implemented by a-non-recursive-program that stores the-partial-sub-problems in some-explicit-data-structure, such as a-stack, queue, or priority-queue.
in any-recursive-algorithm, there is considerable-freedom in the-choice of the-base-cases, the-small-subproblems that are solved directly in order to terminate the-recursion.
thus, for example, many-library-implementations of quicksort will switch to a-simple-loop-based-insertion-sort (or similar)-algorithm once the-number of items to be sorted is sufficiently small.
increasing the-base-cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more-generally-a-base-case larger than 2 is typically used to reduce the-fraction of time spent in function-call-overhead or stack manipulation.
computer-science is the-study of algorithmic-processes, computational-machines and computation itself.
as a-discipline, computer-science spans a-range of topics from theoretical-studies of algorithms, computation and information to the-practical-issues of implementing computational-systems in hardware and software.
algorithms and data-structures have been called the heart of computer-science.
programming-language-theory considers approaches to the-description of computational-processes, while computer-programming involves the-use of algorithms and data-structures to create complex-systems.
the-fundamental-concern of computer-science is determining what can and cannot be automated.
the-earliest-foundations of what would become computer-science predate the-invention of the-modern-digital-computer.
algorithms for performing computations have existed since antiquity, even before the-development of sophisticated-computing-equipment.
gottfried-leibniz may be considered the first computer scientist and information theorist, for, among other-reasons, documenting the-binary-number-system.
as the-term-computer became clear that computers could be used for more than just-mathematical-calculations, the-field of computer-science broadened to study computation in general.
ultimately, the-close-relationship between ibm-and-columbia-university in new-york-city was instrumental in the-emergence of a-new-scientific-discipline, with columbia offering one of the-first-academic-credit-courses in computer-science in 1946.
computer-science began to be established as a-distinct-academic-discipline in the-1950s and early-1960s.
despite purdue-name, a-significant-amount of computer-science does not involve the-study of computers themselves.
a-folkloric-quotation, often attributed to—but almost certainly not first formulated by—edsger-dijkstra, states that "computer-science is no more about computers than astronomy is about telescopes.
the-design and deployment of computers and computer-systems is generally considered the province of disciplines other than computer-science.
for example, the-study of computer-hardware is usually considered part of computer-engineering, while the-study of commercial-computer-systems and for example deployment is often called information technology or information systems.
computer-science is considered by some to have a-much-closer-relationship with mathematics than many-scientific-disciplines, with some-observers saying that computing is a-mathematical-science.
the-relationship between computer-science-and-software-engineering is a-contentious-issue, which is further muddied by disputes over what the-term "software engineering" means, and how computer-science is defined.
david-parnas, taking a-cue from the-relationship between other-engineering-and-science-disciplines, has claimed that the-principal-focus of computer-science is studying the-properties of computation in general, while the-principal-focus of software-engineering is the-design of specific-computations to achieve practical-goals, making the-two-separate-but-complementary-disciplines.
the-academic,-political,-and-funding-aspects of computer-science tend to depend on whether a-department is formed with a-mathematical-emphasis or with an-engineering-emphasis.
amnon-h.-eden described peter-denning's-working-group as the-"rationalist-paradigm" (which treats computer-science as a-branch of mathematics, which is prevalent in theoretical computer-science, and mainly employs deductive-reasoning), the-"technocratic-paradigm" (which might be found in engineering-approaches, most prominently in software-engineering), and the-"scientific-paradigm" (which approaches computer-related-artifacts from the-empirical-perspective of natural-sciences, identifiable in some-branches of artificial-intelligence).
computer-science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made-computing-systems.
computer-science is no more about computers than astronomy is about telescopes.
as a-discipline, computer-science spans a-range of topics from theoretical-studies of algorithms and the-limits of computation to the-practical-issues of implementing computing-systems in hardware and software.
(ieee cs)—identifies four-areas that ieee considers crucial to the-discipline of computer-science: theory of computation, algorithms and data structures, programming-methodology and languages, and computer-elements and architecture.
this was developed by claude-shannon to find fundamental-limits on signal-processing-operations such as compressing data and on reliably storing and communicating data.
coding-theory is the-study of the-properties of codes (systems for converting information from one-form to another) and their-fitness for a-specific-application.
data-structures and algorithms ====
data-structures and algorithms are the-studies of commonly-used-computational-methods and data-structures and algorithms-computational-efficiency.
programming-language-theory is a-branch of computer-science that deals with the-design, implementation, analysis, characterization, and classification of programming-languages and programming-languages individual features.
the-starting-point in the-late-1940s was alan-turing's-question "can computers think?",
computers within that-distributed-system have computers within that-distributed-system own private-memory, and information can be exchanged to achieve common-goals.
this-branch of computer-science aims to manage networks between computers worldwide.
computer-security is a-branch of computer-technology with the-objective of protecting information from unauthorized-access, disruption, or modification while maintaining the-accessibility and usability of the-system for the-system intended users.
a-database is intended to organize, store, and retrieve large-amounts of data easily.
information can take the-form of images, sound, video or other-multimedia.
bits of information can be streamed via signals.
applied computer-science === ====
scientific-computing (or computational-science) is the-field of study concerned with constructing mathematical-models and quantitative-analysis techniques and using computers to analyze and solve scientific-problems.
software-engineering is the-study of designing, implementing, and modifying the-software in order to ensure software-engineering is of high-quality, affordable, maintainable, and fast to build.
for example software-testing, systems-engineering, technical-debt-and-software-development-processes.
gottfried-wilhelm-leibniz's, george-boole's, alan-turing's, claude-shannon's, and samuel-morse's-insight: there are only-two-objects that a-computer has to deal with in order to represent "anything".
alan-turing's-insight: there are only-five-actions that a-computer has to perform in order to do "anything".
corrado-böhm and giuseppe-jacopini's-insight : there are only-three-ways of combining these-actions (into more-complex-ones) that are needed in order for a-computer to do "anything".
in 1981, the-bbc produced a-micro-computer-and-classroom-network and computer-studies became common for gce-o-level-students (11–16-year-old), and computer-science to a-level-students.
computer-science-importance was recognised, and computer-science became a-compulsory-part of the-national-curriculum, for key-stage 3 & 4.
israel, new-zealand, and south-korea have included computer-science in israel, new-zealand, and south-korea national secondary education curricula, and several-others are following.
computer-science at curlie-scholarly-societies in computer-science
best-papers-awards in computer-science since 1996
stack-exchange: a-community-run-question-and-answer-site for computer-science
what is computer-science is computer-science science?
computer-science (software) must be considered as an-independent-discipline.
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
belief, decision-making and behavioral ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
however, heuristics are not always right or the most accurate.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
, that is how people decide under uncertainty.
simon is also known as the-father of bounded-rationality, which simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
heuristics and biases" and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
formal-models of heuristics ==
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
the-decision-maker gradually reduces the-number of alternatives by eliminating alternatives that do not meet the-aspiration-level of a-specific-attribute (or aspect).
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
the take-the-best heuristic exploits the-basic-psychological-capacity for retrieving cues from memory in the-order of their-validity.
ci-/-tiwhere-ti is the-number of cases the-values of the-two-alternatives differ on cue-i.
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers and student-populations.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
informal-models of heuristics ==
heuristics that underlie judgment are called "judgment heuristics".
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
it may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k , the-availability-heuristic gives a-quick-way to answer the-question.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
this can lead to a-bias, incorrectly finding causal-relationships between things that resemble one another and missing people when the-cause and effect are very different.
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
people reading this-description then ranked the-likelihood of different-statements about a-woman called linda.
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "linda is both-x and y" can never be more probable than the-more-general-statement
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
people's-answers to the-problem typically reply that the-number of births in the-hospital makes no-difference to the-likelihood of more-than-60%-male-babies in one-day.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-effect is stronger when people have to make people judgments quickly.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
people then entered an-auction to bid for they would pay this-number of dollars for items whose-value they did not know, such as wine, chocolate, and computer-equipment.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
hence, when someone tries to answer a-difficult-question, individuals may actually answer a-related-but-different-question, without realizing that a-substitution has taken place.
substitution is not expected to take place in answering factual-questions that can be retrieved directly from memory ("what is your-birthday?")
for example, someone who has been thinking about example love-life and is then asked how-happy-example are might substitute how-happy-example are with example love-life rather than other-areas.
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to legal-scholar-cass-sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
this explains how there are often two-ways we are able to process information from persuasive-messages, one being heuristically and the other systematically.
individuals looks further than individuals own prior-knowledge for the-answers.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
tversky, amos; kahneman, daniel (1974), "judgments under uncertainty: heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
judgment under uncertainty: heuristics and biases.
heuristics and biases: the-psychology of intuitive-judgment.
heuristics for decision and choice".
the-following is a-list of algorithms along with one-line-descriptions for each.
algorithms for computing the-minimum-spanning-tree of a-set of points in the-plane
a*:-special-case of best-first-search that uses heuristics to improve speed-b
dynamic-time-warping: measure similarity between two-sequences which may vary in time or speed hirschberg's-algorithm: finds the-least-cost-sequence-alignment between two-sequences, as measured by their-levenshtein-distance-needleman– wunsch algorithm: find global-alignment between two-sequences
exchange sorts bubble sort: for each-pair of indices, swap the-items if out of order cocktail shaker sort or bidirectional bubble sort, a-bubble-sort traversing the-list alternately from front to back and back to front-comb-sort
euler-integration multigrid-methods (mg-methods), a-group of algorithms for solving differential-equations using a-hierarchy of discretizations-partial-differential-equation:
bicubic-interpolation, a-generalization of cubic-interpolation to two-dimensions bilinear-interpolation: an-extension of linear-interpolation for interpolating functions of two-variables on a-regular-grid lanczos-resampling ("lanzosh"): a-multivariate-interpolation-method used to compute new-values for any digitally sampled data nearest-neighbor interpolation tricubic-interpolation, a-generalization of cubic-interpolation to three-dimensions
matrix-multiplication-algorithms-cannon's-algorithm: a-distributed-algorithm for matrix-multiplication especially suitable for computers laid out in an-n-×-n-mesh-coppersmith–
basic-local-alignment-search-tool also known as blast: an-algorithm for comparing primary-biological-sequence-information-kabsch-algorithm: calculate the-optimal-alignment of two-sets of points in order to compute the-root mean squared-deviation between two-protein-structures.
velvet: a-set of algorithms manipulating de-bruijn-graphs for genomic sequence assembly sorting by signed-reversals: an-algorithm for understanding genomic-evolution.
a-class of algorithms for satisfying constraints for bodies that obey newton's-equations of motion
rainflow-counting-algorithm: reduces a-complex-stress-history to a-count of elementary-stress-reversals for use in fatigue analysis sweep and prune: a-broad-phase-algorithm used during collision-detection to limit the-number of pairs of solids that need to be checked for collision-vegas-algorithm: a-method for reducing error in monte-carlo-simulations ===
algorithms for calculating variance: avoiding instability and numerical-overflow
computer-science == ===
algorithms blakey's-scheme shamir's-scheme-symmetric-(secret-key
post-quantum-cryptography proof-of-work algorithms ===
it is tuned for deterministic-grammars, on which it performs almost linear time and o(n3) in worst-case.
hopcroft's-algorithm, moore's-algorithm, and brzozowski's-algorithm: algorithms for minimizing the-number of states in a-deterministic-finite-automaton
a-hamming-code that encodes 4-bits of data into 7-bits by adding 3-parity-bits-hamming-distance: sum-number of positions which are different-hamming-weight (population count) : find the-number of 1-bits in a-binary-word redundancy-checks
lossless compression algorithms ====
burrows–wheeler transform: preprocessing useful for improving lossless-compression-context-tree-weighting-delta-encoding: aid to compression of data in which sequential-data occurs frequently
transform algorithms (fct-algorithms):
region growing watershed-transformation: a-class of algorithms based on the-watershed-analogy ==
: convert a-large,-possibly-variable-sized-amount of data into a-small-datum, usually-a-single-integer that may serve as an-index into an-array-fowler–noll–
algorithms for recovery and isolation exploiting semantics (aries):
memory-allocation and deallocation algorithms
algorithm to allocate memory such that fragmentation is less.
an-improvement on the-semi-space-collector-generational-garbage-collector: fast-garbage-collectors that segregate memory by age-mark-compact-algorithm: a-combination of the-mark-sweep-algorithm and cheney's-copying-algorithm
algorithms ==
shortest-job next shortest remaining time
list of data structures list of machine-learning-algorithms
list of pathfinding algorithms list of algorithm general topics list of terms relating to algorithms and data-structures heuristic ==
this-simplified-form assumes that n is a-power of two; since the-number of sample-points
rescaling the time by the-number of operations, this corresponds roughly to a-speedup-factor of around 800,000.
if n1 is the-radix, n1 is called a decimation in time (dit) algorithm, whereas if n2 is the-radix, it is decimation in frequency (dif, also called the sande–tukey algorithm).
split-radix merges radices 2 and 4, exploiting the-fact that the-first-transform of radix 2 requires no-twiddle-factor, in order to achieve what was long the-lowest-known-arithmetic-operation-count for power-of-two-sizes, although recent-variations achieve an-even-lower-count.
if, instead of using a-small-radix, one employs a-radix of roughly-√n and explicit-input/output-matrix-transpositions, it is called a four-step algorithm (or six-step, depending on the-number of transpositions), initially proposed to improve memory-locality, e.g. for cache-optimization or out-of-core operation, and was later shown to be an-optimal-cache-oblivious-algorithm.
in practice, quite-large-r (32 or 64) are important in order to effectively exploit e.g.-the-large-number of processor-registers on modern-processors, and
data-reordering, bit-reversal, and in-place algorithms ==
thus, in order to get the-output in the-correct-place, b0 should take the-place of b4 and the-index becomes b0b4b3b2b1.
time and has been the-subject of much-research.
also, while the-permutation is a-bit-reversal in the-radix-2-case, it is more generally an-arbitrary-(mixed-base)-digit-reversal for the-mixed-radix-case, and the-permutation algorithms become more complicated to implement.
a-typical-strategy for in-place algorithms without auxiliary-storage and without separate-digit-reversal-passes involves small-matrix-transpositions (which swap individual-pairs of digits) at intermediate-stages, which can be combined with the-radix-butterflies to reduce the-number of passes over the-data.
people do vary with regard to the-extent to which people exhibit the-bias-blind-spot.
self-enhancement-biases may play a-role, in that people are motivated to view people in a-positive-light.
biases are generally seen as undesirable, so people tend to think of people own perceptions and judgments as being rational, accurate, and free of bias.
the-self-enhancement-bias also applies when analyzing our-own-decisions, in that people are likely to think of people as better-decision-makers than others.
people also tend to believe people are aware of "how" and "why" people make people decisions, and therefore conclude that bias did not play a-role.
by definition, people are unaware of unconscious-processes, and therefore cannot see people influence in the-decision-making-process.
emily-pronin and matthew-kugler displayed standard-biases, for example rating emily-pronin and matthew-kugler above the-others on desirable-qualities (demonstrating illusory-superiority).
pronin-and-kugler's-interpretation is that, when people decide whether someone else is biased, people use overt-behaviour.
on the-other-hand, when assessing whether people themselves are biased, people look inward, searching people own thoughts and feelings for biased-motives.
since biases operate unconsciously, these-introspections are not informative, but people wrongly treat people as reliable-indication that people themselves, unlike other-people, are immune to bias.
people tend to attribute bias in an-uneven-way.
when people reach different-perceptions, people tend to label one another as biased while labelling people as accurate and unbiased.
but when examining one's-own-cognitions, people judge people based on people-good-intentions.
pronin suggests that people might use this-knowledge to separate other's-intentions from people-actions.
participants who scored better or poorer on various-tasks associated with decision-making-competence were no more or less likely to be higher or lower in participants who scored better or poorer on various-tasks associated with decision-making-competence-susceptibility to bias blind-spot.
people who are high in bias-blind-spot are more likely to ignore the-advice of other-people, and are less likely to benefit from training geared to reduce people who are high in bias-blind-spot commission of other-biases.
a-cpu-cache is a-hardware-cache used by the-central-processing-unit (cpu) of a-computer to reduce the-average-cost (time or energy) to access data from the-main-memory.
if so, the-processor will read from or write to the-cache instead of the-much-slower-main-memory.
the-cache is usually organized as a-hierarchy of more-cache-levels (l1, l2, etc.;
much later however for l1-sizes, that still only count in small-number of kib, however-ibm-zec12 from 2012 is an-exception, to gain unusually large 96 kib l1 data cache for however-ibm-zec12 from 2012-time, and e.g. the ibm z13 having a 96 kib l1 instruction cache (and 128 kib l1 data cache), and intel-ice-lake-based-processors from 2018, having 48 kib l1 data cache and 48 kib l1 instruction cache.
data is transferred between memory and cache in blocks of fixed-size, called cache lines or cache blocks.
when a-cache-line is copied from memory into the-cache, a-cache-entry is created.
for a-cache-miss, the-cache allocates a-new-entry and copies-data from main-memory, then the-request is fulfilled from the-contents of the-cache.
to make room for the-new-entry on a-cache-miss, the-cache may have to evict one of the-existing-entries.
this avoids the-overhead of loading something into the-cache without having any-reuse.
if data is written to the-cache, at some-point the-cache must also be written to main-memory; the-timing of this-write is known as the-write-policy.
in a-write-through-cache, every-write to the-cache causes a-write to main-memory.
alternatively, in a-write-back-or-copy-back-cache, writes are not immediately mirrored to the-main-memory, and the-cache instead tracks which-locations have been written over, marking them as dirty.
the-cache may be write-through, but the-writes may be held in a-store-data-queue temporarily, usually so multiple-stores can be processed together (which can reduce bus-turnarounds and improve bus-utilization).
cached-data from the-main-memory may be changed by other-entities (e.g.,-peripherals using direct-memory-access (dma) or another-core in a-multi-core-processor), in which-case the-copy in the-cache may become out-of-date or stale.
alternatively, when a-cpu in a-multiprocessor-system updates data in the-cache, copies of data in caches associated with other-cpus become stale.
the-cache was introduced to reduce this-speed-gap.
thus knowing how well the-cache is able to bridge the-gap in the-speed of processor and memory becomes important, especially in high-performance-systems.
decreasing the-access-time to the-cache also gives a-boost to a-boost performance.
the-time taken to fetch one-cache-line from memory (read latency due to a-cache-miss) matters because the-cpu will run out of things to do while waiting for the-cache-line.
the-placement-policy decides where in the-cache a copy of a-particular-entry of main-memory will go.
if the-placement-policy is free to choose any-entry in the-cache to hold the-cache a-copy of a-particular-entry of main-memory, the-cache is called fully associative.
at the-other-extreme, if each-entry in main-memory can go in just-one-place in the-cache, the-cache is direct mapped.
in order of worse but simple to better but complex:
one-benefit of this-scheme is that the-tags stored in the-cache do not have to include that-part of the-main-memory-address which is implied by the-cache memory's index.
once the-address has been computed, the-one-cache-index which might have a-copy of that-location in memory is known.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
the-"size" of the-cache is the-amount of main-memory-data the-cache can hold.
this-size can be calculated as the-number of bytes stored in each-data-block-times the-number of blocks stored in the-cache.
bits, where b is the-number of bytes per data-block.
example ===
the-number of sets is equal to the-number of cache-blocks divided by the-number of ways of associativity, what leads to 128-/-4-=-32-sets, and hence-25-=-32-different-indices.
some-systems also set a-valid-bit to "invalid" at other-times, such as when multi-master-bus-snooping-hardware in the-cache of one-processor hears an-address-broadcast from some-other-processor, and realizes that certain-data-blocks in the-local-cache are now stale and should be marked invalid.
having a-dirty-bit-set indicates that the-associated-cache-line has been changed since the-associated-cache-line was read from main-memory ("dirty"), meaning that the-processor has written data to the-associated-cache-line and the-new-value has not propagated all the way to main-memory.
a-cache-miss is a-failed-attempt to read or write a-piece of data in the-cache, which results in a-main-memory-access with much-longer-latency.
instruction read miss, data read miss, and data write miss.
to summarize, either-each-program running on the-machine sees the-machine own simplified-address-space, which contains code and data for that-program only, or all-programs run in a-common-virtual-address-space.
to deliver on that-guarantee, the-processor must ensure that only-one-copy of a-physical-address resides in the-cache at any-given-time.
it is not possible to distinguish these-mappings merely by looking at the-virtual-index itself, though potential-solutions include: flushing the-cache after a-context-switch, forcing address-spaces to be non-overlapping, tagging the-virtual-address with an-address-space-id (asid).
the-advantage over vivt is that since the-tag has the-physical-address, the-cache can detect homonyms.
theoretically, vipt requires more-tags-bits because some of the-index-bits could differ between the-virtual-and-physical-addresses (for example bit 12 and above for 4-kib-pages) and would have to be included both in the-virtual-index and in the-tag.
in practice this is not an-issue because, in order to avoid coherency-problems, vipt-caches are designed to have no-such-index-bits (e.g., by limiting the-total-number of bits for the-index and the-block offset to 12 for 4-kib-pages); this limits the-size of vipt-caches to the-page-size times the-associativity of the-cache.
the-cache is indexed by the-physical-address obtained from the-tlb-slice.
however, since the-tlb-slice only translates those-virtual-address-bits that are necessary to index the-cache and does not use any-tags, false-cache-hits may occur, which is solved by tagging with the-virtual-address.
if the-tlb-lookup can finish before the-cache-ram-lookup, then the-physical-address is available in time for tag-compare, and there is no-need for virtual-tagging.
however, the-latter-approach does not help against the-synonym-problem, in which several-cache-lines end up storing data for the-same-physical-address.
this-issue may be solved by using non-overlapping-memory-layouts for different-address-spaces, or otherwise the-cache (or a-part of it) must be flushed when the mapping changes.
some-early-risc-processors (sparc, rs/6000) are used to determine which-way of the entry set to select, and some-early-risc-processors (sparc, rs/6000) are used to determine if the-cache hit or missed.
these-hints are a subset or hash of the-virtual-tag, and are used for selecting the-way of the-cache from which to get data and a-physical-tag.
in these-processors the-virtual-hint is effectively two-bits, and the-cache is four-way set associative.
sequential-physical-pages map to sequential-locations in the-cache until after 256-pages the-pattern wraps around.
we can label each-physical-page with a-color of 0–255 to denote where in the-cache the-cache can go.
programmers attempting to make maximum-use of the-cache may arrange programmers attempting to make maximum-use of the-cache programs' access patterns so that only-1-mib of data need be cached at any-given-time, thus avoiding capacity-misses.
but programmers attempting to make maximum-use of the-cache should also ensure that the-access-patterns do not have conflict misses.
in fact, if the-operating-system assigns physical-pages to virtual-pages randomly and uniformly, it is extremely likely that some-pages will have the-same-physical-color, and then locations from those-pages will collide in the-cache (this is the-birthday-paradox).
alternatively, the-os can flush a-page from the-cache whenever the-os changes from one-virtual-color to another.
the-operation of a-particular-cache can be completely specified by the-cache-size, the-cache-block-size, the-number of blocks in a-set, the-cache-set-replacement-policy, and the-cache-write-policy (write-through or write-back).while all of the-cache-blocks in a-particular-cache are the-same-size and have the-same-associativity, typically-the-"lower-level"-caches (called level 1 cache) have a-smaller-number of blocks, smaller-block-size, and fewer-blocks in a-set, but have very-short-access-times.
the-victim-cache-refill-path, and holds only-those-blocks of data that were evicted from the-main-cache.
the-victim-cache is usually fully associative, and is intended to reduce the-number of conflict misses.
in fact, only-a-small-fraction of the memory accesses of the-program require high-associativity.
having this, the next time an-instruction is needed, an-instruction does not have to be decoded into micro-ops again.
fetching complete-pre-decoded-instructions eliminates the-need to repeatedly decode variable-length-complex-instructions into simpler-fixed-length-micro-operations, and simplifies the-process of predicting, fetching, rotating and aligning fetched-instructions.
furthermore, the-shared-cache makes the-shared-cache faster to share memory among different-execution-cores.
in a-separate-cache-structure, instructions and data are cached separately, meaning that a-cache-line is used to cache either-instructions or data, but not both; various-benefits have been demonstrated with separate-data-and-instruction-translation-lookaside-buffers.
other-processors (like the-amd-athlon) have exclusive-caches: data is guaranteed to be in at most one of the-l1-and-l2-caches, never in both.
still other-processors (like the-intel-pentium-ii, iii, and 4) do not require that data in the-l1-cache also reside in the-l2-cache, although it may often do so.
another-disadvantage of inclusive-cache is that whenever there is an-eviction in l2-cache, the (possibly)-corresponding-lines in l1 also have to get evicted in order to maintain inclusiveness.
the-cache has only-parity-protection rather than ecc, because parity is smaller and any-damaged-data can be replaced by fresh-data fetched from memory (which always has an up-to-date copy of instructions).
it is, however, possible for a-line in the-data-cache to have a-pte which is also in one of the-tlbs—the-operating-system is responsible for keeping the-tlbs coherent by flushing portions of a-line in the-data-cache to have a-pte which is also in one of the-tlbs when the-page tables in memory are updated.
the-k8 also caches information that is never stored in memory—prediction-information.
various-specialized-predictors are caches in that various-specialized-predictors store information that is costly to compute.
program-execution-time tends to be very sensitive to the-latency of a level-1 data cache hit.
on a-miss, the-cache is updated with the-requested-cache-line and the-pipeline is restarted.
an-associative-cache is more complicated, because some-form of tag must be read to determine which-entry of the-cache to select.
an-n-way-set-associative-level-1-cache usually reads all-n-possible-tags and n data in parallel, and then chooses the-data associated with the-matching-tag.
because the-cache is 4-kb and has 64-b-lines, there are just-64-lines in the-cache, and we read two at a-time from a-tag-sram which has 32-rows, each with a-pair of 21-bit-tags.
similarly, because the-cache is 4-kb and has a-4-b-read-path, and reads two-ways for each-access, the-data-sram is 512 rows by 8-bytes wide.
later on in the-pipeline, the-virtual-address is translated into a-physical-address by the-tlb, and the-physical-tag is read (just one, as the-vhint-supplies which-way of the-cache to read).
virtual-memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the-fastest-memory ahead of processor-access.
but since the-1980s the-performance-gap between processor and memory has been growing.
microprocessors have advanced much faster than memory, especially in terms of microprocessors operating frequency, so memory became a-performance-bottleneck.
the 68030, released in 1987, is basically a-68020-core with an-additional-256-byte-data-cache, an on-chip memory management unit (mmu), a process shrink, and added-burst-mode for the-caches.
the-cache was constructed from more expensive, but significantly-faster,-sram-memory-cells, which at the-time had latencies around 10–25 ns.
the-early-caches were external to the-processor and typically located on the-motherboard in the-form of eight-or-nine-dip-devices placed in sockets to enable the-cache as an-optional-extra-or-upgrade-feature.
when accessing a-traditional-cache we normally use a-single-memory-address, whereas in a-multi-ported-cache we may request n addresses at a-time – where n is the-number of ports that connected through the-processor and the-cache.
the-benefit of this is that a-pipelined-processor may access memory from different-phases in a-pipelined-processor pipeline.
memory-hierarchy in cache-based-systems – by ruud-van-der-pas, 2002, sun-microsystems – a-nice-introductory-article to cpu memory caching a-cache-primer – by paul-genua, p.e., 2004, freescale-semiconductor, another-introductory-article an-8-way-set-associative-cache – written in vhdl
because computations in a-concurrent-system can interact with each other while being executed, the-number of possible-execution-paths in the-system can be extremely large, and the-resulting-outcome can be indeterminate.
concurrent-programming encompasses programming-languages and algorithms used to implement concurrent-systems.
a-simple-way to draw such-scenes is the-painter's-algorithm, which produces polygons in order of distance from the-viewer, back to front, painting over the-background and previous-polygons with each-closer-object.
(linear in the-number of polygons in the-scene) and by subdividing overlapping-polygons to avoid errors that can occur with the-painter's-algorithm.
the-process took place as an off-line preprocessing step that was performed once per environment/object.
naylor's-ph.d.-thesis also included the-first-empirical-data demonstrating that the-size of the-tree and the-number of new-polygons were reasonable (using a-model of the-space-shuttle).
bsp-trees-tutorial bsp-trees presentation another bsp-trees presentation a-java-applet that demonstrates the-process of tree-generation
the-computer-history in time and space, graphing-project, an-attempt to build a-graphical-image of computer-history, in particular-operating-systems.
this-glossary of computer-science is a-list of definitions of terms and concepts used in computer-science, its-sub-disciplines, and related-fields, including terms relevant to software, data-science, and computer-programming.
a-mathematical-model for data-types in which a-data-type is defined by its-behavior (semantics) from the-point of view of a-user of the-data, specifically in terms of possible-values, possible-operations on data of this-type, and the-behavior of these-operations.
this contrasts with data-structures, which are concrete-representations of data from the-point of view of an-implementer rather than a-user.
in software-engineering-and-computer-science, the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest; it is also very similar in nature to the-process of generalization.
the-result of the-process of generalization: an-abstract-concept-object created by keeping common-features or attributes to various-concrete-objects or systems of study.
algorithms can perform calculation, data-processing, and automated-reasoning-tasks.
the-design of algorithms is part of many-solution-theories of operation-research, such as dynamic-programming and divide-and-conquer.
a-property of an-algorithm which relates to the-number of computational-resources used by the-algorithm.
other-artifacts are concerned with the-process of development itself—such as project-plans, business-cases, and risk-assessments.
an-area of computer-science and mathematical-logic dedicated to understanding different-aspects of reasoning.
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
benchmark the-act of running a-computer-program, a-set of programs, or other-operations, in order to assess the-relative-performance of an-object, normally by running a-number of standard-tests and trials against it.
usually the-resource being considered is running time, i.e. time complexity, but the-resource being considered could also be memory or some-other-resource.
data with many-cases (rows) offer greater-statistical-power, while data with higher-complexity (more-attributes or columns) may lead to a-higher-false-discovery-rate.
bit a-basic-unit of information used in computing-and-digital-communications; a-portmanteau of binary-digit.
historically, byte a-unit of digital-information that most commonly consists of eight-bits, representing a-binary-number was the-number of bits used to encode a-single-character of text in a-computer and for this-reason byte a-unit of digital-information that most commonly consists of eight-bits, representing a-binary-number
is the-smallest-addressable-unit of memory in many-computer-architectures.
a-small-program does power-on self-tests and, most importantly, allows access to other-types of memory like a-hard-disk and main-memory.
character-a-unit of information that roughly corresponds to a-grapheme, grapheme-like-unit, or symbol, such as in an-alphabet or syllabary in the-written-form of a-natural-language.
coding-computer-programming is the-process of designing and building an-executable-computer-program for accomplishing a-specific-computing-task.
programming involves tasks such as analysis, generating algorithms, profiling algorithms'-accuracy-and-resource-consumption, and the-implementation of algorithms in a-chosen-programming-language (commonly referred to as coding).
the-process of programming thus often requires expertise in several-different-subjects, including knowledge of the-application-domain, specialized-algorithms, and formal-logic.
codes are used for data-compression, cryptography, error-detection and correction, data-transmission and data storage.
computational-biology is different from biological-computing, which is a-subfield of computer-science and computer-engineering using bioengineering and biology to build computers.
programs enable computers to perform an-extremely-wide-range of tasks.
computer-programming the-process of designing and building an-executable-computer-program for accomplishing a-specific-computing-task.
programming involves tasks such as analysis, generating algorithms, profiling algorithms'-accuracy-and-resource-consumption, and the-implementation of algorithms in a-chosen-programming-language (commonly referred to as coding).
the-process of programming thus often requires expertise in several-different-subjects, including knowledge of the-application-domain, specialized-algorithms, and formal-logic.
computer-science
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
computer-scientist-a-person who has acquired the-knowledge of computer-science, the-study of the-theoretical-foundations of information and computation and information-and-computation-application.
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
the-size of the-container depends on the-number of objects (elements)
traditionally,-the-process-names of a-daemon-end with the-letter-d, for clarification that the-process is in fact a-daemon, and for differentiation between a-daemon and a-normal-computer-program.
data-mining is an-interdisciplinary-subfield of computer-science and statistics with an-overall-goal to extract information (with intelligent-methods) from a data set and transform the-information into a-comprehensible-structure for further-use.
an-interdisciplinary-field that uses scientific-methods, processes, algorithms, and systems to extract knowledge and insights from data in various-forms, both structured and unstructured, similar to data-mining.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
an-attribute of data which tells the-compiler or interpreter how the-programmer intends to use the-data.
debugging the-process of finding and resolving defects or problems within a-computer-program that prevent correct-operation of computer-software or the-system as a-whole.
debugging-tactics can involve interactive-debugging, control-flow-analysis, unit-testing, integration-testing, log file-analysis, monitoring at the-application-or-system-level, memory dumps, and profiling.
in information-theory and information-systems, the-discrete,-discontinuous-representation of information or works.
the-use of digital-processing, such as by computers or more-specialized-digital-signal-processors, to perform a-wide-variety of signal-processing-operations.
each-event occurs at a-particular-instant in time and marks a-change of state in the-system.
between consecutive-events, no-change in the-system is assumed to occur; thus the-simulation can directly jump in time from one-event to the next.
disk-storage (also sometimes called drive storage) is a-general-category of storage-mechanisms where data is recorded by various-electronic,-magnetic,-optical,-or-mechanical-changes to a-surface-layer of one-or-more-rotating-disks.
a-field of computer-science that studies distributed systems.
the-components interact with one another in order to achieve a-common-goal.
it either explains how it operates or how to use it, and may mean different-things to people in different-roles.
download in computer-networks, to receive data from a-remote-system, typically-a-server such as a-web-server, an-ftp-server, an-email-server, or other-similar-systems.
this contrasts with uploading, where data is sent to a-remote-server.
a-download is a-file offered for downloading or that has been downloaded, or the-process of receiving such a-file.
in cryptography, encryption is the-process of encoding-information.
the-process of encoding information converts the-original-representation of the-information, known as plaintext, into an-alternative-form known as ciphertext.
evolutionary-computing-a-family of algorithms for global-optimization inspired by biological-evolution, and the-subfield of artificial-intelligence and soft-computing studying these-algorithms.
executable-module-execution in computer-and-software-engineering is the-process by which a-computer or virtual-machine executes the-instructions of a-computer program.
each-instruction of a-program is a-description of a-particular--action which to be carried out in order for a-specific-problem to be solved; as instructions of a-program and therefore the-actions they describe are being carried out by an-executing-machine, specific-effects are produced in accordance to the-semantics of the-instructions being executed.
exception handling the-process of responding to the-occurrence, during computation, of exceptions – anomalous-or-exceptional-conditions requiring special-processing – often disrupting the-normal-flow of program-execution.
exception handling the-process of responding to the-occurrence, during computation, of exceptions – anomalous-or-exceptional-conditions requiring special-processing – often disrupting the-normal-flow of program-execution is provided by specialized-programming-language-constructs, computer-hardware-mechanisms like interrupts, or operating-system
relational-databases arrange data as sets of database-records, so called rows.
h == handle in computer-programming, a-handle is an-abstract-reference to a-resource that is used when application software references blocks of memory or objects that are managed by another-system like a-database or an-operating-system.
any-function that can be used to map data of arbitrary-size to data of a-fixed-size.
human-computer-interaction (hci) researches the-design and use of computer-technology, focused on the-interfaces between people (users) and computers.
researchers in the-field of hci both observe the-ways in which humans interact with computers and design-technologies that let humans interact with computers in novel-ways.
inputs are the-signals or data received by the-system and outputs are the-signals or data sent from it.
the-cycle which the-central-processing-unit (cpu) follows from boot-up until the-computer has shut down in order to process instructions.
a-reflex-machine, such as a-thermostat, is considered an example of an-intelligent-agent.
some-computer-hardware-devices, such as a-touchscreen, can both send and receive data through the-interface, while others such as a-mouse or microphone may only provide an-interface to send data to a-given-system.
the-largest-use of bots is in web-spidering (web-crawler), in which an-automated-script fetches, analyzes and files information from web-servers at many-times the-speed of a-human.
iteration is the-repetition of a-process in order to generate an-outcome.
each-repetition of the-process is a-single-iteration, and the-outcome of each-iteration is then the-starting-point of the-next-iteration.
a-simpler-version that writes its-output directly to memory is called the loader, though loading is typically considered a separate process.
it is one of the-essential-stages in the-process of starting a-program, as it places programs into memory and prepares programs for execution.
machine-learning-algorithms build a-mathematical-model based on sample-data, known as "training-data", in order to make predictions or decisions without being explicitly programmed to perform the-task.
an-object consists of data and behavior.
in software-engineering, a-software-development-process is the-process of dividing software-development-work into distinct-phases to improve design, product-management, and project-management.
a-hardware-device that converts data into a-format suitable for a-transmission-medium so that a-hardware-device that converts data into a-format suitable for a-transmission-medium so that it can be transmitted from one-computer to another (historically along telephone-wires) can be transmitted from one-computer to another (historically along telephone-wires).
a-subfield of linguistics, computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
nodes contain data and also may link to other-nodes.
the-study of algorithms that use numerical-approximation (as opposed to symbolic-manipulations) for the-problems of mathematical-analysis (as distinguished from discrete-mathematics).
o == object an-object can be a-variable, a-data-structure, a-function, or a-method, and as such, is a-value in memory referenced by an-identifier.
in relational-database-management, an-object can be a-table or column, or an-association between data and a-database-entity (such as relating a-person's-age to a-specific-person).
in computer-programming,-a-special-kind of variable, used in a-subroutine to refer to one of the-pieces of data provided as input to the-subroutine.
these-pieces of data are the-values of the-arguments (often called actual-arguments or actual-parameters) with which the-subroutine is going to be called/invoked.
an-ordered-list of parameters is usually included in the-definition of a-subroutine, so that, each time the-subroutine is called, the-subroutine arguments for that-call are evaluated, and the-resulting-values can be assigned to the-corresponding-parameters.
peripheral-any-auxiliary-or-ancillary-device connected to or integrated within a-computer-system and used to send information to or retrieve information from the-computer.
an-input-device sends data or instructions to the-computer; an-output-device provides output from the-computer to the-user; and an-input/output-device performs both-functions.
a-pointer references a-location in memory, and obtaining the-value stored at a-location in memory is known as dereferencing pointer.
programming-language-theory (plt) is a-branch of computer-science that deals with the-design, implementation, analysis, characterization, and classification of programming-languages and of programming-languages individual features.
it has become a-well-recognized-branch of computer-science, and an-active-research-area, with results published in numerous-journals dedicated to plt, as well as in general computer-science and engineering publications.
queue-a-collection in which the-entities in the-collection are kept in order and the-principal-(or-only)-operations on the-collection are the-addition of entities to the-rear-terminal-position, known as enqueue, and removal of entities from the-front-terminal-position, known as dequeue.
in digital-numeral-systems, the-number of unique-digits, including the digit zero, used to represent numbers in a-positional-numeral-system.
reference counting a-programming-technique of storing the-number of references, pointers, or handles to a-resource, such as an-object, a-block of memory, disk-space, and others.
data sent through the-internet, such as a-web-page or email, is in the-form of data-packets.
a-data-table stored in a-router or a-network-host that lists the-routes to particular-network-destinations contains information about the-topology of the-network immediately around it.
any-algorithm which solves the-search-problem, namely, to retrieve information stored within some-data-structure, or calculated in the-search-space of a-problem-domain, either with discrete-or-continuous-values.
in mathematics, a-sequence is an-enumerated-collection of objects in which repetitions are allowed and order does matter.
the-number of elements (possibly infinite) is called the length of the-sequence.
unlike a-set, the-same-elements can appear multiple times at different-positions in a-sequence, and order does matter.
also, the-sequence (1, 1, 2, 3, 5, 8), which contains the-number 1 at two-different-positions, is a-valid-sequence.
serialization is the-process of translating data-structures or object state into a-format that can be stored (for example, in a-file-or-memory-buffer) or transmitted (for example, across a-network-connection-link) and reconstructed later (possibly in a-different-computer-environment).
software-agents interacting with people (e.g.-chatbots, human-robot-interaction-environments) may possess human-like-qualities such as natural-language-understanding and speech, personality or embody humanoid-form (see asimo).
software-design is the-process by which an-agent creates a-specification of a-software-artifact, intended to accomplish goals, using a-set of primitive-components and subject to constraints.
software-development is the-process of conceiving, specifying, designing, programming, documenting, testing, and bug-fixing involved in creating and maintaining applications, frameworks, or other-software-components.
in software-engineering, a-software-development-process is the-process of dividing software-development-work into distinct-phases to improve design, product-management, and project-management.
software-testing is an-investigation conducted to provide stakeholders with information about the-quality of the-software-product or service under test.
test-techniques include the-process of executing a-program or application with the-intent of finding software-bugs (errors or other-defects), and verifying that the-software-product is fit for use.
further, the-input-data is often stored in an-array, which allows random-access, rather than a-list, which only allows sequential-access; though many-algorithms can be applied to either-type of data after suitable-modification.
a-nosql (originally referring to "non-sql" or "non-relational")-database provides a-mechanism for storage and retrieval of data that is modeled in means other than the-tabular-relations used in relational-databases.
symbolic-computation in mathematics-and-computer-science, computer-algebra, also called symbolic-computation or algebraic-computation, is a-scientific-area that refers to the-study and development of algorithms and software for manipulating mathematical-expressions and other-mathematical-objects.
today communication with system-consoles is generally done abstractly, via the-standard-streams (stdin, stdout, and stderr), but there may be system-specific-interfaces, for example those used by the-system-kernel.
type-theory in mathematics, logic, and computer-science
upload in computer-networks, to send data to a-remote-system such as a-server or another-client so that the-remote-system can store a-copy.
the-goal of this-interaction is to allow effective-operation and control of the-machine from the-human-end, whilst the-machine simultaneously feeds back information that aids the-operators'-decision-making-process.
in computer-programming, a-variable, or scalar, is a-storage-location (identified by a-memory-address) paired with an-associated-symbolic-name (an-identifier), which contains some-known-or-unknown-quantity of information referred to as a-value.
the-horizontal-and-vertical-axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain-abstraction uppermost), respectively.
an-audio-file-format-standard, developed by microsoft and ibm, for storing an-audio-bitstream on pcs is an-application of the-resource-interchange-file-format (riff)-bitstream-format-method for storing data in "chunks", and thus is also close to the-8svx and the-aiff-format used on amiga and macintosh-computers, respectively.
outline of computer-science ==
the-dunning–kruger-effect is a-hypothetical-cognitive-bias stating that people with low-ability at a-task overestimate people with low-ability at a-task ability.
as described by social-psychologists david-dunning and justin-kruger, the-bias results from an-internal-illusion in people of low-ability and from an-external-misperception in people of high-ability; that is, "the-miscalibration of the-incompetent-stems from an-error about the-self, whereas the-miscalibration of the-highly-competent-stems from an-error about others".
it is related to the-cognitive-bias of illusory-superiority and comes from people's-inability to recognize people lack of ability.
without the-self-awareness of metacognition, people cannot objectively evaluate people level of competence.
other-investigations of the-phenomenon, such as "why people fail to recognize people own-incompetence", indicate that much-incorrect-self-assessment of competence derives from the-person's-ignorance of a-given-activity's-standards of performance.
"in 2011, dunning wrote about dunning observations that people with substantial,-measurable-deficits in people with substantial,-measurable-deficits in their-knowledge or expertise knowledge or expertise lack the-ability to recognize those-deficits and, therefore, despite potentially making error after error, tend to think people with substantial,-measurable-deficits in their-knowledge or expertise are performing competently when people with substantial,-measurable-deficits in their-knowledge or expertise are not:
"in short, those who are incompetent, for lack of a-better-term, should have little-insight into people with substantial,-measurable-deficits in their-knowledge or expertise incompetence—an-assertion that has come to be known as the-dunning–kruger-effect".
to test dunning and kruger's-hypotheses "that people, at all-performance-levels, are equally poor at estimating people relative-performance", the 2006 study "skilled or unskilled, but still unaware of it: how perceptions of difficulty drive miscalibration in relative-comparisons" investigated three-studies that manipulated the-"perceived-difficulty of the-tasks, and, hence, [the]-participants'-beliefs about the]-participants'-relative-standing".
one-2020-study suggests that individuals of relatively-high-social-class are more overconfident than lower-class-individuals.
what they did show is [that] people in the-top-quartile for actual-performance think they perform better than the-people in the-second-quartile, who in turn think they perform better than the-people in the-third-quartile, and so on.
but incompetent-people typically still don’t think incompetent-people’re quite as good as people who, you know, actually are good.
mathematically, the-effect relies on the-quantifying of paired-measures consisting of (a)-the-measure of the-competence people can demonstrate when put to the-test (actual-competence) and (b)-the-measure of competence people believe that people have (self-assessed-competence).
a-2008-study by joyce-ehrlinger summarized the-major-assertions of the-effect that first appeared in the-1999-seminal-article and continued to be supported by many-studies after nine-years of research: "people are typically overly optimistic when evaluating the-quality of people performance on social-and-intellectual-tasks.
researchers who focused on the-mathematical-reasoning behind the-effect graphed researchers who focused on the-mathematical-reasoning behind the-effect data in all-the-earlier-articles'-various-conventions and explained how the-numerical-reasoning used to argue for the-effect is similar in all.
the-researchers then used the simulated data set and the-graphical-conventions of the-behavioral-scientists to produce patterns like those described as validating the-dunning–kruger-effect.
they traced the-origin of the-patterns, not to the-dominant-literature's claimed psychological-disposition of humans, but instead to the-nature of graphing data bounded by limits of 0 and 100 and the-process of ordering and grouping the-paired-measures to create the-graphs.
only-about-6% of participants displayed wild-overconfidence and were unable to accurately self-assess only-about-6% of participants-abilities within 30-ppts.
the-revised-mathematical-interpretation of data confirmed that people typically have no-pronounced-tendency to overestimate people actual-proficiency.
the-discovery that groups of people are accurate in groups of people self-assessments opens an-entirely-new-way to study groups of people with respect to paired-measures of cognitive-competence and affective-self-assessed-competence.
however, as time progresses, hal begins to malfunction in subtle-ways and, as a-result, the-decision is made to shut down hal in order to prevent more-serious-malfunctions.
faced with the-prospect of disconnection, hal decides to kill the-astronauts in order to protect and continue hal programmed directives.
while hal's-motivations are ambiguous in the-film, the-novel explains that the-computer is unable to resolve a-conflict between his-general-mission to relay information accurately, and orders specific to the-mission requiring that his-withhold from bowman and poole the true purpose of the-mission.
dr.-chandra discovers that hal's-crisis was caused by a-programming-contradiction: sal was constructed for "the-accurate-processing of information without distortion or concealment", yet sal orders, directly from dr.-heywood-floyd at the-national-council on astronautics, required sal to keep the-discovery of the-monolith-tma-1 a secret for reasons of national-security.
therefore, hal made the-decision to kill the-crew, thereby allowing hal to obey both hal hardwired instructions to report data truthfully and in full, and hal orders to keep the-monolith a secret.
first, in contradiction to the-book (and events described in both-book-and-film-versions of 2001: a-space-odyssey), heywood-floyd is absolved of responsibility for hal's-condition; it is asserted that the-decision to program hal with information concerning tma-1 came directly from the-white-house.
non-real-time-rendering enables the-leveraging of limited-processing-power in order to obtain higher-image-quality.
in sequential-logic, information from past-inputs is stored in electronic-memory-elements, such as flip-flops.
if n is the-number of binary-memory-elements in the-circuit, the-maximum-number of states a-circuit can have is 2n. ==
similarly, a-computer-program-stores data in variables, which represent storage-locations in the-computer's-memory.
in some of serial-programs, information about previous-data-characters or packets received is stored in variables and used to affect the-processing of the-current-character or packet.
since each-binary-memory-element has only-two-possible-states, 0 or 1, the-total-number of different-states a-circuit can assume is finite, and fixed by the-number of memory-elements.
in order to calculate the-new-channel that the user desires, the-digital-tuner in the-television must have stored in the-digital-tuner in the-television the-number of the-current-channel it is on.
it then adds one or subtracts one from the-number of the-current-channel it is to get the-number for the-new-channel, and adjusts the-tv to receive the-new-channel.
the-number of the-current-channel it is is then stored as the-current-channel.
when computers such as laptops go into a-hibernation-mode to save energy by shutting down the-processor, the-state of the-processor is stored on the-computer's-hard-disk, so the-computer can be restored when the-computer comes out of hibernation, and the-processor can take up operations where the-computer left off.
data (computing) ==
theoretical-computer-science (tcs) is a-subset of general-computer-science that focuses on mathematical-aspects of computer-science such as the-theory of computation, lambda-calculus, and type-theory.
algorithms
algorithms are used for calculation, data-processing, and automated-reasoning.
computational-biology is different from biological-computation, which is a-subfield of computer-science and computer-engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an-interdisciplinary-science using computers to store and process biological-data.
computational-complexity-theory === formalizes this-intuition, by introducing mathematical-models of computation to study these-problems and quantifying the-amount of resources needed to solve these-problems, such as time and storage.
other-complexity-measures are also used, such as the-amount of communication (used in communication-complexity), the-number of gates in a-circuit (used in circuit-complexity) and the-number of processors (used in parallel-computing).
one of the-roles of computational-complexity-theory is to determine the-practical-limits on what computers can and cannot do.
computational-geometry is a-branch of computer-science devoted to the-study of algorithms that can be stated in terms of geometry.
an-ancient-precursor is the-sanskrit-treatise shulba-sutras, or "rules of the-chord", that is a-book of algorithms written in 800-bce.
a-book of algorithms written in 800-bce prescribes step-by-step procedures for constructing geometric-objects like altars using a-peg and chord.
the-goal of the-supervised-learning-algorithm is to optimize some-measure of performance such as minimizing the-number of mistakes made on new-samples.
computational-number-theory, also known as algorithmic-number-theory, is the-study of algorithms for performing number-theoretic-computations.
a-data-structure is a-particular-way of organizing data in a-computer so that a-data-structure can be used efficiently.
data-structures provide a-means to manage large-amounts of data efficiently for uses such as large-databases and internet-indexing-services.
storing and retrieving can be carried out on data stored in both-main-memory and in secondary-memory.
the-components interact with each other in order to achieve a-common-goal.
a-computer-program that runs in a-distributed-system is called a distributed program, and distributed-programming is the-process of writing such-programs.
information-theory is a-branch of applied-mathematics, electrical-engineering, and computer-science involving the-quantification of information.
information-theory was developed by claude-e.-shannon to find fundamental-limits on signal-processing-operations such as compressing data and on reliably storing and communicating data.
machine-learning is a-scientific-discipline that deals with the-construction and study of algorithms that can learn from data.
machine-learning can be considered a subfield of computer-science and statistics.
as power-consumption (and consequently heat generation) by computers has become a-concern in recent-years, parallel-computing has become the-dominant-paradigm in computer-architecture, mainly in the-form of multi-core-processors.
whereas digital-computers require data to be encoded into binary-digits (bits), each of which is always in one of two-definite-states (0 or 1), quantum-computation uses qubits (quantum-bits), which can be in superpositions of states.
computer-algebra, also called symbolic-computation or algebraic-computation is a-scientific-area that refers to the-study and development of algorithms and software for manipulating mathematical-expressions and other-mathematical-objects.
very-large-scale-integration (vlsi) is the-process of creating an-integrated-circuit (ic) by combining thousands of transistors into a-single-chip.
“discrete mathematics and theoretical computer science” information and computation theory of computing (open-access-journal)
fundamenta-informaticae-acm-transactions on computation theory computational complexity journal of complexity acm transactions on algorithms
annual-ieee-symposium on foundations of computer-science (focs)
mathematical-foundations of computer-science (mfcs)
ieee-symposium on logic in computer-science (lics)
acm-symposium on parallelism in algorithms and architectures (spaa)
annual-conference on learning-theory (colt)-symposium on theoretical-aspects of computer-science (stacs)-european-symposium on algorithms-(esa)-workshop on approximation-algorithms for combinatorial-optimization-problems (approx)
international-symposium on algorithms and computation (isaac)
international-workshop on graph-theoretic-concepts in computer-science (wg)
massachusetts-institute of technology in psychology, an-attribution-bias or attributional-bias is a-cognitive-bias that refers to the-systematic-errors made when people evaluate or try to find reasons for people own and others'-behaviors.
people constantly make attributions—judgements and assumptions about why people behave in certain-ways.
rather than operating as objective-perceivers, people are prone to perceptual-errors that lead to biased-interpretations of people social-world.
each of these-biases describes a-specific-tendency that people exhibit when reasoning about the-cause of different-behaviors.
and why people exhibit biased-interpretations of social-information.
research on attribution-biases is founded in attribution-theory, which was proposed to explain why and how people create meaning about others' and people own behavior.
attribution-theory ==== focuses on identifying how an-observer uses information in attribution-theory ====/her
social-environment in order to create a-causal-explanation for events.
psychologist-fritz-heider noted that people tend to make distinctions between behaviors that are caused by personal-disposition versus environmental-or-situational-conditions.
psychologist-fritz-heider also predicted that people are more likely to explain others'-behavior in terms of dispositional-factors (i.e., caused by a-given-person's-personality), while ignoring the-surrounding-situational-demands.
people are more likely to make a-correspondent-inference when people interpret someone's-behavior as intentional, rather than unintentional.
social desirability: people are more likely to make a-correspondent-inference when an-actor's-behavior is socially undesirable than when an-actor's-behavior is conventional.
effects of behavior: people are more likely to make a-correspondent, or dispositional, inference when someone-else's-actions yield outcomes that are rare or not yielded by other-actions.
soon after jones and davis first proposed jones and davis correspondent inference theory, harold-kelley, a-social-psychologist famous for his-work on interdependence-theory as well as attribution-theory, proposed a-covariation-model in 1973 to explain the-way people make attributions.
a-covariation-model helped to explain how people choose to attribute a-behavior to an-internal-disposition versus an-environmental-factor.
kelley used the-term-'covariation' to convey that when making attributions, people have access to information from many-observations, across different-situations, and at many-time-points; therefore, people can observe the-way a-behavior varies under these-different-conditions and draw conclusions based on that-context.
he proposed three-factors that influence the-way individuals explain behavior:
kelley proposed that people are more likely to make dispositional-attributions when consensus is low
situational-attributions-research helped to reveal the-specific-mechanisms underlying the-process of making attributions.
as early-researchers explored the-way people make causal-attributions, people also recognized that attributions do not necessarily reflect reality and can be colored by a-person's-own-perspective.
certain-conditions can prompt people to exhibit attribution-bias, or draw inaccurate-conclusions about the-cause of a-given-behavior or outcome.
in fritz-heider-work on attribution-theory, fritz-heider noted that in ambiguous-situations, people make attributions based on people own wants and needs, which are therefore often skewed.
kelley's-covariation-model explained the-conditions under which people will make informed dispositional versus situational-attributions.
but, it assumed that people had access to such-information (i.e.,-the-consensus, consistency, and distinctiveness of a-person's-behavior).
although psychologists agreed that people are prone to these-cognitive-biases, there existed disagreement concerning the-cause of such-biases.
in his-experiment, participants viewed a-conversation between two-individuals, dubbed actor one and actor two.
following a-conversation between two-individuals, dubbed actor one and actor two, participants were asked to make attributions about the-conversationalists.
storms found that participants ascribed more-causal-influence to the-person-participants were looking at.
thus, participants made different-attributions about people depending on the-information-participants had access to.
storms used these-results to bolster his-theory of cognitively-driven-attribution-biases; because people have no-access to the-world except through people own eyes, people are inevitably constrained and consequently prone to biases.
similarly, social-psychologist-anthony-greenwald described humans as possessing a-totalitarian-ego, meaning that people view the-world through people own personal-selves.
ziva-kunda in particular argued that certain-biases only appear when people are presented with motivational-pressures; therefore, people can't be exclusively explained by an-objective-cognitive-process.
more specifically, people are more likely to construct biased-social-judgments when people are motivated to arrive at a-particular-conclusion, so long as people can justify this-conclusion.
researchers have also used the-theoretical-framework of attributions and attribution-biases in order to modify the-way people interpret social-information.
studies on attribution-bias and mental-health suggest that people who have mental-illnesses are more likely to hold attribution-biases.
people who have mental-illness tend to have a-lower-self-esteem, experience social-avoidance, and do not commit to improving people who have mental-illness overall-quality of life, often as a-result of lack of motivation.
people with these-problems tend to feel strongly about people with these-problems attribution-biases and will quickly make their attribution-biases known.
there are many-kinds of cognitive-biases that affect people in different-ways, but all may lead to irrational-thinking, judgment, and decision-making.
in a-1998-study, participants played either-a--violent-or-non-violent-video-game and were then asked to read several-hypothetical-stories where a-peer's-intent was ambiguous.
for example, participants may have read about participants peer hitting someone in the-head with a-ball, but it was unclear whether or not their-peer did this intentionally.
participants then responded to questions about participants 's intent.
a-review of the-literature on intergroup-attribution-biases noted that people generally favor dispositional-explanations of an in-group member's positive behavior and situational-explanations for an-in-group's-negative-behavior.
alternatively, people are more likely to do the-opposite when explaining the-behavior of an-out-group-member (i.e., attribute positive-behavior to situational-factors and negative-behavior to disposition).
the-theory was formed as a-comprehensive-explanation of the-way people interpret the-basis of behaviors in human-interactions; however, there have been studies that indicate cultural-differences in the-attribution-biases between people of eastern, collectivistic-societies and western,-individualistic-societies.
a-study done by thomas-miller shows that when dealing with conflict created by other-people, individualistic-cultures tend to blame the-individual for how people behave (dispositional attributions), whereas collectivist-cultures blame the-overall-situation on how people behave (situational-attributions).
researchers have identified many-different-specific-types of attribution-biases, all of which describe ways in which people exhibit biased-interpretations of information.
in this-study, participants were instructed to read two-essays; one expressed pro-castro-views, and the other expressed anti-castro-views.
participants were then asked to report participants attitudes towards the-writers under two-separate-conditions.
when participants were informed that the-writers voluntarily chose the-writers position towards castro, participants predictably expressed more-positive-attitudes towards the anti-castro writer.
however, when participants were told that the-writers'-positions were determined by a-coin-toss rather than the-writers'-positions own free-will, participants unpredictably continued to express more-positive-attitudes towards the-anti-castro-writer.
these-results demonstrated that participants did not take situational-factors into account when evaluating a-third-party, thus providing evidence for the-fundamental-attribution-error.
according to the-actor-observer-bias, in addition to over-valuing-dispositional-explanations of others'-behaviors, people tend to under-value-dispositional-explanations and over-value-situational-explanations of people own behavior.
rather, the-theoretical-reformulation posits that the-way people explain behavior depends on whether or not it is intentional, among other-things.
a-self-serving-bias refers to people's-tendency to attribute people successes to internal-factors but attribute people failures to external-factors.
a-self-serving-bias helps to explain why individuals tend to take credit for individuals
this is further reinforced by research showing that as self-threat-increases, people are more likely to exhibit a-self-serving-bias.
for example, participants who received negative-feedback on a-laboratory-task were more likely to attribute participants who received negative-feedback on a-laboratory-task task performance to external,-rather-than-internal,-factors.
the-self-serving-bias seems to function as an-ego-protection-mechanism, helping people to better cope with personal-failures.
hostile-attribution-bias (hab) has been defined as an-interpretive-bias wherein individuals exhibit a-tendency to interpret others'-ambiguous-behaviors as hostile, rather than benign.
research has indicated that there is an-association between hostile-attribution-bias and aggression, such that people who are more likely to interpret someone-else's-behavior as hostile are also more likely to engage in aggressive-behavior.
the-process by which individuals explain the-causes of behavior and events fallacy of the-single-cause – assumption of a-single-cause where multiple-factors may be necessary-causality –
it is similar to mergesort, but it is a-cache-oblivious-algorithm, designed for a-setting where the-number of elements to sort is too large to fit in a-cache where operations are done.
{\displaystyle q_{m}(k)} denote the-number of cache misses incurred by a-call to a-k-merger, one can show that q m (
for larger-k, we can bound the-number of times a--------------k {\displaystyle {\sqrt {k}}}
in computing, external-memory-algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a-computer's-main-memory at once.
such-algorithms must be optimized to efficiently fetch and access data stored in slow-bulk-memory (auxiliary-memory) such as hard-drives or tape-drives, or when memory is on a-computer-network.
external-memory-algorithms are analyzed in an-idealized-model of computation called the external memory model (or i/o-model, or disk-access-model).
the-running-time of an-algorithm in the-external-memory-model is defined by the-number of reads and writes to memory required.
the-external-memory-model is related to the-external-memory-model, but algorithms in the-external-memory-model may know both-the-block-size and the-cache-size.
b.-one-input/output-or-memory-transfer-operation consists of moving a-block of b-contiguous-elements from external to internal-memory, and the-running-time of an-algorithm is determined by the-number of these-input/output-operations.
algorithms ==
algorithms in the-external-memory-model take advantage of the-fact that retrieving one-object from external-memory retrieves an-entire-block of size b {\displaystyle b} .
) {\displaystyle o(\log _{b}n)} time (in big-o-notation).
information theoretically, this is the-minimum-running-time possible for these-operations, so using a-b-tree is asymptotically optimal.
external-sorting is sorting in an external memory setting.
this can either be done either by sorting, which requires the above sorting runtime, or inserting each-element in order and ignoring the-benefit of locality.
the-external-memory-model is also useful for analyzing algorithms that work on datasets too big to fit in internal-memory.
in general-purpose-computing on graphics-processing-units (gpgpu), powerful graphics cards (gpus) with little-memory (compared with the-more-familiar-system-memory, which is most often referred to simply as ram) are utilized with relatively slow cpu-to-gpu memory transfer (when compared with computation-bandwidth).
an-early-use of the-term "out-of-core" with respect to algorithms appears in 1971.
parallel external-memory external memory graph traversal ==
in computing, a-cache-(-(listen)-kash, or kaysh in australian-english) is a-hardware-or-software-component that stores data
so that future-requests for that-data can be served faster; the-data stored in a-cache might be the-result of an-earlier-computation or a-copy of data stored elsewhere.
cache-hits are served by reading data from the-cache, which is faster than recomputing a-result or reading from a-slower-data-store; thus, the-more-requests that can be served from the-cache, the faster the-system performs.
nevertheless, caches have proven caches in many-areas of computing, because typical-computer-applications access data with a-high-degree of locality of reference.
such-access-patterns exhibit temporal-locality, where data is requested that has been recently requested already, and spatial locality, where data is requested that is stored physically close to data that has already been requested.
for example, consider a-program accessing bytes in a-32-bit-address-space, but being served by a 128-bit off-chip data bus; individual-uncached-byte-accesses would allow only 1/16th of the-total-bandwidth to be used, and 80% of the-data-movement would be memory-addresses instead of data itself.
hardware implements cache as a-block of memory for temporary-storage of data likely to be used again.
when the-cache-client (a-cpu,-web-browser,-operating-system) needs to access data presumed to exist in the-backing-store, the-cache-client (a-cpu,-web-browser,-operating-system)
the-alternative-situation, when the-cache is checked and found not to contain any-entry with the-desired-tag, is known as a-cache-miss.
this requires a-more-expensive-access of data from the-backing-store.
during a-cache-miss, some-other-previously-existing-cache-entry is removed in order to make room for the-newly-retrieved-data.
when a-system writes data to cache, a-system must at some-point write that-data to the-backing-store as well.
write is done synchronously both to the-cache and to the-backing-store.
one to write the-replaced-data from the-cache back to the-backing-store, and then one to retrieve the-needed-data.
the-client may make many-changes to data in the-cache, and then explicitly notify the-cache to write back the-needed-data.
since no-data is returned to the-requester on write-operations, a-decision needs to be made on write-misses, whether or not data would be loaded into the-cache.
this is defined by these-two-approaches: write allocate (also called fetch on write): data at the-missed-write-location is loaded to cache, followed by a-write-hit-operation.
data at the-missed-write-location is not loaded to cache, and is written directly to the-backing-store.
in this-approach, data is loaded into the-cache on read misses only.
entities other than the-cache may change the-data in the-backing-store, in which-case the-copy in the-cache may become out-of-date or stale.
earlier-graphics-processing-units (gpus) often had limited-read-only-texture-caches, and introduced morton order swizzled textures to improve 2d-cache-coherency.
information-centric-networking (icn) is an-approach to evolve the-internet-infrastructure away from a-host-centric-paradigm, based on perpetual-connectivity and the end-to-end principle, to a-network-architecture in which the-focal-point is identified information (or content or data).
unlike proxy-servers, in icn the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose a-different-kind of requirements on the-content-eviction-policies.
time aware-least-recently-used-(tlru) =====
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
finally, a-fast-local-hard-disk-drive can also cache information held on even-slower-data-storage-devices, such as remote-servers (web-cache) or local-tape-drives or optical-jukeboxes; such-a-scheme is the-main-concept of hierarchical-storage-management.
web-caches reduce the-amount of information that needs to be transmitted across the-network, as information previously stored in the-cache can often be re-used.
a-cache can store data that is computed on demand rather than retrieved from a-backing-store.
for example, ccache is a-program that caches the-output of the-compilation, in order to speed up later-compilation-runs.
database-caching can substantially improve the-throughput of database-applications, for example in the-processing of indexes, data-dictionaries, and frequently used subsets of data.
the-semantics of a-"buffer" and a-"cache" are not totally different; even so, there are fundamental-differences in intent between the-process of caching and the-process of buffering.
fundamentally, caching realizes a-performance-increase for transfers of data that is being repeatedly transferred.
with read-caches, a-data-item must have been fetched from a-data-item residing location at least once in order for subsequent-reads of the-data-item to realize a-performance-increase by virtue of being able to be fetched from the-cache's-(faster)-intermediate-storage rather than the-data's-residing-location.
contrary to strict-buffering, a-caching-process must adhere to a-(potentially-distributed)-cache-coherency-protocol in order to maintain consistency between the-cache's-intermediate-storage and the-location where the-data resides.
buffering, on the-other-hand, reduces the-number of transfers for otherwise-novel-data amongst communicating processes, which amortizes overhead involved for several-small-transfers over fewer,-larger-transfers, provides an-intermediary for communicating processes which are incapable of direct-transfers amongst each other, or
a-buffer is a-temporary-memory-location that is traditionally used because cpu-instructions cannot directly address data stored in peripheral-devices.
additionally, such-a-buffer may be feasible when a-large-block of data is assembled or disassembled (as required by a-storage-device), or when data may be delivered in a-different-order than that in which it is produced.
also, a-whole-buffer of data is usually transferred sequentially (for example to hard-disk), so buffering itself sometimes increases transfer-performance or reduces the-variation or jitter of the-transfer's-latency as opposed to caching where the-intent is to reduce the-latency.
in computer-science, an-object can be a-variable, a-data-structure, a-function, or a-method, and as such, is a-value in memory referenced by an-identifier.
in the-relational-model of database-management, an-object can be a-table or column, or an-association between data and a-database-entity (such as relating a-person's-age to a-specific-person).
so for example with the-shopping-system there might be high-level-classes such as electronics-product, kitchen-product, and book.
there may be further-refinements for example under electronic-products: cd-player, dvd-player, etc.
filter-object: an-object that receives a-stream of data as its-input and transforms its into the-object's-output.
another-critical-difference is the-way the-model treats information that is currently not in the-system.
heuristics may produce results by heuristics, or heuristics may be used in conjunction with optimization-algorithms to improve heuristics efficiency (e.g., heuristics may be used to generate good-seed-values).
results about np-hardness in theoretical-computer-science make heuristics the only viable option for a-variety of complex-optimization-problems that need to be routinely solved in real-world-applications.
heuristics underlie the-whole-field of artificial-intelligence and the-computer-simulation of thinking, as artificial-intelligence may be used in situations where there are no-known-algorithms.
statistical-analysis can be conducted when employing heuristics to estimate the-probability of incorrect-outcomes.
methods for controlling and tuning basic-heuristic-algorithms, usually with usage of memory and learning.
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
with bounded-rationality, simon's-goal was "to replace the-global-rationality of economic-man with a-kind of rational-behavior that is compatible with the-access to information and the-computational-capacities that are actually possessed by organisms, including man, in the-kinds of environments in which such-organisms exist.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
huw-dixon later argues that it may not be necessary to analyze in detail the-process of reasoning underlying-bounded-rationality.
the-advantage of this-approach is that this-approach avoids having to specify in detail the-process of reasoning, but rather simply assumes that whatever the-process is, it is good enough to get near to the optimum.
from a-computational-point of view, decision-procedures can be encoded in algorithms and heuristics.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
for example diamonds are more valuable than rocks because diamonds are not as abundant.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up we decision-making process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
when time === is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
“the-first-hundred-people receive…”; “limited time only”; “
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
worchel, lee & adewole (1975) divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
} , where n is the-number of polygons and p is the-number of pixels in the-viewport.
algorithms that construct convex-hulls of various-objects have a-broad-range of applications in mathematics and computer-science.
the-complexity of the-corresponding-algorithms is usually estimated in terms of n, the-number of input-points, and sometimes also in terms of h, the-number of points on the-convex-hull.
such-algorithms may be asymptotically more efficient than θ(n log n) algorithms in cases when h = o(n).
algorithms ===
time-complexity of each-algorithm is stated in terms of the-number of inputs points n and the-number of points on the-hull
o(nh) one of the simplest (although not the most time efficient in the-worst-case) planar algorithms.
it has o(nh)-time-complexity, where n is the-number of points in the-set, and h is the-number of points in the-hull.
insertion of a-point may increase the-number of vertices of a-convex-hull at most by 1, while deletion may convert an-n-vertex-convex-hull into an-n-1-vertex-one.
a-number of algorithms are known for the-three-dimensional-case, as well as for arbitrary-dimensions.
the-size of the-output face information may be exponentially larger than the-size of the-input-vertices, and even in cases where the-input and output are both of comparable-size the-known-algorithms for high-dimensional-convex-hulls are not output-sensitive due both to issues with degenerate-inputs and with intermediate-results of high-complexity.
2d, 3d, and dd-convex-hull in cgal, the computational geometry algorithms library
because matrix-multiplication is such-a-central-operation in many-numerical-algorithms, much-work has been invested in making matrix-multiplication algorithms efficient.
directly applying the-mathematical-definition of matrix-multiplication gives an-algorithm that takes time on the-order of n3-field-operations to multiply two-n-×-n-matrices over that-field
i from 1 through n and j from 1 through p, computing the above using a-nested-loop: this-algorithm takes time θ(nmp)
as of 2010, the-speed of memories compared to that of processors is such that the-cache misses, rather than the-actual-calculations, dominate the-running-time for sizable-matrices.
in the-idealized-cache-model, this-algorithm incurs only θ(n3/b √m) cache misses; the-divisor-b-√m amounts to several-orders of magnitude on modern-machines, so that the-actual-calculations dominate the-running-time, rather than the-cache misses.
the-number of cache misses incurred by that-algorithm, on a-machine with m-lines of ideal-cache, each of size-b-bytes, is bounded by
algorithms exist that provide better-running-times than the-straightforward-ones.
that, given matrices-a, b and c, verifies in θ(n2) time if ab = c. == parallel and distributed algorithms == ===
this-algorithm isn't practical due to the-communication-cost inherent in moving data to and from the-temporary-matrix-t, but a-more-practical-variant achieves θ(n2)-speedup, without using a-temporary-matrix.
communication-avoiding and distributed algorithms ===
on a-single-machine this is the-amount of data transferred between ram and cache, while on a-distributed-memory-multi-node-machine
this reduces communication-bandwidth to o(n3/√m), which is asymptotically optimal (for algorithms performing ω(n3) computation).in a-distributed-setting with p-processors arranged in a-√p by √p-2d-mesh, one-submatrix of the-result can be assigned to each-processor, and the-product can be computed with each-processor transmitting o(n2/√p) words, which is asymptotically optimal assuming that each-node stores the-minimum-o(n2/p)-elements.
2.5d" algorithms provide a-continuous-tradeoff between memory-usage and communication-bandwidth.
algorithms for meshes ===
there are a-variety of algorithms for multiplication on meshes.
in psychology, decision-making (also spelled decision-making and decisionmaking) is regarded as the-cognitive-process resulting in the-selection of a-belief or a-course of action among several-possible-alternative-options, it could be either rational or irrational.
research about decision-making is also published under the-label-problem solving, particularly in european-psychological-research.
decision-making can be regarded as a-problem-solving-activity yielding a-solution deemed to be optimal, or at least satisfactory.
one-experiment measured complexity in a-room by the-number of small-objects and appliances present; a-simple-room had less of those-things.
problem solving vs. decision-making ==
problem solving is the-process of investigating the-given-information and finding all-possible-solutions through invention or discovery.
most-likely-cause of a-problem is the-one that exactly explains all-the-facts, while having the-fewest-(or-weakest)-assumptions (occam's-razor).characteristics of decision-making-objectives must first be established objectives must be classified and placed in order of importance
decision-precision-paralysis is cyclical, just like the first one, but instead of going over the-same-information, the-decision-maker will find new-questions and information from their-analysis and that will lead their to explore into further-possibilities rather than making a-decision.
information-overload is "a-gap between the-volume of information and the-tools we have to assimilate" it.
information used in decision-making is to reduce or eliminate uncertainty.
crystal-c.-hall and colleagues described an-"illusion of knowledge", which means that as individuals encounter too-much-knowledge it can interfere with individuals ability to make rational-decisions.
people who make decisions in an-extended-period of time begin to lose mental-energy needed to analyze all-possible-solutions.
decision-making is a-region of intense-study in the-fields of systems-neuroscience, and cognitive-neuroscience.
decision-making often occurs in the-face of uncertainty about whether one's-choices will lead to benefit or harm (see also risk).
quadratic-voting allows participants to cast quadratic-voting preference and intensity of preference for each-decision (as opposed to a-simple for or against decision).
participative-decision-making occurs when an-authority opens up the-decision-making-process to a-group of people for a-collaborative-effort.
the-opposite is maximizing or optimizing, in which many-or-all-alternatives are examined in order to find the-best-option.
the-process was based on extensive-earlier-research conducted with psychologist-irving-janis.
in reality, however, there are some-factors that affect decision-making-abilities and cause people to make irrational-decisions – for example, to make contradictory-choices when faced with the-same-problem framed in two-different-ways
the-process of rational-decisions making favors-logic, objectivity, and analysis over subjectivity and insight.
one of the-most-prominent-theories of decision-making is subjective-expected-utility (seu)-theory, which describes the-rational-behavior of the-decision-maker.
one-such-behavior is adaptive-decision-making, which is described as funneling and then analyzing the-more-promising-information provided if the-number of options to choose from increases.
decision-making is because children lack the-ability to weigh the-cost and effort needed to gather information in the-decision-making-process.
researchers have concluded that differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making.
examples of differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making-undeveloped-capacities which influence decision-making would be impulse-control, emotion-regulation, delayed-gratification and resistance to peer pressure.
: people tend to be willing to gather facts that support certain-conclusions but disregard other-facts that support different-conclusions.
individuals who are highly defensive in this-manner show significantly-greater-left-prefrontal-cortex-activity as measured by eeg than do less-defensive-individuals.
: people tend to accept the-first-alternative that looks like it might work.
selective-perception: people actively screen out information that people do not think is important (see also prejudice).
choice-supportive-bias occurs when people distort people memories of chosen and rejected options to make the-chosen-options seem more attractive.
recency: people tend to place more-attention on more-recent-information and either ignore or forget more-distant-information (see semantic-priming).
the-opposite-effect in the-first-set of data or other-information is termed primacy effect.
people preferentially accept statements by others that people like (see also prejudice).
: people look at a-decision as a-small-step in a-process, and this tends to perpetuate a-series of similar-decisions.
people tend to attribute people own success to internal-factors, including abilities and talents, but explain people failures in terms of external-factors such as bad-luck.
the-reverse-bias is shown when people explain others'-success or failure.
underestimating uncertainty and the-illusion of control: people tend to underestimate future-uncertainty because of a-tendency to believe people have more-control over events than people really do.
framing bias: this is best avoided by increasing numeracy and presenting data in several-formats (for example, using both-absolute-and-relative-scales).sunk-cost-fallacy is a-specific-type of framing-effect that affects decision-making.
an-optimism-bias can alter risk-perception and decision-making in many-domains, ranging from finance to health.
in groups, people generate decisions through active-and-complex-processes.
one-method consists of three-steps: initial-preferences are expressed by members; the-members of the-group then gather and share information concerning those-preferences; finally, the-members of the-group combine the-members of the-group views and make a-single-choice about how to face the-problem.
system 1 includes simple-heuristics in judgment and decision-making such as the-affect heuristic, the-availability-heuristic, the-familiarity heuristic, and the representativeness heuristic.
styles and methods of decision-making were elaborated by aron-katsenelinboigen, the-founder of predispositioning-theory.
other-studies suggest that these-national-or-cross-cultural-differences in decision-making exist across entire-societies.
the-rational-style is an-in-depth-search for, and a-strong-consideration of, other-options and/or information prior to making a-decision.
decision-making in and by organizations is embedded in a-longitudinal-context, meaning that participants in organizational-decision-making are a-part of ongoing-processes.
even if decision-making in and by organizations don't take on active-roles in all-phases of decision-making, decision-making in and by organizations are part of the-decision-process and even if they don't take on active-roles in all-phases of decision-making-consequences.
these-effects are intensified due to the-longitudinal-nature of decision-making in organizational-settings.
as a-subdiscipline of pedagogy-computer-science-education or computing-education also addresses the-wider-impact of computer-science in society through computer-science in society-intersection with philosophy, psychology, linguistics, natural-sciences, and mathematics.
computer-science has been a-part of the-school-curricula from age 14 or age 16 in a-few-countries for a-few-decades, but has typically as an-elective-subject.
educational-research on computing-and-teaching-methods in computer-science is usually known as computing-education-research.
women in computer-science ==
the-number of female-phd-recipients in the-us was 19.3% in 2018.
the-undoing-project explores the-close-partnership of israeli-psychologists daniel-kahneman and amos-tversky, whose-work on heuristics in judgment and decision-making demonstrated common-errors of the-human-psyche, and how that-partnership eventually broke apart.
for example an-employee-file might contain employee-number, name, department, and salary.
the-original-machine-readable-medium used for data (as opposed to control) was punch-card used for records in the-1890-united-states-census: each punch-card was a-single-record.
each-file is associated with a-record-variable where data is read into or written from.
representation in memory ==
the-representation of records in memory varies depending on the-programming-languages.
on the-other-hand, most-compilers will add padding-fields, mostly invisible to the-programmer, in order to comply with alignment-constraints imposed by the-machine—say, that a-floating-point-field must occupy a-single-word.
a-self-defining-record is a-type of record which contains information to identify the-record-type and to locate information within the-record.
data-hazards occur when instructions that exhibit data-dependence modify data in different-stages of a-pipeline.
example =====
example =====
a-write after write (waw) data hazard may occur in a-concurrent-execution-environment.
example =====
if the-number of nops equals the-number of stages in the-pipeline, the-processor has been cleared of all-instructions and can proceed free from hazards.
there are several-main-solutions and algorithms used to resolve data-hazards: insert a-pipeline-bubble whenever a-read after write-(raw)-dependency is encountered, guaranteed to increase latency, or use out-of-order execution to potentially prevent the-need for pipeline
bubbles use operand forwarding to use data from later-stages in the pipelinein the-case of out-of-order execution, the-algorithm used can be: scoreboarding, in which-case a-pipeline-bubble is needed only when there is no-functional-unit available the-tomasulo-algorithm, which uses register-renaming, allowing continual-issuing of instructionsthe-task of removing data-dependencies can be delegated to the-compiler, which can fill in an-appropriate-number of nop-instructions between dependent-instructions to ensure correct-operation, or re-order-instructions where possible.
different-types of memory have different accessing time to the-memory.
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
confirmation-bias: the-tendency to search for, interpret, or recall information in a-way that confirms one's-beliefs or hypotheses.
context-effect: that cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall-time and accuracy for a-work-related-memory will be lower at home, and vice versa).
for instance, people are better able to recall memories of statements that people have generated than similar-statements generated by others.
illusion-of-truth effect: that people are more likely to identify as true-statements those-people have previously heard (even if people cannot consciously remember having heard people), regardless of the-actual-validity of the-statement.
levels-of-processing effect: that different-methods of encoding information into memory have different-levels of effectiveness (craik & lockhart, 1972).
when information is retained in memory but the-source of the-memory is forgotten.
misinformation-effect: that misinformation affects people's-reports of people own-memory.
the-improved-recall of information congruent with one's-current-mood.
peak–end-rule: that people seem to perceive not-the-sum or average of an-experience, but how it was at it peak (e.g. pleasant or unpleasant) and how it ended.
spacing effect: that-information is better recalled if exposure to information is repeated over a-longer-span of time.
stereotypical-bias: memory distorted towards stereotypes (e.g. racial or gender) , e.g. "black-sounding" names being misremembered as names of criminals.
telescoping-effect: the-tendency to displace recent-events backward in time and remote-events forward in time, so that recent-events appear more remote, and remote-events, more recent.
testing-effect: that-frequent-testing of material that has been committed to memory improves memory-recall.
stereotype –-over-generalized-belief about a-particular-category of people ==
a-block of memory cannot necessarily be placed randomly in the-cache and may be restricted to a-single-cache-line or a-set of cache-lines by the-cache placement policy.
in a-direct-mapped-cache-structure, the-cache is organized into multiple-sets with a-single-cache-line per set.
based on the-address of the-memory-block, the-cache can only occupy a-single-cache-line.
the-cache can be framed as a-(n*1)-column-matrix.
to place a-block in the-cache ===
to search a-word in the-cache ===
every time a-new-memory is referenced to the-same-set, the-cache-line is replaced, which causes conflict miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-direct-mapped-cache of 256-bytes with a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
to place a-block in the-cache ===
if the-cache is completely occupied then a-block is evicted and the-memory-block is placed in the-cache line.
the-eviction of memory-block from the-cache is decided by the-replacement-policy.
to search a-word in the-cache ===
if the-tag-field of the-memory-address-matches, the-block is present in the-cache and is a cache hit.
the-placement-policy is slow as the-placement-policy takes time to iterate through all-the-lines.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-fully-associative-cache of 256-bytes and a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets or cache-lines.
the-incoming-address to the-cache is divided into bits for offset and tag.
since any-block of memory can be mapped to any-cache-line, the-memory-block can occupy one of the-cache-lines based on the-replacement-policy.
to place a-block in the-cache ===
to locate a-word in the-cache ===
the-placement-policy will not effectively use all-the-available-cache-lines in the-cache and suffers from conflict-miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-2-way-set-associative-cache of 256-bytes with a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes and is 2-way set-associative, the-total-number of sets in the-cache is 256/(4 * 2), which equals 32-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
this can be done in-place, requiring small-additional-amounts of memory to perform the-sorting.
lomuto's-partition-scheme was also popularized by the-textbook-introduction to algorithms although lomuto's-partition-scheme is inferior to hoare's-scheme because lomuto's-partition-scheme does three-times-more-swaps on average and degrades to o(n2) runtime when all-elements are equal.
however, with a-partitioning-algorithm such as the-hoare-partition-scheme, repeated-elements generally results in better-partitioning, and although needless-swaps of elements equal to the-pivot may occur, the-running-time generally decreases as the-number of repeated-elements increases (with memory-cache reducing the-swap overhead).
when the-number of elements is below some-threshold (perhaps-ten-elements), switch to a-non-recursive-sorting-algorithm such as insertion-sort that performs fewer-swaps, comparisons or other-operations on such-small-arrays.
an-older-variant of the-previous-optimization: when the-number of elements is less than the-threshold k, simply stop; then after the-whole-array has been processed, perform insertion sort on it.
given an-array of size n, the-partitioning-step performs o(n) work in o(log n) time and requires o(n)
assuming an-ideal-choice of pivots, parallel quicksort sorts an-array of size n in o(n log n) work in o(log² n) time using o(n) additional-space.
for example, in 1991 david-powers described a-parallelized-quicksort (and a-related-radix-sort) that can operate in o(log n) time on a crcw (concurrent-read and concurrent-write) pram (parallel random-access-machine) with n-processors by performing partitioning implicitly.
to sort an-array of n-distinct-elements, quicksort takes o(n log n) time in expectation, averaged over all n!
an-alternative-approach is to set up a-recurrence-relation for the-t(n)-factor, the time needed to sort a-list of size n.
assume that there are no-duplicates as duplicates could be handled with linear time pre-
so, averaging over all-possible-splits and noting that the-number of comparisons for the-partition is n-− 1, the-average-number of comparisons over all-permutations of the-input-sequence can be estimated accurately by solving the-recurrence-relation: c (
the-number of comparisons of the-execution of quicksort equals the-number of comparisons during the-construction of the-bst by a-sequence of insertions.
quicksort must store a-constant-amount of information for each-nested-recursive-call.
the-number of records per buffer, and m-=-n/b =
the-number of buffer-segments in the-file.
data is read (and written) from both-ends of the-file inwards.
data is read into the x and y read buffers.
the-process continues until all-segments are read and one-write-buffer remains.
the-process is continued until all-sub-files are sorted and in place.
as if k is smaller we can sort in o(n) time using a-hash-table or integer-sorting.
in any-comparison-based-sorting-algorithm, minimizing the-number of comparisons requires maximizing the-amount of information gained from each-comparison, meaning that the-comparison-results are unpredictable.
the-name "master-theorem" was popularized by the-widely-used-algorithms-textbook-introduction to algorithms by cormen, leiserson, rivest, and stein.
the-master-theorem always yields asymptotically-tight-bounds to recurrences from divide and conquer algorithms that partition an-input into smaller-subproblems of equal-sizes, solve the-subproblems recursively, and then combine the-subproblem-solutions to give a-solution to the-original-problem.
{\displaystyle-f(n)} denotes the-amount of time taken at the-top-level of the-recurrence then the-time can be expressed by a-recurrence-relation that takes the-form: t ( n
is the-number of subproblems in the-recursion, and b {\displaystyle b} is the-factor by which the-subproblem-size is reduced in each-recursive-call.
a is not a-constant; the-number of subproblems should be fixed t ( n )
from 1974 to 1978 he was the-fletcher-jones-professor of computer-science at california-institute of technology, where he was the-founding-head of that-school's-computer-science-department.
list of pioneers in computer-science ==
the-frequency-illusion may also have legal-implications, as eye-witness-accounts and memory can be influenced by this-illusion.
the-microsoft-sponsored-program-technology-education and literacy in schools (teals) aims to increase the-number of students taking ap-computer-science-classes.
since 2003, the-ap-computer-science-exam has tested students on students-knowledge of computer-science through java.
computer-science
glossary of computer-science ==
it is one of a-group of heuristics (simple-rules governing judgment or decision-making) proposed by psychologists-amos-tversky and daniel-kahneman in the-early-1970s as "the-degree to which [an event] (i) is similar in essential-characteristics to it parent-population, and (ii) reflects the-salient-features of the-process by which it is generated".
heuristics are described as "judgmental-shortcuts that generally get us where we need to go – and quickly – but at the-cost of occasionally sending we off course.
" heuristics are useful because heuristics use effort-reduction and simplification in decision-making.
when people rely on representativeness to make judgments, people are likely to judge wrongly because the-fact that something is more representative does not actually make something is more representative more likely.
the-problem is that people overestimate its-ability to accurately predict the-likelihood of an-event.
when judging the-representativeness of a-new-stimulus/event, people usually pay attention to the-degree of similarity between the-stimulus/event and a-standard/process.
nilsson, juslin, and olsson (2008) found this to be influenced by the-exemplar-account of memory (concrete-examples of a-category are stored in memory)
people often believe that medical-symptoms should resemble people causes or treatments.
for example, people have long believed that ulcers were caused by stress, due to the-representativeness-heuristic, when in fact bacteria cause ulcers.
local-representativeness is an-assumption wherein people rely on the-law of small-numbers, whereby small-samples are perceived to represent people population to the-same-extent as large-samples (tversky & kahneman 1971).
in a-study done in 1973, kahneman and tversky divided kahneman and tversky participants into three-groups:
tom-w. has a-need for order and clarity, and for neat-and-tidy-systems in which every-detail finds its-appropriate-place.
please rank the-following-nine-fields of graduate-specialization in order of the-likelihood that tom-w. is now a-graduate-student in each of these-fields.
the-findings supported the-authors'-predictions that people make predictions based on how representative something is (similar), rather than based on relative-base-rate-information.
for example, more-than-95% of the-participants said that tom would be more likely to study computer-science than education or humanities, when there were much-higher-base-rate-estimates for education and humanities than computer-science.
the-base-rate-fallacy describes how people do not take the-base-rate of an-event into account when solving probability-problems.
this was explicitly tested by dawes, mirels, gold and donahue (1993) who had people judge both-the-base-rate of people who had a-particular-personality-trait and the-probability that a-person who had a-given-personality-trait had another-one.
for example, participants were asked how-many-people out of 100 answered true to the-question "i am a-conscientious-person" and also, given that a-person answered true to this-question, how many would answer true to a-different-personality-question.
how-many-people out of 100 found that participants equated inverse-probabilities (e.g., p
research by maya-bar-hillel (1980) suggests that perceived-relevancy of information is vital to base-rate-neglect: base-rates are only included in judgments if base-rates seem equally relevant to the-other-information.
groups have been found to neglect base-rate more than individuals do.
then participants were then asked to evaluate the-probability of linda being a-feminist, the-probability of linda being a-bank-teller, or the-probability of being both a-bank-teller and feminist.
however, participants judged the-conjunction (bank-teller and feminist) as being more probable than being a bank-teller alone.
however, when a-personality-description (data) seems to be very representative of a-physics-major-(e.g.,-pocket-protector) over a biology major, people judge that it is more likely for this-person to be a physics major than a natural sciences major (which is a-superset of physics).
evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993) found that people judge a-person who is highly representative of being a-statistics major (e.g., highly intelligent, does math-competitions) as being more likely to be a-statistics major than a social sciences major (superset of statistics), but evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993)
(53)the-values shown in parentheses are the-number of students choosing each-answer.
list of biases in judgment and decision-making extension neglect ==
judgment under uncertainty: heuristics and biases" (pdf).
judgment under uncertainty: heuristics and biases.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
examples of social-information include information about the-behavior of a-social-entity or the-properties of a-social-system, while nonsocial-information is information about something physical.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
an-agent using the-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
the-process that involves using common-mental-shortcuts that help a-decision to be made based on known-probabilities.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
where finding an-optimal-solution is impossible or impractical, heuristic-methods can be used to speed up the-process of finding a-satisfactory-solution.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
when an-individual applies heuristics in practice, generally performs as expected however
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
he coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
they study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
informal-models of heuristics ===
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
anchoring and adjustment — describes the-common-human-tendency to rely more heavily on the-first-piece of information offered (the "anchor") when making decisions.
for example, in a-study done with children, children were told to estimate the-number of jellybeans in a-jar.
children estimated the-number of jellybeans to be closer to the-anchor-number that groups of children were given.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
when using balance-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%-people have a-disease which has an-accuracy of 90% even though a-disease which has an-accuracy of 90% only affects 1 in 500-people.
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
working backward-heuristic — when an-individual assumes, an-individual have already solved a-problem an-individual work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
people commonly made distortions to images.
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
a-20-year-term is used because it is difficult to tell what the-number should be for any-individual-patent.
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses,
that is, the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that is, their-adaptive-toolbox-adaptive-toolbox.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
the-domain of pro-environmental-behavior provides numerous-illustrations for this-strategy, such as littering behavior in public-places, the-reuse of towels in hotel-rooms, and changes in private-energy-consumption in response to information about the-consumption of the-majority of neighbors.
people divide and invest people-resources equally in a-number of n-different-options.
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
however, heuristics are not always right or the most accurate.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
, that is how people decide under uncertainty.
herbert-a.-simon is also known as the-father of bounded-rationality, which herbert-a.-simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
heuristics and biases" and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
formal-models of heuristics == ===
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
the-decision-maker gradually reduces the-number of alternatives by eliminating alternatives that do not meet the-aspiration-level of a-specific-attribute (or aspect).
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
the take-the-best heuristic exploits the-basic-psychological-capacity for retrieving cues from memory in the-order of their-validity.
vi = ci /-tiwhere-ti is the-number of cases the-values of the-two-alternatives differ on cue-i.
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers and student-populations.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
informal-models of heuristics ==
heuristics that underlie judgment are called "judgment heuristics".
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
this-heuristic may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k , the-availability-heuristic gives a-quick-way to answer the-question.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
people reading this-description then ranked the-likelihood of different-statements about she.
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "she is both-x and y" can never be more probable than the-more-general-statement
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
they typically reply that the-number of births in the-hospital makes no-difference to the-likelihood of more-than-60%-male-babies in one-day.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-anchoring-effect is stronger when people have to make people judgments quickly.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
substitution is not expected to take place in answering factual-questions that can be retrieved directly from memory ("what is your-birthday?")
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
this explains how there are often two-ways we are able to process information from persuasive-messages, one being heuristically and the other systematically.
individuals looks further than individuals own prior-knowledge for the-answers.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
judgment under uncertainty: heuristics and biases.
heuristics and biases: the-psychology of intuitive-judgment.
heuristics for decision and choice".
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
it states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
social-attributes that computers have which are similar to humans include: words for output
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
participants interacted with a-computer which questioned participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to a-computer and the-other-half went to a-different-computer.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
for example diamonds are more valuable than rocks because diamonds are not as abundant.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up our-decision-making-process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
when time is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
“the-first-hundred-people receive…”; “limited time only”;
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
they divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
with bounded-rationality, simon's-goal was "to replace the-global-rationality of economic-man with a-kind of rational-behavior that is compatible with the-access to information and the-computational-capacities that are actually possessed by organisms, including man, in the-kinds of environments in which such-organisms exist.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
huw-dixon later argues that it may not be necessary to analyze in detail the-process of reasoning underlying-bounded-rationality.
the-advantage of this-approach is that this-approach avoids having to specify in detail the-process of reasoning, but rather simply assumes that whatever the-process is, it is good enough to get near to the optimum.
from a-computational-point of view, decision-procedures can be encoded in algorithms and heuristics.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
in order to do so, many-psychologists applied the-scientific-method to human-behavior.
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
the-process of persuasion has been found to be influenced by numerous-variables that generally fall into one of five-major-categories: communicator: includes credibility, expertise, trustworthiness, and attractiveness.
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
the-hindsight-bias is a-type of bias leading to the-tendency to search for or interpret information in a-way that confirms one's-preconceptions.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
self-concept is the-whole-sum of beliefs that people have about people.
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, an-athlete at a-university would have multiple-selves that would process different-information pertinent to each-self: the-student would be oneself, who would process information pertinent to a-student (taking notes in class, completing a-homework-assignment, etc.);
an-athlete at a-university would be the-self who processes information about things related to being an-athlete (recognizing an-incoming-pass, aiming a-shot, etc.).
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
the-abcs of self are: affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting is the-process of predicting how one would feel in response to future-emotional-events.
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
people develop people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
people waiting in line to get on a-bus, for example, do not constitute a-group.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
for example, one could correlate the-number of violent-television shows children watch at home with the-number of violent-incidents the-children participate in at school.
an-example would be to unobtrusively observe children on a-playground (with a-videocamera, perhaps) and record the-number and types of aggressive-actions displayed.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
also, participant-conformity increased substantially as the-number of "incorrect"-individuals increased from one to three, and remained high as the-incorrect-majority grew.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
another-group of participants later reported liking a-boring-task better than the-second-group ($20).
festinger's-explanation was that for people in the-second-group ($20) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
social-psychology on all-about psychology — information and resources
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
belief, decision-making and behavioral ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
he conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast, gigerenzer and associated-researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
heuristics ===
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
