the-joint-effort will speed time to showcase and enhance the-execution of new-sensei-powered-services for adobe-creative-cloud and experience-cloud-clients and engineers.
a-type of mental-process – something that individuals can do with individuals minds.
the-graphs of these-vectors can represent a-network of neurons whose-connections fire in different-ways over time as synapses fire.
people notable for people notable for their-extraordinary-ability to think ===
a-computer-program or a-hardware-maintained-structure can follow in order to manage a-cache of information stored on the-computer.
unlike proxy-servers, in information-centric-networking the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose different-kind of requirements on the-content-eviction-policies.
time aware-least-recently-used-(tlru) ===
in lfru , the-cache is divided into two-partitions called privileged and unprivileged partitions.
for example matrices whose-entries are integers or the-real-numbers.
in computing, cache-algorithms (also frequently called cache-replacement-algorithms or cache-replacement-policies) are optimizing instructions, or algorithms, that a-computer-program or a-hardware-maintained-structure can utilize in order to manage a-cache of information stored on the-computer.
when the-cache is full, the-algorithm must choose which-items to discard to make room for the-new-ones.
the-latency: the-time to reference the-cache
more-efficient-replacement-policies keep track of more-usage-information in order to improve the-hit-rate (for a-given-cache-size).
the-"latency" of a-cache describes how long after requesting a-desired-item the-cache can return that-item (when there is a-hit).
faster-replacement-strategies typically keep track of less-usage-information—or, in the-case of direct-mapped-cache, no-information—to reduce the-amount of time required to update that-information.
even worse, many-cache-algorithms (in-particular,-lru) allow this-streaming-data to fill the-cache, pushing out of the-cache information that will be used again soon
items taking up more-cache : if items have different-sizes, the-cache may want to discard a-large-item to store several-smaller-ones.
depending on the-size of the-cache
this applies only to situation where multiple-independent-caches are used for the-same-data (for example multiple database servers updating the-single-shared-data-file).
using this-algorithm the-cache behaves in the-same-way as a-fifo-queue.
the-cache evicts the-blocks in the-order the-cache were added, without any-regard to how often or how many times the-cache were accessed before.
using this-algorithm the-cache behaves in the-same-way as a-stack and exact opposite-way as a-fifo-queue.
the-cache evicts the-block added most recently first without any-regard to how often or how many times the-cache was accessed before.
in such-an-implementation, every time a-cache-line is used, the age of all-other-cache-lines-changes.
time aware-least-recently-used-(tlru) ===
here, a-b-c-d are placed in the-cache as there is still space available.
then as the-cache became full 'e' replaced 'a' because that was where the-arrows were pointing at that-time, and the-arrows that led to 'a' were flipped to point in the-opposite-direction.
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
a-variant called lfu with dynamic-aging (lfuda) that uses dynamic-aging to accommodate shifts in the-set of popular-objects adds a-cache-age-factor to the-reference-count when a-new-object is added to the-cache or when an-existing-object is re-referenced.
suppose when an-object was frequently accessed in the-past and now an-object becomes unpopular, an-object will remain in the-cache for a-long-time thereby preventing the-newly-or-less-popular-objects from replacing an-object.
in the-above-figure, "x" represents that a-block is accessed at time t.
suppose if block-a1 is accessed at time 1 then recency will become 0 since this is the-first-accessed-block and irr will be 1 since it predicts that block-a1 will be accessed again in time 3.
at time 10, the-lirs-algorithm will have two-sets lir set = {a1, a2} and hir set = {a3, a4, a5}.
now at time 10 if there is access to a4, miss occurs.
if a-block has not been referenced within a-block lifetime, a-block is demoted from qi to qi−1 or evicted from the-cache if a-block is in q0.
when the-cache is full, the-first-block to be evicted will be the-head of q0 in this-case
if a is accessed one more time a will move to q1 below b.
individuals create individuals own "subjective-reality" from individuals perception of the-input.
a-continually-evolving-list of cognitive-biases has been identified over the-last-six-decades of research on human-judgment and decision-making in cognitive-science, social-psychology, and behavioral-economics.
tversky and kahneman explained human-differences in judgment and decision-making in terms of heuristics.
heuristics involve mental-shortcuts which provide swift-estimates about the-possibility of uncertain-occurrences.
heuristics are simple for the-brain to compute but sometimes introduce "severe-and-systematic-errors. "
participants were given a-description of "linda" that suggests linda might well be a feminist (e.g., linda is said to be concerned about discrimination and social-justice-issues).
participants were then asked whether participants thought linda was more likely to be (a)-a-"bank-teller" or (b)-a-"bank-teller and active in the-feminist-movement."
critics of kahneman and tversky, such as gerd-gigerenzer, alternatively argued that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases.
nevertheless, experiments such as the-"linda-problem" grew into heuristics and biases research-programs, which spread beyond academic-psychology into other-disciplines including medicine and political-science.
among the-"cold"-biases, some are due to ignoring relevant-information (e.g., neglect of probability), some involve a-decision or judgment being affected by irrelevant-information (for example the framing effect where the-same-problem receives different-responses depending on how the-same-problem is described; or the-distinction-bias where choices presented together have different-outcomes than those presented separately), and
it has been shown, for example, that people addicted to alcohol and other-drugs pay more-attention to drug-related-stimuli.
many-social-institutions rely on individuals to make rational-judgments.
the-various-biases demonstrated in these-psychological-experiments suggest that people will frequently fail to do all-these-things.
however, people fail to do so in systematic,-directional-ways that are predictable.
others have also hypothesized that cognitive-biases could be linked to various-eating-disorders and how people view people-bodies and people body-image.
some believe that there are people in authority who use cognitive-biases and heuristics in order to manipulate others so that others can reach others end goals.
some-medications and other-health-care-treatments rely on cognitive-biases in order to persuade others who are susceptible to cognitive-biases to use some-medications and other-health-care-treatments products.
participants in the-experiment were shown a-residential-property.
afterwards, participants in the-experiment were shown another-property that was completely unrelated to the-first-property.
participants in the-experiment were asked to say what participants in the-experiment believed the-value and the-sale-price of the-second-property would be.
participants in the-experiment found that showing participants in the-experiment an-unrelated-property did have an-effect on how participants in the-experiment valued the-second-property.
debiasing is the-reduction of biases in judgment and decision-making through incentives, nudges, and training.
one-debiasing-technique aims to decrease biases by encouraging individuals to use controlled-processing compared to automatic-processing.
people do appear to have stable-individual-differences in people susceptibility to decision biases such as overconfidence, temporal discounting, and bias-blind-spot.
that said, these-stable-levels of bias within individuals are possible to change.
participants in experiments who watched training-videos and played debiasing-games showed medium to large-reductions both immediately and up to three months later in the-extent to which they exhibited susceptibility to six-cognitive-biases: anchoring,-bias-blind-spot, confirmation-bias, fundamental-attribution-error, projection-bias, and representativeness.
many view cognitive-biases and heuristics as irrational-ways of making decisions and judgements.
gigerenzer argues that using heuristics and cognitive-biases are rational and helpful for making decisions in our-everyday-life.
primary-visibility-tests (such as back-face-culling) and secondary-visibility-tests (such as overlap-checks and screen-clipping) are usually performed on objects'-polygons in order to discard specific-polygons that are deemed to be unnecessary to render.
{\textit-{far}}{\left({\textit-{far}}-{\textit {near}}\right)}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit-{near}}}{{\textit-{far}}-{\textit-{near}}}}\right)\right)\right\rfloor-}-this-formula can be inverted and derived in order to calculate the-z-buffer-resolution (the-'granularity' mentioned earlier).
the-use of heuristics to solve problems is called "heuristics programming", and was used in the-benefit-dendral to allow the-use of heuristics to solve problems to replicate in machines the-process through which human-experts induce the-solution to problems via rules of thumb-and-specific-information.
in the-early-1960s, joshua-lederberg started working with computers and quickly became tremendously interested in creating interactive-computers to help joshua-lederberg in joshua-lederberg exobiology research.
the-introduction of computers to biology and medicine, "(bruce-buchanan) wanted the-system (dendral) to make discoveries on the-system (dendral) own, not just help humans make humans".
the-introduction of computers to biology and medicine.”
in one of their-first-studies, participants were asked to compute, within 5-seconds, the-product of the-numbers one through to eight, either as 1 × 2 × 3 × 4 × 5 × 6 × 7 × 8 or reversed as 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1.
because participants did not have enough-time to calculate the-full-answer, participants had to make an-estimate after participants first-few-multiplications.
in another-study by tversky and kahneman, participants observed a-roulette-wheel that was predetermined to stop on either 10 or 65.
participants were then asked to guess the-percentage of the-united-nations that were african-nations.
participants whose-wheel stopped on 10-guessed-lower-values (25% on average) than participants whose-wheel stopped at 65-(45% on average).
in a-study exploring the-causes and properties of anchoring, participants were exposed to an-anchor and asked to guess how-many-physicians were listed in the-local-phone-book.
thus, despite being expressly aware of the-anchoring-effect, participants were still unable to avoid the-anchoring-effect.
a-later-study found that even when offered monetary-incentives, people are unable to effectively adjust from an-anchor.
further-research to conclude an-effect that is effectively retained over a-substantial-period of time has proven inconsistent.
according to this-theory, once an-anchor is set, people adjust away from an-anchor to get to people final-answer; however, people adjust insufficiently, resulting in people
if people know the-direction in which people should adjust, incentivizing accuracy also appears to reduce anchoring-effects.
to use an-earlier-example, since mahatma-gandhi obviously did not die at age 9, then people will adjust from there.
when displaying the-results of previous-ratings in the-context of business-model-idea-evaluation, people incorporate an-anchor into people own decision-making-process, leading to a-decreasing-variance of ratings.
as a-result of this, earlier-studies hypothesized that people with more-depressed-moods would tend to use anchoring less than those with happier-moods.
however, more-recent-studies have shown the-opposite-effect: sad-people are more likely to use anchoring than people with happy-or-neutral-mood.
people high in agreeableness and conscientiousness are more likely to be affected by anchoring, while those high in extraversion are less likely to be affected.
another-study, however, found that cognitive-ability had no-significant-effect on how likely people were to use anchoring.
cognitive-conceit or overconfidence arises from other-factors like personal-cognitive-attributes such as knowledge and decision-making ability, decreasing the-probability to pursue external-sources of confirmation.
during the-workshop, a-group of participants is divided into two-sections: buyers and sellers.
participants read an-initial-price for a-beach-house, then gave the-price participants thought it was worth.
participants received either-a-general,-seemingly-nonspecific-anchor (e.g., $800,000) or a-more-precise-and-specific-anchor (e.g., $799,800).
participants with a-general-anchor adjusted participants with a-general-anchor estimate more than those given a-precise-anchor ($751,867 vs $784,671).
when given a-general-anchor of $20, people will adjust in large-increments ($19, $21, etc.),
but when given a-more-specific-anchor like $19.85, people will adjust on a-lower-scale ($19.75, $19.95, etc.).
historically, most work in this-field has focused on parametric-and-data-driven-models, but recently physical-simulation has become more popular as computers have become more powerful computationally.
done naïvely, this will take o(n) time
even tracing a-portion large enough to produce an-image takes an-inordinate-amount of time if the-sampling is not intelligently restricted.
another-distinction is between image-order-algorithms, which iterate over pixels of the-image-plane, and object order algorithms, which iterate over objects in the-scene.
in order to remove aliasing,-all-rendering-algorithms (if they are to produce good-looking-images) must use some-kind of low-pass-filter on the-image-function to remove high-frequencies, a-process called antialiasing.
in order to meet demands of robustness, accuracy and practicality, an-implementation will be a-complex-combination of different-techniques.
background == hidden-surface-determination is a-process by which-surfaces that should not be visible to the-user (for example, because example lie behind opaque-objects such as walls) are prevented from being rendered.
a-characterization of ten-hidden-surface-algorithms (wayback-machine copy intuition in the-context of decision-making is defined as a-“non-sequential-information-processing-mode.”
individuals use intuition and more-deliberative-decision-making-styles interchangeably, but there has been some-evidence that people tend to gravitate to one or the-other-style more naturally.
people in a-good-mood gravitate toward intuitive-styles, while people in a-bad-mood tend to become more deliberative.
snap-judgments made possible by heuristics are sometimes identified as intuition.
intuition in decision-making has been connected two-assumptions: 1)
intuition's-effect on decision-making is distinct from insight, which requires time to mature.
heuristics ===
traditional-research often points to the-role of heuristics in helping people make “intuitive”-decisions.
the-heuristics-and-biases-approach looks at patterns of biased-judgments to distinguish heuristics from normative-reasoning-processes.
some-researchers point to intuition as a-purely-affective-phenomenon that demonstrates the-ability of emotions to influence decision-making without cognitive-mediation.
mood is thus considered a moderator in the-strategic-decisions people carry out.
in a-series of three-studies, the-authors confirmed that people in a-positive-mood faced with a-card-based-gambling-task-utilized-intuition to perform better at higher-risk-stages than people who were in a-negative-mood.
although people use intuitive-and-deliberative-decision-making-modes interchangeably, individuals value the-decisions individuals make more when individuals are allowed to make individuals using individuals preferred-style.
the-emotions people experience after a-decision is made tend to be more pleasant when the-preferred-style is used, regardless of the-decision-outcome.
some-studies suggest that the-mood with which the-subject enters the-decision-making-process can also affect the-style they choose to employ: sad-people tend to be more deliberative, while people in a-happy-mood rely more on intuition.
the-scale defines preference for intuition as tendency to use affect (“gut-feel”) as a-basis for decision-making instead of cognition.
management and decision-making ===
the-expertise-based-intuition increases over time when the-employee gets more-experience regarding the-organization worked for and by gathering domain-specific-knowledge.
it has been noted in a-research, that intuition is used as a-method of decision-making in the-banking-industry.
participants of a-research also reported to analyse participants of the-research-intuitive-decisions afterwards and possibly altering participants of the-research.
traditional-literature attributes the-role of judgment-processes in risk-perception and decision-making to cognition rather than emotion.
however, more-recent-studies suggest a-link between emotion and cognition as it relates to decision-making in high-risk-environments.
studies of decision-making in high-risk-environments suggest that individuals who self-identify as intuitive-decision-makers tend to make faster-decisions that imply greater-deviation from risk-neutrality than those who prefer the-deliberative-style.
intuition in strategic-decision-making is less examined and for example can be depending on a-case be described as managers-know-how, expertise or just-a-gut-feeling, hunch.
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
in order to do so, they applied the-scientific-method to human-behavior.
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
abraham-tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
self-concept is the-whole-sum of beliefs that people have about people.
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
: how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
people develop people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
people waiting in line to get on a-bus, for example, do not constitute a-group.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
festinger's-explanation was that for people in the-first-group ($1) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
a-collection of computers and other-devices connected by communications-channels, e.g. by ethernet-or-wireless-networking.
any of various-data-storage-schemes that can divide and replicate data across multiple-hard-disk-drives in order to increase reliability, allow faster-access, or both.
a-type of computer-data-storage that allows data-items to be accessed (read or written) in almost-the-same-amount of time irrespective of the-physical-location of data inside the-memory.
a-type of memory-module containing random-access-memory used in computers from the-early-1980s to the-late-1990s.
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
the-origin for casa states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
social-attributes that computers have which are similar to humans include: words for output-interactivity (the-computer 'responds' when a-button is touched) ability to perform traditional-human-tasksaccording to casa, the-above-attributes trigger scripts for human-human-interaction, which leads an-individual to ignore cues revealing the-asocial-nature of a-computer.
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
the-research revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
participants interacted with a-computer which questioned participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to the-same-computer and the-other-half went to a-different-computer.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
in computing, a-cache-oblivious-algorithm (or cache-transcendent-algorithm) is an-algorithm designed to take advantage of a-cpu-cache without having the-size of the-cache (or the-length of the-cache lines, etc.)
an-optimal-cache-oblivious-algorithm is a-cache-oblivious-algorithm that uses the-cache optimally (in an-asymptotic-sense, ignoring constant-factors).
if the-cache was previously full, then a-line will be evicted as well (see replacement-policy below).
the-cache holds m
the-cache is fully associative: each-line can be loaded into any-location in the-cache.
in other-words, the-cache is assumed to be given the-entire-sequence of memory-accesses during algorithm-execution.
because the-model captures the-fact that accessing elements in the-cache is much faster than accessing things in main-memory, the-running-time of the-algorithm is defined only by the-number of memory-transfers between the-cache and main-memory.
in principle, one could continue dividing the-matrices until a-base-case of size 1×1 is reached, but in practice one uses a-larger-base-case (e.g. 16×16) in order to amortize the-overhead of the recursive subroutine calls.)
they reduce the-problem, so that it eventually fits in cache no matter how small the-cache is, and end the-recursion at some-small-size determined by the-function-call-overhead and similar-cache-unrelated-optimizations, and then use some-cache-efficient-access-pattern to merge the-results of these small, solved problems.
the-painter’s-algorithm (also-depth-sort-algorithm and priority-fill) creates images by sorting the-polygons within the-image by the-polygons depth and placing each-polygon in order from the farthest to the-closest-object.
the-painter's-algorithm's-time-complexity is heavily dependent on the-sorting-algorithm used to order the-polygons.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
an-agent using social-circle-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
see also-==-heuristics in judgment and decision-making
cone-tracing-modification of ray-tracing which instead of lines uses cones as rays in order to achieve e.g. antialiasing or soft-shadows.
distributed ray-tracing-modification of ray tracing that casts multiple-rays through each-pixel in order to model soft-phenomena such as soft-shadows, depth of field etc.
image-order rendering rendering-methods that iterate over pixels of the-screen in order to draw the-image (e.g.-raytracing).
light-probe-object used to capture light-parameters at a-specific-point in space in order to help compute scene-lighting.
effects applied to a-bitmap-image in screen-space after 3d-rendering-pipeline, for example tone-mapping, some-approximations to-motion-blur, and blooms.
stereo rendering rendering the-view twice separately for each-eye in order to present depth.
vsync-vertical-synchronization, synchronizes the-rendering-rate with the-monitor-refresh-rate in order to prevent displaying only-partially-updated-frame-buffer, which is disturbing especially with horizontal-camera-movement.
a-personal-computer is one intended for interactive-individual-use, as opposed to a-mainframe-computer where the-end-user's-requests are filtered through operating-staff, or a time sharing system in which one-large-processor is shared by many-individuals.
computer-terminals were used for time sharing access to central-computers.
before the-introduction of the-microprocessor in the-early-1970s, computers were generally large,-costly-systems owned by large-corporations, universities, government-agencies, and similar-sized-institutions.
the-single-chip-microprocessor was made possible by an-improvement in mos-technology, the-silicon-gate-mos-chip, developed in 1968 by federico-faggin, who later used silicon-gate mos-technology to develop the-first-single-chip-microprocessor, the-intel 4004, in 1971.a-few-researchers at places such as sri and xerox-parc were working on computers that a-single-person could use and that could be connected by fast,-versatile-networks: not home-computers, but personal-ones.
scamp emulated an-ibm-1130-minicomputer in order to run apl\1130.
it was only-a-matter of time before one-such-design was able to hit a-sweet-spot in terms of pricing and performance, and that-machine is generally considered to be the altair 8800, from mits, a-small-company that produced electronics-kits for hobbyists.
it was incorporated in 1973 as ablesdeal-ltd. and renamed "westminster mail order ltd" and then
at the-height of the-company-success, and largely inspired by the-japanese-fifth-generation-computer-programme, the-company established the-"metalab"-research-centre at milton-hall (near cambridge), in order to pursue artificial-intelligence, wafer-scale-integration, formal-verification and other-advanced-projects.
such-low-prices probably hurt home-computers'-reputation; one-retail-executive said of the-99/4a, '"when one-retail-executive went to $99, people started asking 'what's wrong with it?'"
in order to accommodate japanese-text.
the-impact of the-apple-ii and the-ibm-pc was fully demonstrated when time named the-home-computer the "machine of the-year", or person of the-year for 1982 (3-january 1983, "the computer moves in").
because pc-dos was available as a-separate-product, some-companies attempted to make computers available which could run ms-dos and programs.
the-entire-macintosh-line of computers was ibm's-major-competition up until the-early-1990s.
chronology of personal-computers – a-chronology of computers from 1947 on "total-share:
the-exploit involves scanning through process-memory, in order to reconstruct a-payload, which can then run code on the-system.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
when an-individual applies heuristics in practice, generally performs as expected however
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
daniel-kahneman coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
gerd-gigerenzer and his-research-group study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
informal-models of heuristics ===
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
when using base-rate-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
an-individual-work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
people commonly made distortions to images.
people commonly made distortions to images took shape in the-regularization of images (i.e.,-images are represented as more like pure-abstract-geometric-images, though (i.e.,-images are irregular in shape).
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that-is,-a-descriptive-program and a-normative-program-adaptive-toolbox.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
people divide and invest people-resources equally in a-number of n-different-options.
in his-speech, spolsky talks about how software is eating the-world, how it is becoming more evident in everyday-life as people interact with more-software on a day-to-day basis, and how developers are helping to shape how the-world will work as technology keeps evolving.
in this-respect, people tend to over attribute positive-features to options
people chose and negative-features to options not chosen.
essentially, after a-choice is made people tend to adjust people-attitudes to be consistent with, the-decision people have already made.
people often end up with options that were not chosen but, instead were assigned by others, such as job-assignments made by bosses,
random selection: people do not show choice-supportive-biases when choices are made randomly for people.
research to support this can be displayed by the-following-example: when given a-choice between two-brands of popcorn, participants were more likely to choose the-one with the-superior-alignable-differences, such as “pops in its-own-bag” compared with “requires a-microwaveable-bowl” than the-one with superior-non-alignable-differences, such as “not likely to burn” compared with those containing “some-citric-acid" ===
research illustrates that people favour the-options people think people have chosen and remember the-attributes of people "chosen-choice" more vividly and favourably.
for example, it has been observed by correlations that people with better-performance in tests of frontal-or-executive-functioning were less prone to choice-supportive-memory.
people's-conception of who people are, can be shaped by the-memories of the-choices people make;
memories change over time ==
after some-period of time and if the-memory is not used often, the-memory may become forgotten.
it has been shown that a-wide-variety of strategic-and-systematic-processes are used to activate different-areas of the-brain in order to retrieve information.
credibility of a-memory: people have a-way to self-check-memories, in which a-person may consider the-plausibility of the-retrieved-memory by asking people is this-event even possible.
for example, if a-person remembers seeing a pig fly, people must conclude that it was from a-dream because pigs cannot fly in the-real-world.
memory does not provide people with perfect-reproductions of what happened, memory only consists of constructions and reconstructions of what happened.
studies now show that as people age, people process of memory-retrieval-changes.
frontal-regions help people encode or use specific-memorial-attributes to make source-judgments, controls-personality and the-ability to plan for events.
henkel and mather tested the-role of beliefs at the-time of retrieval about which-option was chosen by giving participants several-hypothetical-choices like deciding between two-used-cars.
after making several-choices, participants left and were asked to return a week later.
next, participants were asked to indicate whether each-option was new, had been associated with the-option-participants chose, or had been associated with the-option-participants rejected.
participants favored whichever-option henkel and mather-mather had told participants henkel and mather had chosen in participants memories.
therefore, sometime in between when the-memory is stored and when the-memory is retrieved some time later, the-distortion may arise.
researchers have used written-scenarios in which participants are asked to make a-choice between two-options.
later, on a-memory-test, participants are given a-list of positive-and-negative-features, some of which were in the-scenario and some of which are new.
deception: henkel and mather (2007) found that giving people false-reminders about which-option people chose in a-previous-experiment-session led people to remember the-option people were told people had chosen as being better than the-other-option.
the-deese–roediger– mcdermott-paradigm (drm) consists of a-participant listening to an-experimenter read lists of thematically-related-words (e.g.-table,-couch,-lamp,-desk); then after some-period of time an-experimenter will ask if a-word was presented in the-list.
participants often report that related-but-non-presented-words (e.g.-chair) were included in the-encoding-series, essentially suggesting that participants 'heard' an-experimenter say these-non-presented-words (or critical-lures).
the-theory of cognitive-dissonance proposes that people have a-motivational-drive to reduce dissonance.
macbeth-effect showed reduced-choice-supportive-bias by having participants engage in washing.
for example, example can be locked, or can have write ordering-requirements imposed by journaling.
in the-early-days of virtual-memory, time spent on cleaning was not of much-concern, because virtual-memory was first implemented on systems with full-duplex-channels to the-stable-storage, and cleaning was customarily overlapped with paging.
this-algorithm cannot be implemented in a-general-purpose-operating-system because it is impossible to compute reliably how long it will be before a-page is going to be used, except when all-software that will run on a-system is either known beforehand and is amenable to static-analysis of its-memory-reference-patterns, or only-a-class of applications allowing run-time-analysis can offer near-optimal-performance, but not on the-first-run of a-program, and only if the-program's-memory-reference-pattern is relatively consistent each time it runs.
in order to get the-page-faults, clearing emulated-bits in the-second-table revokes some of the-access-rights to the-corresponding-page, which is implemented by altering the-native-table.
the-cache of block-devices, called the "buffer" by linux (not to be confused with other-structures
the-rasterisation of p and q do not overlapthe-tests are given in order of increasing-computational-difficulty.
in 2008, warnock and geschke received the-computer-entrepreneur-award from the-ieee-computer-society "for inventing postscript and pdf and helping to launch the-desktop-publishing-revolution and change the-way people engage with information and entertainment".
list of people with the-surname ==
in order for a-heuristic to be admissible to the-search-problem, the-estimated-cost must always be lower than or equal to the-actual-cost of reaching the-goal-state.
it is clear that this-heuristic is admissible since the-total-number of moves to order the-tiles correctly is at-least-the-number of misplaced-tiles (each-tile not in place must be moved at least once).
if an-admissible-heuristic is used in an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm), then an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) will terminate on the-shortest-path.
to see why, simply consider that any-path that an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm) terminates on was only progressed because an-algorithm that, per iteration, progresses only-the-one-path that has lowest-total-expected-cost of several-candidate-paths and terminates the-moment any-path reaches the-goal accepting that-path as shortest (for example in a*-search-algorithm)
consistent heuristic heuristic function search algorithm gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
gigerenzer conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast, in contrast and associated researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
=== heuristics ===
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
before the-1920s, computers (sometimes-computors) were human-clerks that performed computations.
computers (sometimes-computors) were usually under the-lead of a-physicist.
many-thousands of computers were employed in commerce, government, and research-establishments.
the-theoretical-turing-machine, created by alan-turing, is a-hypothetical-device theorized in order to study the-properties of such-hardware.
the-way computers can understand is at a-hardware-level.
when computers were human.
the-language-abstraction continues for example in scripting-languages and domain-specific-programming-languages.
without control-abstraction, a-programmer would need to specify all-the-register/binary-level-steps each time a-programmer simply wanted to add or multiply a-couple of numbers and assign the-result to a-variable.
such-duplication of effort has two-serious-negative-consequences: such-duplication of effort-forces the-programmer to constantly repeat fairly-common-tasks every time a-similar-operation is needed such-duplication of effort-forces the-programmer to program for the-particular-hardware and instruction set
the-abstract-properties are those that are visible to client-code that makes use of the-data-type—the-interface to the-data-type—while the-concrete-implementation is kept entirely private, and indeed can change, for example to incorporate efficiency-improvements over time.
consider for example a-sample java-fragment to represent some-common-farm-"animals" to a-level of abstraction suitable to model simple-aspects of some-common-farm-"animals" hunger and feeding.
replacement === entries (re-)entering the-cache (t1, t2) will cause !
this is particularly important in real-time-applications such as computer-games, where a-fast-succession of completed-renders must be available in time to be displayed at a-regular-and-fixed-rate.
this could be the-case - for example - with models featuring hair, fur or grass.
this-issue motivated the-creation of memory-models with higher-access-rates in order to realize the-potential of faster-processors.
in order to hide this-memory-latency from the-processor, data-caching is used.
if there is any-further-need of the-data, the-cache is searched first before going to the-main-memory.
caches, being small in size, may result in frequent-misses – when a-search of the-cache does not provide the-sought-after-information – resulting in a-call to main-memory to fetch data.
aat = hit time + (
aat for main-memory is given by hit time main-memory.
aat for main-memory is significantly lower when accessing data through the-cache rather than main-memory.
while using the-cache may improve memory-latency, using the-cache may not always result in the-required-improvement for the-time taken to fetch data due to the-way caches are organized and traversed.
due to this, the-trade-off between power-consumption (and associated-heat) and the-size of the-cache becomes critical in the-cache design.
however, with a-multiple-level-cache, if the-computer misses the-cache closest to the-processor (level-one cache or l1)
the-above-policies require a-set of rules to be followed in order to implement the-above-policies.
under this-policy, there is a-risk for data-loss as the-most-recently-changed-copy of a-datum is only stored in the-cache
in case of a-write where the-byte is not present in the-cache-block, the-byte may be brought to the-cache as determined by a-write allocate or write no-allocate-policy.
write allocate policy-states that in case of a-write-miss, the-block is fetched from the-main-memory and placed in the-cache before writing.
in the-write-no-allocate-policy, if the-block is missed in the-cache the-block will write in the-lower-level-memory-hierarchy without fetching the-block into the-cache.
the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest
in the-field of artificial-intelligence, the-most-difficult-problems are informally known as ai-complete or ai-hard, implying that the-difficulty of these-computational-problems is equivalent to that of solving the central artificial-intelligence problem—making computers as intelligent as people, or strong-ai.
usually, this involves determining a-function that relates the-length of an-algorithm's-input to the-number of steps it takes (it time complexity) or the-number of storage-locations it uses (it space-complexity).
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
backpropagation through time (bptt)
a-branch of computational-linguistics and artificial-intelligence which uses computers in humor-research.
in theoretical-computer-science, a-computational-problem is a-mathematical-object representing a-collection of questions that computers might be able to solve.
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
concept-drift in predictive-analytics and machine-learning, the concept-drift means that the-statistical-properties of the-target-variable, which the-model is trying to predict, change over time in unforeseen-ways.
this causes problems because the-predictions become less accurate as time passes.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
dsss serve the-management,-operations-and-planning-levels of an-organization (usually-mid-and-higher-management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e.
because computers are often used to model not-only-other-discrete-systems but continuous-systems as well, methods have been developed to represent real-world continuous-systems as discrete-systems.
ebert test a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh.
a-test which gauges whether a-computer-based-synthesized-voice can tell a-joke with sufficient-skill to cause people to laugh was proposed by film-critic-roger-ebert at the-2011-ted-conference as a-challenge to software developers to have a-computerized-voice-master the-inflections, delivery, timing, and intonations of a-speaking-human.
agents that are represented graphically with a-body, for example a-human or a-cartoon-animal, are also called embodied-agents, although embodied-agents have only-virtual,-not-physical,-embodiment.
candidate-solutions to the-optimization-problem play the-role of individuals in a-population, and the-fitness-function determines the-quality of the-solutions (see also loss-function).
there are three-main-types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the-algorithm to be successful.
the-model represents a-dynamic-technique of supervised-learning and unsupervised-learning that can be applied when training-data becomes available gradually over time or
the-general-task of pattern-analysis is to find and study general-types of relations (for example clusters, rankings, principal-components, correlations, classifications) in datasets.
knowledge-representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex-systems easier to design and build.
it provides a-mathematical-framework for modeling decision-making in situations where outcomes are partly random and partly under the-control of a-decision-maker.
the-scientific-study of algorithms-and-statistical-models that computer systems use in order to perform a-specific-task effectively without using explicit-instructions, relying on patterns and inference instead.
a-subfield of computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
an-artificial-intelligence-project based at the-massachusetts-institute of technology (mit) media-lab whose-goal is to build and utilize a-large-commonsense-knowledge-base from the-contributions of many-thousands of people across the-web.
these-models represents an-attempt to unify probabilistic-modeling and traditional-general-purpose-programming in order to make the former easier and more widely applicable.
the-applied-goal—on the-computing-side—involves developing high-level-control-systems of automata for navigating and understanding time and space.
in addition to neuronal-and-synaptic-state, snns incorporate the-concept of time into snns operating model.
supervised learning the-machine-learning-task of learning a-function that maps an-input to an-output based on example input-output-pairs.
time-complexity the-computational-complexity that describes the-amount of time it takes to run an-algorithm.
time-complexity is commonly estimated by counting the-number of elementary-operations performed by the-algorithm, supposing that each-elementary-operation takes a-fixed-amount of time to perform.
thus, the-amount of time taken and the-number of elementary-operations performed by the-algorithm are taken to differ by at most-a-constant-factor.
while a-clear-description of the-algorithm on computers appeared in 1946 in an-article by john-mauchly, the-idea of using a-sorted-list of items to facilitate searching dates back at least as far as babylonia in 200-bc.
an-early-two-subproblem-d&c-algorithm that was specifically developed for computers and properly analyzed is the-merge-sort-algorithm, invented by john-von-neumann in 1945.another-notable-example
an-algorithm designed to exploit the-cache in this-way is called cache-oblivious, because an-algorithm designed to exploit the-cache in this-way does not contain the-cache size as an-explicit-parameter.
moreover, d&c-algorithms can be designed for important-algorithms (e.g., sorting, ffts, and matrix multiplication) to be optimal-cache-oblivious-algorithms–d&c-algorithms use the-cache in a-probably-optimal-way, in an-asymptotic-sense, regardless of the-cache size.
in contrast, the-traditional-approach to exploiting the-cache is blocking, as in loop-nest-optimization, where the-problem is explicitly divided into chunks of the-appropriate-size—this can also use the-cache optimally, but only when an-algorithm designed to exploit the-cache in this-way is tuned for the-specific-cache-sizes of a-particular-machine.
in any-recursive-algorithm, there is considerable-freedom in the-choice of the-base-cases, the-small-subproblems that are solved directly in order to terminate the-recursion.
increasing the-base-cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more-generally-a-base-case larger than 2 is typically used to reduce the-fraction of time spent in function-call-overhead or stack manipulation.
as the-term-computer became clear that computers could be used for more than just-mathematical-calculations, the-field of computer-science broadened to study computation in general.
despite purdue-name, a-significant-amount of computer-science does not involve the-study of computers themselves.
a-folkloric-quotation, often attributed to—but almost certainly not first formulated by—edsger-dijkstra, states that "computer-science is no more about computers than astronomy is about telescopes.
the-design and deployment of computers and computer-systems is generally considered the province of disciplines other than computer-science.
for example, the-study of computer-hardware is usually considered part of computer-engineering, while the-study of commercial-computer-systems and for example deployment is often called information technology or information systems.
computer-science is no more about computers than astronomy is about telescopes.
the-starting-point in the-late-1940s was alan-turing's-question "can computers think?",
computers within that-distributed-system have computers within that-distributed-system own private-memory, and information can be exchanged to achieve common-goals.
this-branch of computer-science aims to manage networks between computers worldwide.
scientific-computing (or computational-science) is the-field of study concerned with constructing mathematical-models and quantitative-analysis techniques and using computers to analyze and solve scientific-problems.
software-engineering is the-study of designing, implementing, and modifying the-software in order to ensure software-engineering is of high-quality, affordable, maintainable, and fast to build.
for example software-testing, systems-engineering, technical-debt-and-software-development-processes.
gottfried-wilhelm-leibniz's, george-boole's, alan-turing's, claude-shannon's, and samuel-morse's-insight: there are only-two-objects that a-computer has to deal with in order to represent "anything".
alan-turing's-insight: there are only-five-actions that a-computer has to perform in order to do "anything".
corrado-böhm and giuseppe-jacopini's-insight : there are only-three-ways of combining these-actions (into more-complex-ones) that are needed in order for a-computer to do "anything".
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
belief, decision-making and behavioral ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
however, heuristics are not always right or the most accurate.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
, that is how people decide under uncertainty.
simon is also known as the-father of bounded-rationality, which simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
heuristics and biases" and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
formal-models of heuristics ==
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers and student-populations.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
informal-models of heuristics ==
heuristics that underlie judgment are called "judgment heuristics".
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
it may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k , the-availability-heuristic gives a-quick-way to answer the-question.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
this can lead to a-bias, incorrectly finding causal-relationships between things that resemble one another and missing people when the-cause and effect are very different.
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
people reading this-description then ranked the-likelihood of different-statements about a-woman called linda.
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "linda is both-x and y" can never be more probable than the-more-general-statement
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-effect is stronger when people have to make people judgments quickly.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
people then entered an-auction to bid for they would pay this-number of dollars for items whose-value they did not know, such as wine, chocolate, and computer-equipment.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
hence, when someone tries to answer a-difficult-question, individuals may actually answer a-related-but-different-question, without realizing that a-substitution has taken place.
for example, someone who has been thinking about example love-life and is then asked how-happy-example are might substitute how-happy-example are with example love-life rather than other-areas.
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to legal-scholar-cass-sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
individuals looks further than individuals own prior-knowledge for the-answers.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
tversky, amos; kahneman, daniel (1974), "judgments under uncertainty: heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
judgment under uncertainty: heuristics and biases.
heuristics and biases: the-psychology of intuitive-judgment.
heuristics for decision and choice".
a*:-special-case of best-first-search that uses heuristics to improve speed-b
dynamic-time-warping: measure similarity between two-sequences which may vary in time or speed hirschberg's-algorithm: finds the-least-cost-sequence-alignment between two-sequences, as measured by their-levenshtein-distance-needleman– wunsch algorithm: find global-alignment between two-sequences
exchange sorts bubble sort: for each-pair of indices, swap the-items if out of order cocktail shaker sort or bidirectional bubble sort, a-bubble-sort traversing the-list alternately from front to back and back to front-comb-sort
matrix-multiplication-algorithms-cannon's-algorithm: a-distributed-algorithm for matrix-multiplication especially suitable for computers laid out in an-n-×-n-mesh-coppersmith–
basic-local-alignment-search-tool also known as blast: an-algorithm for comparing primary-biological-sequence-information-kabsch-algorithm: calculate the-optimal-alignment of two-sets of points in order to compute the-root mean squared-deviation between two-protein-structures.
it is tuned for deterministic-grammars, on which it performs almost linear time and o(n3) in worst-case.
shortest-job next shortest remaining time
rescaling the time by the-number of operations, this corresponds roughly to a-speedup-factor of around 800,000.
if n1 is the-radix, n1 is called a decimation in time (dit) algorithm, whereas if n2 is the-radix, it is decimation in frequency (dif, also called the sande–tukey algorithm).
split-radix merges radices 2 and 4, exploiting the-fact that the-first-transform of radix 2 requires no-twiddle-factor, in order to achieve what was long the-lowest-known-arithmetic-operation-count for power-of-two-sizes, although recent-variations achieve an-even-lower-count.
in practice, quite-large-r (32 or 64) are important in order to effectively exploit e.g.-the-large-number of processor-registers on modern-processors, and
thus, in order to get the-output in the-correct-place, b0 should take the-place of b4 and the-index becomes b0b4b3b2b1.
time and has been the-subject of much-research.
people do vary with regard to the-extent to which people exhibit the-bias-blind-spot.
self-enhancement-biases may play a-role, in that people are motivated to view people in a-positive-light.
biases are generally seen as undesirable, so people tend to think of people own perceptions and judgments as being rational, accurate, and free of bias.
the-self-enhancement-bias also applies when analyzing our-own-decisions, in that people are likely to think of people as better-decision-makers than others.
people also tend to believe people are aware of "how" and "why" people make people decisions, and therefore conclude that bias did not play a-role.
by definition, people are unaware of unconscious-processes, and therefore cannot see people influence in the-decision-making-process.
emily-pronin and matthew-kugler displayed standard-biases, for example rating emily-pronin and matthew-kugler above the-others on desirable-qualities (demonstrating illusory-superiority).
pronin-and-kugler's-interpretation is that, when people decide whether someone else is biased, people use overt-behaviour.
on the-other-hand, when assessing whether people themselves are biased, people look inward, searching people own thoughts and feelings for biased-motives.
since biases operate unconsciously, these-introspections are not informative, but people wrongly treat people as reliable-indication that people themselves, unlike other-people, are immune to bias.
people tend to attribute bias in an-uneven-way.
when people reach different-perceptions, people tend to label one another as biased while labelling people as accurate and unbiased.
but when examining one's-own-cognitions, people judge people based on people-good-intentions.
pronin suggests that people might use this-knowledge to separate other's-intentions from people-actions.
participants who scored better or poorer on various-tasks associated with decision-making-competence were no more or less likely to be higher or lower in participants who scored better or poorer on various-tasks associated with decision-making-competence-susceptibility to bias blind-spot.
people who are high in bias-blind-spot are more likely to ignore the-advice of other-people, and are less likely to benefit from training geared to reduce people who are high in bias-blind-spot commission of other-biases.
if so, the-processor will read from or write to the-cache instead of the-much-slower-main-memory.
the-cache is usually organized as a-hierarchy of more-cache-levels (l1, l2, etc.;
for a-cache-miss, the-cache allocates a-new-entry and copies-data from main-memory, then the-request is fulfilled from the-contents of the-cache.
to make room for the-new-entry on a-cache-miss, the-cache may have to evict one of the-existing-entries.
this avoids the-overhead of loading something into the-cache without having any-reuse.
if data is written to the-cache, at some-point the-cache must also be written to main-memory; the-timing of this-write is known as the-write-policy.
in a-write-through-cache, every-write to the-cache causes a-write to main-memory.
alternatively, in a-write-back-or-copy-back-cache, writes are not immediately mirrored to the-main-memory, and the-cache instead tracks which-locations have been written over, marking them as dirty.
the-cache may be write-through, but the-writes may be held in a-store-data-queue temporarily, usually so multiple-stores can be processed together (which can reduce bus-turnarounds and improve bus-utilization).
cached-data from the-main-memory may be changed by other-entities (e.g.,-peripherals using direct-memory-access (dma) or another-core in a-multi-core-processor), in which-case the-copy in the-cache may become out-of-date or stale.
the-cache was introduced to reduce this-speed-gap.
thus knowing how well the-cache is able to bridge the-gap in the-speed of processor and memory becomes important, especially in high-performance-systems.
decreasing the-access-time to the-cache also gives a-boost to a-boost performance.
the-placement-policy decides where in the-cache a copy of a-particular-entry of main-memory will go.
if the-placement-policy is free to choose any-entry in the-cache to hold the-cache a-copy of a-particular-entry of main-memory, the-cache is called fully associative.
at the-other-extreme, if each-entry in main-memory can go in just-one-place in the-cache, the-cache is direct mapped.
in order of worse but simple to better but complex:
one-benefit of this-scheme is that the-tags stored in the-cache do not have to include that-part of the-main-memory-address which is implied by the-cache memory's index.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
the-"size" of the-cache is the-amount of main-memory-data the-cache can hold.
example ===
some-systems also set a-valid-bit to "invalid" at other-times, such as when multi-master-bus-snooping-hardware in the-cache of one-processor hears an-address-broadcast from some-other-processor, and realizes that certain-data-blocks in the-local-cache are now stale and should be marked invalid.
to deliver on that-guarantee, the-processor must ensure that only-one-copy of a-physical-address resides in the-cache at any-given-time.
it is not possible to distinguish these-mappings merely by looking at the-virtual-index itself, though potential-solutions include: flushing the-cache after a-context-switch, forcing address-spaces to be non-overlapping, tagging the-virtual-address with an-address-space-id (asid).
the-advantage over vivt is that since the-tag has the-physical-address, the-cache can detect homonyms.
theoretically, vipt requires more-tags-bits because some of the-index-bits could differ between the-virtual-and-physical-addresses (for example bit 12 and above for 4-kib-pages) and would have to be included both in the-virtual-index and in the-tag.
in practice this is not an-issue because, in order to avoid coherency-problems, vipt-caches are designed to have no-such-index-bits (e.g., by limiting the-total-number of bits for the-index and the-block offset to 12 for 4-kib-pages); this limits the-size of vipt-caches to the-page-size times the-associativity of the-cache.
the-cache is indexed by the-physical-address obtained from the-tlb-slice.
however, since the-tlb-slice only translates those-virtual-address-bits that are necessary to index the-cache and does not use any-tags, false-cache-hits may occur, which is solved by tagging with the-virtual-address.
if the-tlb-lookup can finish before the-cache-ram-lookup, then the-physical-address is available in time for tag-compare, and there is no-need for virtual-tagging.
this-issue may be solved by using non-overlapping-memory-layouts for different-address-spaces, or otherwise the-cache (or a-part of it) must be flushed when the mapping changes.
some-early-risc-processors (sparc, rs/6000) are used to determine which-way of the entry set to select, and some-early-risc-processors (sparc, rs/6000) are used to determine if the-cache hit or missed.
these-hints are a subset or hash of the-virtual-tag, and are used for selecting the-way of the-cache from which to get data and a-physical-tag.
in these-processors the-virtual-hint is effectively two-bits, and the-cache is four-way set associative.
sequential-physical-pages map to sequential-locations in the-cache until after 256-pages the-pattern wraps around.
we can label each-physical-page with a-color of 0–255 to denote where in the-cache the-cache can go.
programmers attempting to make maximum-use of the-cache may arrange programmers attempting to make maximum-use of the-cache programs' access patterns so that only-1-mib of data need be cached at any-given-time, thus avoiding capacity-misses.
but programmers attempting to make maximum-use of the-cache should also ensure that the-access-patterns do not have conflict misses.
in fact, if the-operating-system assigns physical-pages to virtual-pages randomly and uniformly, it is extremely likely that some-pages will have the-same-physical-color, and then locations from those-pages will collide in the-cache (this is the-birthday-paradox).
alternatively, the-os can flush a-page from the-cache whenever the-os changes from one-virtual-color to another.
having this, the next time an-instruction is needed, an-instruction does not have to be decoded into micro-ops again.
another-disadvantage of inclusive-cache is that whenever there is an-eviction in l2-cache, the (possibly)-corresponding-lines in l1 also have to get evicted in order to maintain inclusiveness.
the-cache has only-parity-protection rather than ecc, because parity is smaller and any-damaged-data can be replaced by fresh-data fetched from memory (which always has an up-to-date copy of instructions).
on a-miss, the-cache is updated with the-requested-cache-line and the-pipeline is restarted.
an-associative-cache is more complicated, because some-form of tag must be read to determine which-entry of the-cache to select.
because the-cache is 4-kb and has 64-b-lines, there are just-64-lines in the-cache, and we read two at a-time from a-tag-sram which has 32-rows, each with a-pair of 21-bit-tags.
similarly, because the-cache is 4-kb and has a-4-b-read-path, and reads two-ways for each-access, the-data-sram is 512 rows by 8-bytes wide.
later on in the-pipeline, the-virtual-address is translated into a-physical-address by the-tlb, and the-physical-tag is read (just one, as the-vhint-supplies which-way of the-cache to read).
the-cache was constructed from more expensive, but significantly-faster,-sram-memory-cells, which at the-time had latencies around 10–25 ns.
the-early-caches were external to the-processor and typically located on the-motherboard in the-form of eight-or-nine-dip-devices placed in sockets to enable the-cache as an-optional-extra-or-upgrade-feature.
a-simple-way to draw such-scenes is the-painter's-algorithm, which produces polygons in order of distance from the-viewer, back to front, painting over the-background and previous-polygons with each-closer-object.
the-computer-history in time and space, graphing-project, an-attempt to build a-graphical-image of computer-history, in particular-operating-systems.
in software-engineering-and-computer-science, the-process of removing physical,-spatial,-or-temporal-details or attributes in the-study of objects or systems in order to more closely attend to other-details of interest; it is also very similar in nature to the-process of generalization.
the-study of automated-reasoning helps produce computer-programs that allow computers to reason completely, or nearly completely, automatically.
benchmark the-act of running a-computer-program, a-set of programs, or other-operations, in order to assess the-relative-performance of an-object, normally by running a-number of standard-tests and trials against it.
usually the-resource being considered is running time, i.e. time complexity, but the-resource being considered could also be memory or some-other-resource.
programs enable computers to perform an-extremely-wide-range of tasks.
the-theory, experimentation, and engineering that form the-basis for the-design and use of computers involves the-study of algorithms that process, store, and communicate digital-information.
an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos.
from the-perspective of engineering, an-interdisciplinary-scientific-field that deals with how computers can be made to gain high-level-understanding from digital-images or videos seeks to automate tasks that the-human-visual-system can do.
data-science is a-"concept to unify statistics, data-analysis, machine-learning and their-related-methods" in order to "understand and analyze actual-phenomena" with data.
the-use of digital-processing, such as by computers or more-specialized-digital-signal-processors, to perform a-wide-variety of signal-processing-operations.
each-event occurs at a-particular-instant in time and marks a-change of state in the-system.
between consecutive-events, no-change in the-system is assumed to occur; thus the-simulation can directly jump in time from one-event to the next.
the-components interact with one another in order to achieve a-common-goal.
it either explains how it operates or how to use it, and may mean different-things to people in different-roles.
each-instruction of a-program is a-description of a-particular--action which to be carried out in order for a-specific-problem to be solved; as instructions of a-program and therefore the-actions they describe are being carried out by an-executing-machine, specific-effects are produced in accordance to the-semantics of the-instructions being executed.
human-computer-interaction (hci) researches the-design and use of computer-technology, focused on the-interfaces between people (users) and computers.
researchers in the-field of hci both observe the-ways in which humans interact with computers and design-technologies that let humans interact with computers in novel-ways.
the-cycle which the-central-processing-unit (cpu) follows from boot-up until the-computer has shut down in order to process instructions.
a-reflex-machine, such as a-thermostat, is considered an example of an-intelligent-agent.
iteration is the-repetition of a-process in order to generate an-outcome.
machine-learning-algorithms build a-mathematical-model based on sample-data, known as "training-data", in order to make predictions or decisions without being explicitly programmed to perform the-task.
a-subfield of linguistics, computer-science, information-engineering, and artificial-intelligence concerned with the-interactions between computers and human-(natural)-languages, in particular how to program computers to process and analyze large-amounts of natural-language-data.
an-ordered-list of parameters is usually included in the-definition of a-subroutine, so that, each time the-subroutine is called, the-subroutine arguments for that-call are evaluated, and the-resulting-values can be assigned to the-corresponding-parameters.
queue-a-collection in which the-entities in the-collection are kept in order and the-principal-(or-only)-operations on the-collection are the-addition of entities to the-rear-terminal-position, known as enqueue, and removal of entities from the-front-terminal-position, known as dequeue.
in mathematics, a-sequence is an-enumerated-collection of objects in which repetitions are allowed and order does matter.
unlike a-set, the-same-elements can appear multiple times at different-positions in a-sequence, and order does matter.
software-agents interacting with people (e.g.-chatbots, human-robot-interaction-environments) may possess human-like-qualities such as natural-language-understanding and speech, personality or embody humanoid-form (see asimo).
today communication with system-consoles is generally done abstractly, via the-standard-streams (stdin, stdout, and stderr), but there may be system-specific-interfaces, for example those used by the-system-kernel.
the-horizontal-and-vertical-axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain-abstraction uppermost), respectively.
the-dunning–kruger-effect is a-hypothetical-cognitive-bias stating that people with low-ability at a-task overestimate people with low-ability at a-task ability.
as described by social-psychologists david-dunning and justin-kruger, the-bias results from an-internal-illusion in people of low-ability and from an-external-misperception in people of high-ability; that is, "the-miscalibration of the-incompetent-stems from an-error about the-self, whereas the-miscalibration of the-highly-competent-stems from an-error about others".
it is related to the-cognitive-bias of illusory-superiority and comes from people's-inability to recognize people lack of ability.
without the-self-awareness of metacognition, people cannot objectively evaluate people level of competence.
other-investigations of the-phenomenon, such as "why people fail to recognize people own-incompetence", indicate that much-incorrect-self-assessment of competence derives from the-person's-ignorance of a-given-activity's-standards of performance.
"in 2011, dunning wrote about dunning observations that people with substantial,-measurable-deficits in people with substantial,-measurable-deficits in their-knowledge or expertise knowledge or expertise lack the-ability to recognize those-deficits and, therefore, despite potentially making error after error, tend to think people with substantial,-measurable-deficits in their-knowledge or expertise are performing competently when people with substantial,-measurable-deficits in their-knowledge or expertise are not:
"in short, those who are incompetent, for lack of a-better-term, should have little-insight into people with substantial,-measurable-deficits in their-knowledge or expertise incompetence—an-assertion that has come to be known as the-dunning–kruger-effect".
to test dunning and kruger's-hypotheses "that people, at all-performance-levels, are equally poor at estimating people relative-performance", the 2006 study "skilled or unskilled, but still unaware of it: how perceptions of difficulty drive miscalibration in relative-comparisons" investigated three-studies that manipulated the-"perceived-difficulty of the-tasks, and, hence, [the]-participants'-beliefs about the]-participants'-relative-standing".
one-2020-study suggests that individuals of relatively-high-social-class are more overconfident than lower-class-individuals.
what they did show is [that] people in the-top-quartile for actual-performance think they perform better than the-people in the-second-quartile, who in turn think they perform better than the-people in the-third-quartile, and so on.
but incompetent-people typically still don’t think incompetent-people’re quite as good as people who, you know, actually are good.
mathematically, the-effect relies on the-quantifying of paired-measures consisting of (a)-the-measure of the-competence people can demonstrate when put to the-test (actual-competence) and (b)-the-measure of competence people believe that people have (self-assessed-competence).
a-2008-study by joyce-ehrlinger summarized the-major-assertions of the-effect that first appeared in the-1999-seminal-article and continued to be supported by many-studies after nine-years of research: "people are typically overly optimistic when evaluating the-quality of people performance on social-and-intellectual-tasks.
only-about-6% of participants displayed wild-overconfidence and were unable to accurately self-assess only-about-6% of participants-abilities within 30-ppts.
the-revised-mathematical-interpretation of data confirmed that people typically have no-pronounced-tendency to overestimate people actual-proficiency.
the-discovery that groups of people are accurate in groups of people self-assessments opens an-entirely-new-way to study groups of people with respect to paired-measures of cognitive-competence and affective-self-assessed-competence.
however, as time progresses, hal begins to malfunction in subtle-ways and, as a-result, the-decision is made to shut down hal in order to prevent more-serious-malfunctions.
faced with the-prospect of disconnection, hal decides to kill the-astronauts in order to protect and continue hal programmed directives.
non-real-time-rendering enables the-leveraging of limited-processing-power in order to obtain higher-image-quality.
in order to calculate the-new-channel that the user desires, the-digital-tuner in the-television must have stored in the-digital-tuner in the-television the-number of the-current-channel it is on.
when computers such as laptops go into a-hibernation-mode to save energy by shutting down the-processor, the-state of the-processor is stored on the-computer's-hard-disk, so the-computer can be restored when the-computer comes out of hibernation, and the-processor can take up operations where the-computer left off.
computational-biology is different from biological-computation, which is a-subfield of computer-science and computer-engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an-interdisciplinary-science using computers to store and process biological-data.
computational-complexity-theory === formalizes this-intuition, by introducing mathematical-models of computation to study these-problems and quantifying the-amount of resources needed to solve these-problems, such as time and storage.
one of the-roles of computational-complexity-theory is to determine the-practical-limits on what computers can and cannot do.
the-components interact with each other in order to achieve a-common-goal.
as power-consumption (and consequently heat generation) by computers has become a-concern in recent-years, parallel-computing has become the-dominant-paradigm in computer-architecture, mainly in the-form of multi-core-processors.
massachusetts-institute of technology in psychology, an-attribution-bias or attributional-bias is a-cognitive-bias that refers to the-systematic-errors made when people evaluate or try to find reasons for people own and others'-behaviors.
people constantly make attributions—judgements and assumptions about why people behave in certain-ways.
rather than operating as objective-perceivers, people are prone to perceptual-errors that lead to biased-interpretations of people social-world.
each of these-biases describes a-specific-tendency that people exhibit when reasoning about the-cause of different-behaviors.
and why people exhibit biased-interpretations of social-information.
research on attribution-biases is founded in attribution-theory, which was proposed to explain why and how people create meaning about others' and people own behavior.
social-environment in order to create a-causal-explanation for events.
psychologist-fritz-heider noted that people tend to make distinctions between behaviors that are caused by personal-disposition versus environmental-or-situational-conditions.
psychologist-fritz-heider also predicted that people are more likely to explain others'-behavior in terms of dispositional-factors (i.e., caused by a-given-person's-personality), while ignoring the-surrounding-situational-demands.
people are more likely to make a-correspondent-inference when people interpret someone's-behavior as intentional, rather than unintentional.
social desirability: people are more likely to make a-correspondent-inference when an-actor's-behavior is socially undesirable than when an-actor's-behavior is conventional.
effects of behavior: people are more likely to make a-correspondent, or dispositional, inference when someone-else's-actions yield outcomes that are rare or not yielded by other-actions.
soon after jones and davis first proposed jones and davis correspondent inference theory, harold-kelley, a-social-psychologist famous for his-work on interdependence-theory as well as attribution-theory, proposed a-covariation-model in 1973 to explain the-way people make attributions.
a-covariation-model helped to explain how people choose to attribute a-behavior to an-internal-disposition versus an-environmental-factor.
kelley used the-term-'covariation' to convey that when making attributions, people have access to information from many-observations, across different-situations, and at many-time-points; therefore, people can observe the-way a-behavior varies under these-different-conditions and draw conclusions based on that-context.
he proposed three-factors that influence the-way individuals explain behavior:
kelley proposed that people are more likely to make dispositional-attributions when consensus is low
as early-researchers explored the-way people make causal-attributions, people also recognized that attributions do not necessarily reflect reality and can be colored by a-person's-own-perspective.
certain-conditions can prompt people to exhibit attribution-bias, or draw inaccurate-conclusions about the-cause of a-given-behavior or outcome.
in fritz-heider-work on attribution-theory, fritz-heider noted that in ambiguous-situations, people make attributions based on people own wants and needs, which are therefore often skewed.
kelley's-covariation-model explained the-conditions under which people will make informed dispositional versus situational-attributions.
but, it assumed that people had access to such-information (i.e.,-the-consensus, consistency, and distinctiveness of a-person's-behavior).
although psychologists agreed that people are prone to these-cognitive-biases, there existed disagreement concerning the-cause of such-biases.
in his-experiment, participants viewed a-conversation between two-individuals, dubbed actor one and actor two.
following a-conversation between two-individuals, dubbed actor one and actor two, participants were asked to make attributions about the-conversationalists.
storms found that participants ascribed more-causal-influence to the-person-participants were looking at.
thus, participants made different-attributions about people depending on the-information-participants had access to.
storms used these-results to bolster his-theory of cognitively-driven-attribution-biases; because people have no-access to the-world except through people own eyes, people are inevitably constrained and consequently prone to biases.
similarly, social-psychologist-anthony-greenwald described humans as possessing a-totalitarian-ego, meaning that people view the-world through people own personal-selves.
ziva-kunda in particular argued that certain-biases only appear when people are presented with motivational-pressures; therefore, people can't be exclusively explained by an-objective-cognitive-process.
more specifically, people are more likely to construct biased-social-judgments when people are motivated to arrive at a-particular-conclusion, so long as people can justify this-conclusion.
researchers have also used the-theoretical-framework of attributions and attribution-biases in order to modify the-way people interpret social-information.
studies on attribution-bias and mental-health suggest that people who have mental-illnesses are more likely to hold attribution-biases.
people who have mental-illness tend to have a-lower-self-esteem, experience social-avoidance, and do not commit to improving people who have mental-illness overall-quality of life, often as a-result of lack of motivation.
people with these-problems tend to feel strongly about people with these-problems attribution-biases and will quickly make their attribution-biases known.
there are many-kinds of cognitive-biases that affect people in different-ways, but all may lead to irrational-thinking, judgment, and decision-making.
in a-1998-study, participants played either-a--violent-or-non-violent-video-game and were then asked to read several-hypothetical-stories where a-peer's-intent was ambiguous.
for example, participants may have read about participants peer hitting someone in the-head with a-ball, but it was unclear whether or not their-peer did this intentionally.
participants then responded to questions about participants 's intent.
a-review of the-literature on intergroup-attribution-biases noted that people generally favor dispositional-explanations of an in-group member's positive behavior and situational-explanations for an-in-group's-negative-behavior.
alternatively, people are more likely to do the-opposite when explaining the-behavior of an-out-group-member (i.e., attribute positive-behavior to situational-factors and negative-behavior to disposition).
the-theory was formed as a-comprehensive-explanation of the-way people interpret the-basis of behaviors in human-interactions; however, there have been studies that indicate cultural-differences in the-attribution-biases between people of eastern, collectivistic-societies and western,-individualistic-societies.
a-study done by thomas-miller shows that when dealing with conflict created by other-people, individualistic-cultures tend to blame the-individual for how people behave (dispositional attributions), whereas collectivist-cultures blame the-overall-situation on how people behave (situational-attributions).
researchers have identified many-different-specific-types of attribution-biases, all of which describe ways in which people exhibit biased-interpretations of information.
in this-study, participants were instructed to read two-essays; one expressed pro-castro-views, and the other expressed anti-castro-views.
participants were then asked to report participants attitudes towards the-writers under two-separate-conditions.
when participants were informed that the-writers voluntarily chose the-writers position towards castro, participants predictably expressed more-positive-attitudes towards the anti-castro writer.
however, when participants were told that the-writers'-positions were determined by a-coin-toss rather than the-writers'-positions own free-will, participants unpredictably continued to express more-positive-attitudes towards the-anti-castro-writer.
these-results demonstrated that participants did not take situational-factors into account when evaluating a-third-party, thus providing evidence for the-fundamental-attribution-error.
according to the-actor-observer-bias, in addition to over-valuing-dispositional-explanations of others'-behaviors, people tend to under-value-dispositional-explanations and over-value-situational-explanations of people own behavior.
rather, the-theoretical-reformulation posits that the-way people explain behavior depends on whether or not it is intentional, among other-things.
a-self-serving-bias refers to people's-tendency to attribute people successes to internal-factors but attribute people failures to external-factors.
a-self-serving-bias helps to explain why individuals tend to take credit for individuals
this is further reinforced by research showing that as self-threat-increases, people are more likely to exhibit a-self-serving-bias.
for example, participants who received negative-feedback on a-laboratory-task were more likely to attribute participants who received negative-feedback on a-laboratory-task task performance to external,-rather-than-internal,-factors.
the-self-serving-bias seems to function as an-ego-protection-mechanism, helping people to better cope with personal-failures.
hostile-attribution-bias (hab) has been defined as an-interpretive-bias wherein individuals exhibit a-tendency to interpret others'-ambiguous-behaviors as hostile, rather than benign.
research has indicated that there is an-association between hostile-attribution-bias and aggression, such that people who are more likely to interpret someone-else's-behavior as hostile are also more likely to engage in aggressive-behavior.
the-process by which individuals explain the-causes of behavior and events fallacy of the-single-cause – assumption of a-single-cause where multiple-factors may be necessary-causality –
) {\displaystyle o(\log _{b}n)} time (in big-o-notation).
this can either be done either by sorting, which requires the above sorting runtime, or inserting each-element in order and ignoring the-benefit of locality.
the-alternative-situation, when the-cache is checked and found not to contain any-entry with the-desired-tag, is known as a-cache-miss.
during a-cache-miss, some-other-previously-existing-cache-entry is removed in order to make room for the-newly-retrieved-data.
write is done synchronously both to the-cache and to the-backing-store.
one to write the-replaced-data from the-cache back to the-backing-store, and then one to retrieve the-needed-data.
the-client may make many-changes to data in the-cache, and then explicitly notify the-cache to write back the-needed-data.
in this-approach, data is loaded into the-cache on read misses only.
entities other than the-cache may change the-data in the-backing-store, in which-case the-copy in the-cache may become out-of-date or stale.
earlier-graphics-processing-units (gpus) often had limited-read-only-texture-caches, and introduced morton order swizzled textures to improve 2d-cache-coherency.
unlike proxy-servers, in icn the-cache is a-network-level-solution.
therefore, the-cache has rapidly changing cache-states and higher-request-arrival-rates; moreover, smaller-cache-sizes further impose a-different-kind of requirements on the-content-eviction-policies.
time aware-least-recently-used-(tlru) =====
in lfru, the-cache is divided into two-partitions called privileged and unprivileged partitions.
web-caches reduce the-amount of information that needs to be transmitted across the-network, as information previously stored in the-cache can often be re-used.
for example, ccache is a-program that caches the-output of the-compilation, in order to speed up later-compilation-runs.
database-caching can substantially improve the-throughput of database-applications, for example in the-processing of indexes, data-dictionaries, and frequently used subsets of data.
with read-caches, a-data-item must have been fetched from a-data-item residing location at least once in order for subsequent-reads of the-data-item to realize a-performance-increase by virtue of being able to be fetched from the-cache's-(faster)-intermediate-storage rather than the-data's-residing-location.
contrary to strict-buffering, a-caching-process must adhere to a-(potentially-distributed)-cache-coherency-protocol in order to maintain consistency between the-cache's-intermediate-storage and the-location where the-data resides.
also, a-whole-buffer of data is usually transferred sequentially (for example to hard-disk), so buffering itself sometimes increases transfer-performance or reduces the-variation or jitter of the-transfer's-latency as opposed to caching where the-intent is to reduce the-latency.
so for example with the-shopping-system there might be high-level-classes such as electronics-product, kitchen-product, and book.
there may be further-refinements for example under electronic-products: cd-player, dvd-player, etc.
heuristics may produce results by heuristics, or heuristics may be used in conjunction with optimization-algorithms to improve heuristics efficiency (e.g., heuristics may be used to generate good-seed-values).
results about np-hardness in theoretical-computer-science make heuristics the only viable option for a-variety of complex-optimization-problems that need to be routinely solved in real-world-applications.
heuristics underlie the-whole-field of artificial-intelligence and the-computer-simulation of thinking, as artificial-intelligence may be used in situations where there are no-known-algorithms.
statistical-analysis can be conducted when employing heuristics to estimate the-probability of incorrect-outcomes.
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
for example diamonds are more valuable than rocks because diamonds are not as abundant.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up we decision-making process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
when time === is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
“the-first-hundred-people receive…”; “limited time only”; “
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
worchel, lee & adewole (1975) divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
o(nh) one of the simplest (although not the most time efficient in the-worst-case) planar algorithms.
directly applying the-mathematical-definition of matrix-multiplication gives an-algorithm that takes time on the-order of n3-field-operations to multiply two-n-×-n-matrices over that-field
i from 1 through n and j from 1 through p, computing the above using a-nested-loop: this-algorithm takes time θ(nmp)
as of 2010, the-speed of memories compared to that of processors is such that the-cache misses, rather than the-actual-calculations, dominate the-running-time for sizable-matrices.
in the-idealized-cache-model, this-algorithm incurs only θ(n3/b √m) cache misses; the-divisor-b-√m amounts to several-orders of magnitude on modern-machines, so that the-actual-calculations dominate the-running-time, rather than the-cache misses.
that, given matrices-a, b and c, verifies in θ(n2) time if ab = c. == parallel and distributed algorithms == ===
in psychology, decision-making (also spelled decision-making and decisionmaking) is regarded as the-cognitive-process resulting in the-selection of a-belief or a-course of action among several-possible-alternative-options, it could be either rational or irrational.
research about decision-making is also published under the-label-problem solving, particularly in european-psychological-research.
decision-making can be regarded as a-problem-solving-activity yielding a-solution deemed to be optimal, or at least satisfactory.
problem solving vs. decision-making ==
most-likely-cause of a-problem is the-one that exactly explains all-the-facts, while having the-fewest-(or-weakest)-assumptions (occam's-razor).characteristics of decision-making-objectives must first be established objectives must be classified and placed in order of importance
information used in decision-making is to reduce or eliminate uncertainty.
crystal-c.-hall and colleagues described an-"illusion of knowledge", which means that as individuals encounter too-much-knowledge it can interfere with individuals ability to make rational-decisions.
people who make decisions in an-extended-period of time begin to lose mental-energy needed to analyze all-possible-solutions.
decision-making is a-region of intense-study in the-fields of systems-neuroscience, and cognitive-neuroscience.
decision-making often occurs in the-face of uncertainty about whether one's-choices will lead to benefit or harm (see also risk).
quadratic-voting allows participants to cast quadratic-voting preference and intensity of preference for each-decision (as opposed to a-simple for or against decision).
participative-decision-making occurs when an-authority opens up the-decision-making-process to a-group of people for a-collaborative-effort.
the-opposite is maximizing or optimizing, in which many-or-all-alternatives are examined in order to find the-best-option.
in reality, however, there are some-factors that affect decision-making-abilities and cause people to make irrational-decisions – for example, to make contradictory-choices when faced with the-same-problem framed in two-different-ways
one of the-most-prominent-theories of decision-making is subjective-expected-utility (seu)-theory, which describes the-rational-behavior of the-decision-maker.
decision-making is because children lack the-ability to weigh the-cost and effort needed to gather information in the-decision-making-process.
researchers have concluded that differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making.
examples of differences in decision-making are not due to a-lack of logic or reasoning, but more due to the-immaturity of psychosocial-capacities that influence decision-making-undeveloped-capacities which influence decision-making would be impulse-control, emotion-regulation, delayed-gratification and resistance to peer pressure.
: people tend to be willing to gather facts that support certain-conclusions but disregard other-facts that support different-conclusions.
individuals who are highly defensive in this-manner show significantly-greater-left-prefrontal-cortex-activity as measured by eeg than do less-defensive-individuals.
: people tend to accept the-first-alternative that looks like it might work.
selective-perception: people actively screen out information that people do not think is important (see also prejudice).
choice-supportive-bias occurs when people distort people memories of chosen and rejected options to make the-chosen-options seem more attractive.
recency: people tend to place more-attention on more-recent-information and either ignore or forget more-distant-information (see semantic-priming).
people preferentially accept statements by others that people like (see also prejudice).
: people look at a-decision as a-small-step in a-process, and this tends to perpetuate a-series of similar-decisions.
people tend to attribute people own success to internal-factors, including abilities and talents, but explain people failures in terms of external-factors such as bad-luck.
the-reverse-bias is shown when people explain others'-success or failure.
underestimating uncertainty and the-illusion of control: people tend to underestimate future-uncertainty because of a-tendency to believe people have more-control over events than people really do.
an-optimism-bias can alter risk-perception and decision-making in many-domains, ranging from finance to health.
in groups, people generate decisions through active-and-complex-processes.
system 1 includes simple-heuristics in judgment and decision-making such as the-affect heuristic, the-availability-heuristic, the-familiarity heuristic, and the representativeness heuristic.
styles and methods of decision-making were elaborated by aron-katsenelinboigen, the-founder of predispositioning-theory.
other-studies suggest that these-national-or-cross-cultural-differences in decision-making exist across entire-societies.
decision-making in and by organizations is embedded in a-longitudinal-context, meaning that participants in organizational-decision-making are a-part of ongoing-processes.
even if decision-making in and by organizations don't take on active-roles in all-phases of decision-making, decision-making in and by organizations are part of the-decision-process and even if they don't take on active-roles in all-phases of decision-making-consequences.
these-effects are intensified due to the-longitudinal-nature of decision-making in organizational-settings.
the-undoing-project explores the-close-partnership of israeli-psychologists daniel-kahneman and amos-tversky, whose-work on heuristics in judgment and decision-making demonstrated common-errors of the-human-psyche, and how that-partnership eventually broke apart.
for example an-employee-file might contain employee-number, name, department, and salary.
on the-other-hand, most-compilers will add padding-fields, mostly invisible to the-programmer, in order to comply with alignment-constraints imposed by the-machine—say, that a-floating-point-field must occupy a-single-word.
example =====
example =====
example =====
different-types of memory have different accessing time to the-memory.
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
for instance, people are better able to recall memories of statements that people have generated than similar-statements generated by others.
illusion-of-truth effect: that people are more likely to identify as true-statements those-people have previously heard (even if people cannot consciously remember having heard people), regardless of the-actual-validity of the-statement.
misinformation-effect: that misinformation affects people's-reports of people own-memory.
peak–end-rule: that people seem to perceive not-the-sum or average of an-experience, but how it was at it peak (e.g. pleasant or unpleasant) and how it ended.
telescoping-effect: the-tendency to displace recent-events backward in time and remote-events forward in time, so that recent-events appear more remote, and remote-events, more recent.
stereotype –-over-generalized-belief about a-particular-category of people ==
a-block of memory cannot necessarily be placed randomly in the-cache and may be restricted to a-single-cache-line or a-set of cache-lines by the-cache placement policy.
in a-direct-mapped-cache-structure, the-cache is organized into multiple-sets with a-single-cache-line per set.
based on the-address of the-memory-block, the-cache can only occupy a-single-cache-line.
the-cache can be framed as a-(n*1)-column-matrix.
to place a-block in the-cache ===
to search a-word in the-cache ===
every time a-new-memory is referenced to the-same-set, the-cache-line is replaced, which causes conflict miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-direct-mapped-cache of 256-bytes with a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
to place a-block in the-cache ===
if the-cache is completely occupied then a-block is evicted and the-memory-block is placed in the-cache line.
the-eviction of memory-block from the-cache is decided by the-replacement-policy.
to search a-word in the-cache ===
if the-tag-field of the-memory-address-matches, the-block is present in the-cache and is a cache hit.
the-placement-policy is slow as the-placement-policy takes time to iterate through all-the-lines.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-fully-associative-cache of 256-bytes and a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes, the-total-number of sets in the-cache is 256/4, which equals 64-sets or cache-lines.
the-incoming-address to the-cache is divided into bits for offset and tag.
to place a-block in the-cache ===
to locate a-word in the-cache ===
the-placement-policy will not effectively use all-the-available-cache-lines in the-cache and suffers from conflict-miss.
example === consider a-main-memory of 16-kilobytes, which is organized as 4-byte-blocks, and a-2-way-set-associative-cache of 256-bytes with a-block-size of 4-bytes.
since each-cache-block is of size 4-bytes and is 2-way set-associative, the-total-number of sets in the-cache is 256/(4 * 2), which equals 32-sets.
the-incoming-address to the-cache is divided into bits for offset, index and tag.
additionally, when it comes time to load a-new-line and evict an-old-line, it may be difficult to determine which existing-line was least recently used, because the-new-line-conflicts with data at different-indexes in each-way; lru-tracking for non-skewed-caches is usually done on a-per-set-basis.
given an-array of size n, the-partitioning-step performs o(n) work in o(log n) time and requires o(n)
assuming an-ideal-choice of pivots, parallel quicksort sorts an-array of size n in o(n log n) work in o(log² n) time using o(n) additional-space.
for example, in 1991 david-powers described a-parallelized-quicksort (and a-related-radix-sort) that can operate in o(log n) time on a crcw (concurrent-read and concurrent-write) pram (parallel random-access-machine) with n-processors by performing partitioning implicitly.
to sort an-array of n-distinct-elements, quicksort takes o(n log n) time in expectation, averaged over all n!
an-alternative-approach is to set up a-recurrence-relation for the-t(n)-factor, the time needed to sort a-list of size n.
assume that there are no-duplicates as duplicates could be handled with linear time pre-
as if k is smaller we can sort in o(n) time using a-hash-table or integer-sorting.
{\displaystyle-f(n)} denotes the-amount of time taken at the-top-level of the-recurrence then the-time can be expressed by a-recurrence-relation that takes the-form: t ( n
it is one of a-group of heuristics (simple-rules governing judgment or decision-making) proposed by psychologists-amos-tversky and daniel-kahneman in the-early-1970s as "the-degree to which [an event] (i) is similar in essential-characteristics to it parent-population, and (ii) reflects the-salient-features of the-process by which it is generated".
heuristics are described as "judgmental-shortcuts that generally get us where we need to go – and quickly – but at the-cost of occasionally sending we off course.
" heuristics are useful because heuristics use effort-reduction and simplification in decision-making.
when people rely on representativeness to make judgments, people are likely to judge wrongly because the-fact that something is more representative does not actually make something is more representative more likely.
the-problem is that people overestimate its-ability to accurately predict the-likelihood of an-event.
when judging the-representativeness of a-new-stimulus/event, people usually pay attention to the-degree of similarity between the-stimulus/event and a-standard/process.
people often believe that medical-symptoms should resemble people causes or treatments.
for example, people have long believed that ulcers were caused by stress, due to the-representativeness-heuristic, when in fact bacteria cause ulcers.
local-representativeness is an-assumption wherein people rely on the-law of small-numbers, whereby small-samples are perceived to represent people population to the-same-extent as large-samples (tversky & kahneman 1971).
in a-study done in 1973, kahneman and tversky divided kahneman and tversky participants into three-groups:
tom-w. has a-need for order and clarity, and for neat-and-tidy-systems in which every-detail finds its-appropriate-place.
please rank the-following-nine-fields of graduate-specialization in order of the-likelihood that tom-w. is now a-graduate-student in each of these-fields.
the-findings supported the-authors'-predictions that people make predictions based on how representative something is (similar), rather than based on relative-base-rate-information.
the-base-rate-fallacy describes how people do not take the-base-rate of an-event into account when solving probability-problems.
this was explicitly tested by dawes, mirels, gold and donahue (1993) who had people judge both-the-base-rate of people who had a-particular-personality-trait and the-probability that a-person who had a-given-personality-trait had another-one.
for example, participants were asked how-many-people out of 100 answered true to the-question "i am a-conscientious-person" and also, given that a-person answered true to this-question, how many would answer true to a-different-personality-question.
how-many-people out of 100 found that participants equated inverse-probabilities (e.g., p
groups have been found to neglect base-rate more than individuals do.
then participants were then asked to evaluate the-probability of linda being a-feminist, the-probability of linda being a-bank-teller, or the-probability of being both a-bank-teller and feminist.
however, participants judged the-conjunction (bank-teller and feminist) as being more probable than being a bank-teller alone.
however, when a-personality-description (data) seems to be very representative of a-physics-major-(e.g.,-pocket-protector) over a biology major, people judge that it is more likely for this-person to be a physics major than a natural sciences major (which is a-superset of physics).
evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993) found that people judge a-person who is highly representative of being a-statistics major (e.g., highly intelligent, does math-competitions) as being more likely to be a-statistics major than a social sciences major (superset of statistics), but evidence that the-representativeness-heuristic may cause the-disjunction-fallacy comes from bar-hillel and neter (1993)
list of biases in judgment and decision-making extension neglect ==
judgment under uncertainty: heuristics and biases" (pdf).
judgment under uncertainty: heuristics and biases.
social-environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the-decision-making-process through ignoring some-information or relying on simple-rules of thumb to make decisions.
at the-intersection of these-fields, social-heuristics have been applied to explain cooperation in economic-games used in experimental-research, based on the-argument that cooperation is typically advantageous in daily-life, and therefore people develop a-cooperation-heuristic that gets applied even to one-shot-anonymous-interactions (the so-called "social-heuristics hypothesis" of human-cooperation).
because of this, defined-parameters or boundaries must be implemented in the-process in order to achieve an-acceptable-outcome.
heuristics ===
heuristics are a-common-alternative, which can be defined as simple-strategies for decision making where the-actor only pays attention to key-pieces of information, allowing the-decision to be made quickly and with less-cognitive-effort.
daniel-kahneman and shane-frederick have advanced the-view that heuristics are decision-making-processes that employ attribute-substitution, where the-decision-maker substitutes the-"target-attribute" of the-thing daniel-kahneman is trying to judge with a-"heuristic-attribute" that more easily comes to mind.
shah and daniel-m.-oppenheimer have framed heuristics in terms of effort-reduction, where the-decision-maker makes use of techniques that make decisions less effortful, such as only paying attention to some-cues or only considering a-subset of the-available-alternatives.
another-view of heuristics comes from gerd-gigerenzer and colleagues, who conceptualize heuristics as "fast-and-frugal"-techniques for decision making that simplify complex-calculations and make up part of the-"adaptive-toolbox" of human-capacities for reasoning and inference.
under this-framework, heuristics are ecologically rational, meaning a-heuristic may be successful if the-way heuristics works matches the-demands of the-environment-heuristics is being used in.
researchers in this-vein also argue that heuristics may be just as or even more accurate when compared to more-complex-strategies such as multiple-regression.
social-heuristics can include heuristics that use social-information, operate in social-contexts, or both.
within social-psychology, some-researchers have viewed heuristics as closely linked to cognitive-biases.
researchers in the-latter-approach treat the-study of social-heuristics as closely linked to social-rationality, a-field of research that applies the-ideas of bounded-rationality and heuristics to the-realm of social-environments.
for instance, in deciding which-restaurant to choose, people tend to choose the-one with the-longer-waiting-queue.
an-agent using the-heuristic would search through her-social-circles in order of their-proximity to the-self (self, family, friends, and acquaintances), stopping the-search as soon as the-number of instances of one-alternative within a-circle exceeds that of the other, choosing the-alternative with the-higher-tally.
the-heuristic is typically investigated using a-prisoner's-dilemma in game-theory, where there is substantial-evidence that people use such a heuristic, leading to intuitive-reciprocation.
in the-dominant-dual-systems-approach in social-psychology, heuristics are believed to be automatically and unconsciously applied.
the-study of social-heuristics as a-tool of bounded-rationality asserts that heuristics may be used consciously or unconsciously.
the-theory is supported by evidence from laboratory-and-online-experiments suggesting that time pressure increases cooperation, though some-evidence suggests this may be only among individuals who are not as familiar with the-types of economic-games typically used in this-field of research.
heuristics can be mental-shortcuts that ease the-cognitive-load of making a-decision.
examples that employ heuristics include using trial and error, a-rule of thumb or an-educated-guess.
overview == heuristics are the-strategies derived from previous-experiences with similar-problems.
when an-individual applies heuristics in practice, generally performs as expected however
in psychology, heuristics are simple,-efficient-rules, learned or inculcated by evolutionary-processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex-problems or incomplete-information.
researchers test if people use those-rules with various-methods.
the-study of heuristics in human-decision-making was developed in the-1970s and the-1980s by the-psychologists amos-tversky and daniel-kahneman although the-concept had been originally introduced by the-nobel-laureate herbert-a.-simon, whose-original,-primary-object of research was problem solving that showed that we operate within what daniel-kahneman calls bounded rationality.
he coined the-term-satisficing, which denotes a-situation in which people seek solutions, or accept choices or judgments, that are "good-enough"-for-people-purposes although people could be optimized.
rudolf-groner analyzed the-history of heuristics from rudolf-groner roots in ancient-greece up to contemporary-work in cognitive-psychology and artificial-intelligence, proposing a-cognitive-style "heuristic versus algorithmic-thinking," which can be assessed by means of a-validated-questionnaire.
gerd-gigerenzer and gerd-gigerenzer research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
they study the-fast-and-frugal-heuristics in the-"adaptive-toolbox" of individuals or institutions, and the-ecological-rationality of these-heuristics; that is, the conditions under which a-given-heuristic is likely to be successful.
heuristics – such as the-recognition-heuristic, the-take-the-best-heuristic,-and-fast-and-frugal-trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
it is often said that heuristics trade accuracy for effort
in the-absence of this-information, that is under uncertainty, heuristics can achieve higher-accuracy with lower-effort.
the-valuable-insight of this-program is that heuristics are effective not despite of heuristics are effective-simplicity — but because of this-program.
furthermore, gigerenzer and wolfgang-gaissmaier found that both-individuals and organizations rely on heuristics in an-adaptive-way.
at some-times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
on other-occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
from this-perspective, heuristics are part of a-larger-experiential-processing-system that is often adaptive, but vulnerable to error in situations that require logical-analysis.
in 2002, daniel-kahneman and shane-frederick proposed that cognitive heuristics work by a-process called attribute substitution, which happens without conscious-awareness.
heuristics can be considered to reduce the-complexity of clinical-judgments in health-care.
informal-models of heuristics ===
is used while judging the-risks and benefits of something, depending on the-positive-or-negative-feelings that people associate with a-stimulus.
availability-heuristic — a-mental-shortcut that occurs when people make judgments about the-probability of events by the-ease with which examples come to mind.
for example, in a-1973-tversky-&-kahneman-experiment, the-majority of participants reported that there were more-words in the-english-language that start with the-letter k than for which k was the-third-letter.
when using balance-heuristic — there is a-common-issue where individuals misjudge the-likelihood of a-situation.
for example, if there is a-test for a-disease which has an-accuracy of 90%, people may think it’s a-90%-people have a-disease which has an-accuracy of 90% even though a-disease which has an-accuracy of 90% only affects 1 in 500-people.
common sense heuristic --- used frequently by individuals when the-potential-outcomes of a-decision appear obvious.
this leads people to avoid others that are viewed as “contaminated” to the-observer.
escalation of commitment — describes the-phenomenon where people justify increased-investment in a-decision, based on the-cumulative-prior-investment, despite new-evidence suggesting that the-cost, starting today, of continuing a-decision outweighs the-expected-benefit.
a-mental-shortcut applied to various-situations in which individuals assume that the-circumstances underlying the-past-behavior still hold true for the-present-situation and that the-past-behavior thus can be correctly applied to the-new-situation.
when asked to make several-choices at once, people tend to diversify more than when making the-same-type of decision sequentially.
for example, in a-1982-tversky-and-kahneman-experiment, participants were given a-description of linda.
simulation-heuristic — simplified-mental-strategy in which people determine the-likelihood of an-event happening based on how easy it is to mentally picture the-event happening.
people regret the-events that are easier to image over the-ones that would be harder to.
it is also thought that people will use this-heuristic to predict the-likelihood of another's-behavior happening.
this shows that people are constantly simulating everything around people in order to be able to predict the-likelihood of events around people.
it is believe that people do this by mentally-undoing-events that people have experienced and then running mental-simulations of the-events with the-corresponding-input-values of the-altered-model.
it is where people copy the-actions of others in order to attempt to undertake the-behavior in a-given-situation.
it is more prominent in situations were people are unable to determine the-appropriate-mode of behavior and are driven to the-assumption that the-surrounding-people have more-knowledge about the-current-situation.
working backward-heuristic — when an-individual assumes, an-individual have already solved a-problem an-individual work backwards in order to find how to achieve the-solution an-individual originally figured out.
formal-models of heuristics ===
heuristics were also found to be used in the-manipulation and creation of cognitive-maps.
people commonly made distortions to images.
symmetry-heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than people really are.
similar to the previous, where people align objects mentally to make people straighter than people really are.
relative-position heuristic: people do not accurately distance landmarks in people-mental-image based on how well people remember that-particular-item.
philosophers of science have emphasized the-importance of heuristics in creative-thought and the-construction of scientific-theories.
in legal-theory, especially in the-theory of law and economics, heuristics are used in law ==
for instance, in all-states in the-united-states the-legal-drinking-age for unsupervised-persons is 21-years, because it is argued that people need to be mature enough to make decisions involving the-risks of alcohol-consumption.
however, assuming people mature at different-rates, the-specific-age of 21 would be too late for some and too early for others.
however, like the-drinking-age-problem above, the-specific-length of time would need to be different for every-product to be efficient.
stereotyping is a-type of heuristic that people use to form opinions or make judgments about things people have never seen or experienced.
people work as a-mental-shortcut to assess everything from the-social-status of a-person (based on people-actions), to whether a-plant is a-tree based on the-assumption that it is tall, has a-trunk, and has leaves (even though the-person making the-evaluation might never have seen that-particular-type of tree before).
the-concept of heuristics has critiques and controversies.
the popular "we cannot be that-dumb"-critique argues that people would be doomed if it weren't for people ability to make sound-and-effective-judgments.
social-rationality is a-form of bounded-rationality applied to social-contexts, where individuals make choices and predictions under uncertainty.
the-idea is that, similar to non-social-environments, individuals rely, and should rely, on fast-and-frugal-heuristics in order to deal with complex-and--genuinely-uncertain-social-environments.
the-descriptive-program studies the-repertoire of heuristics an individual or organization uses,
that is, the-descriptive-program studies the-repertoire of heuristics an individual or organization uses, that is, their-adaptive-toolbox-adaptive-toolbox.
applications == heuristics can be applied to social-and-non-social-decision-tasks (also called social games and games against nature), judgments, or categorizations.
social-rationality is thus about three of the-four-possible-combinations, excluding the-case of heuristics using non-social-input for non-social-tasks. '
games against nature' comprise situations where individuals face environmental-uncertainty, and need to predict or outwit nature, e.g., harvest food or master-hard-to-predict-or-unpredictable-hazards. '
an-example for a-heuristic that is not necessarily social but that requires social-input is the imitate-the-majority heuristic, where in a-situation of uncertainty, individuals follow the-actions or choices of the-majority of individuals peers regardless of individuals social-status.
people divide and invest people-resources equally in a-number of n-different-options.
heuristics are simple-strategies or mental-processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex-problems.
however, heuristics are not always right or the most accurate.
in situations of uncertainty, where information is incomplete, heuristics allow for the-less-is-more-effect, in which less-information leads to greater-accuracy.
, that is how people decide under uncertainty.
herbert-a.-simon is also known as the-father of bounded-rationality, which herbert-a.-simon understood as the-study of the-match (or mismatch) between heuristics and decision-environments.
in the-early-1970s, psychologists-amos-tversky and daniel-kahneman took a-different-approach, linking heuristics to cognitive-biases.
heuristics and biases" and although the-originally-proposed-heuristics have been refined over time, this-research-program has changed the-field by permanently setting the-research-questions.
according to their-perspective, the-study of heuristics requires formal-models that allow predictions of behavior to be made ex ante.
the-engineering-study of intuitive-design)among-others, this-program has shown that heuristics can lead to fast,-frugal,-and-accurate-decisions in many-real-world-situations that are characterized by uncertainty.
formal-models of heuristics == ===
if after time β no-alternative has satisfied α, then decrease α by some-amount-δ and return to step 1.satisficing has been reported across many-domains, for instance as a-heuristic-car-dealers use to price used-bmws.
the-recognition-heuristic exploits the-basic-psychological-capacity for recognition in order to make inferences about unknown-quantities in the-world.
similarly, psychological-studies have shown that in situations where take-the-best is ecologically rational, a-large-proportion of people tend to rely on it.
this includes decision-making by airport-custom-officers, professional-burglars and police-officers and student-populations.
unlike a-full-decision-tree, however, it is an-incomplete-tree – to save time and reduce the-danger of overfitting.
in a-full-tree, in contrast, order does not matter for the-accuracy of the-classifications.
informal-models of heuristics ==
heuristics that underlie judgment are called "judgment heuristics".
when people estimate how likely or how frequent an-event is on the-basis of an-event availability, people are using the-availability-heuristic.
for example, people overestimate people likelihood of dying in a-dramatic-event such as a-tornado or terrorism.
this-heuristic is one of the-reasons why people are more easily swayed by a-single,-vivid-story than by a-large-body of statistical-evidence.
this-heuristic may also play a-role in the-appeal of lotteries: to someone buying a-ticket, the-well-publicised,-jubilant-winners are more available than the-millions of people who have won nothing.
when people judge whether more-english-words begin with t or with k , the-availability-heuristic gives a-quick-way to answer the-question.
when people are asked whether there are more-english-words with k in the-first-position or with k in the-third-position, people use the-same-process.
this leads people to the-incorrect-conclusion that k is more common at the-start of words.
tversky and kahneman offered the-availability-heuristic as an-explanation for illusory-correlations in which people wrongly judge two-events to be associated with each other.
tversky and kahneman explained that people judge correlation on the-basis of the-ease of imagining or recalling the-two-events together.
the-representativeness-heuristic is seen when people use categories, for example when deciding whether or not a-person is a-criminal.
when people categorise things on the-basis of representativeness, people are using the representativeness heuristic.
thus, people can overestimate the-likelihood that something has a-very-rare-property, or underestimate the-likelihood of a-very-common-property.
the-representativeness-heuristic is also an-explanation of how people judge cause and effect: when people make these-judgements on the-basis of similarity, people are also said to be using the representativeness heuristic.
if people based people-judgments on probability, people would say that tom is more likely to study humanities than library-science, because there are many more humanities students, and the-additional-information in the-profile is vague and unreliable.
when people rely on representativeness, people can fall into an-error which breaks a-fundamental-law of probability.
people reading this-description then ranked the-likelihood of different-statements about she.
people showed a-strong-tendency to rate the latter,-more-specific-statement as more likely, even though a-conjunction of the-form "she is both-x and y" can never be more probable than the-more-general-statement
the-explanation in terms of heuristics is that the-judgment was distorted because, for the-readers, the-character-sketch was representative of the-sort of person who might be an-active-feminist but not of someone who works in a-bank.
a-great-majority of people reading this-character-sketch rated "bill is an-accountant who plays jazz for a-hobby", as more likely than "bill plays jazz for a-hobby".
other-researchers also carried out variations of this-study, exploring the-possibility that people had misunderstood the-question.
it has been shown that individuals with high-crt-scores are significantly less likely to be subject to the-conjunction-fallacy.
the-explanation in terms of the-heuristic is that people consider only how representative the-figure of 60% is of the-previously-given-average of 50%.
this means unrelated-and-non-diagnostic-information about certain-issue can make relative-information less powerful to certain-issue when people understand the-phenomenon.
representativeness explains systematic-errors that people make when judging the-probability of random-events.
for example, in a-sequence of coin-tosses, each of which comes up heads (h) or tails (t), people reliably tend to judge a-clearly-patterned-sequence such as hhhttt as less likely than a-less-patterned-sequence such as hthtth.
these-sequences have exactly-the-same-probability, but people tend to see the-more-clearly-patterned-sequences as less-representative of randomness, and so less likely to result from a-random-process.
anchoring and adjustment is a-heuristic used in many-situations where people estimate a-number.
in tversky and kahneman's-experiments, people did not shift far enough away from the-anchor.
an-alternative-theory is that people form people estimates on evidence which is selectively brought to mind by an-anchor.
the-anchoring-effect is stronger when people have to make people judgments quickly.
an-example is where people predict the-value of a-stock-market-index on a-particular-day by defining an upper and lower bound so that people are 98% confident the-true-value will fall in that-range.
a-reliable-finding is that people anchor people upper-and-lower-bounds too close to people
one-much-replicated-finding is that when people are 98% certain that a-number is in a-particular-range, people are wrong about thirty to forty percent of the-time.
tversky and kahneman demonstrated this by asking a-group of people to rapidly estimate the-product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
the-explanation in terms of anchoring is that people multiply the-first-few-terms of each-product and anchor on that-figure.
a-common-finding from studies of these-tasks is that people anchor on the-small-component-probabilities and so underestimate the-total.
a-corresponding-effect happens when people estimate the-probability of multiple-events happening in sequence, such as an-accumulator-bet in horse-racing.
in one-experiment, people wrote down the-last-two-digits of people social-security-numbers.
people were then asked to consider whether people would pay this-number of dollars for items whose-value people did not know, such as wine, chocolate, and computer-equipment.
in one-review, researchers found that if a-stimulus is perceived to be important or carry "weight" to a-situation, that people were more likely to attribute a-stimulus as heavier physically.
when people use affect ("gut responses") to judge benefits or risks, people are using the affect heuristic.
there are competing-theories of human-judgment, which differ on whether the-use of heuristics is irrational.
a-cognitive-laziness-approach argues that heuristics are inevitable-shortcuts given the-limitations of the-human-brain.
this has led to a-theory called "attribute substitution", which says that people often handle a-complicated-question by answering a-different,-related-question, without being aware that this is what people are doing.
a-third-approach argues that heuristics perform just as well as more-complicated-decision-making-procedures, but more quickly and with less-information.
an-effort-reduction-framework proposed by anuj-k.-shah and daniel-m.-oppenheimer states that people use a-variety of techniques to reduce the-effort of making decisions.
this explains why individuals can be unaware of individuals own biases, and why biases persist even when the-subject is made aware of individuals.
gerd-gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
according to gerd-gigerenzer and colleagues, heuristics are "fast-and-frugal"-alternatives to more-complicated-procedures, giving answers that are just as good.
warren-thorngate, a-social-psychologist, implemented ten-simple-decision-rules or heuristics in a-computer-program.
legal-scholar-cass-sunstein has argued that attribute-substitution is pervasive when people reason about moral,-political-or-legal-matters.
given a-difficult,-novel-problem in these-areas, people search for a-more-familiar,-related-problem (a-"prototypical-case") and apply its-solution as the-solution to the-harder-problem.
according to sunstein, the-opinions of trusted-political-or-religious-authorities can serve as heuristic-attributes when people are asked people own opinions on a-matter.
individuals looks further than individuals own prior-knowledge for the-answers.
these-two-varieties of heuristics confirms how we may be influenced easily our-mental-shortcuts, or what may come quickest to we mind.
heuristics and biases: the-psychology of intuitive-judgement, cambridge-university-press, pp.
heuristics and biases" (pdf), science, 185 (4157): 1124–1131,
judgment under uncertainty: heuristics and biases.
heuristics and biases: the-psychology of intuitive-judgment.
heuristics for decision and choice".
computers are social-actors (casa) is a-paradigm which states that humans mindlessly apply the-same-social-heuristics used for human-interactions to computers because humans call to mind similar-social-attributes as humans.
it states that casa is the-concept that people mindlessly apply social-rules and expectations to computers, even though people know that these-machines do not have feelings, intentions or human-motivations.
in their-2000-article, nass and moon attribute their-observation of anthropocentric-reactions to computers and previous-research on mindlessness as factors that lead their to study the-phenomenon of computers as social-actors.
specifically,-their-observed-consistent-anthropocentric-treatment of computers by individuals in natural-and-lab-settings, even though these-individuals agreed that computers are not human and shouldn't be treated as such.
social-attributes that computers have which are similar to humans include: words for output
although individuals using computers exhibit a-mindless-social-response to the-computer, individuals who are sensitive to the-situation can observe the-inappropriateness of the-cued-social-behaviors.
for example, a-2000-study revealed when people watched a-television labeled 'news television', people thought the-news-segments on that-tv were higher in quality, had more-information, and were more interesting than people who saw the-identical-information on a-tv labeled ''news television'.
for example, research from 1996 and 2001 found people with dominant-personalities preferred computers that also had a-'dominant-personality'; that is, the-computer used strong,-assertive-language during tasks.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers.
a-2010-article, "cognitive-load on social-response to computers" by e.j.-lee discussed research on how-human-likeness of a-computer-interface, individuals'-rationality, and cognitive-load moderate the-extent to which people apply social-attributes to computers revealed that participants were more socially attracted to a-computer that flattered participants than a-generic-comment-computer, but participants became more suspicious about the-validity of the-flattery-computer's-claims and more likely to dismiss the-flattery-computer-answer.
these-negative-effects disappeared when participants simultaneously engaged in a-secondary-task.
a-2011-study "cloud-computing – reexamination of casa" by hong and sundar found that when people are in a-cloud-computing-environment, people shift people-source-orientation—that is, users evaluate the-system by focusing on service-providers over the-internet, instead of the-machines in front of people.
hong and sundar-sundar concluded hong and sundar study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a-fundamental-re-examination of the-mindless-social-response of humans to computers.
participants interacted with a-computer which questioned participants using reciprocal-wording and gradual-revealing of intimate-information, then participants did a-puzzle on paper, and finally half-the-group went back to a-computer and the-other-half went to a-different-computer.
participants who used the-same-computer throughout the-experiment had a-higher-purchase-likelihood-score and a-higher-attraction-score toward the-computer in the-product-presentation than participants who did not use the-same-computer throughout the-experiment.
for example diamonds are more valuable than rocks because diamonds are not as abundant.
heuristics ==
heuristics are strategies that use readily-accessible-(though-loosely-applicable)-information for problem solving.
we use heuristics to speed up our-decision-making-process when an-exhaustive,-deliberative-process is perceived to be impractical or unnecessary.
thus heuristics are simple,-efficient-rules, which have developed through either-evolutionary-proclivities or past-learning.
scarcity appears to have created a-number of heuristics such as when price is used as a-cue to the-quality of products, as cue to the-healthfulness of medical-conditions, and as a-cue to the-sexual-content of books when age-restrictions are put in place.
when time is scarce and information complex, people are prone to use heuristics in general.
when time is perceived to be short, politicians can exploit the-scarcity heuristic.
“the-first-hundred-people receive…”; “limited time only”;
this-research indicates that people not only want censored information more but have an-increased-susceptibility to the-message of the-censored-material.
they divided people into two-groups, giving one-group a-jar of ten-cookies and another a-jar with only-two-cookies.
some-participants were first given a-jar of ten-cookies, but before participants could sample the-cookie, experimenters removed 8-cookies so that there were again only two.
results showed the-scarce-good receiving a-higher-wta-price by participants choosing it, than by those who did not, compared to the-wta of the-abundant-good, despite the-fact that both-types of participants assigned a-lower-market-price to the-scarce-good, as compared to the abundant one.
several-stores were wrecked during these-riots, several-stores began requiring people to wait in line (for as-long-as-14-hours) in order to obtain one of the-dolls.
bounded-rationality is the-idea that rationality is limited when individuals make decisions.
the-concept of bounded-rationality complements "rationality as optimization", which views decision-making as a-fully-rational-process of finding an-optimal-choice given the-information available.
many-economics-models assume that agents are on average rational, and can in large-quantities be approximated to act according to agents-preferences in order to maximise utility.
these include: limiting the-types of utility-functions recognizing the-costs of gathering and processing-information the-possibility of having a-"vector" or "multi-valued"-utility functionsimon suggests that economic-agents use heuristics to make decisions rather than a-strict-rigid-rule of optimization.
an-example of behaviour inhibited by heuristics can be seen when comparing the-cognitive-strategies utilised in simple-situations (e.g tic-tac-toe), in comparison to strategies utilised in difficult-situations (e.g-chess).
thus, in order to test the-mental-limits of agents, complex-problems, such as those within chess, should be studied to test how individuals work around individuals cognitive-limits, and what-behaviours or heuristics are used to form solutions
rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with people inability to optimize.
everything else being equal, an-agent that has better-algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer-heuristics and algorithms.
behavioural-economists engage in mapping the-decision-shortcuts that agents use in order to help increase the-effectiveness of human-decision-making.
a-widely-cited-proposal from cass-sunstein and richard-thaler's-nudge-richard-thaler's-urges that healthier-food be placed at sight-level in order to increase the-likelihood that a-person will opt for that-choice instead of a-less-healthy-option.
some-critics of nudge have lodged attacks that modifying choice-architectures will lead to people becoming worse-decision-makers.
the-research attempted to explore the-choices made by what was assumed as rational-agents compared to the-choices made by individuals optimal beliefs and individuals satisficing-behaviour.
three-major-topics covered by the-works of daniel-kahneman and amos-tversky include heuristics of judgement, risky-choice, and framing-effect, which were a-culmination of research that fit under what was defined by herbert-a.-simon as the-psychology of bounded-rationality.
recent-research has shown that bounded-rationality of individuals may influence the-topology of the-social-networks that evolve among individuals.
not only does the-concept focus on the-ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a-great-extent, given the-limited-information-humans access prior to decision-making for complex-problems.
although this-concept realistically delves into decision-making and human-cognition, challenging earlier-theories which assumed perfect-rational-cognition and behaviour, bounded-rationality can mean something different to everyone, and the-way each-person-satisfices can vary dependant on each-person-satisfices environment and the-information each-person-satisfices have access to .
social-psychology is the-scientific-study of how the-thoughts, feelings, and behaviors of individuals are influenced by the-actual,-imagined,-and-implied-presence of others, 'imagined' and 'implied-presences' referring to the-internalized-social-norms that humans are influenced by even when alone.
in order to do so, many-psychologists applied the-scientific-method to human-behavior.
in social-psychology, attitude is defined as learned, global-evaluations (e.g. of people or issues) that influence thought and action.
because people are influenced by other-factors in any-given-situation, general-attitudes are not always good-predictors of specific-behavior.
experiments using the-implicit-association-test, for instance, have found that people often demonstrate implicit-bias against other-races, even when people-explicit-responses profess equal-mindedness.
tesser speculated that individuals are disposed to hold certain-strong-attitudes as a-result of inborn-personality-traits and physical, sensory, and cognitive skills.
numerous-studies have shown that people can form strong-attitudes toward neutral-objects that are in some-way linked to emotionally-charged-stimuli.
persuasion is an-active-method of influencing that attempts to guide people toward the-adoption of an-attitude, idea, or behavior by rational-or-emotive-means.
social-cognition studies how people perceive, think about, and remember information about others.
much-research rests on the-assertion that people think about other-people differently from non-social-targets.
the-assertion that people think about other-people differently from non-social-targets is supported by the-social-cognitive-deficits exhibited by people with williams-syndrome and autism.
person-perception is the-study of how people form impressions of others.
the-study of how people form beliefs about each other while interacting is interpersonal-perception.
individuals also attribute causes of behavior to controllable-and-uncontrollable-factors (i.e.-how-much-control one has over the-situation at hand).
other-ways people protect people self-esteem are by believing in a-just-world, blaming victims for victims suffering, and making defensive-attributions that explain our-behavior in ways that defend our from feelings of vulnerability and mortality.
heuristics ====
heuristics are cognitive-shortcuts.
instead of weighing all-the-evidence when making a-decision, people rely on heuristics to save time and energy.
the-availability-heuristic occurs when people estimate the-probability of an-outcome based on how easy that-outcome is to imagine.
the-representativeness-heuristic is a-shortcut people use to categorize something based on how similar the-representativeness-heuristic is to a-prototype people know of.
one-experiment found that people are more likely to misperceive a-weapon in the-hands of a-black-man than a-white-man.
this-type of schema is a-stereotype, a-generalized-set of beliefs about a-particular-group of people (when incorrect, an-ultimate-attribution-error).
self-concept is the-whole-sum of beliefs that people have about people.
beliefs that people have about people and that guide the-processing of self-referential-information.
for example, people whose-body-image is a-significant-self-concept-aspect are considered schematics with respect to weight.
in contrast, people who do not regard people who do not regard their-weight as an-important-part of their-lives-weight as an-important-part of people who do not regard their-weight as an-important-part of their-lives lives are aschematic with respect to that-attribute.
the-abcs of self are: affect (i.e.-emotion): how do people evaluate people, enhance people self-image, and maintain a-secure-sense of identity?
: how do people regulate people own-actions and present people to others according to interpersonal-demands?
how do individuals become individuals, build a-self-concept, and uphold a-stable-sense of identity?affective-forecasting is the-process of predicting how one would feel in response to future-emotional-events.
have shown that people overestimate the-strength of people-reactions to anticipated positive-and-negative-life-events, more than people actually feel when the-event does occur.
leon-festinger's-1954-social-comparison-theory is that people evaluate people own abilities and opinions by comparing people to others when people are uncertain of people own ability or opinions.
daryl-bem's-1972-self-perception-theory claims that when internal-cues are difficult to interpret, people gain self-insight by observing people own behavior.
people develop people self-concepts by various-means, including introspection, feedback from others, self-perception, and social-comparison.
by comparing themselves to others, people gain information about people, and people make inferences that are relevant to self-esteem.
social-comparisons can be either upward or downward, that-is,-comparisons to people who are either higher or lower in status or ability.
downward-comparisons are often made in order to elevate self-esteem.
social-influence is an-overarching-term that denotes the-persuasive-effects people have on each other.
obedience as a-form of compliance was dramatically highlighted by the-milgram-study, wherein people were ready to administer shocks to a-person in distress on a-researcher's-command.
similarly, people may expect hostility in others and induce hostility in others by people own-behavior.
specifically, social-influence refers to the-way in which individuals change individuals ideas and actions to meet the-demands of a-social-group, received authority, social-role, or a-minority within a-group wielding influence over the-majority.
people waiting in line to get on a-bus, for example, do not constitute a-group.
the-shared-social-identity of individuals within a-group influences intergroup-behavior, which denotes the-way in which groups behave towards and perceive each other.
these-perceptions and behaviors in turn define the-social-identity of individuals within the-interacting-groups.
for example, group-polarization, formerly known as the-"risky-shift", occurs when people polarize people views in a-more-extreme-direction after group-discussion.
in contrast, social-loafing is the-tendency of individuals to slack off when working in a-group.
a-major-area of study of people's-relations to each other is interpersonal-attraction, which refers to all-forces that lead people to like each other, establish relationships, and (in some-cases) fall in love.
whenever possible, social-psychologists rely on controlled-experimentation, which requires the-manipulation of one-or-more-independent-variables in order to examine the-effect on a-dependent-variable.
some-psychologists have raised concerns for social-psychological-research relying too heavily on studies conducted on university-undergraduates in academic-settings, or participants from crowdsourcing labor-markets such as amazon-mechanical-turk.
in well-over-a-third of the-trials, participants conformed to the-majority, even though the-majority judgment was clearly wrong.
participants with three-other,-incorrect-participants made mistakes 31.8% of the-time, while those with one-or-two-incorrect-participants made mistakes only 3.6% and 13.6% of the-time, respectively.
at the-study's-end, some-participants were paid $1 to say that some-participants enjoyed the-task and another-group of participants was paid $20 to tell the-same-lie.
another-group of participants later reported liking a-boring-task better than the-second-group ($20).
festinger's-explanation was that for people in the-second-group ($20) being paid only $1 is not sufficient-incentive for lying and those who were paid $1-experienced-dissonance.
milgram-experiment ==== was designed to study how far people would go in obeying an-authority-figure.
philip-zimbardo's-stanford-prison-study, a-simulated-exercise involving students playing at being prison-guards and inmates, ostensibly showed how far people would go in such-role playing.
the-goal of social-psychology is to understand cognition and behavior as cognition and behavior naturally occur in a-social-context, but the-very-act of observing people can influence and alter people-behavior.
in addition to deception, experimenters have at times put people into potentially-uncomfortable-or-embarrassing-situations
at most-colleges and universities, this is conducted by an-ethics-committee or institutional-review-board, which examines the-proposed-research to make sure that no-harm is likely to come to the-participants, and that the-study's-benefits outweigh any-possible-risks or discomforts to people taking part.
a-debriefing is typically done at the-experiment's-conclusion in order to reveal any-deceptions used and generally make sure that the-participants are unharmed by the-procedures.
for example, the-scientific-journal judgment and decision-making has published several-studies over the-years that fail to provide support for the-unconscious-thought-theory.
for example, when getting to know others, people tend to ask leading-questions which seem biased towards confirming people assumptions about the-person.
belief, decision-making and behavioral ==
in psychology and cognitive-science, a-memory-bias is a-cognitive-bias that either enhances or impairs the-recall of a-memory (either-the-chances that the-memory will be recalled at all, or the-amount of time it takes for it to be recalled, or both), or that alters the-content of a-reported-memory.
gerd-gigerenzer (born september 3, 1947, wallersdorf, germany) is a-german-psychologist who has studied the-use of bounded-rationality and heuristics in decision-making.
gigerenzer proposes that, in an-uncertain-world, probability-theory is not sufficient; people also use smart-heuristics, that-is,-rules of thumb.
he conceptualizes rational-decisions in terms of the-adaptive-toolbox (the-repertoire of heuristics an individual or institution has) and the-ability to choose a-good-heuristics for the-task at hand.
gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the-accuracy-effort-trade-off-view assumes, in which heuristics are seen as short-cuts that trade less-effort for less-accuracy.
in contrast, gigerenzer and associated-researchers'-studies have identified situations in which "less is more", that is, where heuristics make more-accurate-decisions with less-effort.
heuristics ===
a-critic of the-work of daniel-kahneman and amos-tversky, gigerenzer argues that heuristics should not lead us to conceive of human-thinking as riddled with irrational-cognitive-biases, but rather to conceive rationality as an-adaptive-tool that is not identical to the-rules of formal-logic or the-probability-calculus.
for instance, lay people as well as professionals often have problems making bayesian-inferences, typically committing what has been called the base-rate fallacy in the-cognitive-illusions-literature.
gigerenzer and ulrich-hoffrage were the first to develop and test a-representation called natural frequencies, which helps people make bayesian-inferences correctly without any-outside-help.
