Adobe Inc. ( ə-DOH-bee) is an American multinational computer software company.
Incorporated in Delaware 
and headquartered in San Jose, California, it has historically specialized in software for the creation and publication of a wide range of content, including graphics, photography, illustration, animation, multimedia/video, motion pictures and print.
The company has expanded into digital marketing management software.
Adobe has millions of users worldwide.
Flagship products include: Photoshop image editing software, Adobe Illustrator vector-based illustration software, Adobe Acrobat Acrobat Reader and the Portable Document Format (PDF), plus a host of tools primarily for audio-visual content creation, editing and publishing.
The company began by leading in the desktop publishing revolution of the mid-eighties, went on to lead in animation and multi-media through its acquisition of Macromedia, from which it acquired animation technology Adobe Flash, Developed inDesign and subsequently gained a leadership position in publishing over Quark and PageMaker, developed video editing and compositing technology in Premiere, pioneered low-code web development with Muse, and emerged with a suite of solutions for marketing management.
Adobe offered a bundled solution of its products named Adobe Creative Suite, which evolved into a SaaS subscription offering Adobe Creative Cloud.
Adobe was founded in December 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC to develop and sell the PostScript page description language.
In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
As of 2019, Adobe has more than 21,000 employees worldwide, about 40% of whom work in San Jose.
Adobe also has major development operations in the United States in Newton, New York City, Minneapolis, Lehi, Seattle, Austin and San Francisco.
It also has major development operations in Noida and Bangalore in India.

==
History ==
The company was started in John Warnock's garage.
The name of the company, Adobe, comes from Adobe Creek in Los Altos, California, which ran behind Warnock's house.
That creek is so named because of the type of clay found there, which alludes to the creative nature of the company's software.
Adobe's corporate logo features a stylized "A" and was designed by Marva Warnock, a graphic designer who is also John Warnock's wife.
Steve Jobs attempted to buy the company for $5 million in 1982, but Warnock and Geschke refused.
Their investors urged them to work something out with Jobs, so they agreed to sell him shares worth 19 percent of the company.
Jobs paid a five-times multiple of their company's valuation at the time, plus a five-year license fee for PostScript, in advance.
The purchase and advance made Adobe the first company in the history of Silicon Valley to become profitable in its first year.
Warnock and Geschke considered various business options including a copy-service business and a turnkey system for office printing.
Then they chose to focus on developing specialized printing software and created the Adobe PostScript page description language.
PostScript was the first truly international standard for computer printing as it included algorithms describing the letter-forms of many languages.
Adobe added kanji printer products in 1988.
Warnock and Geschke were also able to bolster the credibility of PostScript by connecting with a typesetting manufacturer.
They weren't able to work with Compugraphic, but then worked with Linotype to license the Helvetica and Times Roman fonts (through the Linotron 100).
By 1987, PostScript had become the industry-standard printer language with more than 400 third-party software programs and licensing agreements with 19 printer companies.
Warnock described the language as "extensible", in its ability to apply graphic arts standards to office printing.
Adobe's first products after PostScript were digital fonts, which they released in a proprietary format called Type 1, worked on by Bill Paxton after he left Stanford.
Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft.

In the mid-1980s, Adobe entered the consumer software market with Illustrator, a vector-based drawing program for the Apple Macintosh.
Illustrator, which grew from the firm's in-house font-development software, helped popularize PostScript-enabled laser printers.

Adobe entered the NASDAQ Composite index in August 1986.
Its revenue has grown from roughly $1 billion in 1999 to $4 billion in 2012.
Adobe's fiscal years run from December to November.

For example, the 2007 fiscal year ended on November 30, 2007.

In 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop.
Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.

In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software.
PDF is now an International Standard: ISO 32000-1:2008.

In December 1991, Adobe released Adobe Premiere, which Adobe rebranded as Adobe Premiere Pro in 2003.
In 1992, Adobe acquired OCR Systems,
Inc. In 1994, Adobe acquired the Aldus Corporation and added PageMaker and After Effects to its product line later in the year; it also controls the TIFF file format.
In the same year, Adobe acquired LaserTools Corp and
Compution Inc. In 1995, Adobe added FrameMaker, the long-document DTP application, to its product line after Adobe acquired
Frame Technology Corp. In 1996, Adobe acquired
Ares Software Corp. In 2002, Adobe acquired Canadian company Accelio (also known as JetForm).In May 2003
Adobe purchased audio editing and multitrack recording software Cool Edit Pro from Syntrillium Software for $16.5 million, as well as a large loop library called "Loopology".
Adobe then renamed Cool Edit Pro to "Adobe Audition" and included it in the Creative Suite.

On December 3, 2005, Adobe acquired its main rival, Macromedia, in a stock swap valued at about $3.4 billion, adding ColdFusion, Contribute, Captivate, Breeze (rebranded as Adobe Connect), Director, Dreamweaver, Fireworks, Flash, FlashPaper, Flex, FreeHand, HomeSite, JRun, Presenter, and Authorware to Adobe's product line.
Adobe released Adobe Media Player in April 2008.
On April 27, Adobe discontinued development and sales of its older HTML/web development software, GoLive, in favor of Dreamweaver.
Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance.
On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work.
Creative Suite 4, which includes Design, Web, Production Premium, and Master Collection came out in October 2008 in six configurations at prices from about US$1,700 to $2,500 or by individual application.
The Windows version of Photoshop includes 64-bit processing.
On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.

On September 15, 2009, Adobe Systems announced that it would acquire online marketing and web analytics company Omniture for $1.8 billion.
The deal was completed on October 23, 2009.
Former Omniture products were integrated into the Adobe Marketing Cloud.

On November 10, 2009, the company laid off a further 680 employees.
Adobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products.
Former Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wish to maintain control over the iOS platform.
In April 2010, Steve Jobs published a post titled "Thoughts on Flash" where he outlined his thoughts on Flash and the rise of HTML 5.

In July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and MobileIn January 2011, Adobe acquired DemDex, Inc. with the intent of adding DemDex's audience-optimization software to its online marketing suite.
At Photoshop World 2011, Adobe unveiled a new mobile photo service.
Carousel is a new application for iPhone, iPad, and Mac that uses Photoshop Lightroom technology for users to adjust and fine-tune images on all platforms.
Carousel will also allow users to automatically sync, share and browse photos.
The service was later renamed to "Adobe Revel".
In October 2011, Adobe acquired Nitobi Software, the makers of the mobile application development framework PhoneGap.
As part of the acquisition, the source code of PhoneGap was submitted to the Apache Foundation, where it became Apache Cordova.
In November 2011, Adobe announced that they would cease development of Flash for mobile devices following version 11.1.
Instead, it would focus on HTML 5 for mobile devices.
In December 2011, Adobe announced that it entered into a definitive agreement to acquire privately held Efficient Frontier.
In December 2012, Adobe opened a new 280,000-square-foot (26,000 m2) corporate campus in Lehi, Utah.
In 2013, Adobe endured a major security breach.
Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download.
In 2012, about 40 million sets of payment card information were compromised by a hack of Adobe.
A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013.
In May 2014, it was revealed the four companies, Adobe, Apple, Google, and Intel had reached agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.

In March 2018, at Adobe Summit, the company and NVIDIA publicized a key association to quickly upgrade their industry-driving AI and profound learning innovations.
Expanding on years of coordinated effort, the organizations will work to streamline the Adobe Sensei AI and machine learning structure for NVIDIA GPUs.
The joint effort will speed time to showcase and enhance the execution of new Sensei-powered services for Adobe Creative Cloud and Experience Cloud clients and engineers.

Adobe and NVIDIA have co-operated for over 10 years on empowering GPU quickening for a wide arrangement of Adobe's creative and computerized encounter items.
This incorporates Sensei-powered features, for example, auto lip-sync in Adobe Character Animator CC and face-aware editing in Photoshop CC, and also cloud-based AI/ML items and features, for example, picture investigation for Adobe Stock and Lightroom CC and auto-labeling in Adobe Experience Supervisor.
In May 2018, Adobe stated they would buy e-commerce services provider Magento Commerce from private equity firm Permira for $1.68 billion.
This deal will help bolster its Experience Cloud business, which provides services including analytics, advertising, and marketing.
The deal is expected to close during Adobe's fiscal third quarter in 2018.In September 2018, Adobe announced its acquisition of marketing automation software company Marketo.
In October 2018, Adobe officially changed its name from Adobe Systems Incorporated to Adobe Inc.
In January 2019, Adobe announced its acquisition of 3D texturing company Allegorithmic.
In 2020, the annual Adobe Summit was canceled due to the COVID-19 pandemic.
The event is said to take place online this year.
The software giant has imposed a ban on the political ads features on its digital advert sales platform as the United States presidential elections approach.
On November 9, 2020, Adobe announced it will spend US$1.5 billion to acquire Workfront, a provider of marketing collaboration software.

==
Finances ==


== Products ==
Graphic design software
Adobe Photoshop, Adobe Pagemaker, Adobe Lightroom, Adobe InDesign, Adobe InCopy, Adobe ImageReady, Adobe Illustrator, Adobe Freehand, Adobe FrameMaker, Adobe Fireworks, Adobe Acrobat, Adobe XD
Web design programs
Adobe Muse, Adobe GoLive, Adobe Flash Builder, Adobe Flash, Adobe Edge, Adobe Dreamweaver, Adobe Contribute
Video editing, animation, and visual effects
Adobe Ultra, Adobe Spark Video, Adobe Premiere Pro, Adobe Premiere Elements, Adobe Prelude, Adobe Encore, Adobe Director, Adobe Animate, Adobe
After Effects, Adobe Character Animator
Audio editing software
Adobe Soundbooth, Adobe Audition
eLearning software
Adobe Captivate Prime (LMS platform), Adobe Captivate, Adobe Presenter Video Express and Adobe Connect (also a Web conferencing platform)
Digital Marketing Management Software
Adobe Marketing Cloud, Adobe Experience Manager (AEM 6.2)
, XML Documentation add-on (for AEM),
Mixamo
Server software
Adobe ColdFusion, Adobe Content Server and Adobe LiveCycle Enterprise Suite, Adobe BlazeDS
Formats
Portable Document Format (PDF), PDF's predecessor PostScript, ActionScript, Shockwave Flash (SWF), Flash Video (FLV), and Filmstrip (.flm)
Web-hosted services
Adobe Color, Photoshop Express, Acrobat.com, and Adobe Spark3D and
AR
Adobe Aero, Dimension, Mixamo, Substance by Adobe
Adobe Renderer
Adobe Media EncoderAdobe Stock
A microstock agency that presently provides over 57 million high-resolution, royalty-free images and videos available to license (via subscription or credit purchase methods).
In 2015, Adobe acquired Fotolia, a stock content marketplace founded in 2005 by Thibaud Elziere, Oleg Tscheltzoff, and Patrick Chassany which operated in 23 countries.
It is run as a stand-alone website.

Adobe Experience Platform
In March 2019, Adobe released its Adobe Experience Platform, which consists family of content, development, and customer relationship management products, with what it calls the "next generation" of its Sensei artificial intelligence and machine learning framework.

== Reception ==
From 1995 to 2013, Fortune ranked Adobe as "an outstanding place to work".
Adobe was rated the 5th best U.S. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013.
In October 2008, Adobe Systems Canada Inc. was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc. and was featured in Maclean's newsmagazine.
Adobe has a five-star privacy rating from the Electronic Frontier Foundation.

==
Criticisms ==


===
Pricing ===
Adobe has been criticized for its pricing practices, with retail prices being up to twice as much in non-US countries.
For example, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.
After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was £1,000 higher for European customers, a petition to protest over "unfair pricing" was published and signed by 10,000 users.
In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users were not allowed to buy from the US store.
Adobe's Reader and Flash programs were listed on "The 10 most hated programs of all time" article by TechRadar.
In April 2021, Adobe received heavy criticism for the company’s cancellation fees after a customer shared a tweet showing they had been charged a $291.45 cancellation fee for their Adobe Creative Cloud subscription.
Many also showed their cancellation fees for Adobe Creative Cloud, with this leading to many encouraging piracy of Adobe products and/or purchase of alternatives with lower prices.

===
Security ===
Hackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers.
Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player).
A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.
Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture.
When users became aware, Adobe explained what the suspicious software did and admitted that they: "could and should do a better job taking security concerns into account".
When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade.
Following a fierce backlash Adobe decided to provide the software patch.
Adobe has been criticized for pushing unwanted software including third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.

===
Customer data breach ===
On October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in security breach which included encrypted credit card information.
Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts.
The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.
A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org.
LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them.
Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort.
According to IT expert Simon Bain, Adobe has failed its customers and 'should hang their heads in shame'.
Many of the credit cards were tied to the Creative Cloud software-by-subscription service.
Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers.
When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.
After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013.
Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned.
Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as "one of the worst in US history".
Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.
Published on a server of a Russian-speaking hacker group, the "disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data" and may have opened the gateway to new generation zero-day attacks.
Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach.
They also used a ColdFusion exploit to breach Washington state court and expose up to 200,000 Social Security numbers.

=== Anti-competitive practices ===
In 1994, Adobe acquired Aldus Corp., a software vendor that sold FreeHand, a competing product.
FreeHand was direct competition to Adobe Illustrator, Adobe's flagship vector-graphics editor.
The Federal Trade Commission intervened and forced Adobe to sell FreeHand back to Altsys, and also banned Adobe from buying back FreeHand or any similar program for the next 10 years (1994–2004).
Altsys was then bought by Macromedia, which released versions 5 to 11.
When Adobe acquired Macromedia in December 2005, it stalled development of FreeHand in 2007, effectively rendering it obsolete.
With FreeHand and Illustrator, Adobe controlled the only two products that compete in the professional illustration program market for Macintosh operating systems.
In 2011, a group of 5,000 FreeHand graphic designers convened under the banner Free FreeHand, and filed a civil antitrust complaint in the US District Court for the Northern District of California against Adobe.
The suit alleged that Adobe has violated federal and state antitrust laws by abusing its dominant position in the professional vector graphic illustration software market and that Adobe has engaged in a series of exclusionary and anti-competitive acts and strategies designed to kill FreeHand, the dominant competitor to Adobe's Illustrator software product, instead of competing on the basis of product merit according to the principals of free market capitalism.
Adobe had no response to the claims and the lawsuit was eventually settled.
The FreeHand community believes Adobe should release the product to an open-source community if it cannot update it internally.
As of 2010, on its FreeHand product page, Adobe stated, "While we recognize FreeHand has a loyal customer base, we encourage users to migrate to the new Adobe Illustrator CS4 software which supports both PowerPC and Intel-based Macs and Microsoft Windows XP and Windows Vista.
"
As of 2016, the FreeHand page no longer exists; instead, it simply redirects to the Illustrator page.
Adobe's software FTP server still contains a directory for FreeHand, but it is empty.

== See also ==
Adobe MAX
Digital rights management (DRM)
List of acquisitions by Adobe
List of Adobe software
US v. ElcomSoft
Sklyarov


== References ==


==
External links ==
Official website
Business data for Adobe Inc.:
"Patents owned by Adobe Inc".
US Patent & Trademark Office.
Retrieved December 8, 2005.
The following outline is provided as an overview of and topical guide to thought (thinking):
Thought (also called thinking) is the mental process in which beings form psychological associations and models of the world.

Thinking is manipulating information, as when we form concepts, engage in problem solving, reason and make decisions.

Thought, the act of thinking, produces more thoughts.

A thought may be an idea, an image, a sound or even control an emotional feeling.

==
Nature of thought ==
Thought (or thinking) can be described as all of the following:
An activity taking place in a:
brain – organ that serves as the center of the nervous system in all vertebrate and most invertebrate animals
(only a few invertebrates such as sponges, jellyfish, adult sea squirts and starfish do not have a brain).
It is the physical structure associated with the mind.

mind – abstract entity with the cognitive faculties of consciousness, perception, thinking, judgement, and memory.
Having a mind is a characteristic of living creatures.
Activities taking place in a mind are called mental processes or cognitive functions.

computer (see § Machine thought below) – general purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically.
Since a sequence of operations (an algorithm) can be readily changed, the computer can solve more than one kind of problem.

An activity of intelligence – intelligence is the intellectual process of  which is marked by cognition, motivation, and self-awareness.
Through intelligence, living creatures possess the cognitive abilities to learn, form concepts, understand, apply logic, and reason, including the capacities to recognize patterns, comprehend ideas, plan, problem solve, make decisions, retaining, and use language to communicate.
Intelligence enables living creatures to experience and think.

A type of mental process – something that individuals can do with their minds.
Mental processes include perception, memory, thinking, volition, and emotion.
Sometimes the term cognitive function is used instead.

Thought as a biological adaptation
mechanismNeural Network explanation:
Thoughts are created by the summation of neural outputs and connections of which vectors form.
These vectors describe the magnitude and direction of the connections and action between neurons.
The graphs of these vectors can represent a network of neurons whose connections fire in different ways over time as synapses fire.

These large thought vectors in the brain cause other vectors of activity.

For example: An input from the environment is received by the neural network.

The network changes the magnitude and outputs of individual neurons.

The altered network outputs the symbols needed to make sense of the input.
==
Types of thoughts ==


===
Content of thoughts ===


==
Types of thought (thinking) ==
Listed below are types of thought, also known as thinking processes.

===
Animal thought ===


===
Human thought ===
Human thought


====
Classifications of thought ====
Bloom's taxonomy – Classification system in education
Dual process theory –
Psychological theory of how thought can arise in two different ways
Fluid and crystallized intelligence – Factors of general intelligence
Higher-order thinking – A concept of education reform
Theory of multiple intelligences – Theory of intelligence proposed by Howard Gardner
Three-stratum theory
Williams' taxonomy


====
Creative processes ====


====
Decision-making ====


====
Erroneous thinking ====


====
Emotional intelligence (emotionally based thinking) ====
Emotional intelligence
– Capability to understand one's emotions and use it to guide thinking and behavior


====
Problem solving ====
Problem solving –
Generic and ad hoc approach to problem solving


====
Reasoning ====
Reasoning


===
Machine thought ===


===
Organizational thought ===
Organizational thought (thinking by organizations)
Management information system – Information system used for decision-making
Organizational communication
Organizational planning
Strategic planning
Strategic thinking
Systems theory –
Interdisciplinary study of systems


==
Aspects of the thinker ==
Aspects of the thinker which may affect (help or hamper)
his or her thinking:


==
Properties of thought ==


==
Fields that study
thought ==


==
Thought tools and thought research ==
Cognitive model
Design tool
Diagram – Symbolic representation of information using visualization techniques
Argument map
Concept map –
Diagram showing relationships among concepts
Mind map – System or map
used to visually organize information
DSRP
Intelligence amplification
Language – Capacity to communicate using signs, such as words or gestures
Meditation – Mental practice of focus on a particular object, thought or activity to improve one's mind
Six Thinking Hats
Synectics


==
History of thinking ==
History of reasoning

History of artificial intelligence – Overview of the history of artificial intelligence
History of cognitive science
History of the concept of creativity
History of ideas
History of logic – Study of the history of the science of valid inference
History of psychometrics


== Nootropics (cognitive enhancers and smart drugs)
==
Nootropic – Drug, supplement, or other substance that improves cognitive function

Substances that improve mental performance:


===
Organizational thinking concepts ===


==
Teaching methods and skills ==


==
Awards related to thinking ==


===
Awards for acts of genius ===
Nobel Prize –
Set of annual international awards, primarily 5 established in 1895 by Alfred Nobel
Pulitzer Prize – Award for achievements in journalism, literature, and musical composition within the United States
MacArthur Fellows Program – prize awarded annually by the John D. and Catherine T. MacArthur Foundation


==
Organizations ==
Associations pertaining to thought
Association for Automated Reasoning
Association for Informal Logic and Critical Thinking
International Joint Conference on Automated Reasoning
High IQ societies
Mega Society – High IQ society
Mensa – Largest and oldest high IQ society in the world
Mind Sports Organisations
World Mind
Sports Games
Think tank – Organization that performs policy research and advocacys


==
Media ==
===
Publications ===
====
Books ====
Handbook of Automated Reasoning


====
Periodicals ====
Journal of Automated Reasoning
Journal of Formalized Reasoning
Positive Thinking Magazine


===
Television programs ===
Thinkabout (U.S. TV series)


==
Persons associated with thinking ==


===
People notable for their extraordinary ability to think ===
Geniuses
List of Nobel laureates (see also Nobel Prize)
Polymaths


===
Scientists in fields that study thought ===
List of cognitive scientists


===
Scholars of thinking ===
Aaron T. Beck
Edward de Bono
David D. Burns – author of Feeling Good:
The New Mood Therapy and The Feeling Good Handbook.
Burns popularized Aaron T. Beck's cognitive behavioral therapy (CBT) when his book became a best seller during the 1980s.

Tony Buzan
Noam Chomsky
Albert Ellis
Howard Gardner
Eliyahu M. Goldratt
Douglas Hofstadter
Ray Kurzweil
Marvin Minsky
Steven Pinker
Baruch Spinoza
Robert Sternberg


==
Related concepts ==
Cognition
Knowledge
Multiple intelligences
Strategy
Structure
System


===
Awareness and perception ===


===
Learning and memory ===


==
See also ==
Artificial intelligence
Outline of artificial intelligence
Human intelligence
Outline of human intelligence
Neuroscience
Outline of neuroscience
Psychology
Gestalt psychology (theory of mind)
Outline of psychologyMiscellaneous
Thinking
Lists


==
References ==


==
External links ==
The Psychology of Emotions, Feelings and Thoughts, Free Online Book
In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions‍—‌or algorithms‍—‌that a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer.
When the cache is full, the algorithm must choose which items to discard to make room for the new ones.
Due to the inherent caching capability of nodes in Information-centric networking ICN, the ICN can be viewed as a loosely connect network of caches, which has unique requirements of Caching policies.
Unlike proxy servers, in Information-centric networking the cache is a network level solution.
Therefore, it has rapidly changing cache states and higher request arrival rates; moreover, smaller cache sizes further impose different kind of requirements on the content eviction policies.
In particular, eviction policies for Information-centric networking should be fast and lightweight.
Various cache replication and eviction schemes for different Information-centric networking architectures and applications are proposed.

==
Policies ==


===
Time aware least recently used (TLRU) ===
The Time aware Least Recently Used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid life time.
The algorithm is suitable in network cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
TLRU introduces a new term: TTU (Time to Use).
TTU is a time stamp of a content/page which stipulates the usability time for the content based on the locality of the content and the content publisher announcement.
Owing to this locality based time stamp, TTU provides more control to the local administrator to regulate in network storage.

In the TLRU algorithm, when a piece of content arrives, a cache node calculates the local TTU value based on the TTU value assigned by the content publisher.
The local TTU value is calculated by using a locally defined function.
Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node.
The TLRU ensures that less popular and small life content should be replaced with the incoming content.

=== Least frequent recently used (LFRU) ===
The Least Frequent Recently Used (LFRU) cache replacement scheme combines the benefits of LFU and LRU schemes.
LFRU  is suitable for ‘in network’ cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
In LFRU, the cache is divided into two partitions called privileged and unprivileged partitions.
The privileged partition can be defined as a protected partition.
If content is highly popular, it is pushed into the privileged partition.
Replacement of the privileged partition is done as follows:  LFRU evicts content from the unprivileged partition, pushes content from privileged partition to unprivileged partition, and finally inserts new content into the privileged partition.

In the above procedure the LRU is used for the privileged partition and an approximated LFU (ALFU) scheme is used for the unprivileged partition, hence the abbreviation LFRU.

The basic idea is to filter out the locally popular contents with ALFU scheme and push the popular contents to one of the privileged partition.

==
References ==
In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication.
It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.

Strassen's algorithm works for any ring, such as plus/multiply, but not all semirings, such as min-plus or boolean algebra, where the naive algorithm still works, and so called combinatorial matrix multiplication.

==
History ==
Volker Strassen first published this algorithm in 1969 and proved that the n3 general matrix multiplication algorithm wasn't optimal.
The Strassen algorithm is only slightly better than that, but its publication resulted in much more research about matrix multiplication that led to faster approaches, such as the Coppersmith
–Winograd algorithm.

==
Algorithm ==
Let A, B be two square matrices over a ring R,
for example matrices whose entries are integers or the real numbers.
We want to calculate the matrix product C as
C
        
        =
A
B
        
      
    
    {\displaystyle \mathbf {C} =\mathbf {A} \mathbf {B} }
In the following exposition of the algorithm, we will assume that all of these matrices have sizes that are powers of two (i.e.,
A
        
        ,
B
        
        ,
C
        
        ∈
R
2
              
                n
×
2
n
{\displaystyle \mathbf {A} ,\mathbf {B} ,\mathbf {C} \in R^{2^{n}\times 2^{n}}}
  ),
but this is only conceptually necessary --
if the matrices A, B are not of type 2n × 2n we can think conceptually about filling the "missing" rows and columns with zeros to obtain matrices with sizes of powers of two -- though real implementations of the algorithm will of course not actually do this in practice.

We then partition A, B and C into equally sized block matrices

  
    
      
        
          A
        
        =
[
            
              
                
                  
                    
                      A
                    
                    
                      1
                      ,
                      1
A
                    
                    
                      1
                      ,
                      2
A
                    
                    
                      2
                      ,
                      1
A
                    
                    
                      2
                      ,
                      2
                    
                  
                
              
            
            ]
          
        
        
          
             , 
          
        
        
          B
        
        =
[
            
              
                
                  
                    
                      B
                    
                    
                      1
                      ,
                      1
B
                    
                    
                      1
,
                      2
B
                    
                    
                      2
                      ,
                      1
B
                    
                    
                      2
,
                      2
                    
                  
                
              
            
            ]
          
        
        
          
             , 
          
        
        
          C
        
        =
[
            
              
                
                  
                    
                      C
                    
                    
                      1
                      ,
                      1
C
                    
                    
                      1
                      ,
                      2
C
                    
                    
                      2
,
                      1
C
                    
                    
                      2
,
                      2
]
          
        
      
    
    {\displaystyle \mathbf {A} ={\begin{bmatrix}\mathbf {A} _{1,1}&\mathbf {A} _{1,2}\\\mathbf {A} _{2,1}&\mathbf {A} _{2,2}\end{bmatrix}}{\mbox{ , }}\mathbf {B} ={\begin{bmatrix}\mathbf {B} _{1,1}&\mathbf {B} _{1,2}\\\mathbf {B} _{2,1}&\mathbf {B} _{2,2}\end{bmatrix}}{\mbox{ , }}\mathbf {C} ={\begin{bmatrix}\mathbf {C} _{1,1}&\mathbf {C} _{1,2}\\\mathbf {C} _{2,1}&\mathbf {C} _{2,2}\end{bmatrix}}}
  with

  
    
      
        
          
            A
i
            ,
            j
          
        
        ,
        
          
            B
i
            ,
            j
          
        
        ,
        
          
            C
i
            ,
j
          
        
        ∈
        
          R
2
              
                n
−
1
×
            
              2
n
−
1
              
            
          
        
      
    
    {\displaystyle \mathbf {A} _{i,j},\mathbf {B} _{i,j},\mathbf
{C} _{i,j}\in R^{2^{n-1}\times 2^{n-1}}}
.The
naive algorithm would be:

  
    
      
        
          
            C
          
          
            1
,
            1
          
        
        =
A
          
          
            1
            ,
1
          
        
        
          
            B
          
          
            1
            ,
            1
+
A
          
          
            1
            ,
2
B
          
          
            2
            ,
            1
          
        
      
    
    {\displaystyle \mathbf {C} _{1,1}=\mathbf {A} _{1,1}\mathbf {B} _{1,1}+\mathbf {A} _{1,2}\mathbf {B} _{2,1}}
  

  
    
      
        
          
            C
          
          
            1
            ,
2
          
        
        =
A
          
          
            1
            ,
            1
          
        
        
          
            B
          
          
            1
            ,
            2
+
A
          
          
            1
            ,
2
B
          
          
            2
            ,
2
          
        
      
    
    {\displaystyle \mathbf {C} _{1,2}=\mathbf {A} _{1,1}\mathbf {B} _{1,2}+\mathbf {A} _{1,2}\mathbf {B} _{2,2}}
C
          
          
            2
            ,
1
          
        
        =
A
          
          
            2
            ,
1
          
        
        
          
            B
          
          
            1
            ,
            1
+
A
          
          
            2
            ,
2
B
          
          
            2
            ,
            1
          
        
      
    
    {\displaystyle \mathbf {C} _{2,1}=\mathbf {A} _{2,1}\mathbf {B} _{1,1}+\mathbf {A} _{2,2}\mathbf {B} _{2,1}}
C
          
          
            2
            ,
2
          
        
        =
A
          
          
            2
            ,
            1
          
        
        
          
            B
          
          
            1
            ,
            2
+
A
          
          
            2
            ,
2
B
          
          
            2
            ,
2
          
        
      
    
    {\displaystyle \mathbf {C} _{2,2}=\mathbf {A} _{2,1}\mathbf {B} _{1,2}+\mathbf {A} _{2,2}\mathbf {B} _{2,2}}
With this construction we have not reduced the number of multiplications.
We still need 8 multiplications of matrix blocks to calculate the 
  
    
      
        
          C
i
            ,
j
          
        
      
    
    {\displaystyle C_{i,j}}
   matrices, the same number of multiplications we need when using standard matrix multiplication.

The Strassen algorithm defines instead new matrices:
M
          
          
            1
          
        
        :=
(
A
          
          
            1
            ,
            1
+
A
          
          
            2
            ,
            2
          
        
        )
(
        
          
            B
          
          
            1
            ,
            1
+
        
          
            B
          
          
            2
            ,
2
          
        
        )
      
    
    {\displaystyle \mathbf {M} _{1}:=(\mathbf {A} _{1,1}+\mathbf {A} _{2,2})(\mathbf {B} _{1,1}+\mathbf {B} _{2,2})}
M
          
          
            2
          
        
        :=
(
A
          
          
            2
            ,
            1
+
A
          
          
            2
            ,
            2
          
        
        )
B
          
          
            1
            ,
            1
          
        
      
    
    {\displaystyle \mathbf {M} _{2}:=(\mathbf {A} _{2,1}+\mathbf {A} _{2,2})\mathbf {B} _{1,1}}
M
          
          
            3
          
        
        :=
A
          
          
            1
            ,
            1
(
        
          
            B
          
          
            1
            ,
2
          
        
        −
B
          
          
            2
            ,
2
          
        
        )
      
    
    {\displaystyle \mathbf {M} _{3}:=\mathbf {A} _{1,1}(\mathbf {B} _{
1,2}-\mathbf {B} _{2,2})}
M
          
          
            4
          
        
        :=
A
          
          
            2
            ,
            2
(
        
          
            B
          
          
            2
            ,
            1
          
        
        −
B
          
          
            1
            ,
1
          
        
        )
      
    
    {
\displaystyle \mathbf {M} _{4}:=\mathbf {A} _{2,2}(\mathbf {B} _{
2,1}-\mathbf {B} _{1,1})}
M
          
          
            5
          
        
        :=
(
A
          
          
            1
            ,
            1
+
A
          
          
            1
            ,
            2
          
        
        )
B
          
          
            2
            ,
2
          
        
      
    
    {
\displaystyle \mathbf {M} _{5}:=(\mathbf {A} _{1,1}+\mathbf {A} _{1,2})\mathbf {B} _{2,2}}
M
          
          
            6
          
        
        :=
(
A
          
          
            2
            ,
1
          
        
        −
A
          
          
            1
            ,
            1
          
        
        )
(
        
          
            B
          
          
            1
            ,
            1
+
B
          
          
            1
            ,
            2
          
        
        )
      
    
    {\displaystyle
\mathbf {M} _{6}:=(\mathbf {A} _{2,1}-\mathbf {A} _{1,1})(\mathbf {B} _{1,1}+\mathbf {B} _{1,2})}
M
          
          
            7
          
        
        :=
(
A
          
          
            1
            ,
2
          
        
        −
A
          
          
            2
            ,
            2
          
        
        )
(
        
          
            B
          
          
            2
            ,
            1
+
        
          
            B
          
          
            2
            ,
2
          
        
        )
      
    
    {\displaystyle \mathbf {M} _{7}:=(\mathbf {A} _{1,2}-\mathbf {A} _{2,2})(\mathbf {B} _{2,1}+\mathbf {B} _{2,2})}
  only using 7 multiplications (one for each
M
k
          
        
      
    
    {\displaystyle M_{k}}
  ) instead of 8.
We may now express the 
  
    
      
        
          C
i
            ,
j
          
        
      
    
    {\displaystyle C_{i,j}}
   in terms of 
  
    
      
        
          M
          
            k
          
        
      
    
    {\displaystyle M_{k}}
  :
C
          
          
            1
            ,
1
          
        
        =
M
          
          
            1
+
M
          
          
            4
          
        
        −
M
          
          
            5
+
M
          
          
            7
          
        
      
    
    {\displaystyle \mathbf {C} _{1,1}=\mathbf {M} _{1}+\mathbf {M} _{4}-\mathbf {M} _{5}+\mathbf {M} _{7}}
C
          
          
            1
            ,
2
=
M
          
          
            3
+
M
          
          
            5
          
        
      
    
    {\displaystyle \mathbf {C} _{1,2}=\mathbf {M} _{3}+\mathbf {M} _{5}}
  

  
    
      
        
          
            C
2
,
            1
          
        
        =
M
          
          
            2
+
        
          
            M
          
          
            4
          
        
      
    
    {\displaystyle \mathbf {C} _{2,1}=\mathbf {M} _{2}+\mathbf {M} _{4}}
C
          
          
            2
            ,
2
=
M
          
          
            1
          
        
        −
M
          
          
            2
+
        
          
            M
          
          
            3
+
        
          
            M
          
          
            6
          
        
      
    
    {\displaystyle \mathbf {C} _{2,2}=\mathbf
{M} _{1}-\mathbf {M} _{2}+\mathbf {M} _{3}+\mathbf {M} _{6}}
We recursively iterate this division process until the submatrices degenerate into numbers (elements of the ring R).
If, as mentioned above, the original matrix had a size that was not a power of 2, then the resulting product will have zero rows and columns just like A and B, and these will then be stripped at this point to obtain the (smaller) matrix C we really wanted.

Practical implementations of Strassen's algorithm switch to standard methods of matrix multiplication for small enough submatrices, for which those algorithms are more efficient.
The particular crossover point for which Strassen's algorithm is more efficient depends on the specific implementation and hardware.
Earlier authors had estimated that Strassen's algorithm is faster for matrices with widths from 32 to 128 for optimized implementations.
However, it has been observed that this crossover point has been increasing in recent years, and a 2010 study found that even a single step of Strassen's algorithm is often not beneficial on current architectures, compared to a highly optimized traditional multiplication,
until matrix sizes exceed 1000 or more, and even for matrix sizes of several thousand the benefit is typically marginal at best (around 10% or less).

A more recent study (2016) observed benefits for matrices as small as 512 and a benefit around 20%.

==
Asymptotic complexity ==
The outline of the algorithm above showed that one can get away with just 7, instead of the traditional 8, matrix-matrix multiplications for the sub-blocks of the matrix.
On the other hand, one has to do additions and subtractions of blocks, though this is of no concern for the overall complexity:
Adding matrices of size N/2 requires only (N/2)2 operations whereas multiplication is substantially more expensive (traditionally 2(N/2)3) addition or multiplication operations).

The question then is how many operations exactly one needs for Strassen's algorithms, and how this compares with the standard matrix multiplication that takes approximately 2N3 (where N = 2n)
arithmetic operations, i.e. an asymptotic complexity Θ(N3).

The number of additions and multiplications required in the Strassen algorithm can be calculated as follows: let f(n)
be the number of operations for a 2n × 2n matrix.
Then by recursive application of the Strassen algorithm, we see that f(n) =
7f(n−1)
+ ℓ4n, for some constant ℓ that depends on the number of additions performed at each application of the algorithm.
Hence f(n) =
(7 + o(1))n, i.e., the asymptotic complexity for multiplying matrices of size N = 2n using the Strassen algorithm is

  
    
      
        O
(
[
        7
+
        o
(
        1
        )
        
          ]
n
          
        
        )
        =
O
(
        
          N
          
            
              log
              
                2
⁡
7
+
o
(
            1
            )
          
        
        )
≈
        O
(
        
          N
2.8074
          
        
        )
{\displaystyle O([7+o(1)]^{n})=O(N^{\log _{2}7+o(1)})\approx O(N^{2.8074})}
.The reduction in the number of arithmetic operations however comes at the price of a somewhat reduced numerical stability, and the algorithm also requires significantly more memory compared to the naive algorithm.
Both initial matrices must have their dimensions expanded to the next power of 2, which results in storing up to four times as many elements, and the seven auxiliary matrices each contain a quarter of the elements in the expanded ones.

Strassen's algorithm needs to be compared to the "naive" way of doing the matrix multiplication that would require 8 instead of 7 multiplications of sub-blocks.
This would then give rise to the complexity one expects from the standard approach:
O
(
        
          8
log
              
                2
⁡
n
          
        
        )
        =
O
(
        
          N
          
            
              log
              
                2
⁡
            8
          
        
        )
        =
O
(
        
          N
          
            3
          
        
        )
{\displaystyle O(8^{\log _{2}n})=O(N^{\log _{2}8})=O(N^{3})}
.The
comparison of these two algorithms shows that asymptotically, Strassen's algorithm is faster: There exists a size Nthreshold so that matrices that are larger are more efficiently multiplied with Strassen's algorithm than the "traditional" way.
However, the asymptotic statement does not imply that Strassen's algorithm is always faster even for small matrices, and in practice this is in fact not the case: For small matrices, the cost of the additional additions of matrix blocks outweighs the savings in the number of multiplications.
There are also other factors not captured by the analysis above, such as the difference in cost on today's hardware between loading data from memory onto processors vs. the cost of actually doing operations on this data.
As a consequence of these sorts of considerations, Strassen's algorithm is typically only used on "large" matrices.
This kind of effect is even more pronounced with alternative algorithms such as the one by Coppersmith and Winograd:
While asymptotically even faster, the cross-over point
Nthreshold is so large that the algorithm is not generally used on matrices one encounters in practice.

=== Rank or bilinear complexity ===
The bilinear complexity or rank of a bilinear map is an important concept in the asymptotic complexity of matrix multiplication.

The rank of a bilinear map
ϕ
        :
A
        
        ×
        
          B
        
        →
        
          C
        
      
    
    {\displaystyle \phi :
\mathbf {A} \times \mathbf {B} \rightarrow \mathbf {C} }
   over a field F is defined as (somewhat of an abuse of notation)

  
    
      
        R
(
        ϕ
        
          /
        
        
          F
        
        )
        =
min
        
          {
r
|
              
                ∃
                
                  f
i
                  
                
                ∈
A
                  
                  
                    ∗
                  
                
                ,
                
                  g
i
                  
                
                ∈
                
                  
                    B
∗
                  
                
                ,
                
                  w
i
                  
                
                ∈
C
                
                ,
                ∀
a
                
                ∈
A
                
                ,
b
                
                ∈
                
                  B
                
                ,
ϕ
(
                
                  a
                
                ,
                
                  b
                
                )
=
∑
i
                    =
1
                  
                  
                    r
                  
                
                
                  f
i
(
                
                  a
                
                )
                
                  g
i
(
                
                  b
                
                )
                
                  w
i
}
{\displaystyle R(\phi /\mathbf {F} )=
\min \left\{r\left|\exists f_{i}\in \mathbf {A} ^{*},g_{i}\in \mathbf {B} ^{*},w_{i}\in \mathbf {C} ,\forall \mathbf {a} \in \mathbf {A} ,
\mathbf {b} \in \mathbf {B} ,\phi
(\mathbf {a} ,\mathbf {b} )=
\sum _{i=1}^{r}f_{i}(\mathbf {a} )g_{i}(\mathbf {b} )w_{i}\right.\right\}}
In other words, the rank of a bilinear map is the length of its shortest bilinear computation.

The existence of Strassen's algorithm shows that the rank of 2×2 matrix multiplication is no more than seven.

To see this, let us express this algorithm (alongside the standard algorithm) as such a bilinear computation.

In the case of matrices, the dual spaces
A* and B* consist of maps into the field F induced by a scalar double-dot product,
(i.e. in this case the sum of all the entries of a Hadamard product.)

It can be shown that the total number of elementary multiplications L required for matrix multiplication is tightly asymptotically bound to the rank R, i.e. 
  
    
      
        L
        =
        Θ
        (
        R
)
      
    
    {\displaystyle L=\Theta (R)}
  , or more specifically, since the constants are known, 
  
    
      
        
          
            1
            2
          
        
        R
        ≤
L
        ≤
        R
        .
{
\displaystyle {\frac {1}{2}}R\leq L\leq R.}
One useful property of the rank is that it is submultiplicative for tensor products, and this enables one to show that 2n×2n×2n matrix multiplication can be accomplished with no more than 7n elementary multiplications for any n.
(This n-fold tensor product of the 2×2×2 matrix multiplication map with itself—an nth tensor power—is realized by the recursive step in the algorithm shown.)
===
Cache behavior ===
Strassen's algorithm is cache oblivious.
Analysis of its cache behavior algorithm has shown it to incur

  
    
      
        Θ
        
          (
          
            1
+
            
              
                
                  n
                  
                    2
                  
                
                b
+
            
              
                
                  n
log
                      
                        2
⁡
7
b
                  
                    
                      M
)
{\displaystyle \Theta \left(1+{\frac {n^{2}}{b}}+{\frac {n^{\log _{2}7}}{b{\sqrt {M}}}}\right)}
cache misses during its execution, assuming an idealized cache of size
M (i.e. with 
  
    
      
        
          
            M
            b
          
        
      
    
    {\displaystyle {\frac {M}{b}}}
   lines of length b).
==
Implementation considerations ==
The description above states that the matrices are square, and the size is a power of two, and that padding should be used if needed.
This restriction allows the matrices to be split in half, recursively, until limit of scalar multiplication is reached.
The restriction simplifies the explanation, and analysis of complexity, but is not actually necessary;
and in fact, padding the matrix as described will increase the computation time and can easily eliminate the fairly narrow time savings obtained by using the method in the first place.

A good implementation will observe the following:

It is not necessary or desirable to use the Strassen algorithm down to the limit of scalars.
Compared to conventional matrix multiplication, the algorithm adds a considerable  
  
    
      
        O
(
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
workload in addition/subtractions; so below a certain size, it will be better to use conventional multiplication.
Thus, for instance, a 1600x1600 does not need to be padded to 2048x2048, since it could be subdivided down to 25x25 matrices and conventional multiplication can then be used at that level.

The method can indeed be applied to square matrices of any dimension.
If the dimension is even, they are split in half as described.
If the dimension is odd, zero padding by one row and one column is applied first.

Such padding can be applied on-the-fly and lazily, and the extra rows and columns discarded as the result is formed.
For instance, suppose the matrices are 199x199.
They can be split so that the upper-left portion is 100x100
and the lower-right is 99x99.
Wherever the operations require it, dimensions of 99 are zero padded to 100 first.
Note, for instance, that the product
M
2
          
        
      
    
    {\displaystyle M_{2}}
   is only used in the lower row of the output, so is only required to be 99 rows high; and thus the left factor 
  
    
      
        (
        
          A
          
            2
            ,
            1
+
A
          
            2
            ,
2
          
        
        )
      
    
    {
\displaystyle (A_{2,1}+A_{2,2})
}
   used to generate it need only be 99 rows high; accordingly, there is no need to pad that sum to 100 rows; it is only necessary to pad
A
          
            2
            ,
2
          
        
      
    
    {\displaystyle A_{2,2}}
to 100 columns to match
A
          
            2
            ,
            1
          
        
      
    
    {\displaystyle A_{2,1}}
.Furthermore, there is no need for the matrices to be square.
Non-square matrices can be split in half using the same methods, yielding smaller non-square matrices.
If the matrices are sufficiently non-square it will be worthwhile reducing the initial operation to more square products, using simple methods which are essentially  
  
    
      
        O
(
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
  ,
for instance:

A product of size [2N x N] * [N x 10N] can be done as 20 separate [N x N]
*
[N x N] operations, arranged to form the result;
A product of size [N x 10N]
*
[10N x N] can be done as 10 separate [N x N] *
[N x N]
operations, summed to form the result.
These techniques will make the implementation more complicated, compared to simply padding to a power-of-two square; however, it is a reasonable assumption that anyone undertaking an implementation of Strassen, rather than conventional, multiplication, will place a higher priority on computational efficiency than on simplicity of the implementation.

In practice, Strassen's algorithm can be implemented to attain better performance than conventional multiplication even for small matrices, for matrices that are not at all square, and without requiring workspace beyond buffers that are already needed for a high-performance conventional multiplication.

== See also ==
Computational complexity of mathematical operations
Gauss–Jordan elimination
Coppersmith–
Winograd algorithm
Z-order matrix representation
Karatsuba algorithm, for multiplying n-digit integers in 
  
    
      
        O
(
        
          n
          
            
              log
              
                2
⁡
            3
          
        
        )
{\displaystyle O(n^{\log _{2}3})}
instead of in 
  
    
      
        O
(
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle O(n^{2})}
time
Toom-Cook algorithm, a faster generalization of the Karatsuba algorithm that permits recursive divide-and-conquer decomposition into more than 2 blocks at a time
Gauss's complex multiplication algorithm multiplies two complex numbers using 3 real multiplications instead of
4


==
References ==
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms, Second Edition.
MIT Press and McGraw-Hill, 2001.
ISBN 0-262-03293-7.
Chapter 28:
Section 28.2:
Strassen's algorithm for matrix multiplication, pp.
735–741.

==
External links ==
Weisstein, Eric W. "Strassen's Formulas".
MathWorld.
(
also includes formulas for fast matrix inversion)
Tyler J. Earnest, Strassen's Algorithm on the Cell Broadband Engine
In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer.
Caching improves performance by keeping recent or often-used data items in memory locations that are faster or computationally cheaper to access than normal memory stores.
When the cache is full, the algorithm must choose which items to discard to make room for the new ones.

==
Overview ==
The average memory reference time is

  
    
      
        T
=
        m
×
T
m
          
        
        +
        
          T
          
            h
+
E
      
    
    {\displaystyle
T=m\times
T_{m}+T_{h}+E}
where

  
    
      
        m
      
    
    {\displaystyle m}
   = miss ratio = 1 - (hit ratio)
T
m
          
        
      
    
    {\displaystyle T_{m}}
   =
time to make a main memory access when there is a miss (or, with multi-level cache, average memory reference time for the next-lower cache)
T
          
            h
          
        
      
    
    {\displaystyle T_{h}}
= the latency:
the time to reference the cache (should be the same for hits and misses)
E
      
    
    {\displaystyle E}
   =
various secondary effects, such as queuing effects in multiprocessor systemsThere are two primary figures of merit of a cache:
The latency, and the hit rate.

There are also a number of secondary factors affecting cache performance.
The "hit ratio" of a cache describes how often a searched-for item is actually found in the cache.

More efficient replacement policies keep track of more usage information in order to improve the hit rate (for a given cache size).

The "latency" of a cache describes how long after requesting a desired item the cache can return that item (when there is a hit).

Faster replacement strategies typically keep track of less usage information—or, in the case of direct-mapped cache, no information—to reduce the amount of time required to update that information.

Each replacement strategy is a compromise between hit rate and latency.

Hit rate measurements are typically performed on benchmark applications.

The actual hit ratio varies widely from one application to another.
In particular, video and audio streaming applications often have a hit ratio close to zero, because each bit of data in the stream is read once for the first time (a compulsory miss), used, and then never read or written again.

Even worse, many cache algorithms (in particular, LRU) allow this streaming data to fill the cache, pushing out of the cache information that will be used again soon (cache pollution).Other things to consider:
Items with different cost: keep items that are expensive to obtain, e.g. those that take a long time to get.

Items taking up more cache
: If items have different sizes, the cache may want to discard a large item to store several smaller ones.

Items that expire with time: Some caches keep information that expires (e.g. a news cache, a DNS cache, or a web browser cache).
The computer may discard items because they are expired.
Depending on the size of the cache
no further caching algorithm to discard items may be necessary.
Various algorithms also exist to maintain cache coherency.
This applies only to situation where multiple independent caches are used for the same data (for example multiple database servers updating the single shared data file).

==
Policies ==


===
Bélády's algorithm ===
The most efficient caching algorithm would be to always discard the information that will not be needed for the longest time in the future.
This optimal result is referred to as Bélády's optimal algorithm/simply optimal replacement policy or the clairvoyant algorithm.
Since it is generally impossible to predict how far in the future information will be needed, this is generally not implementable in practice.
The practical minimum can be calculated only after experimentation, and one can compare the effectiveness of the actually chosen cache algorithm.

At the moment when a page fault occurs, some set of pages is in memory.
In the example, the sequence of '5', '0', '1' is accessed by Frame 1, Frame 2, Frame 3 respectively.
Then when '2' is accessed, it replaces value '5', which is in frame 1 since it predicts that value '5' is not going to be accessed in the near future.
Because a real-life general purpose operating system cannot actually predict when '5' will be accessed, Bélády's Algorithm cannot be implemented on such a system.

===
First in first out (FIFO) ===
Using this algorithm the cache behaves in the same way as a FIFO queue.
The cache evicts the blocks in the order they were added, without any regard to how often or how many times they were accessed before.
===
Last in first out (LIFO) or First in last out (FILO) ===
Using this algorithm the cache behaves in the same way as a stack and exact opposite way as a FIFO queue.
The cache evicts the block added most recently first without any regard to how often or how many times it was accessed before.
===
Least recently used (LRU) ===
Discards the least recently used items first.
This algorithm requires keeping track of what was used when, which is expensive if one wants to make sure the algorithm always discards the least recently used item.
General implementations of this technique require keeping "age bits" for cache-lines and track the "Least Recently Used" cache-line based on age-bits.
In such an implementation, every time a cache-line is used, the age of all other cache-lines changes.
LRU is actually a family of caching algorithms with members including 2Q by Theodore Johnson and Dennis Shasha, and LRU/K by Pat O'Neil, Betty O'Neil and Gerhard Weikum.
The access sequence for the below example is A B C D E D F.
In the above example once A B C D gets installed in the blocks with sequence numbers (Increment 1 for each new Access) and when E is accessed, it is a miss
and it needs to be installed in one of the blocks.
According to the LRU Algorithm, since A has the lowest Rank(A(0)), E will replace A.
In the second last step D is accessed and therefore the sequence number is updated.

LRU, like many other replacement policies, can be characterized using a state transition field in a vector space, which decides the dynamic cache state changes similar to how an electromagnetic field determines the movement of a charged particle placed in it.

===
Time aware least recently used (TLRU) ===
The Time aware Least Recently Used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid life time.
The algorithm is suitable in network cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
TLRU introduces a new term: TTU (Time to Use).
TTU is a time stamp of a content/page which stipulates the usability time for the content based on the locality of the content and the content publisher announcement.
Owing to this locality based time stamp, TTU provides more control to the local administrator to regulate in network storage.

In the TLRU algorithm, when a piece of content arrives, a cache node calculates the local TTU value based on the TTU value assigned by the content publisher.
The local TTU value is calculated by using a locally defined function.
Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node.
The TLRU ensures that less popular and small life content should be replaced with the incoming content.

===
Most recently used (MRU) ===
Discards, in contrast to LRU, the most recently used items first.
In findings presented at the 11th VLDB conference, Chou and DeWitt noted that "When a file is being repeatedly scanned in a [Looping Sequential] reference pattern, MRU is the best replacement algorithm."
Subsequently, other researchers presenting at the 22nd VLDB conference noted that for random access patterns and repeated scans over large datasets (sometimes known as cyclic access patterns) MRU cache algorithms have more hits than LRU due to their tendency to retain older data.
MRU algorithms are most useful in situations where the older an item is,
the more likely it is to be accessed.

The access sequence for the below example is A B C D E C D B.
Here, A B C D are placed in the cache as there is still space available.
At the 5th access E, we see that the block which held D is now replaced with E as this block was used most recently.
Another access to C and at the next access to D, C is replaced as it was the block accessed just before D and so on.
===
Pseudo-LRU (PLRU) ===
For CPU caches with large associativity (generally >4 ways), the implementation cost of LRU becomes prohibitive.
In many CPU caches, a scheme that almost always discards one of the least recently used items is sufficient, so many CPU designers choose a PLRU algorithm which only needs one bit per cache item to work.

PLRU typically has a slightly worse miss ratio, has a slightly better latency, uses slightly less power than LRU and lower overheads compared to LRU.

The following example shows how Bits work as a binary tree of 1-bit pointers that point to the less recently used subtree.
Following the pointer chain to the leaf node identifies the replacement candidate.
Upon an access all pointers in the chain from the accessed way's leaf node to the root node are set to point to subtree that does not contain the accessed way.

The access sequence is A B C D E.
The principle here is simple to understand if we only look at the arrow pointers.
When there is an access to a value, say 'A', and we cannot find it in the cache, then we load it from memory and place it at the block where the arrows are currently pointing, going from top to bottom.

After we have placed that block we flip those same arrows
so they point the opposite way.

In the above example we see how 'A' was placed, followed by 'B', 'C and 'D'.
Then as the cache became full 'E' replaced 'A' because that was where the arrows were pointing at that time, and the arrows that led to 'A' were flipped to point in the opposite direction.
The arrows then led to 'B', which will be the block replaced on the next cache miss.

===
Random replacement (RR) ===
Randomly selects a candidate item and discards it to make space when necessary.
This algorithm does not require keeping any information about the access history.
For its simplicity, it has been used in ARM processors.
It admits efficient stochastic simulation.
The access sequence for the below example is
A B C D E B D F


===
Segmented LRU (SLRU) ===
SLRU cache is divided into two segments, a probationary segment and a protected segment.
Lines in each segment are ordered from the most to the least recently accessed.
Data from misses is added to the cache at the most recently accessed end of the probationary segment.
Hits are removed from wherever they currently reside and added to the most recently accessed end of the protected segment.
Lines in the protected segment have thus been accessed at least twice.
The protected segment is finite, so migration of a line from the probationary segment to the protected segment may force the migration of the LRU line in the protected segment to the most recently used (MRU) end of the probationary segment, giving this line another chance to be accessed before being replaced.
The size limit on the protected segment is an SLRU parameter that varies according to the I/O workload patterns.
Whenever data must be discarded from the cache, lines are obtained from the LRU end of the probationary segment.

=== Least-frequently used (LFU) ===

Counts how often an item is needed.
Those that are used least often are discarded first.
This works very similar to LRU except that instead of storing the value of how recently a block was accessed, we store the value of how many times it was accessed.
So of course while running an access sequence we will replace a block which was used fewest times from our cache.
E.g., if A was used (accessed) 5 times and B was used 3 times and others C and D were used 10 times each, we will replace B.


===
Least frequent recently used (LFRU) ===
The Least Frequent Recently Used (LFRU) cache replacement scheme combines the benefits of LFU and LRU schemes.
LFRU  is suitable for ‘in network’ cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
In LFRU, the cache is divided into two partitions called privileged and unprivileged partitions.
The privileged partition can be defined as a protected partition.
If content is highly popular, it is pushed into the privileged partition.
Replacement of the privileged partition is done as follows:  LFRU evicts content from the unprivileged partition, pushes content from privileged partition to unprivileged partition, and finally inserts new content into the privileged partition.

In the above procedure the LRU is used for the privileged partition and an approximated LFU (ALFU) scheme is used for the unprivileged partition, hence the abbreviation LFRU.

The basic idea is to filter out the locally popular contents with ALFU scheme and push the popular contents to one of the privileged partition.

===
LFU with dynamic aging (LFUDA) ===
A variant called LFU with Dynamic Aging (LFUDA) that uses dynamic aging to accommodate shifts in the set of popular objects.
It adds a cache age factor to the reference count when a new object is added to the cache or when an existing object is re-referenced.
LFUDA increments the cache ages when evicting blocks by setting it to the evicted object’s key value.
Thus, the cache age is always less than or equal to the minimum key value in the cache.
Suppose when an object was frequently accessed in the past and now it becomes unpopular, it will remain in the cache for a long time thereby preventing the newly or less popular objects from replacing it.
So this Dynamic aging is introduced to bring down the count of such objects thereby making them eligible for replacement.
The advantage of LFUDA is it reduces the cache pollution caused by LFU when cache sizes are very small.
When Cache sizes are large few replacement decisions are sufficient and cache pollution will not be a problem.

===
Low inter-reference recency set (LIRS) ===
LIRS is a page replacement algorithm with an improved performance over LRU and many other newer replacement algorithms.
This is achieved by using reuse distance as a metric for dynamically ranking accessed pages to make a replacement decision.
LIRS effectively address the limits of LRU by using recency to evaluate Inter-Reference Recency (IRR) for making a replacement decision.

In the above figure, "x" represents that a block is accessed at time t.
Suppose if block A1 is accessed at time 1 then Recency will become 0 since this is the first accessed block and IRR will be 1 since it predicts that A1 will be accessed again in time 3.
In the time 2 since A4 is accessed, the recency will become 0 for A4 and 1 for A1 because A4 is the most recently accessed Object and IRR will become 4 and it will go on.
At time 10, the LIRS algorithm will have two sets LIR set = {A1, A2} and HIR set = {A3, A4, A5}.
Now at time 10 if there is access to A4, miss occurs.
LIRS algorithm will now evict A5 instead of A2 because of its largest recency.

===
CLOCK-Pro ===
LRU algorithm cannot be directly implemented in the critical path of computer systems, such as operating systems, due to its high overhead.
An approximation of LRU, called CLOCK is commonly used for the implementation.

Similarly, CLOCK-Pro is an approximation of LIRS for an low cost implementation in systems.
CLOCK-Pro is under the basic CLOCK framework, but has three major distinct merits.
First, CLOCK-Pro has three "clock hands" in contrast to a simple structure of CLOCK where only one "hand" is used.
With the three hands, CLOCK-Pro is able to measure the reuse distance of data accesses in an approximate way.
Second, all the merits of LIRS are retained, such as quickly evicting one-time accessing and/or low locality data items.
Third, the complexity of the CLOCK-Pro is same as that of CLOCK, thus it is easy to implement at a low cost.
The buffer cache replacement implementation in the current version of Linux is a combination of LRU and CLOCK-Pro.
===
Adaptive replacement cache (ARC) ===
Constantly balances between LRU and LFU, to improve the combined result.
ARC improves on SLRU by using information about recently evicted cache items to dynamically adjust the size of the protected segment and the probationary segment to make the best use of the available cache space.
Adaptive replacement algorithm is explained with the example.

===
AdaptiveClimb (AC) ===
Uses recent hit/miss to adjust the jump where in climb any hit switches the position one slot to the top, and in LRU hit switches the position of the hit to the top.
Thus, benefiting from the optimality of climb when the program is in a fixed scope, and the rapid adaption to a new scope, as LRU does.

Also support cache sharing among cores by releasing extras when the references are to the top part of the cache.
===
Clock with adaptive replacement (CAR) ===
Combines the advantages of Adaptive Replacement Cache (ARC) and CLOCK.
CAR has performance comparable to ARC, and substantially outperforms both LRU and CLOCK.
Like ARC, CAR is self-tuning and requires no user-specified magic parameters.
It uses 4 doubly linked lists: two clocks T1 and T2 and two simple LRU lists B1 and B2.
T1 clock stores pages based on "recency" or "short term utility" whereas T2 stores pages with "frequency" or "long term utility".
T1 and T2 contain those pages that are in the cache, while B1 and B2 contain pages that have recently been evicted from T1 and T2 respectively.
The algorithm tries to maintain the size of these lists B1≈T2 and B2≈T1.
New pages are inserted in T1 or T2.
If there is a hit in B1 size of T1 is increased and similarly if there is a hit in B2 size of T1 is decreased.
The adaptation rule used has the same principle as that in ARC, invest more in lists that will give more hits when more pages are added to it.

===
Multi queue (MQ) ===
The multi queue algorithm or MQ was developed to improve the performance of second level buffer cache for e.g. a server buffer cache.
It is introduced in a paper by Zhou, Philbin, and Li.
The MQ cache contains an m number of LRU queues: Q0, Q1, ..., Qm-1.
Here, the value of m represents a hierarchy based on the lifetime of all blocks in that particular queue.
For example, if j>
i, blocks in Qj will have a longer lifetime than those in Qi.
In addition to these there is another history buffer Qout, a queue which maintains a list of all the Block Identifiers along with their access frequencies.
When Qout is full the oldest identifier is evicted.
Blocks stay in the LRU queues for a given lifetime, which is defined dynamically by the MQ algorithm to be the maximum temporal distance between two accesses to the same file or the number of cache blocks, whichever is larger.
If a block has not been referenced within its lifetime, it is demoted from Qi to Qi−1 or evicted from the cache if it is in Q0.
Each queue also has a maximum access count; if a block in queue Qi is accessed more than 2i times, this block is promoted to Qi+1 until it is accessed more than 2i+1 times or its lifetime expires.
Within a given queue, blocks are ranked by the recency of access, according to LRU.

We can see from Fig.
how the m LRU queues are placed in the cache.
Also see from Fig.
how the Qout  stores the block identifiers and their corresponding access frequencies.
a was placed in Q0 as it was accessed only once recently
and we can check in Qout how b and c were placed in Q1 and Q2 respectively as their access frequencies are 2 and 4.
The queue in which a block is placed is dependent on access frequency(f) as log2(f).
When the cache is full, the first block to be evicted will be the head of Q0 in this case
a.
If a is accessed one more time it will move to Q1 below
b.


===
Pannier: Container-based caching algorithm for compound objects ===
Pannier  is a container-based flash caching mechanism that identifies divergent (heterogeneous) containers where blocks held therein have highly varying access patterns.
Pannier uses a priority-queue based survival queue structure to rank the containers based on their survival time, which is proportional to the live data in the container.
Pannier is built based on Segmented LRU (S2LRU), which segregates hot and cold data.
Pannier also uses a multi-step feedback controller to throttle flash writes to ensure flash lifespan.

== See also ==
Cache-oblivious algorithm
Locality of reference
Distributed cache


== References ==


==
External links ==
Definitions of various cache algorithms
Caching algorithm for flash/SSDs
A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment.
Individuals create their own "subjective reality" from their perception of the input.
An individual's construction of reality, not the objective input, may dictate their behavior in the world.
Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or
what is broadly called irrationality.
Although it may seem like such misperceptions would be aberrations, biases can help humans find commonalities and shortcuts to assist in the navigation of common situations in life.
Some cognitive biases are presumably adaptive.
Cognitive biases may lead to more effective actions in a given context.
Furthermore, allowing cognitive biases enables faster decisions which can be desirable when timeliness is more valuable than accuracy, as illustrated in heuristics.
Other cognitive biases are a "by-product" of human processing limitations, resulting from a lack of appropriate mental mechanisms (bounded rationality), impact of individual's constitution and biological state (see embodied cognition), or simply from a limited capacity for information processing.
A continually evolving list of cognitive biases has been identified over the last six decades of research on human judgment and decision-making in cognitive science, social psychology, and behavioral economics.
Daniel Kahneman and  Tversky (1996) argue that cognitive biases have efficient practical implications for areas including clinical judgment, entrepreneurship, finance, and management.

==
Overview ==
The notion of cognitive biases was introduced by Amos Tversky and Daniel Kahneman in 1972 and grew out of their experience of people's innumeracy, or inability to reason intuitively with the greater orders of magnitude.
Tversky, Kahneman and colleagues demonstrated several replicable ways in which human judgments and decisions differ from rational choice theory.
Tversky and Kahneman explained human differences in judgment and decision-making in terms of heuristics.
Heuristics involve mental shortcuts which provide swift estimates about the possibility of uncertain occurrences.
Heuristics are simple for the brain to compute but sometimes introduce "severe and systematic errors.
"For
example, the representativeness heuristic is defined as “The tendency to judge the frequency or likelihood" of an occurrence by the extent of which the event "resembles the typical case".
The "Linda Problem" illustrates the representativeness heuristic (Tversky & Kahneman, 1983).
Participants were given a description of "Linda" that suggests Linda might well be a feminist (e.g., she is said to be concerned about discrimination and social justice issues).
They were then asked whether they thought Linda was more likely to be (a) a "bank teller" or (b) a "bank teller and active in the feminist movement."
A majority chose answer (b).
This error (mathematically, answer (b) cannot be more likely than answer (a)) is an example of the "conjunction fallacy"; Tversky and Kahneman argued that respondents chose (b) because it seemed more "representative" or typical of persons who might fit the description of Linda.
The representativeness heuristic may lead to errors such as activating stereotypes and inaccurate judgments of others (Haselton et al.,
2005, p. 726).

Critics of Kahneman and Tversky, such as Gerd Gigerenzer, alternatively argued that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases.
They should rather conceive rationality as an adaptive tool, not identical to the rules of formal logic or the probability calculus.
Nevertheless, experiments such as the "Linda problem" grew into heuristics and biases research programs, which spread beyond academic psychology into other disciplines including medicine and political science.

== Types ==
Biases can be distinguished on a number of dimensions.
For a more complete list, see list of cognitive biases.
Examples of cognitive biases include:

Biases specific to groups (such as the risky shift) versus biases at the individual level.

Biases that affect decision-making, where the desirability of options has to be considered (e.g., sunk costs fallacy).

Biases, such as illusory correlation, that affect judgment of how likely something is or whether one thing is the cause of another.

Biases that affect memory, such as consistency bias (remembering one's past attitudes and behavior as more similar to one's present attitudes).

Biases that reflect a subject's motivation, for example, the desire for a positive self-image leading to egocentric bias and the avoidance of unpleasant cognitive dissonance.
Other biases are due to the particular way the brain perceives, forms memories and makes judgments.
This distinction is sometimes described as "hot cognition" versus "cold cognition", as motivated reasoning can involve a state of arousal.
Among the "cold" biases,

some are due to ignoring relevant information (e.g., neglect of probability),
some involve a decision or judgment being affected by irrelevant information (for example the framing effect where the same problem receives different responses depending on how it is described; or the distinction bias where choices presented together have different outcomes than those presented separately), and
others give excessive weight to an unimportant but salient feature of the problem (e.g., anchoring).The fact that some biases reflect motivation, specifically the motivation to have positive attitudes to oneself, accounts for the fact that many biases are self-serving or self-directed (e.g., illusion of asymmetric insight, self-serving bias).
There are also biases in how subjects evaluate in-groups or out-groups; evaluating in-groups as more diverse and "better" in many respects, even when those groups are arbitrarily defined (ingroup bias, outgroup homogeneity bias).

Some cognitive biases belong to the subgroup of attentional biases, which refers to paying increased attention to certain stimuli.
It has been shown, for example, that people addicted to alcohol and other drugs pay more attention to drug-related stimuli.
Common psychological tests to measure those biases are the Stroop task and the dot probe task.

Individuals' susceptibility to some types of cognitive biases can be measured by the Cognitive Reflection Test (CRT) developed by Shane Frederick (2005).
===
List of biases ===
The following is a list of the more commonly studied cognitive biases:


==
Practical significance ==
Many social institutions rely on individuals to make rational judgments.

The securities regulation regime largely assumes that all investors act as perfectly rational persons.
In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.

A fair jury trial, for example, requires that the jury ignore irrelevant features of the case, weigh the relevant features appropriately, consider different possibilities open-mindedness and resist fallacies such as appeal to emotion.
The various biases demonstrated in these psychological experiments suggest that people will frequently fail to do all these things.
However, they fail to do so in systematic, directional ways that are predictable.
Cognitive biases are also related to the persistence of theory-of-everything thinking, to large social issues such as prejudice, and they also work as a hindrance in the acceptance of scientific non-intuitive knowledge by the public.
However, in some academic disciplines, the study of bias is very popular.
For instance, bias is a wide spread and well studied phenomenon because most decisions that concern the minds and hearts of entrepreneurs are computationally intractable.
Cognitive biases can create other issues that arise in everyday life.
One study showed the connection between cognitive bias, specifically approach bias, and inhibitory control on how much unhealthy snack food a person would eat.
They found that the participants who ate more of the unhealthy snack food, tended to have less inhibitory control and more reliance on approach bias.
Others have also hypothesized that cognitive biases could be linked to various eating disorders and how people view their bodies and their body image.
It has also been argued that cognitive biases can be used in destructive ways.
Some believe that there are people in authority who use cognitive biases and heuristics in order to manipulate others so that they can reach their end goals.
Some medications and other health care treatments rely on cognitive biases in order to persuade others who are susceptible to cognitive biases to use their products.
Many see this as taking advantage of one’s natural struggle of judgement and decision-making.
They also believe that it is the government’s responsibility to regulate these misleading ads.

Cognitive biases also seem to play a role in property sale price and value.
Participants in the experiment were shown a residential property.
Afterwards, they were shown another property that was completely unrelated to the first property.
They were asked to say what they believed the value and the sale price of the second property would be.
They found that showing the participants an unrelated property did have an effect on how they valued the second property.

==
Reducing ==
Because they cause systematic errors, cognitive biases cannot be compensated for using a wisdom of the crowd technique of averaging answers from several people.

Debiasing is the reduction of biases in judgment and decision-making through incentives, nudges, and training.
Cognitive bias mitigation and cognitive bias modification are forms of debiasing specifically applicable to cognitive biases and their effects.
Reference class forecasting is a method for systematically debiasing estimates and decisions, based on what Daniel Kahneman has dubbed the outside view.

Similar to Gigerenzer (1996), Haselton et al.
(
2005) state
the content and direction of cognitive biases are not "arbitrary" (p. 730).
Moreover, cognitive biases can be controlled.
One debiasing technique aims to decrease biases by encouraging individuals to use controlled processing compared to automatic processing.
In relation to reducing the FAE, monetary incentives and informing participants they will be held accountable for their attributions have been linked to the increase of accurate attributions.
Training has also shown to reduce cognitive bias.
Carey K. Morewedge and colleagues (2015) found that research participants exposed to one-shot training interventions, such as educational videos and debiasing games that taught mitigating strategies, exhibited significant reductions in their commission of six cognitive biases immediately and up to 3 months later.
Cognitive bias modification refers to the process of modifying cognitive biases in healthy people and also refers to a growing area of psychological (non-pharmaceutical) therapies for anxiety, depression and addiction called cognitive bias modification therapy (CBMT).
CBMT is sub-group of therapies within a growing area of psychological therapies based on modifying cognitive processes with or without accompanying medication and talk therapy, sometimes referred to as applied cognitive processing therapies (ACPT).
Although cognitive bias modification can refer to modifying cognitive processes in healthy individuals, CBMT is a growing area of evidence-based psychological therapy, in which cognitive processes are modified to relieve suffering from serious depression, anxiety, and addiction.
CBMT techniques are technology assisted therapies that are delivered via a computer with or without clinician support.
CBM combines evidence and theory from the cognitive model of anxiety, cognitive neuroscience, and attentional models.
Cognitive bias modification has also been used to help those who are suffering with obsessive compulsive beliefs and obsessive-compulsive disorder.
This therapy has shown that it decreases the obsessive-compulsive beliefs and behaviors.

==
Common theoretical causes of some cognitive biases ==
Bias arises from various processes that are sometimes difficult to distinguish.
These include:
Bounded rationality — limits on optimization and rationality
Prospect theory
Mental accounting
Adaptive bias — basing decisions on limited information and biasing them based on the costs of being wrong
Attribute substitution — making a complex, difficult judgment by unconsciously replacing it with an easier judgment
Attribution theory
Salience
Naïve realism
Cognitive dissonance, and related:
Impression management
Self-perception theory
Information-processing shortcuts (heuristics), including:
Availability heuristic — estimating what is more likely by what is more available in memory, which is biased toward vivid, unusual, or emotionally charged examples
Representativeness heuristic — judging probabilities based on resemblance
Affect heuristic — basing a decision on an emotional reaction rather than a calculation of risks and benefits
Emotional and moral motivations deriving, for example, from:
The two-factor theory of emotion
The somatic markers hypothesis
Introspection illusion
Misinterpretations or misuse of statistics; innumeracy.

Social influence
The brain's limited information processing capacity
Noisy information processing (distortions during storage in and retrieval from memory).
For example, a 2012 Psychological Bulletin article suggests that at least eight seemingly unrelated biases can be produced by the same information-theoretic generative mechanism.
The article shows that noisy deviations in the memory-based information processes that convert objective evidence (observations) into subjective estimates (decisions) can produce regressive conservatism, the belief revision (Bayesian conservatism), illusory correlations, illusory superiority (better-than-average effect) and worse-than-average effect, subadditivity effect, exaggerated expectation, overconfidence, and the hard–easy effect.

==
Individual differences in cognitive biases ==
People do appear to have stable individual differences in their susceptibility to decision biases such as overconfidence, temporal discounting, and bias blind spot.
That said, these stable levels of bias within individuals are possible to change.
Participants in experiments who watched training videos and played debiasing games showed medium to large reductions both immediately and up to three months later in the extent to which they exhibited susceptibility to six cognitive biases: anchoring, bias blind spot, confirmation bias, fundamental attribution error, projection bias, and representativeness.
Individual differences in cognitive bias have also been linked to varying levels of cognitive abilities and functions.
The Cognitive Reflection Test (CRT) has been used to help understand the connection between cognitive biases and cognitive ability.
There have been inconclusive results when using the Cognitive Reflection Test to understand ability.
However, there does seem to be a correlation; those who gain a higher score on the Cognitive Reflection Test, have higher cognitive ability and rational-thinking skills.
This in turn helps predict the performance on cognitive bias and heuristic tests.
Those with higher CRT scores tend to be able to answer more correctly on different heuristic and cognitive bias tests and tasks.
Age is another individual difference that has an effect on one’s ability to be susceptible to cognitive bias.
Older individuals tend to be more susceptible to cognitive biases and have less cognitive flexibility.
However, older individuals were able to decrease their susceptibility to cognitive biases throughout ongoing trials.
These experiments had both young and older adults complete a framing task.
Younger adults had more cognitive flexibility than older adults.
Cognitive flexibility is linked to helping overcome preexisting biases.

==
Criticisms
==
Criticisms against theories of cognitive biases are usually founded in the fact that both sides of a debate often claim the other's thoughts to be subject to human nature and the result of cognitive bias, while claiming their own viewpoint to be above the cognitive bias and the correct way to "overcome" the issue.
This rift ties to a more fundamental issue that stems from a lack of consensus in the field, thereby creating arguments that can be non-falsifiably used to validate any contradicting viewpoint.
Gerd Gigerenzer is one of the main opponents to cognitive biases and heuristics.
Gigerenzer believes that cognitive biases are not biases, but rules of thumb, or as he would put it “gut feelings” that can actually help us make accurate decisions in our lives.
His view shines a much more positive light on cognitive biases than many other researchers.
Many view cognitive biases and heuristics as irrational ways of making decisions and judgements.
Gigerenzer argues that using heuristics and cognitive biases are rational and helpful for making decisions in our everyday life.

== See also ==


==
References ==


==
Further reading ==


==
External links ==
The Roots of Consciousness:
To Err Is human
Cognitive bias in the financial arena
A Visual Study Guide To Cognitive Biases
Why smart people may be more likely to fall for fake news
A depth buffer, also known as a z-buffer, is a type of data buffer used in computer graphics used to represent depth information of objects in 3D space from a particular perspective.
Depth buffers are an aid to rendering a scene to ensure that the correct polygons properly occlude other polygons.
Z-buffering was first described in 1974 by Wolfgang Straßer in his PhD thesis on fast algorithms for rendering occluded objects.
A similar solution to determining overlapping polygons is the painter's algorithm, which is capable of handling non-opaque scene elements, though at the cost of efficiency and incorrect results.

In a 3d-rendering pipeline, when an object is projected on the screen, the depth (z-value) of a generated fragment in the projected screen image is compared to the value already stored in the buffer (depth test), and replaces it if the new value is closer.
It works in tandem with the rasterizer, which computes the colored values.
The fragment outputted by the rasterizer is saved if it is not overlapped by another fragment.

When viewing an image containing partially or fully overlapping opaque objects or surfaces, it is not possible to fully see those objects that are farthest away from the viewer and behind other objects (i.e., some surfaces are hidden behind others).
If there were no mechanism for managing overlapping surfaces, surfaces would render on top of each other, not caring if they are meant to be behind other objects.
The identification and removal of these surfaces are called the hidden-surface problem.
To check for overlap, the computer calculates the z-value of a pixel corresponding to the first object and compares it with the z-value at the same pixel location in the z-buffer.
If the calculated z-value is smaller than the z-value already in the z-buffer (i.e., the new pixel is closer), then the current z-value in the z-buffer is replaced with the calculated value.
This is repeated for all objects and surfaces in the scene (often in parallel).
In the end, the z-buffer will allow correct reproduction of the usual depth perception: a close object hides one further away.
This is called z-culling.

The z-buffer has the same internal data structure as an image, namely a 2d-array, with the only difference being that it stores a single value for each screen pixel instead of color images that use 3 values to create color.
This makes the z-buffer appear black-and-white because it is not storing color information.
The buffer has the same dimensions as the screen buffer for consistency.

Primary visibility tests (such as back-face culling) and secondary visibility tests (such as overlap checks and screen clipping) are usually performed on objects' polygons in order to discard specific polygons that are deemed to be unnecessary to render.
Z-buffer, by comparison, is comparatively expensive, so performing primary and secondary visibility tests relieve the z-buffer of some duty.

The granularity of a z-buffer has a great influence on the scene quality: the traditional 16-bit z-buffer can result in artifacts (called "z-fighting" or stitching) when two objects are very close to each other.
A more modern 24-bit or 32-bit z-buffer behaves much better, although the problem cannot be entirely eliminated without additional algorithms.
An 8-bit z-buffer is almost never used since it has too little precision.

== Uses ==
The Z-buffer is a technology used in almost all contemporary computers, laptops, and mobile phones for performing 3D computer graphics.
The primary use now is for video games, which require fast and accurate processing of 3d scenes.
The Z-buffer is implemented in hardware within consumer graphics cards.
The Z-buffer is also used (implemented as software as opposed to hardware) for producing computer-generated special effects for films.
Furthermore, Z-buffer data obtained from rendering a surface from a light's point-of-view permits the creation of shadows by the shadow mapping technique.

==
Developments ==
Even with small enough granularity, quality problems may arise when precision in the z-buffer's distance values are not spread evenly over distance.
Nearer values are much more precise (and hence can display closer objects better) than values that are farther away.
Generally, this is desirable, but sometimes it will cause artifacts to appear as objects become more distant.
A variation on z-buffering which results in more evenly distributed precision is called w-buffering (see below).

At the start of a new scene, the z-buffer must be cleared to a defined value, usually, 1.0, because this value is the upper limit (on a scale of 0 to 1) of depth, meaning that no object is present at this point through the viewing frustum.

The invention of the z-buffer concept is most often attributed to Edwin Catmull, although Wolfgang Straßer described this idea in his 1974 Ph.D. thesis months before Catmull's invention.
On more recent PC graphics cards (1999–2005), z-buffer management uses a significant chunk of the available memory bandwidth.
Various methods have been employed to reduce the performance cost of z-buffering, such as lossless compression (computer resources to compress/decompress are cheaper than bandwidth) and ultra-fast hardware z-clear
that makes obsolete the "one frame positive, one frame negative" trick (skipping inter-frame clear altogether using signed numbers to cleverly check depths).

== Z-culling ==
In rendering, z-culling is early pixel elimination based on depth, a method that provides an increase in performance when rendering of hidden surfaces is costly.
It is a direct consequence of z-buffering, where the depth of each pixel candidate is compared to the depth of the existing geometry behind which it might be hidden.

When using a z-buffer, a pixel can be culled (discarded) as soon as its depth is known, which makes it possible to skip the entire process of lighting and texturing a pixel that would not be visible anyway.
Also, time-consuming pixel shaders will generally not be executed for the culled pixels.
This makes z-culling a good optimization candidate in situations where fillrate, lighting, texturing, or pixel shaders are the main bottlenecks.

While z-buffering allows the geometry to be unsorted, sorting polygons by increasing depth (thus using a reverse painter's algorithm) allows each screen pixel to be rendered fewer times.
This can increase performance in fillrate-limited scenes with large amounts of overdraw, but if not combined with z-buffering it suffers from severe problems such as:

polygons might occlude one another in a cycle (e.g.:
triangle A occludes B
, B occludes C
, C occludes A), and
there is no canonical "closest" point on a triangle (e.g.: no matter whether one sorts triangles by their centroid or closest point or furthest point, one can always find two triangles A and B such that A is "closer"
but in reality B should be drawn first).As such, a reverse painter's algorithm cannot be used as an alternative to Z-culling (without strenuous re-engineering), except as an optimization to Z-culling.
For example, an optimization might be to keep polygons sorted according to x/y-location and z-depth to provide bounds, in an effort to quickly determine if two polygons might possibly have an occlusion interaction.

==
Mathematics ==
The range of depth values in camera space to be rendered is often defined between a 
  
    
      
        
          
            near
          
        
      
    
    {\displaystyle {\textit {near}}}
   and 
  
    
      
        
          
            far
          
        
      
    
    {\displaystyle {\textit {far}}}
   value of 
  
    
      
        z
      
    
    {\displaystyle z}
  .
After a perspective transformation, the new value of 
  
    
      
        z
      
    
    {\displaystyle z}
  , or 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
  , is defined by:

  
    
      
        
          z
′
        
        =
far
                
              
              +
              
                
                  near
far
                
              
              −
near
+
        
          
            1
z
(
          
            
              
                −
2
⋅
                
                  
                    far
⋅
                
                  
                    near
far
−
near
                  
                
              
            
          
          )
        
      
    
    {\displaystyle z'={\frac {{\textit {far}}+{\textit {near}}}{{\textit {far}}-{\textit {near}}}}+{\frac {1}{z}}\left({\frac {-2\cdot {\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)}
After an orthographic projection, the new value of 
  
    
      
        z
      
    
    {\displaystyle z}
  , or 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
  , is defined by:

  
    
      
        
          z
          ′
        
        =
2
⋅
z
              
              −
near
far
                
              
              −
near
                
              
            
          
        
        −
1
      
    
    {\displaystyle z'=2\cdot {\frac {{z}-{\textit {near}}}{{\textit {far}}-{\textit {near}}}}-1}
where 
  
    
      
        z
{\displaystyle z}
   is the old value of 
  
    
      
        z
      
    
    {\displaystyle z}
   in camera space, and is sometimes called 
  
    
      
        w
      
    
    {\displaystyle w}
   or
w
          ′
        
      
    
    {\displaystyle w'}
  .

The resulting values of 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
   are normalized between the values of -1 and 1, where the 
  
    
      
        
          
            near
          
        
      
    
    {\displaystyle {\textit {near}}}
   plane is at -1 and the 
  
    
      
        
          
            f
a
            r
          
        
      
    
    {\displaystyle {\mathit {far}}}
   plane is at 1.
Values outside of this range correspond to points which are not in the viewing frustum, and shouldn't be rendered.

=== Fixed-point representation ===
Typically, these values are stored in the z-buffer of the hardware graphics accelerator in fixed point format.
First they are normalized to a more common range which is [0, 1] by substituting the appropriate conversion 
  
    
      
        
          z
          
            2
′
        
        =
1
            2
(
z
              
                1
              
              ′
+
1
          
          )
        
      
    
    {\displaystyle z'_{2}={\frac {1}{2}}\left(z'_{1}+1\right)}
into the previous formula:

  
    
      
        
          z
′
        
        =
far
                
              
              +
              
                
                  near
                
              
            
            
              2
⋅
(
                
                  
                    
                      far
                    
                  
                  −
                  
                    
                      near
                    
                  
                
                )
+
        
          
            1
            2
+
        
          
            1
z
(
          
            
              
                −
                
                  
                    far
⋅
                
                  
                    near
far
−
near
                  
                
              
            
          
          )
        
      
    
    {\displaystyle z'={\frac {{\textit {far}}+{\textit {near}}}{2\cdot \left({\textit {far}}-{\textit {near}}\right)}}+{\frac {1}{2}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)}
Simplifying:

  
    
      
        
          z
′
        
        =
far
(
              
                
                  
                    far
                  
                
                −
                
                  
                    near
                  
                
              
              )
+
        
          
            1
z
(
          
            
              
                −
                
                  
                    far
⋅
                
                  
                    near
far
−
near
                  
                
              
            
          
          )
        
      
    
    {\displaystyle z'={\frac {\textit {far}}{\left({\textit {far}}-{\textit {near}}\right)}}+{\frac {
1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)}
Second, the above formula is multiplied by 
  
    
      
        S
        =
        
          2
          
            d
−
1
      
    
    {\displaystyle S=2^{d}-1
}
   where d is the depth of the z-buffer (usually 16, 24 or 32 bits) and rounding the result to an integer:

  
    
      
        
          z
          ′
        
        =
f
(
z
        )
=
⌊
(
              
                
                  2
                  
                    d
−
1
              
              )
            
            ⋅
(
              
                
                  
                    
                      far
(
                      
                        
                          
                            far
                          
                        
                        −
                        
                          
                            near
                          
                        
                      
                      )
+
                
                  
                    1
z
(
                  
                    
                      
                        −
far
                          
                        
                        ⋅
                        
                          
                            near
                          
                        
                      
                      
                        
                          
                            far
                          
                        
                        −
                        
                          
                            near
                          
                        
                      
                    
                  
                  )
                
              
              )
⌋
        
      
    
    {\displaystyle
z'=f(z)=\left\lfloor \left(2^{d}-1\right)\cdot \left({\frac {\textit {far}}{\left({\textit {far}}-{\textit {near}}\right)}}+{\frac {1}{z}}\left({\frac {-{\textit {far}}\cdot {\textit {near}}}{{\textit {far}}-{\textit {near}}}}\right)\right)\right\rfloor }
This formula can be inverted and derived in order to calculate the z-buffer resolution (the 'granularity' mentioned earlier).
The inverse of the above 
  
    
      
        f
(
        z
)
        
      
    
    {\displaystyle f(z)\,}
  :
z
        =
−
far
⋅
              
                
                  near
z
′
                  
                  S
(
                
                  
                    
                      far
                    
                  
                  −
                  
                    
                      near
                    
                  
                
                )
              
              −
far
                
              
            
          
        
        =
−
S
⋅
              
                
                  far
                
              
              ⋅
              
                
                  near
                
              
            
            
              
                z
′
(
                
                  
                    
                      far
                    
                  
                  −
                  
                    
                      near
                    
                  
                
                )
              
              −
far
⋅
              S
            
          
        
      
    
    {\displaystyle z={\frac {-{\textit {far}}\cdot {\textit {near}}}{{\frac {z'}{S}}\left({\textit {far}}-{\textit {near}}\right)-{\textit {far}}}}={\frac {-S\cdot {\textit {far}}\cdot {\textit {near}}}{z'\left({\textit {far}}-{\textit {near}}\right)-{\textit {far}}\cdot S}}}
where 
  
    
      
        S
        =
        
          2
          
            d
−
1
      
    
    {
\displaystyle S=2^{d}-1}
The z-buffer resolution in terms of camera space would be the incremental value resulted from the smallest change in the integer stored in the z-buffer, which is +1 or -1.
Therefore, this resolution can be calculated from the derivative of 
  
    
      
        z
      
    
    {\displaystyle z}
   as a function of 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
  :
d
              z
            
            
              d
              
                z
′
              
            
          
        
        =
−
1
⋅
              −
1
⋅
S
⋅
              
                
                  far
                
              
              ⋅
              
                
                  near
                
              
            
            
              
                (
                
                  
                    z
                    ′
(
                    
                      
                        
                          far
                        
                      
                      −
                      
                        
                          near
                        
                      
                    
                    )
                  
                  −
far
⋅
                  S
                
                )
              
              
                2
⋅
(
          
            
              
                far
              
            
            −
            
              
                near
)
{\displaystyle {\frac {dz}{dz'}}={\frac {-1\cdot -1\cdot S\cdot {\textit {far}}\cdot {\textit {near}}}{\left(z'\left({\textit {far}}-{\textit {near}}\right)-{\textit {far}}\cdot S\right)^{2}}}\cdot \left({\textit {far}}-{\textit {near}}\right)}
Expressing it back in camera space terms, by substituting 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
   by the above
f
(
        z
)
        
      
    
    {\displaystyle f(z)\,}
  :
d
                      z
                    
                    
                      d
                      
                        z
                        ′
                      
                    
                  
                
              
              
                
                =
−
1
⋅
                      −
1
⋅
S
⋅
far
⋅
                      
                        
                          near
                        
                      
                      ⋅
(
                        
                          
                            
                              far
                            
                          
                          −
                          
                            
                              near
                            
                          
                        
                        )
(
                        
                          S
                          ⋅
(
                            
                              
                                
                                  
                                    −
                                    
                                      
                                        far
⋅
                                    
                                      
                                        near
                                      
                                    
                                  
                                  z
+
                              
                                
                                  far
                                
                              
                            
                            )
−
far
⋅
                          S
                        
                        )
                      
                      
                        2
=
(
                        
                          
                            
                              far
                            
                          
                          −
                          
                            
                              near
                            
                          
                        
                        )
⋅
                      
                        z
                        
                          2
S
⋅
far
⋅
                      
                        
                          near
                        
                      
                    
                  
                
              
            
            
              
              
                
                =
z
2
                      
                    
                    
                      S
                      ⋅
                      
                        
                          near
                        
                      
                    
                  
                
                −
z
                      
                        2
                      
                    
                    
                      S
⋅
                      
                        
                          far
                        
                      
                    
                  
                
                ≈
                
                  
                    
                      z
2
                      
                    
                    
                      S
⋅
                      
                        
                          near
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {dz}{dz'}}&={\frac
{-1\cdot -1\cdot S\cdot {\textit {far}}\cdot {\textit {near}}\cdot \left({\textit {far}}-{\textit {near}}\right)}{\left(S\cdot \left({\frac {-{\textit {far}}\cdot {\textit {near}}}{z}}+{\textit {far}}\right)-{\textit {far}}\cdot S\right)^{2}}}\\&={\frac {\left({\textit {far}}-{\textit {near}}\right)\cdot z^{2}}{S\cdot {\textit {far}}\cdot {\textit {near}}}}\\&={\frac {z^{2}}{S\cdot {
\textit {near}}}}-{\frac {z^{2}}{S\cdot {\textit {far}}}}\approx {
\frac {z^{2}}{S\cdot {\textit {near}}}}\end{aligned}}}
This shows that the values of 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
   are grouped much more densely near the
near
          
        
      
    
    {\displaystyle {\textit {near}}}
   plane, and much more sparsely farther away, resulting in better precision closer to the camera.
The smaller 
  
    
      
        n
        e
a
        r
      
    
    {\displaystyle near}
is, the less precision there is far away—
having the 
  
    
      
        n
        e
a
        r
      
    
    {\displaystyle near}
   plane set too closely is a common cause of undesirable rendering artifacts in more distant objects.
To implement a z-buffer, the values of 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
   are linearly interpolated across screen space between the vertices of the current polygon, and these intermediate values are generally stored in the z-buffer in fixed point format.
=== W-buffer ===
To implement a w-buffer, the old values of 
  
    
      
        z
      
    
    {\displaystyle z}
   in camera space, or 
  
    
      
        w
      
    
    {\displaystyle w}
  , are stored in the buffer, generally in floating point format.
However, these values cannot be linearly interpolated across screen space from the vertices—they usually have to be inverted, interpolated, and then inverted again.
The resulting values of 
  
    
      
        w
      
    
    {\displaystyle w}
  , as opposed to 
  
    
      
        
          z
′
        
      
    
    {\displaystyle z'}
  , are spaced evenly between 
  
    
      
        
          
            near
          
        
      
    
    {\displaystyle {\textit {near}}}
   and 
  
    
      
        
          
            far
          
        
      
    
    {\displaystyle {\textit {far}}}
  .
There are implementations of the w-buffer that avoid the inversions altogether.

Whether a z-buffer or w-buffer results in a better image depends on the application.

==
Algorithmics ==
The following pseudocode demonstrates the process of z-buffering:

// First of all, initialize the depth of each pixel.

d(i, j) =
infinite //
Max length
// Initialize the color value for each pixel to the background color
c(i, j) =
background color
// For each polygon, do the following steps :
for (each pixel in polygon's projection)
{
    // Find depth i.e, z of polygon
    //
at (x, y) corresponding to pixel (i, j)   
    if (z < d(i, j))
    {
        d(i, j) =
z;
        c(i, j) =
color;
    }
}


==
See also ==
Z-fighting
Irregular Z-buffer
Z-order
A-buffer
Depth map
HyperZ
Stencil buffer


==
References ==


==
External links ==
Learning to Love your Z-buffer
Alpha-blending and the Z-buffer
== Notes ==
Dendral was a project in artificial intelligence (AI) of the 1960s, and the computer software expert system that it produced.
Its primary aim was to study hypothesis formation and discovery in science.

For that, a specific task in science was chosen:  help organic chemists in identifying unknown organic molecules, by analyzing their mass spectra and using knowledge of chemistry.

It was done at Stanford University by Edward Feigenbaum, Bruce G. Buchanan, Joshua Lederberg, and Carl Djerassi, along with a team of highly creative research associates and students.

It began in 1965 and spans approximately half the history of AI research.
The software program Dendral is considered the first expert system because it automated the decision-making process and problem-solving behavior of organic chemists.

The project consisted of research on two main programs  Heuristic Dendral and Meta-Dendral, and several sub-programs.

It was written in the Lisp programming language, which was considered the language of AI because of its flexibility.
Many systems were derived from Dendral, including MYCIN, MOLGEN, PROSPECTOR, XCON, and STEAMER.

There are many other programs today for solving the mass spectrometry inverse problem, see List of mass spectrometry software, but they are no longer described as 'artificial intelligence', just as structure searchers.

The name Dendral is an acronym of the term "Dendritic Algorithm".

==
Heuristic Dendral ==
Heuristic Dendral is a program that uses mass spectra or other experimental data together with a knowledge base of chemistry to produce a set of possible chemical structures that may be responsible for producing the data.

A mass spectrum of a compound is produced by a mass spectrometer, and is used to determine its molecular weight, the sum of the masses of its atomic constituents.
For example, the compound water (H2O), has a molecular weight of 18 since hydrogen has a mass of 1.01 and oxygen 16.00, and its mass spectrum has a peak at 18 units.
Heuristic Dendral would use this input mass and the knowledge of atomic mass numbers and valence rules, to determine the possible combinations of atomic constituents whose mass would add up to 18.

As the weight increases and the molecules become more complex, the number of possible compounds increases drastically.
Thus, a program that is able to reduce this number of candidate solutions through the process of hypothesis formation is essential.

New graph-theoretic algorithms were invented by Lederberg, Harold Brown, and others that generate all graphs with a specified set of nodes and connection-types (chemical atoms and bonds) -- with or without cycles.

Moreover, the team was able to prove mathematically that the generator is complete, in that it produces all graphs with the specified nodes and edges, and that it is non-redundant, in that the output contains no equivalent graphs (e.g., mirror images).

The CONGEN program, as it became known, was developed largely by computational chemists Ray Carhart, Jim Nourse, and Dennis Smith.

It was useful to chemists as a stand-alone program to generate chemical graphs showing a complete list of structures that satisfy the constraints specified by a user.

== Meta-Dendral ==
Meta-Dendral is a machine learning system that receives the set of possible chemical structures and corresponding mass spectra as input, and proposes a set of rules of mass spectrometry that correlate structural features with processes that produce the mass spectrum.
These rules would be fed back to Heuristic Dendral (in the planning and testing programs described below) to test their applicability.

Thus, "Heuristic Dendral is a performance system and
Meta-Dendral is a learning system".

The program is based on two important features: the plan-generate-test paradigm and knowledge engineering.

=== Plan-generate-test paradigm ===
The plan-generate-test paradigm is the basic organization of the problem-solving method, and is a common paradigm used by both Heuristic Dendral and Meta-Dendral systems.
The generator (later named CONGEN) generates potential solutions for a particular problem, which are then expressed as chemical graphs in Dendral.
However, this is feasible only when the number of candidate solutions is minimal.
When there are large numbers of possible solutions, Dendral has to find a way to put constraints that rules out large sets of candidate solutions.
This is the primary aim of Dendral planner, which is a “hypothesis-formation” program that employs “task-specific knowledge to find constraints for the generator”.
Last but not least, the tester analyzes each proposed candidate solution and discards those that fail to fulfill certain criteria.
This mechanism of plan-generate-test paradigm is what holds Dendral together.

===
Knowledge Engineering ===
The primary aim of knowledge engineering is to attain a productive interaction between the available knowledge base and problem solving techniques.
This is possible through development of a procedure in which large amounts of task-specific information is encoded into heuristic programs.
Thus, the first essential component of knowledge engineering is a large “knowledge base.”
Dendral has specific knowledge about the mass spectrometry technique, a large amount of information that forms the basis of chemistry and graph theory, and information that might be helpful in finding the solution of a particular chemical structure elucidation problem.
This “knowledge base” is used both to search for possible chemical structures that match the input data, and to learn new “general rules” that help prune searches.
The benefit Dendral provides the end user, even a non-expert, is a minimized set of possible solutions to check manually.

== Heuristics ==
A heuristic is a rule of thumb, an algorithm that does not guarantee a solution, but reduces the number of possible solutions by discarding unlikely and irrelevant solutions.

The use of heuristics to solve problems is called "heuristics programming", and was used in Dendral to allow it to replicate in machines the process through which human experts induce the solution to problems via rules of thumb and specific information.

Heuristics programming was a major approach and a giant step forward in artificial intelligence, as it allowed scientists to finally automate certain traits of human intelligence.
It became prominent among scientists in the late 1940s through George Polya’s book, How to Solve It:
A New Aspect of Mathematical Method.

As Herbert A. Simon said in The Sciences of the Artificial, "if you take a heuristic conclusion as certain, you may be fooled and disappointed; but if you neglect heuristic conclusions altogether you will make no progress at all."
==
History ==
During the mid 20th century, the question "can machines think?
"
became intriguing and popular among scientists, primarily to add humanistic characteristics to machine behavior.
John McCarthy, who was one of the prime researchers of this field, termed this concept of machine intelligence as "artificial intelligence" (AI) during the Dartmouth summer in 1956.
AI is usually defined as the capacity of a machine to perform operations that are analogous to human cognitive capabilities.
Much research to create AI was done during the 20th century.

Also around the mid 20th century, science, especially biology, faced a fast-increasing need to develop a "man-computer symbiosis", to aid scientists in solving problems.
For example, the structural analysis of myogoblin, hemoglobin, and other proteins relentlessly needed instrumentation development due to its complexity.

In the early 1960s, Joshua Lederberg started working with computers and quickly became tremendously interested in creating interactive computers to help him in his exobiology research.
Specifically, he was interested in designing computing systems to help him study alien organic compounds.
As he was not an expert in either chemistry or computer programming, he collaborated with Stanford chemist Carl Djerassi to help him with chemistry, and Edward Feigenbaum with programming, to automate the process of determining chemical structures from raw mass spectrometry data.
Feigenbaum was an expert in programming languages and heuristics, and helped Lederberg design a system that replicated the way Djerassi solved structure elucidation problems.
They devised a system called Dendritic Algorithm (Dendral) that was able to generate possible chemical structures corresponding to the mass spectrometry data as an output.
Dendral then was still very inaccurate in assessing spectra of ketones, alcohols, and isomers of chemical compounds.
Thus, Djerassi "taught" general rules to Dendral that could help eliminate most of the "chemically implausible" structures, and produce a set of structures that could now be analyzed by a "non-expert" user to determine the right structure.
The Dendral team recruited Bruce Buchanan to extend the Lisp program initially written by Georgia Sutherland.

Buchanan had similar ideas to Feigenbaum and Lederberg, but his special interests were scientific discovery and hypothesis formation.

As Joseph November said in Digitizing Life:
The Introduction of Computers to Biology and Medicine, "(Buchanan) wanted the system (Dendral) to make discoveries on its own, not just help humans make them".
Buchanan, Lederberg and Feigenbaum designed "Meta-Dendral", which was a "hypothesis maker".

Heuristic Dendral "would serve as a template for similar knowledge-based systems in other areas" rather than just concentrating in the field of organic chemistry.

Meta-Dendral was a model for knowledge-rich learning systems that was later codified in Tom Mitchell's influential Version Space Model of learning.

== Notes ==


==
References ==
Berk, A A. LISP: the Language of Artificial Intelligence.
New York:
Van Nostrand Reinhold Company, 1985.
1-25.

Lederberg, Joshua.
An Instrumentation Crisis in Biology.
Stanford University Medical School.
Palo Alto, 1963.

Lederberg, Joshua.
How Dendral Was Conceived and Born.
ACM Symposium on the History of Medical Informatics, 5 November 1987, Rockefeller University.
New York:
National Library of Medicine, 1987.

Lindsay, Robert K., Bruce G. Buchanan, Edward A. Feigenbaum, and Joshua Lederberg.
Applications of Artificial Intelligence for Organic Chemistry:
The Dendral Project.
McGraw-Hill Book Company, 1980.

Lindsay, Robert K., Bruce G. Buchanan, E. A. Feigenbaum, and Joshua Lederberg.
DENDRAL:
A Case Study of the First Expert System for Scientific Hypothesis Formation.
Artificial Intelligence 61, 2 (1993): 209-261.

November, Joseph A. “Digitizing Life:
The Introduction of Computers to Biology and Medicine.”
Doctoral dissertation, Princeton University, 2006
The anchoring effect is a cognitive bias whereby an individual's decisions are influenced by a particular reference point or 'anchor'.
Once the value of the anchor is set, subsequent arguments, estimates, etc.
made by an individual may change from what they would have otherwise been without the anchor.
For example, an individual may be more likely to purchase a car if it is placed alongside a more expensive model (the anchor).
Prices discussed in negotiations that are lower than the anchor may seem reasonable, perhaps even cheap to the buyer, even if said prices are still relatively higher than the actual market value of the car.

Another example may be when estimating the orbit of Mars, one might start with the Earth's orbit (365 days) and then adjust upward until they reach a value that seems reasonable (usually less than 687 days, the correct answer).

The original description of the anchoring effect came from psychophysics.
When judging stimuli along a continuum, it was noticed that the first and last stimuli were used to compare the other stimuli (this is also referred to as "end anchoring").
This was applied to attitudes by Sherif et al.
in their 1958 article "Assimilation and effects of anchoring stimuli on judgments".

==
Experimental Findings ==
The anchoring and adjustment heuristic was first theorized by Amos Tversky and Daniel Kahneman.
In one of their first studies, participants were asked to compute, within 5 seconds, the product of the numbers one through to eight, either as 1 × 2 × 3 × 4 × 5 × 6 × 7 × 8 or reversed as 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1.
Because participants did not have enough time to calculate the full answer, they had to make an estimate after their first few multiplications.
When these first multiplications gave a small answer – because the sequence started with small numbers – the median estimate was 512; when the sequence started with the larger numbers, the median estimate was 2,250.
(
The correct answer is 40,320.)
In another study by Tversky and Kahneman, participants observed a roulette wheel that was predetermined to stop on either 10 or 65.
Participants were then asked to guess the percentage of the United Nations that were African nations.
Participants whose wheel stopped on 10 guessed lower values (25% on average) than participants whose wheel stopped at 65 (45% on average).
The pattern has held in other experiments for a wide variety of different subjects of estimation.

As a second example, in a study by Dan Ariely, an audience is first asked to write the last two digits of their social security number and consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate and computer equipment.
They were then asked to bid for these items, with the result that the audience members with higher two-digit numbers would submit bids that were between 60 percent and 120 percent higher than those with the lower social security numbers, which had become their anchor.
When asked if they believed the number was informative of the value of the item, quite a few said yes.
Trying to avoid this confusion, a small number of studies used procedures that were clearly random, such as Excel random generator button and die roll, and failed to replicate anchoring effects.

The anchoring effect was also found to be present in a study in the Journal of Real Estate Research in relation to house prices.
In this investigation, it was established that the 2-year and 9-year highs on the Case-Shiller House Price Index could be used as anchors in predicting current house prices.
The findings were used to indicate that, in forecasting house prices, these 2-year and 9-years highs might be relevant.

===
Difficulty of avoiding ===
Various studies have shown that anchoring is very difficult to avoid.
For example, in one study students were given anchors that were wrong.
They were asked whether Mahatma Gandhi died before or after age 9, or before or after age 140.
Clearly neither of these anchors can be correct, but when the two groups were asked to suggest when they thought he had died, they guessed significantly differently (average age of 50 vs. average age of 67).Other studies have tried to eliminate anchoring much more directly.
In a study exploring the causes and properties of anchoring, participants were exposed to an anchor and asked to guess how many physicians were listed in the local phone book.
In addition, they were explicitly informed that anchoring would "contaminate" their responses, and that they should do their best to correct for that.
A control group received no anchor and no explanation.
Regardless of how they were informed and whether they were informed correctly, all of the experimental groups reported higher estimates than the control group.
Thus, despite being expressly aware of the anchoring effect, participants were still unable to avoid it.
A later study found that even when offered monetary incentives, people are unable to effectively adjust from an anchor.

===
Durability of anchoring ==
=
Anchoring effects are also shown to remain adequately present given the accessibility of knowledge pertaining to the target.
This, in turn, suggests that despite a delay in judgement towards a target, the extent of anchoring effects have seen to remain unmitigated within a given time period.
A series of three experiments were conducted to test the longevity of anchoring effects.
It was observed that despite a delay of one week being introduced for half the sample population of each experiment, similar results of immediate judgement and delayed judgement of the target were achieved.

The experiments concluded that external information experienced within the delayed judgement period shows little influence relative to self-generated anchors even with commonly encountered targets (temperature) used in one of the experiments, showing that anchoring effects may precede priming in duration especially when the anchoring effects were formed during the task.
Further research to conclude an effect that is effectively retained over a substantial period of time has proven inconsistent.
===
Anchoring bias in groups ===
Given the old saying that 'Two Heads are Better than One', it is often presumed that groups come to a more unbiased decision relative to individuals.
However, this assumption is supported with varied findings that could not come to a general consensus.
Nevertheless, while some groups are able to perform better than an individual member, they are found to be just as biased or even more biased relative to their individual counterparts.
A possible cause would be the discriminatory fashion in which information is communicated, processed and aggregated based on each individual's anchored knowledge and belief.
This results in a diminished quality in the decision-making process and consequently, amplifies the pre-existing anchored biases.

The cause of group anchoring remains obscure.
Group anchors may have been established at the group level or may simply be the culmination of several individual's personal anchors.
Previous studies have shown that when given an anchor before the experiment, individual members consolidated the respective anchors to attain a decision in the direction of the anchor placed.
However, a distinction between individual and group-based anchor biases does exist, with groups tending to ignore or disregard external information due to the confidence in the  joint decision-making process.
The presence of pre-anchor preferences also impeded the extent to which external anchors affected the group decision, as groups tend to allocate more weight to self-generated anchors, according to the 'competing anchor hypothesis'.
A series of experiments were conducted to investigate anchoring bias in groups and possible solutions to avoid or mitigate anchoring.
The first experiment established that groups are indeed influenced by anchors while the other two experiments highlighted methods to overcome group anchoring bias.
Utilized methods include the use of process accountability and motivation through competition instead of cooperation to reduce the influence of anchors within groups.

===
Business Intelligence ===
A peer-reviewed study sought to investigate the effect of business intelligence (BI) systems on the anchoring effect.
Business intelligence denotes an array of software and services used by businesses to gather valuable insights into an organisation's performance.
The extent to which cognitive bias is mitigated by using these systems was the overarching question in this study.
While the independent variable was the use of the BI system, the dependent variable was the outcome of the decision-making process.
The subjects were presented with a 'plausible' anchor and a 'spurious' anchor in a forecasting decision.
It was found that, while the BI system mitigated the negative effects of the spurious anchor, it had no influence on the effects of the plausible anchor.
This is important in a business context, because it shows that humans are still susceptible to cognitive biases, even when using sophisticated technological systems.
One of the subsequent recommendations from the experimenters was to implement a forewarning into BI systems as to the anchoring effect.

==
Causes ==
Several theories have been put forth to explain what causes anchoring, and although some explanations are more popular than others, there is no consensus as to which is best.
In a study on possible causes of anchoring, two authors described anchoring as easy to demonstrate, but hard to explain.
At least one group of researchers has argued that multiple causes are at play, and that what is called "anchoring" is actually several different effects.

===
Anchoring-and-adjusting ===
In their original study, Tversky and Kahneman put forth a view later termed anchoring-as-adjustment.
According to this theory, once an anchor is set, people adjust away from it to get to their final answer; however, they adjust insufficiently, resulting in their final guess  being closer to the anchor than it would be otherwise.
Other researchers also found evidence supporting the anchoring-and-adjusting explanation.
Factors that influence the capacity for judgmental correction, like alcohol intoxication and performing a taxing cognitive load (rehearsing a long string of digits in working memory) tend to increase anchoring effects.
If people know the direction in which they should adjust, incentivizing accuracy also appears to reduce anchoring effects.
This model is not without its critiques.
Proponents of alternative theories have researchers criticized this model, claiming it is only applicable when the initial anchor is outside the range of acceptable answers.
To use an earlier example, since Mahatma Gandhi obviously did not die at age 9, then people will adjust from there.
If a reasonable number were given, though, there would be no adjustment.
Therefore, this theory cannot, according to its critics, explain all cases of anchoring effect.

===
Selective accessibility ===
An alternate explanation regarding selective accessibility  is derived from a theory called "confirmatory hypothesis testing".
In short, selective accessibility proposes that when given an anchor, a judge (i.e. a person making some judgment) will evaluate the hypothesis that the anchor is a suitable answer.
Assuming it is not, the judge moves on to another guess, but not before accessing all the relevant attributes of the anchor itself.
Then, when evaluating the new answer, the judge looks for ways in which it is similar to the anchor, resulting in the anchoring effect.
Various studies have found empirical support for this hypothesis.
This explanation assumes that the judge considers the anchor to be a plausible value so that it is not immediately rejected, which would preclude considering its relevant attributes.
For example, an online-experiment showed that ratings of previous members of the crowd could act as an anchor.
When displaying the results of previous ratings in the context of business model idea evaluation, people incorporate the displayed anchor into their own decision making process, leading to a decreasing variance of ratings.

===
Attitude change ===
More recently, a third explanation of anchoring has been proposed concerning attitude change.
According to this theory, providing an anchor changes someone's attitudes to be more favorable to the particular attributes of that anchor, biasing future answers to have similar characteristics as the anchor.
Leading proponents of this theory consider it to be an alternate explanation in line with prior research on anchoring-and-adjusting and selective accessibility.

==
Influencing Factors ==


=== Mood ===
A wide range of research has linked sad or depressed moods with more extensive and accurate evaluation of problems.
As a result of this, earlier studies hypothesized that people with more depressed moods would tend to use anchoring less than those with happier moods.
However, more recent studies have shown the opposite effect: sad people are more likely to use anchoring than people with happy or neutral mood.

===
Experience ===
Early research found that experts (those with high knowledge, experience, or expertise in some field) were more resistant to the anchoring effect.
Since then, however, numerous studies have demonstrated that while experience can sometimes reduce the effect, even experts are susceptible to anchoring.
In a study concerning the effects of anchoring on judicial decisions, researchers found that even experienced legal professionals were affected by anchoring.
This remained true even when the anchors provided were arbitrary and unrelated to the case in question.
Also, this relates to goal setting, where more experienced individuals will set goals based on their past experiences which consequently affects end results in negotiations.

===
Personality ===
Research has correlated susceptibility to anchoring with most of the Big Five personality traits.
People high in agreeableness and conscientiousness are more likely to be affected by anchoring, while those high in extraversion are less likely to be affected.
Another study found that those high in openness to new experiences were more susceptible to the anchoring effect.

===
Cognitive ability ===
The impact of cognitive ability on anchoring is contested.
A recent study on willingness to pay for consumer goods found that anchoring decreased in those with greater cognitive ability, though it did not disappear.
Another study, however, found that cognitive ability had no significant effect on how likely people were to use anchoring.

===
Overconfidence ===
Cognitive conceit or overconfidence arises from other factors like personal cognitive attributes such as knowledge and decision-making ability, decreasing the probability to pursue external sources of confirmation.
This factor has also been shown to arise with tasks with greater difficulty.
Even within subject matter experts, they were also prey to such behaviour of overconfidence and should more so, actively reduce such behaviour.
Following the study of estimations under uncertain, despite several attempts to curb overconfidence proving unsuccessful, Tversky and Kahneman (1971) research suggest an effective solution to overconfidence is for subjects to explicitly establish anchors to help reduce overconfidence in their estimates.

==
Anchoring in Negotiations ==
In the negotiation process anchoring serves to determine an accepted starting point for the subsequent negotiations.
As soon as one side states their first price offer, the (subjective) anchor is set.
The counterbid (counter-anchor) is the second-anchor.
In addition to the initial research conducted by Tversky and Kahneman, multiple other studies have shown that anchoring can greatly influence the estimated value of an object.
For instance, although negotiators can generally appraise an offer based on multiple characteristics, studies have shown that they tend to focus on only one aspect.
In this way, a deliberate starting point can strongly affect the range of possible counteroffers.
The process of offer and counteroffer results in a mutually beneficial arrangement.
However, multiple studies have shown that initial offers have a stronger influence on the outcome of negotiations than subsequent counteroffers.
An example of the power of anchoring has been conducted during the Strategic Negotiation Process Workshops.
During the workshop, a group of participants is divided into two sections: buyers and sellers.
Each side receives identical information about the other party before going into a one-on-one negotiation.
Following this exercise, both sides debrief about their experiences.
The results show that where the participants anchor the negotiation had a significant effect on their success.
Anchoring affects everyone, even people who are highly knowledgeable in a field.
Northcraft and Neale conducted a study to measure the difference in the estimated value of a house between students and real-estate agents.
In this experiment, both groups were shown a house and then given different listing prices.
After making their offer, each group was then asked to discuss what factors influenced their decisions.
In the follow-up interviews, the real-estate agents denied being influenced by the initial price, but the results showed that both groups were equally influenced by that anchor.
Anchoring can have more subtle effects on negotiations as well.
Janiszewski and Uy investigated the effects of precision of an anchor.
Participants read an initial price for a beach house, then gave the price they thought it was worth.
They received either a general, seemingly nonspecific anchor (e.g., $800,000) or a more precise and specific anchor (e.g., $799,800).
Participants with a general anchor adjusted their estimate more than those given a precise anchor ($751,867 vs $784,671).
The authors propose that this effect comes from difference in scale; in other words, the anchor affects not only the starting value, but also the starting scale.
When given a general anchor of $20, people will adjust in large increments ($19, $21, etc.),
but when given a more specific anchor like $19.85, people will adjust on a lower scale ($19.75, $19.95, etc.).
Thus, a more specific initial price will tend to result in a final price closer to the initial one.

As for the question of setting the first or second anchor, the party setting the second anchor has the advantage in that the counter-anchor determines the point midway between both anchors.
Due to a possible lack of knowledge the party setting the first anchor can also set it too low, i.e. against their own interests.
Generally negotiators who set the first anchor also tend to be less satisfied with the negotiation outcome, than negotiators who set the counter-anchor.
This may be due to the regret or sense that they did not achieve or rather maximise the full potential of the negotiations.
However, studies suggest that negotiators who set the first offer frequently achieve economically more advantageous results.

== See also ==
List of cognitive biases
Poisoning the well
Primacy effect
Negotiation strategies
Law of the instrument


==
References ==


==
Further reading ==
Serfas, S. (2010).
Cognitive Biases in the Capital Investment Context:
Theoretical Considerations and Empirical Experiments on Violations of Normative Rationality.
Gabler research.
Gabler Verlag.
pp.
67–70.
ISBN 978-3-8349-6485-4.
Retrieved April 9, 2019.
Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content.

Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.

==
Overview ==
Computer graphics studies the manipulation of visual and geometric information using computational techniques.

It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues.

Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.

Connected studies include:
Applied mathematics
Computational geometry
Computational topology
Computer vision
Image processing
Information visualization
Scientific visualizationApplications of computer graphics include:
Print design
Digital art
Special effects
Video games
Visual effects


==
History ==
There are several international conferences and journals where the most significant results in computer graphics are published.
Among them are the SIGGRAPH and Eurographics conferences and the Association for Computing Machinery (ACM) Transactions on Graphics journal.
The joint Eurographics and ACM SIGGRAPH symposium series features the major venues for the more specialized sub-fields: Symposium on Geometry Processing, Symposium on Rendering, Symposium on Computer Animation, and High Performance Graphics.
As in the rest of computer science, conference publications in computer graphics are generally more significant than journal publications (and subsequently have lower acceptance rates).

==
Subfields ==
A broad classification of major subfields in computer graphics might be:
Geometry: ways to represent and process surfaces
Animation: ways to represent and manipulate motion
Rendering:
algorithms to reproduce light transport
Imaging: image acquisition or image editing


===
Geometry ===
The subfield of geometry studies the representation of three-dimensional objects in a discrete digital setting.

Because the appearance of an object depends largely on its exterior, boundary representations are most commonly used.

Two dimensional surfaces are a good representation for most objects, though they may be non-manifold.

Since surfaces are not finite, discrete digital approximations are used.
Polygonal meshes (and to a lesser extent subdivision surfaces) are by far the most common representation, although point-based representations have become more popular recently
(see for instance the Symposium on Point-Based Graphics).
These representations are Lagrangian, meaning the spatial locations of the samples are independent.

Recently, Eulerian surface descriptions (i.e., where spatial samples are fixed) such as level sets have been developed into a useful representation for deforming surfaces which undergo many topological changes (with fluids being the most notable example).Geometry subfields include:

Implicit surface modeling – an older subfield which examines the use of algebraic surfaces, constructive solid geometry, etc.,
for surface representation.

Digital geometry processing – surface reconstruction, simplification, fairing, mesh repair, parameterization, remeshing, mesh generation, surface compression, and surface editing
all fall under this heading.

Discrete differential geometry – a nascent field which defines geometric quantities for the discrete surfaces used in computer graphics.

Point-based graphics – a recent field which focuses on points as the fundamental representation of surfaces.

Subdivision surfaces
Out-of-core mesh processing – another recent field which focuses on mesh datasets that do not fit in main memory.
===
Animation ===
The subfield of animation studies descriptions for surfaces (and other phenomena) that move or deform over time.

Historically, most work in this field has focused on parametric and data-driven models, but recently physical simulation has become more popular as computers have become more powerful computationally.

Animation subfields include:
Performance capture
Character animation
Physical simulation (e.g. cloth modeling,  animation of fluid dynamics, etc.)
===
Rendering ===
Rendering generates images from a model.

Rendering may simulate light transport to create realistic images or it may create images that have a particular artistic style in non-photorealistic rendering.

The two basic operations in realistic rendering are transport (how much light passes from one place to another) and scattering (how surfaces interact with light).

See
Rendering (computer graphics) for more information.

Rendering subfields include:
Transport describes how illumination in a scene gets from one place to another.
Visibility is a major component of light transport.

Scattering: Models of scattering (how light interacts with the surface at a given point) and shading (how material properties vary across the surface) are used to describe the appearance of a surface.

In graphics these problems are often studied within the context of rendering since they can substantially affect the design of rendering algorithms.

Descriptions of scattering are usually given in terms of a bidirectional scattering distribution function (BSDF).

The latter issue addresses how different types of scattering are distributed across the surface (i.e., which scattering function applies where).

Descriptions of this kind are typically expressed with a program called a shader.

(Note that there is some confusion since the word "shader" is sometimes used for programs that describe local geometric variation.)

Non-photorealistic rendering
Physically based rendering – concerned with generating images according to the laws of geometric optics
Real-time rendering – focuses on rendering for interactive applications, typically using specialized hardware like GPUs
Relighting – recent area concerned with quickly re-rendering scenes


==
Notable researchers ==


==
See also ==


==
References ==


==
Further reading ==
Foley et al.
Computer Graphics:
Principles and Practice.

Shirley.
Fundamentals of Computer Graphics.

Watt.
3D Computer Graphics.

==
External links ==
A Critical History of Computer Graphics and Animation
History of Computer Graphics series of articles


===
Industry ===
Industrial labs doing "blue sky" graphics research include:

Adobe Advanced Technology Labs
MERL
Microsoft Research – Graphics
Nvidia ResearchMajor film studios notable for graphics research include:
ILM
PDI/Dreamworks Animation
Pixar
In mathematics and computational geometry,  a Delaunay triangulation (also known as a Delone triangulation) for a given set P of discrete points in a general position is a triangulation DT(P) such that no point in P is inside the circumcircle of any triangle in DT(P).
Delaunay triangulations maximize the minimum angle of all the angles of the triangles in the triangulation; they tend to avoid sliver triangles.
The triangulation is named after Boris Delaunay for his work on this topic from 1934.For a set of points on the same line there is no Delaunay triangulation (the notion of triangulation is degenerate for this case).

For four or more points on the same circle (e.g., the vertices of a rectangle)
the Delaunay triangulation is not unique: each of the two possible triangulations that split the quadrangle into two triangles satisfies the "Delaunay condition", i.e., the requirement that the circumcircles of all triangles have empty interiors.

By considering circumscribed spheres, the notion of Delaunay triangulation extends to three and higher dimensions.

Generalizations are possible to metrics other than Euclidean distance.
However, in these cases a Delaunay triangulation is not guaranteed to exist or be unique.

==
Relationship with the Voronoi diagram ==
The Delaunay triangulation of a discrete point set P in general position corresponds to the dual graph of the Voronoi diagram for P.
The circumcenters of Delaunay triangles are the vertices of the Voronoi diagram.

In the 2D case, the Voronoi vertices are connected via edges, that can be derived from adjacency-relationships of the Delaunay triangles: If two triangles share an edge in the Delaunay triangulation, their circumcenters are to be connected with an edge in the Voronoi tesselation.

Special cases where this relationship does not hold, or is ambiguous, include cases like:

Three or more collinear points, where the circumcircles are of infinite radii.

Four or more points on a perfect circle, where the triangulation is ambiguous and all circumcenters are trivially identical.

Edges of the Voronoi diagram going to infinity are not defined by this relation in case of a finite set P.
If the Delaunay triangulation is calculated using the Bowyer–Watson algorithm then the circumcenters of triangles having a common vertex with the "super" triangle should be ignored.
Edges going to infinity start from a circumcenter
and they are perpendicular to the common edge between the kept and ignored triangle.

==
d-dimensional Delaunay ==
For a set P of points in the (d-dimensional) Euclidean space, a Delaunay triangulation is a triangulation DT(P) such that no point in P is inside the circum-hypersphere of any d-simplex in DT(P).

It is known that there exists a unique Delaunay triangulation for P if P is a set of points in general position; that is, the affine hull of P is d-dimensional and no set of d + 2 points in P lie on the boundary of a ball whose interior does not intersect P.
The problem of finding the Delaunay triangulation of a set of points in d-dimensional Euclidean space can be converted to the problem of finding the convex hull of a set of points in (d + 1)-dimensional space.
This may be done by giving each point p an extra coordinate equal to |p|2, thus turning it into a hyper-paraboloid (this is termed "lifting"); taking the bottom side of the convex hull (as the top end-cap faces upwards away from the origin, and must be discarded); and mapping back to d-dimensional space by deleting the last coordinate.
As the convex hull is unique, so is the triangulation, assuming all facets of the convex hull are simplices.
Nonsimplicial facets only occur when d +
2 of the original points lie on the same d-hypersphere, i.e., the points are not in general position.

==
Properties ==
Let n be the number of points and d
the number of dimensions.

The union of all simplices in the triangulation is the convex hull of the points.

The Delaunay triangulation contains O(n⌈d / 2⌉) simplices.

In the plane (d = 2), if there are b vertices on the convex hull, then any triangulation of the points has at most 2n − 2
− b triangles, plus one exterior face (see Euler characteristic).

If points are distributed according to a Poisson process in the plane with constant intensity, then each vertex has on average six surrounding triangles.
More generally for the same process in d dimensions the average number of neighbors is a constant depending only on d.
In the plane, the Delaunay triangulation maximizes the minimum angle.

Compared to any other triangulation of the points, the smallest angle in the Delaunay triangulation is at least as large as the smallest angle in any other.

However, the Delaunay triangulation does not necessarily minimize the maximum angle.
The Delaunay triangulation also does not necessarily minimize the length of the edges.

A circle circumscribing any Delaunay triangle does not contain any other input points in its interior.

If a circle passing through two of the input points doesn't contain any other input points in its interior, then the segment connecting the two points is an edge of a Delaunay triangulation of the given points.

Each triangle of the Delaunay triangulation of a set of points in d-dimensional spaces corresponds to a facet of convex hull of the projection of the points onto a (d + 1)-dimensional paraboloid, and vice versa.

The closest neighbor b to any point p is on an edge bp in the Delaunay triangulation since the nearest neighbor graph is a subgraph of the Delaunay triangulation.

The Delaunay triangulation is a geometric spanner: In the plane (d = 2), the shortest path between two vertices, along Delaunay edges, is known to be no longer than 1.998 times the Euclidean distance between them.

==
Visual Delaunay definition: Flipping ==
From the above properties
an important feature arises: Looking at two triangles ABD and BCD with the common edge BD (see figures), if the sum of the angles α and γ is less than or equal to 180°, the triangles meet the Delaunay condition.

This is an important property because it allows the use of a flipping technique.
If two triangles do not meet the Delaunay condition, switching the common edge BD for the common edge
AC produces two triangles that do meet the Delaunay condition:
This operation is called a flip, and can be generalised to three and higher dimensions.

==
Algorithms ==
Many algorithms for computing Delaunay triangulations rely on fast operations for detecting when a point is within a triangle's circumcircle and an efficient data structure for storing triangles and edges.

In two dimensions, one way to detect if point D lies in the circumcircle of A, B, C is to evaluate the determinant:
|
A
                            
                              x
A
                            
                              y
A
x
                            
                            
                              2
                            
                          
                          +
A
                            
                              y
                            
                            
                              2
                            
                          
                        
                        
                          1
B
                            
                              x
B
                            
                              y
B
                            
                              x
                            
                            
                              2
+
                          
                            B
                            
                              y
2
1
C
                            
                              x
C
                            
                              y
C
x
                            
                            
                              2
+
                          
                            C
                            
                              y
2
                            
                          
                        
                        
                          1
D
                            
                              x
D
                            
                              y
D
x
2
+
                          
                            D
                            
                              y
2
1
                        
                      
                    
                    |
=
|
A
                            
                              x
                            
                          
                          −
                          
                            D
x
A
                            
                              y
                            
                          
                          −
D
                            
                              y
(
                          
                            A
                            
                              x
                            
                            
                              2
                            
                          
                          −
                          
                            D
x
                            
                            
                              2
                            
                          
                          )
+
                          (
                          
                            A
                            
                              y
2
                            
                          
                          −
D
                            
                              y
2
                            
                          
                          )
B
                            
                              x
                            
                          
                          −
                          
                            D
                            
                              x
B
y
                            
                          
                          −
D
                            
                              y
(
                          
                            B
                            
                              x
                            
                            
                              2
                            
                          
                          −
                          
                            D
x
                            
                            
                              2
                            
                          
                          )
+
                          (
                          
                            B
                            
                              y
2
−
D
                            
                              y
2
                            
                          
                          )
C
                            
                              x
                            
                          
                          −
                          
                            D
                            
                              x
C
                            
                              y
                            
                          
                          −
D
                            
                              y
(
                          
                            C
                            
                              x
                            
                            
                              2
                            
                          
                          −
                          
                            D
x
                            
                            
                              2
                            
                          
                          )
+
                          (
                          
                            C
                            
                              y
2
−
D
                            
                              y
2
                            
                          
                          )
|
                  
                
              
            
            
              
                =
|
A
                            
                              x
                            
                          
                          −
                          
                            D
x
A
                            
                              y
                            
                          
                          −
D
                            
                              y
(
                          
                            A
                            
                              x
                            
                          
                          −
D
x
                            
                          
                          
                            )
                            
                              2
+
(
                          
                            A
                            
                              y
                            
                          
                          −
                          
                            D
                            
                              y
                            
                          
                          
                            )
                            
                              2
B
                            
                              x
                            
                          
                          −
                          
                            D
                            
                              x
B
y
                            
                          
                          −
D
                            
                              y
(
                          
                            B
x
                            
                          
                          −
D
x
                            
                          
                          
                            )
                            
                              2
+
                          (
                          
                            B
                            
                              y
                            
                          
                          −
                          
                            D
                            
                              y
                            
                          
                          
                            )
                            
                              2
C
                            
                              x
                            
                          
                          −
                          
                            D
                            
                              x
C
                            
                              y
                            
                          
                          −
D
                            
                              y
(
                          
                            C
x
                            
                          
                          −
D
x
                            
                          
                          
                            )
                            
                              2
+
                          (
                          
                            C
                            
                              y
                            
                          
                          −
                          
                            D
                            
                              y
                            
                          
                          
                            )
                            
                              2
                            
                          
                        
                      
                    
                    |
>
0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\begin{vmatrix}A_{x}&A_{y}&A_{x}^{2}+A_{y}^{2}&1\\B_{x}&B_{y}&B_{x}^{2}+B_{y}^{2}&1\\C_{x}&C_{y}&C_{x}^{2}+C_{y}^{2}&1\\D_{x}&D_{y}&D_{x}^{2}+D_{y}^{2}&1\end{vmatrix}}={\begin{vmatrix}A_{x}-D_{x}&A_{y}-D_{y}&(A_{x}^{2}-D_{x}^{2})+(A_{y}^{2}-D_{y}^{2})\\B_{x}-D_{x}&B_{y}-D_{y}&(B_{x}^{2}-D_{x}^{2})+(B_{y}^{2}-D_{y}^{2})\\C_{x}-D_{x}&C_{y}-D_{y}&(C_{x}^{2}-D_{x}^{2})+(C_{y}^{2}-D_{y}^{2})\end{vmatrix}}\\[8pt]={}&{\begin{vmatrix}A_{x}-D_{x}&A_{y}-D_{y}&(A_{x}-D_{x})^{2}+(A_{y}-D_{y})^{2}\\B_{x}-D_{x}&B_{y}-D_{y}&(B_{x}-D_{x})^{2}+(B_{y}-D_{y})^{2}\\C_{x}-D_{x}&C_{y}-D_{y}&(C_{x}-D_{x})^{2}+(C_{y}-D_{y})^{2}\end{vmatrix}}>0\end{aligned}}}
When A, B and C are sorted in a counterclockwise order, this determinant is positive if and only if D lies inside the circumcircle.

=== Flip algorithms ===
As mentioned above, if a triangle is non-Delaunay, we can flip one of its edges.

This leads to a straightforward algorithm: construct any triangulation of the points, and then flip edges until no triangle is non-Delaunay.
Unfortunately, this can take Ω(n2) edge flips.
While this algorithm can be generalised to three and higher dimensions, its convergence is not guaranteed in these cases, as it is conditioned to the connectedness of the underlying flip graph: this graph is connected for two dimensional sets of points, but may be disconnected in higher dimensions.

===
Incremental ===
The most straightforward way of efficiently computing the Delaunay triangulation is to repeatedly add one vertex at a time, retriangulating the affected parts of the graph.

When a vertex v is added, we split in three the triangle that contains v, then we apply the flip algorithm.

Done naïvely, this will take O(n) time
: we search through all the triangles to find the one that contains v, then we potentially flip away every triangle.

Then the overall runtime is O(n2).

If we insert vertices in random order, it turns out (by a somewhat intricate proof) that each insertion will flip, on average
, only O(1) triangles –
although sometimes it will flip many more.

This still leaves the point location time to improve.

We can store the history of the splits and flips performed: each triangle stores a pointer to the two or three triangles that replaced it.

To find the triangle that contains v, we start at a root triangle, and follow the pointer that points to a triangle that contains v, until we find a triangle that has not yet been replaced.

On average, this will also take O(log n) time.

Over all vertices, then, this takes O(n log n) time.

While the technique extends to higher dimension (as proved by Edelsbrunner and Shah), the runtime can be exponential in the dimension even if the final Delaunay triangulation is small.

The Bowyer–Watson algorithm provides another approach for incremental construction.

It gives an alternative to edge flipping for computing the Delaunay triangles containing a newly inserted vertex.

Unfortunately the flipping-based algorithms are generally hard to be parallelized, since adding some certain point (e.g. the center point of a wagon wheel) can lead to up to O(n) consecutive flips.

Blelloch et al.
proposed another version of incremental algorithm based on rip-and-tent, which is practical and highly parallelized with polylogarithmic span.

=== Divide and conquer ===
A divide and conquer algorithm for triangulations in two dimensions was developed by Lee and Schachter and improved by Guibas and Stolfi and later by Dwyer.

In this algorithm, one recursively draws a line to split the vertices into two sets.
The Delaunay triangulation is computed for each set, and then the two sets are merged along the splitting line.

Using some clever tricks, the merge operation can be done in time O(n), so the total running time is
O(n log n).For certain types of point sets, such as a uniform random distribution, by intelligently picking the splitting lines
the expected time can be reduced to O(n log log n) while still maintaining worst-case performance.

A divide and conquer paradigm to performing a triangulation in d dimensions is presented in "DeWall: A fast divide and conquer Delaunay triangulation algorithm in Ed" by P. Cignoni, C. Montani, R. Scopigno.
The divide and conquer algorithm has been shown to be the fastest DT generation technique.

===
Sweephull ===
Sweephull is a hybrid technique for 2D Delaunay triangulation that uses a radially propagating sweep-hull, and a flipping algorithm.
The sweep-hull is created sequentially by iterating a radially-sorted set of 2D points, and connecting triangles to the visible part of the convex hull, which gives a non-overlapping triangulation.
One can build a convex hull in this manner so long as the order of points guarantees no point would fall within the triangle.
But, radially sorting should minimize flipping by being highly Delaunay to start.
This is then paired with a final iterative triangle flipping step.

==
Applications ==
The Euclidean minimum spanning tree of a set of points is a subset of the Delaunay triangulation of the same points, and this can be exploited to compute it efficiently.

For modelling terrain or other objects given a set of sample points, the Delaunay triangulation gives a nice set of triangles to use as polygons in the model.

In particular, the Delaunay triangulation avoids narrow triangles (as they have large circumcircles compared to their area).
See triangulated irregular network.

Delaunay triangulations can be used to determine the density or intensity of points samplings by means of the Delaunay tessellation field estimator (DTFE).

Delaunay triangulations are often used to generate meshes for space-discretised solvers such as the finite element method and the finite volume method of physics simulation, because of the angle guarantee and because fast triangulation algorithms have been developed.

Typically, the domain to be meshed is specified as a coarse simplicial complex; for the mesh to be numerically stable, it must be refined, for instance by using Ruppert's algorithm.

The increasing popularity of finite element method and boundary element method techniques increases the incentive to improve automatic meshing algorithms.
However, all of these algorithms can create distorted and even unusable grid elements.
Fortunately, several techniques exist which can take an existing mesh and improve its quality.
For example, smoothing (also referred to as mesh refinement) is one such method, which repositions nodes to minimize element distortion.
The stretched grid method allows the generation of pseudo-regular meshes that meet the Delaunay criteria easily and quickly in a one-step solution.

Constrained Delaunay triangulation has found applications in path planning in automated driving and topographic surveying.

== See also ==


==
References ==


==
External links ==
"Delaunay triangulation".
Wolfram MathWorld.
Retrieved April 2010.

===
Software ===
Delaunay triangulation in CGAL, the Computational Geometry Algorithms Library:
Mariette Yvinec.
2D Triangulation.
Retrieved April 2010.

Pion, Sylvain; Teillaud, Monique.
3D Triangulations.
Retrieved April 2010.

Hornus, Samuel; Devillers, Olivier; Jamin, Clément.
dD Triangulations.

Hert, Susan; Seel, Michael.
dD Convex Hulls and Delaunay Triangulations.
Retrieved April 2010.

"Poly2Tri:
Incremental constrained Delaunay triangulation.
Open source C++ implementation.
Retrieved April 2019.

"Divide & Conquer Delaunay triangulation construction".
Open source C99 implementation.
Retrieved April 2019.
Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.

The resulting image is referred to as the render.

Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.

The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene.
The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file.
The term "rendering" is analogous to the concept of an artist's impression of a scene.

The term "rendering" is also used to describe the process of calculating effects in a video editing program to produce the final video output.

Rendering is one of the major sub-topics of 3D computer graphics, and in practice it is always connected to the others.
It is the last major step in the graphics pipeline, giving models and animation their final appearance.
With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject.

Rendering has uses in architecture, video games, simulators, movie and TV visual effects, and design visualization, each employing a different balance of features and techniques.
A wide variety of renderers are available for use.
Some are integrated into larger modeling and animation packages, some are stand-alone, and some are free open-source projects.
On the inside, a renderer is a carefully engineered program based on multiple disciplines, including light physics, visual perception, mathematics, and software development.

Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image on a screen from a 3D representation stored in a scene file are handled by the graphics pipeline in a rendering device such as a GPU.
A GPU is a purpose-built device that assists a CPU in performing complex rendering calculations.
If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software must solve the rendering equation.
The rendering equation doesn't account for all lighting phenomena, but instead acts as a general lighting model for computer-generated imagery.

In the case of 3D graphics, scenes can be pre-rendered or generated in realtime.

Pre-rendering is a slow, computationally intensive process that is typically used for movie creation, where scenes can be generated ahead of time, while real-time rendering is often done for 3D video games and other applications that must dynamically create scenes.

3D hardware accelerators can improve realtime rendering performance.

==
Usage ==
When the pre-image (a wireframe sketch usually) is complete, rendering is used, which adds in bitmap textures or procedural textures, lights, bump mapping and relative position to other objects.
The result is a completed image the consumer or intended viewer sees.

For movie animations, several images (frames) must be rendered, and stitched together in a program capable of making an animation of this sort.
Most 3D image editing programs can do this.

==
Features ==
A rendered image can be understood in terms of a number of visible features.
Rendering research and development has been largely motivated by finding ways to simulate these efficiently.
Some relate directly to particular algorithms and techniques, while others are produced together.

Shading –
how the color and brightness of a surface varies with lighting
Texture-mapping –
a method of applying detail to surfaces
Bump-mapping –  a method of simulating small-scale bumpiness on surfaces
Fogging/participating medium –
how light dims when passing through non-clear atmosphere or air
Shadows –
the effect of obstructing light
Soft shadows –  varying darkness caused by partially obscured light sources
Reflection –
mirror-like or highly glossy reflection
Transparency (optics), transparency (graphic) or opacity –  sharp transmission of light through solid objects
Translucency –  highly scattered transmission of light through solid objects
Refraction –  bending of light associated with transparency
Diffraction –  bending, spreading, and interference of light passing by an object or aperture that disrupts the ray
Indirect illumination –  surfaces illuminated by light reflected off other surfaces, rather than directly from a light source (also known as global illumination)
Caustics (a form of indirect illumination) –  reflection of light off a shiny object, or focusing of light through a transparent object, to produce bright highlights on another object
Depth of field –  objects appear blurry or out of focus when too far in front of or behind the object in focus
Motion blur
–  objects appear blurry due to high-speed motion, or the motion of the camera
Non-photorealistic rendering –  rendering of scenes in an artistic style, intended to look like a painting or drawing


==
Techniques ==
Many rendering algorithms have been researched, and software used for rendering may employ a number of different techniques to obtain a final image.

Tracing every particle of light in a scene is nearly always completely impractical and would take a stupendous amount of time.
Even tracing a portion large enough to produce an image takes an inordinate amount of time if the sampling is not intelligently restricted.

Therefore, a few loose families of more-efficient light transport modelling techniques have emerged:
rasterization, including scanline rendering, geometrically projects objects in the scene to an image plane, without advanced optical effects;
ray casting considers the scene as observed from a specific point of view, calculating the observed image based only on geometry and very basic optical laws of reflection intensity, and perhaps using Monte Carlo techniques to reduce artifacts;
ray tracing is similar to ray casting, but employs more advanced optical simulation, and usually uses Monte Carlo techniques to obtain more realistic results at a speed that is often orders of magnitude faster.
The fourth type of light transport technique, radiosity is not usually implemented as a rendering technique, but instead calculates the passage of light as it leaves the light source and illuminates surfaces.
These surfaces are usually rendered to the display using one of the other three techniques.

Most advanced software combines two or more of the techniques to obtain good-enough results at reasonable cost.

Another distinction is between image order algorithms, which iterate over pixels of the image plane, and object order algorithms, which iterate over objects in the scene.
Generally object order is more efficient, as there are usually fewer objects in a scene than pixels.

===
Scanline rendering and rasterization ===
A high-level representation of an image necessarily contains elements in a different domain from pixels.

These elements are referred to as primitives.

In a schematic drawing, for instance, line segments and curves might be primitives.

In a graphical user interface, windows and buttons might be the primitives.

In rendering of 3D models, triangles and polygons in space might be primitives.

If a pixel-by-pixel (image order) approach to rendering is impractical or too slow for some task, then a primitive-by-primitive (object order) approach to rendering may prove useful.

Here, one loops through each of the primitives, determines which pixels in the image it affects, and modifies those pixels accordingly.

This is called rasterization, and is the rendering method used by all current graphics cards.

Rasterization is frequently faster than pixel-by-pixel rendering.

First, large areas of the image may be empty of primitives; rasterization will ignore these areas, but pixel-by-pixel rendering must pass through them.

Second, rasterization can improve cache coherency and reduce redundant work by taking advantage of the fact that the pixels occupied by a single primitive tend to be contiguous in the image.

For these reasons, rasterization is usually the approach of choice when interactive rendering is required; however, the pixel-by-pixel approach can often produce higher-quality images and is more versatile because it does not depend on as many assumptions about the image as rasterization.

The older form of rasterization is characterized by rendering an entire face (primitive) as a single color.

Alternatively, rasterization can be done in a more complicated manner by first rendering the vertices of a face and then rendering the pixels of that face as a blending of the vertex colors.

This version of rasterization has overtaken the old method as it allows the graphics to flow without complicated textures (a rasterized image when used face by face tends to have a very block-like effect if not covered in complex textures; the faces are not smooth because there is no gradual color change from one primitive to the next).

This newer method of rasterization utilizes the graphics card's more taxing shading functions and still achieves better performance because the simpler textures stored in memory use less space.
Sometimes designers will use one rasterization method on some faces and the other method on others based on the angle at which that face meets other joined faces, thus increasing speed and not hurting the overall effect.
===
Ray casting ===
In ray casting the geometry which has been modeled is parsed pixel by pixel, line by line, from the point of view outward, as if casting rays out from the point of view.
Where an object is intersected, the color value at the point may be evaluated using several methods.
In the simplest, the color value of the object at the point of intersection becomes the value of that pixel.
The color may be determined from a texture-map.
A more sophisticated method is to modify the colour value by an illumination factor, but without calculating the relationship to a simulated light source.
To reduce artifacts, a number of rays in slightly different directions may be averaged.

Ray casting involves calculating the "view direction" (from camera position), and incrementally following along that "ray cast" through "solid 3d objects" in the scene, while accumulating the resulting value from each point in 3D space.

This is related and similar to "ray tracing" except that the raycast is usually not "bounced" off surfaces (where the "ray tracing" indicates that it is tracing out the lights path including bounces).
"
Ray casting" implies that the light ray is following a straight path (which may include travelling through semi-transparent objects).

The ray cast is a vector that can originate from the camera or from the scene endpoint ("back to front", or "front to back").

Sometimes the final light value is derived from a "transfer function" and sometimes it's used directly.

Rough simulations of optical properties may be additionally employed: a simple calculation of the ray from the object to the point of view is made.
Another calculation is made of the angle of incidence of light rays from the light source(s), and from these as well as the specified intensities of the light sources, the value of the pixel is calculated.
Another simulation uses illumination plotted from a radiosity algorithm, or a combination of these two.
===
Ray tracing ===
Ray tracing aims to simulate the natural flow of light, interpreted as particles.
Often, ray tracing methods are utilized to approximate the solution to the rendering equation by applying Monte Carlo methods to it.
Some of the most used methods are path tracing, bidirectional path tracing, or Metropolis light transport, but also semi realistic methods are in use, like Whitted Style Ray Tracing, or hybrids.
While most implementations let light propagate on straight lines, applications exist to simulate relativistic spacetime effects.
In a final, production quality rendering of a ray traced work, multiple rays are generally shot for each pixel, and traced not just to the first object of intersection, but rather, through a number of sequential 'bounces', using the known laws of optics such as "angle of incidence equals angle of reflection" and more advanced laws that deal with refraction and surface roughness.

Once the ray either encounters a light source, or more probably once a set limiting
number of bounces has been evaluated, then the surface illumination at that final point is evaluated using techniques described above, and the changes along the way through the various bounces evaluated to estimate a value observed at the point of view.
This is all repeated for each sample, for each pixel.

In distribution ray tracing, at each point of intersection, multiple rays may be spawned.
In path tracing, however, only a single ray or none is fired at each intersection, utilizing the statistical nature of Monte Carlo experiments.

As a brute-force method, ray tracing has been too slow to consider for real-time, and until recently too slow even to consider for short films of any degree of quality, although it has been used for special effects sequences, and in advertising, where a short portion of high quality (perhaps even photorealistic) footage is required.

However, efforts at optimizing to reduce the number of calculations needed in portions of a work where detail is not high or does not depend on ray tracing features have led to a realistic possibility of wider use of ray tracing.
There is now some hardware accelerated ray tracing equipment, at least in prototype phase, and some game demos which show use of real-time software or hardware ray tracing.
==
Radiosity ==
Radiosity is a method which attempts to simulate the way in which directly illuminated surfaces act as indirect light sources that illuminate other surfaces.

This produces more realistic shading and seems to better capture the 'ambience' of an indoor scene.
A classic example is the way that shadows 'hug' the corners of rooms.

The optical basis of the simulation is that some diffused light from a given point on a given surface is reflected in a large spectrum of directions and illuminates the area around it.

The simulation technique may vary in complexity.
Many renderings have a very rough estimate of radiosity, simply illuminating an entire scene very slightly with a factor known as ambiance.
However, when advanced radiosity estimation is coupled with a high quality ray tracing algorithm, images may exhibit convincing realism, particularly for indoor scenes.

In advanced radiosity simulation, recursive, finite-element algorithms 'bounce' light back and forth between surfaces in the model, until some recursion limit is reached.
The colouring of one surface in this way influences the colouring of a neighbouring surface, and vice versa.
The resulting values of illumination throughout the model (sometimes including for empty spaces) are stored and used as additional inputs when performing calculations in a ray-casting or ray-tracing model.

Due to the iterative/recursive nature of the technique, complex objects are particularly slow to emulate.
Prior to the standardization of rapid radiosity calculation, some digital artists used a technique referred to loosely as false radiosity by darkening areas of texture maps corresponding to corners, joints and recesses, and applying them via self-illumination or diffuse mapping for scanline rendering.
Even now, advanced radiosity calculations may be reserved for calculating the ambiance of the room, from the light reflecting off walls, floor and ceiling, without examining the contribution that complex objects make to the radiosity—or complex objects may be replaced in the radiosity calculation with simpler objects of similar size and texture.

Radiosity calculations are viewpoint independent which increases the computations involved, but makes them useful for all viewpoints.
If there is little rearrangement of radiosity objects in the scene, the same radiosity data may be reused for a number of frames, making radiosity an effective way to improve on the flatness of ray casting, without seriously impacting the overall rendering time-per-frame.

Because of this, radiosity is a prime component of leading real-time rendering methods, and has been used from beginning-to-end to create a large number of well-known recent feature-length animated 3D-cartoon films.

==
Sampling and filtering ==
One problem that any rendering system must deal with, no matter which approach it takes, is the sampling problem.

Essentially, the rendering process tries to depict a continuous function from image space to colors by using a finite number of pixels.

As a consequence of the Nyquist–Shannon sampling theorem (or Kotelnikov theorem), any spatial waveform that can be displayed must consist of at least two pixels, which is proportional to image resolution.

In simpler terms, this expresses the idea that an image cannot display details, peaks or troughs in color or intensity, that are smaller than one pixel.

If a naive rendering algorithm is used without any filtering, high frequencies in the image function will cause ugly aliasing to be present in the final image.

Aliasing typically manifests itself as jaggies, or jagged edges on objects where the pixel grid is visible.

In order to remove aliasing, all rendering algorithms (if they are to produce good-looking images) must use some kind of low-pass filter on the image function to remove high frequencies, a process called antialiasing.
==
Optimization ==
Due to the large number of calculations, a work in progress is usually only rendered in detail appropriate to the portion of the work being developed at a given time, so in the initial stages of modeling, wireframe and ray casting may be used, even where the target output is ray tracing with radiosity.
It is also common to render only parts of the scene at high detail, and to remove objects that are not important to what is currently being developed.

For real-time, it is appropriate to simplify one or more common approximations, and tune to the exact parameters of the scenery in question, which is also tuned to the agreed parameters to get the most 'bang for the buck'.

==
Academic core ==
The implementation of a realistic renderer always has some basic element of physical simulation or emulation — some computation which resembles or abstracts a real physical process.

The term "physically based" indicates the use of physical models and approximations that are more general and widely accepted outside rendering.
A particular set of related techniques have gradually become established in the rendering community.

The basic concepts are moderately straightforward, but intractable to calculate; and a single elegant algorithm or approach has been elusive for more general purpose renderers.
In order to meet demands of robustness, accuracy and practicality, an implementation will be a complex combination of different techniques.

Rendering research is concerned with both the adaptation of scientific models and their efficient application.

===
The rendering equation ===
This is the key academic/theoretical concept in rendering.
It serves as the most abstract formal expression of the non-perceptual aspect of rendering.
All more complete algorithms can be seen as solutions to particular formulations of this equation.

L
          
            o
(
        x
        ,
        
          
            
              w
              →
            
          
        
        )
        =
        
          L
          
            e
(
x
        ,
        
          
            
              w
              →
            
          
        
        )
        +
∫
          
            Ω
          
        
        
          f
          
            r
          
        
        (
        x
        ,
        
          
            
              
                w
                →
              
            
          
          ′
        
        ,
        
          
            
              w
              →
            
          
        
        )
        
          L
          
            i
          
        
        (
        x
        ,
        
          
            
              
                w
                →
              
            
          
          ′
        
        )
        (
        
          
            
              
                w
→
              
            
          
          ′
⋅
        
          
            
              n
              →
            
          
        
        )
        
          d
w
→
′
{\displaystyle L_{o}(x,{\vec {w}})=L_{e}(x,{\vec {w}})+\int _{\Omega }f_{r}(x,{\vec {w}}',{\vec {w}})L_{i}(x,{\vec {w}}')({\vec {w}}'\cdot {\vec {n}})\mathrm {d} {\vec {w}}'}
Meaning:
at a particular position and direction, the outgoing light (Lo) is the sum of the emitted light (Le) and the reflected light.
The reflected light being the sum of the incoming light (Li) from all directions, multiplied by the surface reflection and incoming angle.
By connecting outward light to inward light, via an interaction point, this equation stands for the whole 'light transport' — all the movement of light — in a scene.

===
The bidirectional reflectance distribution function ===
The bidirectional reflectance distribution function (BRDF) expresses a simple model of light interaction with a surface as follows:
f
          
            r
(
        x
        ,
        
          
            
              
                w
                →
              
            
          
          ′
        
        ,
        
          
            
              w
→
            
          
        
        )
        =
        
          
            
              
                d
              
              
                L
                
                  r
(
              x
              ,
              
                
                  
                    w
→
                  
                
              
              )
L
i
(
              x
              ,
              
                
                  
                    
                      w
→
                    
                  
                
                ′
              
              )
              (
              
                
                  
                    
                      w
                      →
                    
                  
                
                ′
              
              ⋅
              
                
                  
                    n
                    →
                  
                
              
              )
              
                d
w
                      →
                    
                  
                
                ′
              
            
          
        
      
    
    {\displaystyle f_{r}(x,{\vec {w}}',{\vec {w}})={\frac {\mathrm {d} L_{r}(x,{\vec {w}})}{L_{i}(x,{\vec {w}}')({\vec {w}}'\cdot {\vec {n}})\mathrm {d} {\vec {w}}'}}}
Light interaction is often approximated by the even simpler models: diffuse reflection and specular reflection, although both can ALSO be BRDFs.

===
Geometric optics ===
Rendering is practically exclusively concerned with the particle aspect of light physics — known as geometrical optics.
Treating light, at its basic level, as particles bouncing around is a simplification, but appropriate: the wave aspects of light are negligible in most scenes, and are significantly more difficult to simulate.
Notable wave aspect phenomena include diffraction (as seen in the colours of CDs and DVDs) and polarisation (as seen in LCDs).
Both types of effect, if needed, are made by appearance-oriented adjustment of the reflection model.

===
Visual perception ===
Though it receives less attention, an understanding of human visual perception is valuable to rendering.
This is mainly because image displays and human perception have restricted ranges.
A renderer can simulate a wide range of light brightness and color, but current displays — movie screen, computer monitor, etc. —
cannot handle so much, and something must be discarded or compressed.
Human perception also has limits, and so does not need to be given large-range images to create realism.
This can help solve the problem of fitting images into displays, and, furthermore, suggest what short-cuts could be used in the rendering simulation, since certain subtleties won't be noticeable.
This related subject is tone mapping.

Mathematics used in rendering includes: linear algebra, calculus, numerical mathematics, signal processing, and Monte Carlo methods.

Rendering for movies often takes place on a network of tightly connected computers known as a render farm.

The current state of the art in 3-D image description for movie creation is the Mental Ray scene description language designed at Mental Images and RenderMan Shading Language designed at Pixar (compare with simpler 3D fileformats such as VRML or APIs such as OpenGL and DirectX tailored for 3D hardware accelerators).

Other renderers (including proprietary ones) can and are sometimes used, but most other renderers tend to miss one or more of the often needed features like good texture filtering, texture caching, programmable shaders, highend geometry types like hair, subdivision or nurbs surfaces with tesselation on demand, geometry caching, raytracing with geometry caching, high quality shadow mapping, speed or patent-free implementations.

Other highly sought features these days may include interactive photorealistic rendering (IPR) and hardware rendering/shading.

==
Chronology of important published ideas ==


==
See also ==


==
References ==


==
Further reading ==


==
External links ==
GPU Rendering Magazine, online CGI magazine about advantages of GPU rendering
SIGGRAPH
The ACMs special interest group in graphics — the largest academic and professional association and conference.

https://web.archive.org/web/20040923075327/http://www.cs.brown.edu/~tor/ List of links to (recent, as of 2004)
siggraph papers (and some others) on the web.
In 3D computer graphics, hidden-surface determination (also known as shown-surface determination, hidden-surface removal (HSR), occlusion culling (OC) or visible-surface determination (VSD)) is the process of identifying what surfaces and parts of surfaces can be seen from a particular viewing angle.
A hidden-surface determination algorithm is a solution to the visibility problem, which was one of the first major problems in the field of 3D computer graphics.
The process of hidden-surface determination is sometimes called hiding, and such an algorithm is sometimes called a hider.
When referring to line rendering it is known as hidden-line removal.
Hidden-surface determination is necessary to render a scene correctly, so that one may not view features hidden behind the model itself, allowing only the naturally viewable portion of the graphic to be visible.

==
Background ==
Hidden-surface determination is a process by which surfaces that should not be visible to the user (for example, because they lie behind opaque objects such as walls) are prevented from being rendered.
Despite advances in hardware capability, there is still a need for advanced rendering algorithms.
The responsibility of a rendering engine is to allow for large world spaces, and as the world’s size approaches infinity, the engine should not slow down but remain at a constant speed.
Optimizing this process relies on being able to ensure the deployment of as few resources as possible towards the rendering of surfaces that will not end up being displayed to the user.

There are many techniques for hidden-surface determination.
They are fundamentally an exercise in sorting and usually vary in the order in which the sort is performed and how the problem is subdivided.
Sorting large quantities of graphics primitives is usually done by divide and conquer.

==
Algorithms ==
Considering the rendering pipeline, the projection, the clipping, and the rasterization steps are handled differently by the following algorithms:

Z-buffering
During rasterization, the depth/Z value of each pixel (or sample in the case of anti-aliasing, but without loss of generality the term pixel is used) is checked against an existing depth value.
If the current pixel is behind the pixel in the Z-buffer, the pixel is rejected, otherwise, it is shaded and its depth value replaces the one in the Z-buffer.
Z-buffering supports dynamic scenes easily and is currently implemented efficiently in graphics hardware.
This is the current standard.
The cost of using Z-buffering is that it uses up to 4 bytes per pixel and that the rasterization algorithm needs to check each rasterized sample against the Z-buffer.
The Z-buffer can also suffer from artifacts due to precision errors (also known as Z-fighting).

Coverage buffers (C-buffer) and surface buffer (S-buffer)
Faster than Z-buffers and commonly used in games in the Quake
I era.
Instead of storing the Z value per pixel, they store a list of already displayed segments per line of the screen.
New polygons are then cut against already displayed segments that would hide them.
An S-buffer can display unsorted polygons, while a C-buffer requires polygons to be displayed from the nearest to the furthest.
Because the C-buffer technique does not require a pixel to be drawn more than once, the process is slightly faster.
This was commonly used with binary space partitioning (BSP) trees, which would provide sorting for the polygons.

Sorted active edge list
Used in Quake 1, this was storing a list of the edges of already displayed polygons (see scanline rendering).
Polygons are displayed from the nearest to the furthest.
New polygons are clipped against already displayed polygons' edges, creating new polygons to display then storing the additional edges.
It's much harder to implement than S/C/Z-buffers, but it scales much better with increases in resolution.

Painter's algorithm
Sorts polygons by their barycenter and draws them back to front.
This produces few artifacts when applied to scenes with polygons of similar size forming smooth meshes and back-face culling turned on.
The cost here is the sorting step and the fact that visual artifacts can occur.
This algorithm is broken by design for general scenes, as it cannot handle polygons in various common configurations, such as surfaces that intersect each other.

Binary space partitioning (BSP)
Divides a scene along planes corresponding to polygon boundaries.
The subdivision is constructed in such a way as to provide an unambiguous depth ordering from any point in the scene when the BSP tree is traversed.
The disadvantage here is that the BSP tree is created with an expensive pre-process.
This means that it is less suitable for scenes consisting of dynamic geometry.
The advantage is that the data is pre-sorted and error-free, ready for the previously mentioned algorithms.
Note that the BSP is not a solution to HSR, only an aid.

Ray tracing
Attempts to model the path of light rays to a viewpoint by tracing rays from the viewpoint into the scene.
Although not a hidden-surface removal algorithm as such, it implicitly solves the hidden-surface removal problem by finding the nearest surface along each view-ray.
Effectively this is equivalent to sorting all the geometry on a per-pixel basis.

The Warnock algorithm
Divides the screen into smaller areas and sorts triangles within these.
If there is ambiguity (i.e., polygons overlap in-depth extent within these areas), then further subdivision occurs.
At the limit, the subdivision may occur down to the pixel level.

==
Culling and visible-surface determination ==
A related area to visible-surface determination (VSD) is culling, which usually happens before VSD in a rendering pipeline.

Primitives or batches of primitives can be rejected in their entirety, which usually reduces the load on a well-designed system.

The advantage of culling early on in the pipeline is that entire objects that are invisible do not have to be fetched, transformed, rasterized, or shaded.

Here are some types of culling algorithms:


===
Viewing-frustum culling ===
The viewing frustum is a geometric representation of the volume visible to the virtual camera.
Naturally, objects outside this volume will not be visible in the final image, so they are discarded.
Often, objects lie on the boundary of the viewing frustum.
These objects are cut into pieces along this boundary in a process called clipping, and the pieces that lie outside the frustum are discarded as there is no place to draw them.

===
Back-face culling ===
With 3D objects, some of the object's surface is facing the camera, and the rest is facing away from the camera, i.e. is on the backside of the object, hindered by the front side.
If the object is completely opaque, those surfaces never need to be drawn.
They are determined by the vertex winding order: if the triangle drawn has its vertices in clockwise order on the projection plane when facing the camera, they switch into counter-clockwise order when the surface turns away from the camera.

Incidentally, this also makes the objects completely transparent when the viewpoint camera is located inside them, because then all the surfaces of the object are facing away from the camera and are culled by the renderer.
To prevent this the object must be set as double-sided (i.e. no back-face culling is done) or have separate inside surfaces.

===
Contribution culling ===
Often, objects are so far away that they do not contribute significantly to the final image.
These objects are thrown away if their screen projection is too small.

See Clipping plane.

===
Occlusion culling ===
Objects that are entirely behind other opaque objects may be culled.

This is a very popular mechanism to speed up the rendering of large scenes that have a moderate to high depth complexity.

There are several types of occlusion culling approaches:
Potentially visible set (PVS) rendering divides a scene into regions and pre-computes visibility for them.

These visibility sets are then indexed at run-time to obtain high-quality visibility sets (accounting for complex occluder interactions) quickly.

Portal rendering divides a scene into cells/sectors (rooms) and portals (doors), and computes which sectors are visible by clipping them against portals.
Hansong Zhang's dissertation "Effective Occlusion Culling for the Interactive Display of Arbitrary Models" describes an occlusion culling approach.

== Divide and conquer ==
A popular theme in the VSD literature is divide and conquer.
The Warnock algorithm pioneered dividing the screen.
Beam tracing is a ray-tracing approach that divides the visible volumes into beams.
Various screen-space subdivision approaches reducing the number of primitives considered per region, e.g. tiling, or screen-space BSP clipping.
Tiling may be used as a preprocess to other techniques.
Z-buffer hardware may typically include a coarse "hi-Z", against which primitives can be rejected early without rasterization, this is a form of occlusion culling.

Bounding volume hierarchies (BVHs) are often used to subdivide the scene's space (examples are the BSP tree, the octree and the kd-tree).

This allows visibility determination to be performed hierarchically: effectively, if a node in the tree is considered to be invisible, then all of its child nodes are also invisible, and no further processing is necessary (they can all be rejected by the renderer).
If a node is considered visible, then each of its children needs to be evaluated.

This traversal is effectively a tree walk, where invisibility/occlusion or reaching a leaf node determines whether to stop or whether to recurse respectively.

== See also ==
Clipping


== Sources ==
Hidden Surface Determination
A Characterization of Ten Hidden-Surface Algorithms (Wayback Machine copy
Intuition in the context of decision-making is defined as a “non-sequential information-processing mode.”
It is distinct from insight (a much more protracted process) and can be contrasted with the deliberative style of decision-making.
Intuition can influence judgment through either emotion or cognition, and there has been some suggestion that it may be a means of bridging the two.
Individuals use intuition and more deliberative decision-making styles interchangeably, but there has been some evidence that people tend to gravitate to one or the other style more naturally.
People in a good mood gravitate toward intuitive styles, while people in a bad mood tend to become more deliberative.
The specific ways in which intuition actually influences decisions remain poorly understood.
Snap judgments made possible by heuristics are sometimes identified as intuition.

== Definition and related terms ==
Intuitive decision-making can be described as the process by which information acquired through associated learning and stored in long-term memory is accessed unconsciously to form the basis of a judgment or decision.

This information can be transferred through affect induced by exposure to available options, or through unconscious cognition.
Intuition is based on the implicit knowledge available to the decision-maker.
For example, owning a dog as a child imbues someone with implicit knowledge about canine behavior, which may then be channeled into a decision-making process as the emotion of fear or anxiety before taking a certain kind of action around an angry dog.
Intuition is the mechanism by which this implicit knowledge is brought to the forefront of the decision-making process.
Some definitions of intuition in the context of decision-making point to the importance of recognizing cues and patterns in one's environment and then using them to improve one's problem solving.
Intuition in decision-making has been connected two assumptions: 1)
Tacit decision - previous decisions are affecting and 2)
Explicit decision - emotions are affecting.
Intuition's effect on decision-making is distinct from insight, which requires time to mature.
A month spent pondering a math problem may lead to a gradual understanding of the answer, even if one does not know where that understanding came from.
Intuition, in contrast, is a more instantaneous, immediate understanding upon first being confronted with the math problem.
Intuition is also distinct from implicit knowledge and learning, which inform intuition but are separate concepts.
Intuition is the mechanism by which implicit knowledge is made available during an instance of decision-making.

==
Channels of intuitive influence ==


===
Heuristics ===
Traditional research often points to the role of heuristics in helping people make “intuitive” decisions.

Those following the heuristics-and-biases school of thought developed by Amos Tversky and Daniel Kahneman believe that intuitive judgments are derived from an “informal and unstructured mode of reasoning” that ultimately does not include any methodical calculation.

Tversky and Kahneman identify availability, representativeness, and anchoring/adjustment as three heuristics that influence many intuitive judgments made under uncertain conditions.

The heuristics-and-biases approach looks at patterns of biased judgments to distinguish heuristics from normative reasoning processes.
Early studies supporting this approach associated each heuristic with a set of biases.
These biases were “departures from the normative rational theory” and helped identify the underlying heuristics.

Use of the availability heuristic, for example, leads to error whenever the memory retrieved is a biased recollection of actual frequency.
This can be attributed to an individual's tendency to remember dramatic cases.
Heuristic processes are quick intuitive responses to basic questions such as frequency.

===
Affect ===
Some researchers point to intuition as a purely affective phenomenon that demonstrates the ability of emotions to influence decision-making without cognitive mediation.
This supports the dual processing theory of affect and cognition, under which conscious thought is not required for emotions to be experienced, but nevertheless positive conscious thoughts towards person's will have positive emotional affects on them.

In studies comparing affect and cognition, some researchers have found that positive mood is associated with reliance on affective signals while negative mood is associated with more deliberative thought processes.
Mood is thus considered a moderator in the strategic decisions people carry out.
In a series of three studies, the authors confirmed that people in a positive mood faced with a card-based gambling task utilized intuition to perform better at higher-risk stages than people who were in a negative mood.

Other theories propose that intuition has both cognitive and affective elements, bridging the gap between these two fundamentally different kinds of human information processing.

==
Comparison to other decision-making styles ==
Intuitive decision-making can be contrasted with deliberative decision-making, which is based on cognitive factors like beliefs, arguments, and reasons, commonly referred to as one's explicit knowledge.
Intuitive decision-making is based on implicit knowledge
relayed to the conscious mind at the point of decision through affect or unconscious cognition.
Some studies also suggest that intuitive decision-making relies more on the mind's parallel processing functions, while deliberative decision-making relies more on sequential processing.

==
Prevalence of intuitive judgment and measurement of use ==
Although people use intuitive and deliberative decision-making modes interchangeably, individuals value the decisions they make more when they are allowed to make them using their preferred style.

This specific kind of regulatory fit is referred to as decisional fit.
The emotions people experience after a decision is made tend to be more pleasant when the preferred style is used, regardless of the decision outcome.
Some studies suggest that the mood with which the subject enters the decision-making process can also affect the style they choose to employ: sad people tend to be more deliberative, while people in a happy mood rely more on intuition.
The Preference for Intuition and Deliberation Scale developed by Coralie Bestch in 2004 measures propensity toward intuitiveness.
The scale defines preference for intuition as tendency to use affect (“gut-feel”) as a basis for decision-making instead of cognition.
The Myers-Briggs Type Indicator is also sometimes used.

==
Intuitive decision-making in specific environments ==
===
Management and decision-making ===
Researchers have also explored the efficacy of intuitive judgments and the debate on the function of intuition versus analysis in decisions that require specific expertise, as in management of organizations.
In this context, intuition is interpreted as an “unconscious expertise” rather than a traditionally purely heuristic response.
Research suggests that this kind of intuition is based on a “broad constellation of past experiences, knowledge, skills, perceptions and feelings.”

The efficacy of intuitive decision-making in the management environment is largely dependent on the decision context and decision maker's expertise.

The expertise-based intuition increases over time when the employee gets more experience regarding the organization worked for and by gathering domain-specific knowledge.
In this context the so-called intuition is not just series of random guesses, but rather a process of combining expertise and know-how with the employee's instincts.
Intuitions can, however be difficult to prove to be right in terms of decision-making.
It is in most situations likely, that decisions based on intuition are harder to justify than those that are based in rational analysis.
Especially in the context of business and organizational decision-making, one should be able to justify their decisions, thus making them purely intuitively is often not possible.
It is debated upon whether intuition is accurate, but evidence has been shown that under aforementioned conditions it can.
The organizations should not base their decisions on just intuitive or rational analysis.
The effective organizations need both rational and intuitive decision-making processes and combination of those.
When it comes to the decision maker him/herself, mainly two factors affect the effectiveness of intuitive decision-making.
These factors have been found to be the amount of expertise the person has and the individuals processing style.

===
Finance ===
A study of traders from the four largest investment banks in London looked at the role that emotion and expert intuition play in financial trading decisions.
This study reported on the differences between how higher and lower performing traders incorporate intuition in their decision strategy, and attributed the success of some higher performing traders to their great disposition to reflect critically about their intuitions.
This propensity to think critically about intuition and the source of those hunches served as a distinguishing factor between the higher and lower performing traders included in the study.
While successful traders were more open to this critical introspection, lower performing traders were reported to rely on their feelings alone rather than further explore the affective influences for their decisions.
Reflection on the origin of feelings by expert traders may be particularly salient given affect-as-information model, which holds that the impact of emotions on behavior is reduced or even disappears when the relevance of those emotions is explicitly called into question.
It has been noted in a research, that intuition is used as a method of decision-making in the banking industry.
Record shows that intuition is used in combination with pre-existing solution models and previous experiences.
Participants of the research also reported to analyse their intuitive decisions afterwards and possibly altering them.

=== High-risk situations ===
Traditional literature attributes the role of judgment processes in risk perception and decision-making to cognition rather than emotion.
However, more recent studies suggest a link between emotion and cognition as it relates to decision-making in high-risk environments.
Studies of decision-making in high-risk environments suggest that individuals who self-identify as intuitive decision-makers tend to make faster decisions that imply greater deviation from risk neutrality than those who prefer the deliberative style.

For example, risk-averse intuitive decision-makers will choose to not participate in a dangerous event more quickly than deliberative decision-makers, but will choose not to participate in more instances than their deliberative counterparts.

===
Strategic decisions ===
Strategic decisions are usually made by the top management in the organizations.
Usually strategic decisions also effect on the future of the organization.
Rationality has been the guideline and also justified way to make decisions because they are based on facts.
Intuition in strategic decision making is less examined and for example can be depending on a case be described as managers know-how, expertise or just a gut feeling, hunch.

== See also ==
Intuitionistic logic


== Sources ==
Social psychology is the scientific study of how the thoughts, feelings, and behaviors of individuals are influenced by the actual, imagined, and implied presence of others, 'imagined' and 'implied presences' referring to the internalized social norms that humans are influenced by even when alone.
Social psychologists typically explain human behavior as being a result of the relationship between mental state and social situation, studying the conditions under which thoughts, feelings, and behaviors occur and how these variables influence social interactions.

Social psychology has bridged the gap between psychology and sociology to an extent, but a divide still exists between the two fields.
Nevertheless, sociological approaches to psychology remain an important counterpart to conventional psychological research.
In addition to the split between psychology and sociology, there is difference in emphasis between American and European social psychologists, as the former traditionally have focused more on the individual, whereas the latter have generally paid more attention to group-level phenomena.

==
History ==
Although issues in social psychology already had been discussed in philosophy for much of human history—such as the writings of the Islamic philosopher Al-Farabi, which dealt with similar issues—the modern, scientific discipline began in the United States at the end of the 19th century.

===
19th century ===
In the 19th century, social psychologist was an emerging field from the larger field of psychology.
At the time, many psychologists were concerned with developing concrete explanations for the different aspects of human nature.
They attempted to discover concrete cause-and-effect relationships that explained social interactions.
In order to do so, they applied the scientific method to human behavior.
The first published study in the field was Norman Triplett's 1898 experiment on the phenomenon of social facilitation.
These psychological experiments later went on to form the foundation of much of 20th century social psychological findings.

===
Early 20th century ===
During the 1930s, many Gestalt psychologists, most notably Kurt Lewin, fled to the United States from Nazi Germany.
They were instrumental in developing the field as an area separate from the dominant behavioral and psychoanalytic schools of that time.
Attitudes and small group phenomena were the topics most commonly studied in this era.
During World War II, social psychologists were primarily engaged with studies of persuasion and propaganda for the U.S. military (see also psychological warfare).
Following the war, researchers became interested in a variety of social problems, including issues of gender and racial prejudice.
Most notable and contentious of these were the Milgram experiments.
During the years immediately following World War II, there were frequent collaborations between psychologists and sociologists.
The two disciplines, however, have become increasingly specialized and isolated from each other in recent years, with sociologists generally focusing on macro features whereas psychologists generally focusing on more micro features.

===
Late 20th century and modernity ==
=
In the 1960s, there was growing interest in topics such as cognitive dissonance, bystander intervention, and aggression.
By the 1970s, however, social psychology in America had reached a crisis, as heated debates emerged over issues such as ethical concerns about laboratory experimentation, whether attitude could actually predict behavior, and how much science could be done in a cultural context.
This was also a time when situationism came to challenge the relevance of self and personality in psychology.
Throughout the 1980s and 1990s, social psychology reached a more mature level, especially in regard to theory and methodology.
Now, careful ethical standards regulate research, and pluralistic and multicultural perspectives have emerged.
Modern researchers are interested in many phenomena, though attribution, social cognition, and the self-concept are perhaps the areas of greatest growth in recent years.
Social psychologists have also maintained their applied interests with contributions in the social psychology of health, education, law, and the workplace.

==
Intrapersonal phenomena ==


===
Attitudes ===
In social psychology, attitude is defined as learned, global evaluations (e.g. of people or issues) that influence thought and action.
Attitudes are basic expressions of approval and disapproval, or as Bem (1970) suggests, likes and dislikes
(e.g. enjoying chocolate ice cream, or endorsing the values of a particular political party).
Because people are influenced by other factors in any given situation, general attitudes are not always good predictors of specific behavior.
For example, a person may value the environment but may not recycle a plastic bottle on a particular day.

Research on attitudes has examined the distinction between traditional, self-reported attitudes and implicit, unconscious attitudes.
Experiments using the implicit-association test, for instance, have found that people often demonstrate implicit bias against other races, even when their explicit responses profess equal mindedness.
Likewise, one study found that in interracial interactions, explicit attitudes correlate with verbal behavior while implicit attitudes correlate with nonverbal behavior.
One hypothesis on how attitudes are formed, first proposed in 1983 by Abraham Tesser, is that strong likes and dislikes are ingrained in our genetic make-up.
Tesser speculated that individuals are disposed to hold certain strong attitudes as a result of inborn personality traits and physical, sensory, and cognitive skills.
Attitudes are also formed as a result of exposure to different experiences, environments, and through the learning process.
Numerous studies have shown that people can form strong attitudes toward neutral objects that are in some way linked to emotionally charged stimuli.
Attitudes are also involved in several other areas of the discipline, such as conformity, interpersonal attraction, social perception, and prejudice.

===
Persuasion ===
Persuasion is an active method of influencing that attempts to guide people toward the adoption of an attitude, idea, or behavior by rational or emotive means.
Persuasion relies on appeals rather than strong pressure or coercion.
The process of persuasion has been found to be influenced by numerous variables that generally fall into one of five major categories:
Communicator: includes credibility, expertise, trustworthiness, and attractiveness.

Message: includes varying degrees of reason, emotion (e.g. fear), one-sided or two sided arguments, and other types of informational content.

Audience: includes a variety of demographics, personality traits, and preferences.

Channel/medium:
includes printed word, radio, television, the internet, or face-to-face interactions.

Context: includes environment, group dynamics, and preliminary information to that of Message (category #2).Dual-process theories of persuasion (such as the elaboration likelihood model) maintain that persuasion is mediated by two separate routes: central and peripheral.
The central route of persuasion is more fact-based and results in longer-lasting change, but requires motivation to process.
The peripheral route is more superficial and results in shorter-lasting change, but does not require as much motivation to process.
An example of peripheral persuasion is a politician using a flag lapel pin, smiling, and wearing a crisp, clean shirt.
This does not require motivation to be persuasive, but should not last as long as central persuasion.
If that politician were to outline what they believe and their previous voting record, he would be centrally persuasive, resulting in longer-lasting change at the expense of greater motivation required for processing.

===
Social cognition ===
Social cognition studies how people perceive, think about, and remember information about others.
Much research rests on the assertion that people think about other people differently from non-social targets.
This assertion is supported by the social-cognitive deficits exhibited by people with Williams syndrome and autism.
Person perception is the study of how people form impressions of others.
The study of how people form beliefs about each other while interacting is interpersonal perception.

A major research topic in social cognition is attribution.
Attributions are how we explain people's behavior, either our own behavior or the behavior of others.
One element of attribution ascribes the cause of a behavior to internal and external factors.
An internal, or dispositional, attribution reasons that behavior is caused by inner traits such as personality, disposition, character, and ability.
An external, or situational, attribution reasons that behaviour is caused by situational elements such as the weather.
A second element of attribution ascribes the cause of behavior to stable and unstable factors (i.e. whether the behavior will be repeated or changed under similar circumstances).
Individuals also attribute causes of behavior to controllable and uncontrollable factors (i.e. how much control one has over the situation at hand).

Numerous biases in the attribution process have been discovered.
For instance, the fundamental attribution error is the tendency to make dispositional attributions for behavior, overestimating the influence of personality and underestimating the influence of the situational.
The actor-observer bias is a refinement of this; it is the tendency to make dispositional attributions for other people's behavior and situational attributions for our own.
The self-serving bias is the tendency to attribute dispositional causes for successes, and situational causes for failure, particularly when self-esteem is threatened.
This leads to assuming one's successes are from innate traits, and one's failures are due to situations.
Other ways people protect their self-esteem are by believing in a just world, blaming victims for their suffering, and making defensive attributions that explain our behavior in ways that defend us from feelings of vulnerability and mortality.
Researchers have found that mildly depressed individuals often lack this bias and actually have more realistic perceptions of reality as measured by the opinions of others.

==== Heuristics ====
Heuristics are cognitive shortcuts.
Instead of weighing all the evidence when making a decision, people rely on heuristics to save time and energy.
The availability heuristic occurs when people estimate the probability of an outcome based on how easy that outcome is to imagine.
As such, vivid or highly memorable possibilities will be perceived as more likely than those that are harder to picture or difficult to understand, resulting in a corresponding cognitive bias.
The representativeness heuristic is a shortcut people use to categorize something based on how similar it is to a prototype they know of.
Numerous other biases have been found by social cognition researchers.
The hindsight bias is a false memory of having predicted events, or an exaggeration of actual predictions, after becoming aware of the outcome.
The confirmation bias is a type of bias leading to the tendency to search for or interpret information in a way that confirms one's preconceptions.

====
Schemas ====
Another key concept in social cognition is the assumption that reality is too complex to easily discern.
As a result, we tend to see the world according to simplified schemas or images of reality.
Schemas are generalized mental representations that organize knowledge and guide information processing.
Schemas often operate automatically and unintentionally, and can lead to biases in perception and memory.
Schemas may induce expectations that lead us to see something that is not there.
One experiment found that people are more likely to misperceive a weapon in the hands of a black man than a white man.
This type of schema is a stereotype, a generalized set of beliefs about a particular group of people (when incorrect, an ultimate attribution error).
Stereotypes are often related to negative or preferential attitudes (prejudice) and behavior (discrimination).
Schemas for behaviors (e.g., going to a restaurant, doing laundry) are known as scripts.

===
Self-concept ===
Self-concept is the whole sum of beliefs that people have about themselves.
The self-concept is made up of cognitive aspects called self-schemas—
beliefs that people have about themselves and that guide the processing of self-referential information.
For example, an athlete at a university would have multiple selves that would process different information pertinent to each self: the student would be oneself, who would process information pertinent to a student (taking notes in class, completing a homework assignment, etc.);
the athlete would be the self who processes information about things related to being an athlete (recognizing an incoming pass, aiming a shot, etc.).
These selves are part of one's identity and the self-referential information is that which relies on the appropriate self to process and react to it.
If a self is not part of one's identity, then it is much more difficult for one to react.
For example, a civilian may not know how to handle a hostile threat as well as a trained Marine would.
The Marine contains a self that would enable him/her to process the information about the hostile threat and react accordingly, whereas a civilian may not contain that self, lessening the civilian's ability to properly assess the threat and act accordingly.

The self-concept comprises multiple self-schemas.
For example, people whose body image is a significant self-concept aspect are considered schematics with respect to weight.
In contrast, people who do not regard their weight as an important part of their lives are aschematic with respect to that attribute.
For individuals, a range of otherwise mundane events—grocery shopping, new clothes, eating out, or going to the beach—can trigger thoughts about the self.
The self is a special object of our attention.
Whether one is mentally focused on a memory, a conversation, a foul smell,
the song that is stuck in one's head, or this sentence
, consciousness is like a spotlight.
This spotlight can shine on only one object at a time, but it can switch rapidly from one object to another.
In this spotlight the self is front and center: things relating to the self have the spotlight more often.
The ABCs of self are:
Affect (i.e. emotion): How do people evaluate themselves, enhance their self-image, and maintain a secure sense of identity?

Behavior: How do people regulate their own actions and present themselves to others according to interpersonal demands?

Cognition: How do individuals become themselves, build a self-concept, and uphold a stable sense of identity?Affective forecasting
is the process of predicting how one would feel in response to future emotional events.
Studies done in 2003 by Timothy Wilson and Daniel Gilbert
have shown that people overestimate the strength of their reactions to anticipated positive and negative life events, more than they actually feel when the event does occur.
There are many theories on the perception of our own behavior.
Leon Festinger's 1954 social comparison theory is that people evaluate their own abilities and opinions by comparing themselves to others when they are uncertain of their own ability or opinions.
Daryl Bem's 1972 self-perception theory claims that when internal cues are difficult to interpret, people gain self-insight by observing their own behavior.
There is also the facial feedback hypothesis: changes in facial expression can lead to corresponding changes in emotion.
The self-concept is often divided into a cognitive component, known as the self-schema, and an evaluative component, the self-esteem.
The need to maintain a healthy self-esteem is recognized as a central human motivation.
Self-efficacy beliefs are associated with the self-schema.
These are expectations that performance of some task will be effective and successful.
Social psychologists also study such self-related processes as self-control and self-presentation.
People develop their self-concepts by various means, including introspection, feedback from others, self-perception, and social comparison.
By comparing themselves to others, people gain information about themselves, and they make inferences that are relevant to self-esteem.
Social comparisons can be either upward or downward, that is, comparisons to people who are either higher or lower in status or ability.
Downward comparisons are often made in order to elevate self-esteem.
Self-perception is a specialized form of attribution that involves making inferences about oneself after observing one's own behavior.
Psychologists have found that too many extrinsic rewards (e.g. money) tend to reduce intrinsic motivation through the self-perception process, a phenomenon known as overjustification.
People's attention is directed to the reward, and they lose interest in the task when the reward is no longer offered.
This is an important exception to reinforcement theory.

==
Interpersonal phenomena ==


===
Social influence ===
Social influence is an overarching term that denotes the persuasive effects people have on each other.
It is seen as a fundamental value in social psychology.
The study of it overlaps considerably with research into attitudes and persuasion.
The three main areas of social influence include: conformity, compliance, and obedience.
Social influence is also closely related to the study of group dynamics, as most effects of influence are strongest when they take place in social groups.

The first major area of social influence is conformity.
Conformity is defined as the tendency to act or think like other members of a group.
The identity of members within a group (i.e. status), similarity, expertise, as well as cohesion, prior commitment, and accountability to the group help to determine the level of conformity of an individual.
Individual variations among group members plays a key role in the dynamic of how willing people will be to conform.
Conformity is usually viewed as a negative tendency in American culture, but a certain amount of conformity is adaptive in some situations, as is nonconformity in other situations.

The second major area of social influence research is compliance, which refers to any change in behavior that is due to a request or suggestion from another person.
The foot-in-the-door technique is a compliance method in which the persuader requests a small favor and then follows up with requesting a larger favor, e.g., asking for the time and then asking for ten dollars.
A related trick is the bait and switch.
The third major form of social influence is obedience; this is a change in behavior that is the result of a direct order or command from another person.
Obedience as a form of compliance was dramatically highlighted by the Milgram study, wherein people were ready to administer shocks to a person in distress on a researcher's command.
An unusual kind of social influence is the self-fulfilling prophecy.
This is a prediction that, in being made, causes itself to become true.
For example, in the stock market, if it is widely believed that a crash is imminent, investors may lose confidence, sell most of their stock, and thus cause a crash.
Similarly, people may expect hostility in others and induce this hostility by their own behavior.
Psychologists have spent decades studying the power of social influence, and the way in which it manipulates people's opinions and behavior.
Specifically, social influence refers to the way in which individuals change their ideas and actions to meet the demands of a social group, received authority, social role, or a minority within a group wielding influence over the majority.

===
Group dynamics ===
A group can be defined as two or more individuals who are connected to each another by social relationships.
Groups tend to interact, influence each other, and share a common identity.
They have a number of emergent qualities that distinguish them from coincidental, temporary gatherings, which are termed social aggregates:
Norms:
Implicit rules and expectations for group members to follow (e.g. saying thank you, shaking hands).

Roles:
Implicit rules and expectations for specific members within the group (e.g. the oldest sibling, who may have additional responsibilities in the family).

Relations:
Patterns of liking within the group, and also differences in prestige or status (e.g. leaders, popular people).Temporary groups and aggregates share few or none of these features and do not qualify as true social groups.
People waiting in line to get on a bus, for example, do not constitute a group.
Groups are important not only because they offer social support, resources, and a feeling of belonging, but because they supplement an individual's self-concept.
To a large extent, humans define themselves by the group memberships which form their social identity.
The shared social identity of individuals within a group influences intergroup behavior, which denotes the way in which groups behave towards and perceive each other.
These perceptions and behaviors in turn define the social identity of individuals within the interacting groups.
The tendency to define oneself by membership in a group may lead to intergroup discrimination, which involves favorable perceptions and behaviors directed towards the in-group, but negative perceptions and behaviors directed towards the out-group.
On the other hand, such discrimination and segregation may sometimes exist partly to facilitate a diversity that strengthens society.
Intergroup discrimination leads to prejudicial stereotyping, while the processes of social facilitation and group polarization encourage extreme behaviors towards the out-group.

Groups often moderate and improve decision making, and are frequently relied upon for these benefits, such as in committees and juries.
A number of group biases, however, can interfere with effective decision making.
For example, group polarization, formerly known as the "risky shift", occurs when people polarize their views in a more extreme direction after group discussion.
More problematic is the phenomenon of groupthink, which is a collective thinking defect that is characterized by a premature consensus or an incorrect assumption of consensus, caused by members of a group failing to promote views that are not consistent with the views of other members.
Groupthink occurs in a variety of situations, including isolation of a group and the presence of a highly directive leader.
Janis offered the 1961 Bay of Pigs Invasion as a historical case of groupthink.
Groups also affect performance and productivity.
Social facilitation, for example, is a tendency to work harder and faster in the presence of others.
Social facilitation increases the dominant response's likelihood, which tends to improve performance on simple tasks and reduce it on complex tasks.
In contrast, social loafing is the tendency of individuals to slack off when working in a group.
Social loafing is common when the task is considered unimportant and individual contributions are not easy to see.
Social psychologists study group-related (collective) phenomena such as the behavior of crowds.
An important concept in this area is deindividuation, a reduced state of self-awareness that can be caused by feelings of anonymity.
Deindividuation is associated with uninhibited and sometimes dangerous behavior.
It is common in crowds and mobs, but it can also be caused by a disguise, a uniform, alcohol, dark environments, or online anonymity.
===
Interpersonal attraction ===
A major area of study of people's relations to each other is interpersonal attraction, which refers to all forces that lead people to like each other, establish relationships, and (in some cases) fall in love.
Several general principles of attraction have been discovered by social psychologists.
One of the most important factors in interpersonal attraction is how similar two particular people are.
The more similar two people are in general attitudes, backgrounds, environments, worldviews, and other traits, the more likely they will be attracted to each other.
Physical attractiveness is an important element of romantic relationships, particularly in the early stages characterized by high levels of passion.
Later on, similarity and other compatibility factors become more important, and the type of love people experience shifts from passionate to companionate.
In 1986, Robert Sternberg suggested that there are actually three components of love: intimacy, passion, and commitment.
When two (or more) people experience all three, they are said to be in a state of consummate love.

According to social exchange theory, relationships are based on rational choice and cost-benefit analysis.
A person may leave a relationship if their partner's "costs" begin to outweigh their benefits, especially if there are good alternatives available.
This theory is similar to the minimax principle proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games).
With time, long-term relationships tend to become communal rather than simply based on exchange.

==
Research ==


===
Methods ===
Social psychology is an empirical science that attempts to answer questions about human behavior by testing hypotheses, both in the laboratory and in the field.
Careful attention to research design, sampling, and statistical analysis is important; results are published in peer-reviewed journals such as the Journal of Experimental Social Psychology, Personality and Social Psychology Bulletin and the Journal of Personality and Social Psychology.
Social psychology studies also appear in general science journals such as Psychological Science and Science.

Experimental methods involve the researcher altering a variable in the environment and measuring the effect on another variable.
An example would be allowing two groups of children to play violent or nonviolent videogames and then observing their subsequent level of aggression during the free-play period.
A valid experiment is controlled and uses random assignment.

Correlational methods examine the statistical association between two naturally occurring variables.
For example, one could correlate the number of violent television shows children watch at home with the number of violent incidents the children participate in at school.
Note that this study would not prove that violent TV causes aggression in children: it is quite possible that aggressive children choose to watch more violent TV.

Observational methods are purely descriptive and include naturalistic observation, contrived observation, participant observation, and archival analysis.
These are less common in social psychology but are sometimes used when first investigating a phenomenon.
An example would be to unobtrusively observe children on a playground (with a videocamera, perhaps) and record the number and types of aggressive actions displayed.

Whenever possible, social psychologists rely on controlled experimentation, which requires the manipulation of one or more independent variables in order to examine the effect on a dependent variable.
Experiments are useful in social psychology because they are high in internal validity, meaning that they are free from the influence of confounding or extraneous variables, and so are more likely to accurately indicate a causal relationship.
However, the small samples used in controlled experiments are typically low in external validity, or the degree to which the results can be generalized to the larger population.
There is usually a trade-off between experimental control (internal validity) and being able to generalize to the population (external validity).

Because it is usually impossible to test everyone, research tends to be conducted on a sample of persons from the wider population.
Social psychologists frequently use survey research when they are interested in results that are high in external validity.
Surveys use various forms of random sampling to obtain a sample of respondents that is representative of a population.
This type of research is usually descriptive or correlational because there is no experimental control over variables.
Some psychologists have raised concerns for social psychological research relying too heavily on studies conducted on university undergraduates in academic settings, or participants from crowdsourcing labor markets such as Amazon Mechanical Turk.
In a 1986 study by David O. Sears, over 70% of experiments used North American undergraduates as subjects, a subset of the population that is unrepresentative of the population as a whole.
Regardless of which method has been chosen, the significance of the results is reviewed before accepting them in evaluating an underlying hypothesis.
There are two different types of tests that social psychologists use to review their results.
Statistics and probability testing define what constitutes a significant finding, which can be as low as 5% or less, that is unlikely due to chance.
Replications testing is important in ensuring that the results are valid and not due to chance.
False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are a hazard in the field.

===
Famous experiments ===


====
Asch conformity experiments ====
The Asch conformity experiments demonstrated the power of the impulse to conform within small groups, by the use of a line-length estimation task that was designed to be easy to assess but where deliberately wrong answers were given by at least some, oftentimes most, of the other participants.
In well over a third of the trials, participants conformed to the majority, even though the majority judgment was clearly wrong.
Seventy-five percent of the participants conformed at least once during the experiment.
Additional manipulations of the experiment showed that participant conformity decreased when at least one other individual failed to conform but increased when the individual began conforming or withdrew from the experiment.
Also, participant conformity increased substantially as the number of "incorrect" individuals increased from one to three, and remained high as the incorrect majority grew.
Participants with three other, incorrect participants made mistakes 31.8% of the time, while those with one or two incorrect participants made mistakes only 3.6% and 13.6% of the time, respectively.

==== Festinger (cognitive dissonance) ====
In Leon Festinger's cognitive dissonance experiment, after being divided into two groups participants were asked to perform a boring task and later asked to dishonestly give their opinion of the task, afterwards being rewarded according to two different pay scales.
At the study's end, some participants were paid $1 to say that they enjoyed the task and another group of participants was paid $20 to tell the same lie.
The first group ($1) later reported liking the task better than the second group ($20).
Festinger's explanation was that for people in the first group being paid only $1 is not sufficient incentive for lying and those who were paid $1 experienced dissonance.
They could only overcome that dissonance by justifying their lies by changing their previously unfavorable attitudes about the task.
Being paid $20 provides a reason for doing the boring task resulting in no dissonance.

====
Milgram experiment ====
The Milgram experiment was designed to study how far people would go in obeying an authority figure.
Following the events of The Holocaust in World War II, the experiment showed that normal American citizens were capable of following orders even when they believed they were causing an innocent person to suffer.

====
Stanford prison experiment ====
Philip Zimbardo's Stanford prison study, a simulated exercise involving students playing at being prison guards and inmates, ostensibly showed how far people would go in such role playing.
In just a few days, the guards became brutal and cruel, and the prisoners became miserable and compliant.
This was initially argued to be an important demonstration of the power of the immediate social situation and its capacity to overwhelm normal personality traits.
Subsequent research has contested the initial conclusions of the study.
For example, it has been pointed out that participant self-selection may have affected the participants' behavior, and that the participants' personalities influenced their reactions in a variety of ways, including how long they chose to remain in the study.
The 2002 BBC prison study, designed to replicate the conditions in the Stanford study, produced conclusions that were drastically different from the initial findings.

====
Others ====
Muzafer Sherif's robbers' cave study divided boys into two competing groups to explore how much hostility and aggression would emerge.
Sherif's explanation of the results became known as realistic group conflict theory, because the intergroup conflict was induced through competition for resources.
Inducing cooperation and superordinate goals later reversed this effect.

Albert Bandura's Bobo doll experiment demonstrated how aggression is learned by imitation.
This set of studies fueled debates regarding media violence, a topic that continues to be debated among scholars.

===
Ethics ===
The goal of social psychology is to understand cognition and behavior as they naturally occur in a social context, but the very act of observing people can influence and alter their behavior.
For this reason, many social psychology experiments utilize deception to conceal or distort certain aspects of the study.
Deception may include false cover stories, false participants (known as confederates or stooges),
false feedback given to the participants, and so on.
The practice of deception has been challenged by psychologists who maintain that deception under any circumstances is unethical and that other research strategies (e.g., role-playing) should be used instead.
Unfortunately, research has shown that role-playing studies do not produce the same results as deception studies, and this has cast doubt on their validity.
In addition to deception, experimenters have at times put people into potentially uncomfortable or embarrassing situations
(e.g., the Milgram experiment and Stanford prison experiment), and this has also been criticized for ethical reasons.

To protect the rights and well-being of research participants, and at the same time discover meaningful results and insights into human behavior, virtually all social psychology research must pass an ethical review.
At most colleges and universities, this is conducted by an ethics committee or Institutional Review Board, which examines the proposed research to make sure that no harm is likely to come to the participants, and that the study's benefits outweigh any possible risks or discomforts to people taking part.

Furthermore, a process of informed consent is often used to make sure that volunteers know what will asked of them in the experiment and understand that they are allowed to quit the experiment at any time.
A debriefing is typically done at the experiment's conclusion in order to reveal any deceptions used and generally make sure that the participants are unharmed by the procedures.
Today, most research in social psychology involves no more risk of harm than can be expected from routine psychological testing or normal daily activities.

===
Adolescents ===
Social psychology studies what plays key roles in a child's development.
During this time, teens are faced with many issues and decisions that can impact their social development.
They are faced with self-esteem issues, peer pressure, drugs, alcohol, tobacco, sex, and social media.
Psychologists today are not fully aware of the effect of social media.
Social media is worldwide, so one can be influenced by something they will never encounter in real life.
In 2019, social media became the single most important activity in adolescents' and even some older adults' lives.

===
Replication crisis ===
Many social psychological research findings have proven difficult to replicate, leading some to argue that social psychology is undergoing a replication crisis.
Replication failures are not unique to social psychology and are found in all fields of science.
Some factors have been identified in social psychological research that has led the field to undergo its current crisis.

Firstly, questionable research practices have been identified as common.
Such practices, while not necessarily intentionally fraudulent, involve converting undesired statistical outcomes into desired outcomes via the manipulation of statistical analyses, sample sizes, or data management systems, typically to convert non-significant findings into significant ones.
Some studies have suggested that at least mild versions of these practices are prevalent.
One of the criticisms of Daryl Bem in the feeling the future controversy is that the evidence for precognition in the study could be attributed to questionable practices.

Secondly, some social psychologists have published fraudulent research that has entered into mainstream academia, most notably the admitted data fabrication by Diederik Stapel as well as allegations against others.
Fraudulent research is not the main contributor to the replication crisis.
Several effects in social psychology have been found to be difficult to replicate even before the current replication crisis.
For example, the scientific journal Judgment and Decision Making has published several studies over the years that fail to provide support for the unconscious thought theory.
Replications appear particularly difficult when research trials are pre-registered and conducted by research groups not highly invested in the theory under questioning.

These three elements together have resulted in renewed attention to replication supported by Daniel Kahneman.
Scrutiny of many effects have shown that several core beliefs are hard to replicate.
A 2014 special edition of Social Psychology focused on replication studies, and a number of previously held beliefs were found to be difficult to replicate.
Likewise, a 2012 special edition of Perspectives on Psychological Science focused on issues ranging from publication bias to null-aversion that contribute to the replication crisis in psychology.
It is important to note that this replication crisis does not mean that social psychology is unscientific.
Rather, this reexamination is
a healthy if sometimes acrimonious part of the scientific process in which old ideas or those that cannot withstand careful scrutiny are pruned.
The consequence is that some areas of social psychology once considered solid, such as social priming, have come under increased scrutiny due to failure to replicate findings.

===
Academic journals ===


==
See also ==
== Notes ==


==
References ==


==
External links ==
Social Psychology Network
Introduction to Social Psychology
Social Psychology — basics
Social psychology  on PLOS —
subject area page
Social psychology on All About Psychology — information and resources
page
What is Social Psychology?
on YouTube
This glossary of computer hardware terms is a list of definitions of terms and concepts related to computer hardware, i.e. the physical and structural components of computers, architectural issues, and peripheral devices.

==
A ==
Accelerated Graphics Port (AGP)
A high-speed point-to-point channel for attaching a video card to a computer's motherboard, primarily to assist in the acceleration of 3D computer graphics.

accelerator
A microprocessor, ASIC, or expansion card designed to offload a specific task from the CPU, often containing fixed function hardware.
A common example is a graphics processing unit.

accumulator
A register in a CPU in which intermediate arithmetic and logic results are stored.

address
The unique integer number that specifies a memory location in an address space.

address space
A mapping of logical addresses into physical memory or other memory-mapped devices.

Advanced Technology eXtended (ATX)
A motherboard form factor specification developed by Intel in 1995 to improve on previous DE factor standards like the AT form factor.

AI accelerator
An accelerator aimed at running artificial neural networks or other machine learning and machine vision algorithms (either training or deployment), e.g. Movidius Myriad 2, TrueNorth, tensor processing unit, etc.

Advanced Configuration and Power Interface
An open standard for operating systems to discover, configure, manage, and monitor status of the hardware.

==
B ==
Blu-ray Disc (BD)
An optical disc storage medium designed to supersede the DVD format.

bus
A subsystem that transfers data between computer components inside a computer or between computers.

==
C ==
cache
A small, fast local memory that transparently buffers access to a larger but slower or more distant/higher latency memory  or storage device, organised into cache lines.
Automatically translates accesses to the underlying resources address space to locations in the cache.

cache coherency
The process of keeping data in multiple caches synchronised in a multiprocessor shared memory system, also required when DMA modifies the underlying memory.

cache eviction
Freeing up data from within a cache to make room for new cache entries to be allocated; controlled by a cache replacement policy.
Caused by a cache miss whilst a cache is already full.

cache hit
Finding data in a local cache, preventing the need to search for that resource in a more distant location (or to repeat a calculation).

cache line
A small block of memory within a cache; the granularity of allocation, refills, eviction; typically 32–128 bytes in size.

cache miss
Not finding data in a local cache, requiring use of the cache policy to allocate and fill this data, and possibly performing evicting other data to make room.

cache thrashing
A pathological situation where access in a cache cause cyclical cache misses by evicting data that is needed in the near future.

cache ways
The number of potential cache lines in an associative cache that specific physical addresses can be mapped to; higher values reduce potential collisions in allocation.

cache-only memory architecture (COMA)
A multiprocessor memory architecture where an address space is dynamically shifted between processor nodes based on demand.

card reader
Any data input device that reads data from a card-shaped storage medium.

channel
I/O
A generic term that refers to a high-performance input/output (I/O) architecture that is implemented in various forms on a number of computer architectures, especially on mainframe computers.

chipset
Also chip set.

A group of integrated circuits, or chips, that are designed to work together.
They are usually marketed as a single product.

Compact Disc-Recordable (CD-R)
A variation of the optical compact disc which can be written to once.

Compact Disc-ReWritable (CD-RW)
A variation of the optical compact disc which can be written to many times.

Compact Disc Read-Only
Memory (CD-ROM)
A pre-pressed compact disc which contains data or music playback and which cannot be written to.

computer case
Also chassis, cabinet, box, tower, enclosure, housing, system unit, or simply case.

The enclosure that contains most of the components of a computer, usually excluding the display, keyboard, mouse, and various other peripherals.

computer fan
An active cooling system forcing airflow inside or around a computer case using a fan to cause air cooling.

computer form factor
The name used to denote the dimensions, power supply type, location of mounting holes, number of ports on the back panel, etc.

computer monitor
An electronic visual display for computers.
A monitor usually comprises the display device, circuitry, casing, and power supply.
The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) or a flat panel LED display, whereas older monitors used a cathode ray tube (CRT).

control store
The memory that stores the microcode of a CPU.

Conventional Peripheral Component Interconnect (Conventional PCI)
Also simply PCI.

A computer bus for attaching hardware devices in a computer.

core
The portion of the CPU which actually performs arithmetic and logical operations; many CPUs have multiple cores (e.g. "a quad-core processor").

core memory
In modern usage, a synonym for main memory, dating back from the pre-semiconductor-chip times when the dominant main memory technology was magnetic core memory.

Central Processing Unit (CPU)
The portion of a computer system that executes the instructions of a computer program.

== D ==
data cache (D-cache)
A cache in a CPU or GPU servicing data load and store requests, mirroring main memory (or VRAM for a GPU).

data storage
A technology consisting of computer components and recording media used to retain digital data.
It is a core function and fundamental component of computers.

device memory
local memory associated with a hardware device such as a graphics processing unit or OpenCL compute device, distinct from main memory.

Digital Video Disc (DVD)
Also Digital Versatile Disc.

An optical compact disc - of the same dimensions as compact discs (CDs), but store more than six times as much data.

Digital Visual Interface (DVI)
A video display interface developed by the Digital Display Working Group (DDWG).
The digital interface is used to connect a video source to a display device, such as a computer monitor.

Direct Access Storage Device (DASD)
A mainframe terminology introduced by IBM denoting secondary storage with random access, typically (arrays of) hard disk drives.

direct mapped cache
A cache where each physical address may only be mapped to one cache line, indexed using the low bits of the address.
Simple but highly prone to allocation conflicts.

direct memory access (DMA)
The ability of a hardware device such as a disk drive or network interface to access main memory without intervention from the CPU,  provided by one or more DMA channels in a system.

DisplayPort
A digital display interface developed by the Video Electronics Standards Association (VESA).
The interface is primarily used to connect a video source to a display device such as a computer monitor, though it can also be used to transmit audio, USB, and other forms of data.

drive bay
A standard-sized area within a computer case for adding hardware (hard drives, CD drives, etc.)
to a computer.

dual in-line memory module (DIMM)
A series of dynamic random-access memory integrated circuits.
These modules are mounted on a printed circuit board and designed for use in personal computers, workstations and servers.
Contrast SIMM.

dual issue
A superscalar pipeline capable of executing two instructions simultaneously.

dynamic random-access memory (DRAM)
A type of random-access memory that stores each bit of data in a separate capacitor within an integrated circuit and which must be periodically refreshed to retain the stored data.

== E ==
expansion bus
A computer bus which moves information between the internal hardware of a computer system (including the CPU and RAM) and peripheral devices.
It is a collection of wires and protocols that allows for the expansion of a computer.

expansion card
A printed circuit board that can be inserted into an electrical connector or expansion slot on a computer motherboard, backplane, or riser card to add functionality to a computer system via an expansion bus.

== F ==
firewall
Any hardware device or software program designed to protect a computer from viruses, trojans, malware, etc.

firmware
Fixed programs and data that internally control various electronic devices.

flash memory
A type of non-volatile computer storage chip that can be electrically erased and reprogrammed.

floppy disk
A data storage medium that is composed of a disk of thin, flexible ("floppy") magnetic storage medium encased in a square or rectangular plastic shell.

floppy disk drive
A device for reading floppy disks.

floppy-disk controller
free and open-source graphics device driver


== G ==
graphics hardware
Graphics Processing Unit (GPU)


==
H ==
hard disk drive (HDD)
Any non-volatile storage device that stores data on rapidly rotating rigid (i.e. hard) platters with magnetic surfaces.

hardware
The physical components of a computer system.

Harvard architecture
A memory architecture where program machine code and data are held in separate memories, more commonly seen in microcontrollers and digital signal processors.

High-Definition Multimedia Interface (HDMI)
A compact interface for transferring encrypted uncompressed digital audio and video data to a device such as a computer monitor, video projector or digital television.

==
I ==
input device
Any peripheral equipment used to provide data and control signals to an information processing system.

input/output (I/O)
The communication between an information processing system (such as a computer), and the outside world.

Input/Output Operations Per Second (IOPS)
A common performance measurement used to benchmark computer storage devices like hard disk drives.

instruction
A group of several bits in a computer program that contains an operation code and usually one or more memory addresses.

instruction cache
I-cache
A cache in a CPU or GPU servicing instruction fetch requests for program code (or shaders for a GPU), possibly implementing modified Harvard architecture if program machine code is stored in the same address space and physical memory as data.

instruction fetch
A stage in a pipeline that loads the next instruction referred to by the program counter.

integrated circuit
Also chip.

A miniaturised electronic circuit that has been manufactured in the surface of a thin substrate of semiconductor material.

==
J ==
jump drive
Another name for a USB flash drive.

==
K ==
keyboard
An input device, partially modeled after the typewriter keyboard, which uses an arrangement of buttons or keys to act as mechanical levers or electronic switches.

==
L ==
load/store instructions
instructions used to transfer data between memory and processor registers.

load-store architecture
An instruction set architecture where arithmetic/logic instructions may only be performed between processor registers, relying on separate load/store instructions for all data transfers.

local memory
memory associated closely with a processing element, e.g. a cache, scratchpad, the memory connected to one processor node in a NUMA or COMA system, or device memory (such as VRAM) in an accelerator.

== M ==
magneto-optical drive

mainframe
An especially powerful computer used mainly by large organizations for bulk data processing such as census, industry and consumer statistics, enterprise resource planning, and financial transaction processing.

main memory
The largest random-access memory in a memory hierarchy (before offline storage) in a computer system; i.e. distinct from caches or scratchpads; usually consists of DRAM.

mask ROM
A type of read-only memory (ROM) whose contents are programmed by the integrated circuit manufacturer.

memory
Devices that are used to store data or programs on a temporary or permanent basis for use in an electronic digital computer.

memory access pattern
The pattern with which software or some other system (such as an accelerator or DMA channel) accesses memory, affecting locality of reference and parallelism.

memory address
The address of a location in a memory or other address space.

memory architecture
A memory architecture in a computer system, e.g. NUMA, uniform memory access, COMA, etc.

memory card

mini-VGA
Small connectors used on some laptops and other systems in place of the standard VGA connector.

microcode
A layer of hardware-level instructions involved in the implementation of higher level machine code instructions in many computers and other processors.

modem

modified Harvard architecture
A variation of Harvard architecture used for most CPUs with separate non-coherent instruction and data caches
(assuming that code is immutable), but still mirroring the same main memory address space, and possibly sharing higher levels of the same cache hierarchy.

monitor
An electronic visual display for computers.

motherboard
The central printed circuit board (PCB) in many modern computers which holds many of the crucial components of the system, usually while also providing connection space for peripherals.

mouse
A pointing device that functions by detecting two-dimensional motion relative to its supporting surface; motion is usually mapped to a cursor in screen space; typically used to control a graphical user interface on a desktop computer or for CAD, etc.

==
N ==
network
A collection of computers and other devices connected by communications channels, e.g. by Ethernet or wireless networking.

network interface controller
Also LAN card or network card.

network on a chip (NOC)
A computer network on a single semiconductor chip, connecting processing elements, fixed function units, or even memories and caches.
Increasingly common in system on a chip designs.

non-uniform memory access (NUMA)

non-volatile memory
memory that can retain the stored data even when not powered, as opposed to volatile memory.

non-volatile random-access memory
Random-access memory (RAM) that retains its data when power is turned off.
==
O ==
operating system
The set of software that manages computer hardware resources and provides common services for computer programs, typically loaded by the BIOS on booting.

operation code
Several bits in a computer program instruction that specify which operation to perform.

optical disc drive
A type of disk drive that uses laser light or electromagnetic waves near the light spectrum as part of the process of reading or writing data to or from optical discs.

==
P ==
pen drive
Another name for a USB flash drive.

pentest
Another name for a penetration test.

peripheral
Any device attached to a computer but not part of it.

personal computer (PC)
Any general-purpose computer whose size, capabilities, and original sales price make it useful for individuals, and which is intended to be operated directly by an end user, with no intervening computer operator.

power supply
A unit of the computer that converts mains AC to low-voltage regulated DC for the power of all the computer components.

power supply unit (PSU)
Converts mains AC to low-voltage regulated DC power for the internal components of a computer.
Modern personal computers universally use switched-mode power supplies.
Some power supplies have a manual switch for selecting input voltage, while others automatically adapt to the mains voltage.

prefetch
The process of pre-loading instructions or data into a cache ahead of time, either under manual control via prefetch instructions or automatically by a prefetch unit which may use runtime heuristics to predict the future memory access pattern.

prefetching
The pre-loading of instructions or data before either is needed by dedicated cache control instructions or predictive hardware, to mitigate latency.

printer
A peripheral which produces a text or graphics of documents stored in electronic form, usually on physical print media such as paper or transparencies.

process node
Refers to a level of semiconductor manufacturing technology, one of several successive transistor shrinks.

processing element
An electronic circuit (either a microprocessor or an internal component of one) that may function autonomously or under external control, performing arithmetic and logic operations on data, possibly containing local memory, and possibly connected to other processing elements via a network, network on a chip, or cache hierarchy.

processor node
A processor in a multiprocessor system or cluster, connected by dedicated communication channels or a network.

programmable read-only memory (PROM)
A type of non-volatile memory chip that may be programmed after the device is constructed.

programmer
Any electronic equipment that arranges written software to configure programmable non-volatile integrated circuits (called programmable devices) such as  EPROMs, EEPROMs, Flashes, eMMC, MRAM, FRAM, NV RAM, PALs, FPGAs or programmable logic circuits.

PCI Express (PCIe)
An expansion bus standard designed to replace the older PCI, PCI-X, and AGP bus standards.

PCI-eXtended (PCI-X)
An expansion bus and expansion card standard that enhances the 32-bit PCI Local Bus for higher bandwidth demanded by servers.

== R ==
Redundant Array of Independent Disks (RAID)
Any of various data storage schemes that can divide and replicate data across multiple hard disk drives in order to increase reliability, allow faster access, or both.

random-access memory (RAM)
A type of computer data storage that allows data items to be accessed (read or written) in almost the same amount of time irrespective of the physical location of data inside the memory.
RAM contains multiplexing and demultiplexing circuitry to connect the data lines to the addressed storage for reading or writing the entry.
Usually more than one bit of storage is accessed by the same address, and RAM devices often have multiple data lines and are said to be '8-bit' or '16-bit' etc.
devices.
In today's technology, random-access memory takes the form of integrated circuits.

read-only memory (ROM)
A type of memory chip that retains its data when its power supply is switched off.

==
S ==
server
A computer which may be used to provide services to clients.

software
Any computer program or other kind of information that can be read and/or written by a computer.

single in-line memory module (SIMM)
A type of memory module containing random-access memory used in computers from the early 1980s to the late 1990s.
Contrast DIMM.

solid-state drive
Also solid-state disk or electronic disk.

Any data storage device that uses integrated circuit assemblies as memory to store data persistently.
Though they are sometimes referred to as solid-state disks, these devices contain neither an actual disk nor a drive motor to spin a disk.

static random-access memory (SRAM)
A type of semiconductor memory that uses bistable latching circuitry to store each bit.
The term static differentiates it from DRAM, which must be periodically refreshed.

sound card
Also audio card.

An internal expansion card that facilitates economical input and output of audio signals to and from a computer under control of computer programs.

storage device

synchronous dynamic random-access memory (SDRAM)
A type of dynamic random access memory that is synchronized with the system bus.

SuperDisk
A high-speed, high-capacity alternative to the 90 mm (3.5 in), 1.44 MB floppy disk.
The SuperDisk hardware was created by 3M's storage products group Imation in 1997.

Sata
Also Serial ATA (SATA, abbreviated from Serial AT Attachment)
A computer bus interface that connects host bus adapters to mass storage devices such as hard disk drives, optical drives, and solid-state drives.

==
T ==
tape drive
A peripheral storage device that allows only sequential access, typically using magnetic tape.

terminal
An electronic or electromechanical hardware device that is used for entering data into, and displaying data from, a computer or a computing system.

trackpad
Also touchpad.

A pointing device consisting of specialized surface that can translate the motion and position of a user's fingers or a stylus to a relative position on a screen.

TV tuner card


==
U ==
Universal Serial Bus (USB)
A specification to establish communication between devices and a host controller (usually a personal computer).

uop cache
A cache of decoded micro-operations in a CISC processor (e.g x86).

USB flash drive
A flash memory device integrated with a USB interface.
USB flash drives are typically removable and rewritable.

==
V ==
video card
Also graphics card.

An expansion card which generates a feed of output images to a display (such as a computer monitor).

Video Graphics Array (VGA)
The last graphical standard introduced by IBM to which the majority of PC clone manufacturers conformed.

volatile memory
Memory that requires power to maintain the stored information, as opposed to non-volatile memory.
==
W ==
webcam
A video camera that feeds its images in real time to a computer or computer network, often via USB, Ethernet, or Wi-Fi.

write back cache
A cache where store operations are buffered in cache lines, only reaching main memory when the entire cache line is evicted.

write through cache
A cache where store operations are immediately written to the underlying main memory.

working set
The set of data used by a processor during a certain time interval, which should ideally fit into a CPU cache for optimum performance.

==
Z ==
zip drive
The Zip drive is a removable floppy disk storage system that was introduced by Iomega in late 1994.
Considered medium-to-high-capacity at the time of its release, Zip disks were originally launched with capacities of 100 MB.

== See also ==
List of computer term etymologies
Glossary of backup terms
Glossary of computer graphics
Glossary of computer science
Glossary of computer software termsGlossary of energy efficient hardware/software
Glossary of Internet-related terminology
Glossary of reconfigurable computing


==
References ==


==
External links ==
Dictionary: JESD88 | JEDEC
Recency bias is a cognitive bias that favors recent events over historic ones.

A memory bias, recency bias gives "greater importance to the most recent event", such as the final lawyer's closing argument a jury hears before being dismissed to deliberate.

Recency Bias should not be confused with anchoring or confirmation bias.

It commonly appears in employee evaluations, as a distortion in favor of recently completed activities or recollections, and can be reinforced or offset by the Halo effect.
Recency bias can skew investors into not accurately evaluating economic cycles, causing them to continue to remain invested in a bull market even when they should grow cautious of its potential continuation, and refrain from buying assets in a bear market because they remain pessimistic about its prospects of recovery.

When it comes to investing, recency bias often manifests in terms of direction or momentum.
It convinces us that a rising market or individual stock will continue to appreciate, or that a declining market or stock is likely to keep falling.
This bias often leads us to make emotionally charged choices—decisions that could erode our earning potential by tempting us to hold a stock for too long or pull out too soon.

Lists of superlatives such as "Top 10 Superbowls", Greatest of All Time (G.O.A.T.), and sports awards (such as MVP trophies, Rookie of the Year, etc.)
all are prone to distortion due to recency bias.

Sports betting is also impacted by recency bias.
Recency bias is related to the serial-position effect known as the recency effect.

It is not to be confused with recency illusion, the belief or impression that a word or language usage is of recent origin when in reality it is long-established.

== See also ==


==
References ==


==
Further reading ==
Liebermann, David A. Learning and memory: An integrative approach.
Belmont, CA: Thomson/Wadsworth, 2004, ISBN 978-0-534-61974-9.
Computers are social actors (CASA) is a paradigm which states that humans mindlessly apply the same social heuristics used for human interactions to computers because they call to mind similar social attributes as humans.

==
History and context ==
Clifford Nass and Youngme Moon's article, "Machines and Mindlessness:
Social Responses to Computers", published in 2000, is the origin for CASA.
It states that CASA is the concept that people mindlessly apply social rules and expectations to computers, even though they know that these machines do not have feelings, intentions or human motivations.

In their 2000 article, Nass and Moon attribute their observation of anthropocentric reactions to computers and previous research on mindlessness as factors that lead them to study the phenomenon of computers as social actors.
Specifically, they observed consistent anthropocentric treatment of computers by individuals in natural and lab settings, even though these individuals agreed that computers are not human and shouldn't be treated as such.

Additionally, Nass and Moon found a similarity between this behavior and research by Harvard psychology professor Ellen Langer on mindlessness.
Langer states that mindlessness is when a specific context triggers an individual to rely on categories, associations, and habits of thought from the past with little to no conscious awareness.
When these contexts are triggered, the individual becomes oblivious to novel or alternative aspects of the situation.
In this respect, mindlessness is similar to habits and routines, but different in that with only one exposure to information, a person will create a cognitive commitment to the information and freeze its potential meaning.
With mindlessness, alternative meanings or uses of the information become unavailable for active cognitive use.
Social attributes that computers have which are similar to humans include:

Words for output
Interactivity (the computer 'responds' when a button is touched)
Ability to perform traditional human tasksAccording to CASA, the above attributes trigger scripts for human-human interaction, which leads an individual to ignore cues revealing the asocial nature of a computer.
Although individuals using computers exhibit a mindless social response to the computer, individuals who are sensitive to the situation can observe the inappropriateness of the cued social behaviors.
CASA has been extended to include robots and AI.
However, recently, there have been challenges to the CASA paradigm.

To account for the advances in technology, MASA has been forwarded as a significant extension of CASA.

==
Attributes ==
Cued social behaviors observed in research settings include some of the following:
Gender stereotyping:
When voice outputs are used on computers, this triggers gender stereotype scripts, expectations, and attributions from individuals.
For example, a 1997 study revealed that female-voiced tutor computers were rated as more informative about love and relationships than male-voiced computers, whereas male-voiced computers were more proficient in technical subjects than female-voiced computers.

Reciprocity: When a computer provides help, favours, or benefits, this triggers the mindless response of the participant feeling obliged to 'help' the computer.
For example, an experiment in 1997 found that when a specific computer 'helped' a person, that person was more likely to do more 'work' for that computer.

Specialist versus generalist: When a technology is labeled as 'specialist', this triggers a mindless response by influencing people's perceptions of the content the labeled technology presents.
For example, a 2000 study revealed when people watched a television labeled 'News Television', they thought the news segments on that TV were higher in quality, had more information, and were more interesting than people who saw the identical information on a TV labeled 'News and Entertainment Television'.

Personality: When a computer user mindlessly creates a personality for a computer based on verbal or paraverbal cues in the interface.
For example, research from 1996 and 2001 found people with dominant personalities preferred computers that also had a 'dominant personality'; that is, the computer used strong, assertive language during tasks.

==
Academic research ==
Three research articles have represented some of the advances in the field of CASA.
Specifically, researchers in this field are looking at how novel variables, manipulations, and new computer software influence mindlessness.

A 2010 article, "Cognitive load on social response to computers" by E.J. Lee discussed research on how human likeness of a computer interface, individuals' rationality, and cognitive load moderate the extent to which people apply social attributes to computers.
The research revealed that participants were more socially attracted to a computer that flattered them than a generic-comment computer, but they became more suspicious about the validity of the flattery computer's claims and more likely to dismiss its answer.
These negative effects disappeared when participants simultaneously engaged in a secondary task.

A 2011 study, "Computer emotion – impacts on trust" by Dimitrios Antos, Celso De Melo, Jonathan Gratch, and Barbara Grosz investigated whether computer agents can use the expression of emotion to influence human perceptions of trustworthiness in the context of a negotiation activity followed by a trust activity.
They found that computer agents displaying emotions congruent with their actions were preferred as partners in the trust game over computer agents whose emotion expressions and actions did not match.
They also found that when emotion did not carry useful new information, it did not strongly influence human decision-making behavior in a negotiation setting.

A 2011 study "Cloud computing – reexamination of CASA" by Hong and Sundar found that when people are in a cloud computing environment, they shift their source orientation—that is, users evaluate the system by focusing on service providers over the internet, instead of the machines in front of them.
Hong and Sundar concluded their study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a fundamental re-examination of the mindless social response of humans to computers.
"One
example of how CASA research can impact consumer behaviour and attitude is Moon's experiment, which tested the application of the principle of reciprocity and disclosure in a consumer context.

He tested this principle with intimate self-disclosure of high-risk information (when disclosure makes the person feel vulnerable) to a computer, and observed how that disclosure affects future attitudes and behaviors.
Participants interacted with a computer which questioned them using reciprocal wording and gradual revealing of intimate information, then participants did a puzzle on paper, and finally half the group went back to the same computer and the other half went to a different computer.

Both groups were shown 20 products and asked if they would purchase them.
Participants who used the same computer throughout the experiment had a higher purchase likelihood score and a higher attraction score toward the computer in the product presentation than participants who did not use the same computer throughout the experiment.

==
References ==
In computing, a cache-oblivious algorithm (or cache-transcendent algorithm) is an algorithm designed to take advantage of a CPU cache without having the size of the cache (or the length of the cache lines, etc.)
as an explicit parameter.
An optimal cache-oblivious algorithm is a cache-oblivious algorithm that uses the cache optimally (in an asymptotic sense, ignoring constant factors).

Thus, a cache-oblivious algorithm is designed to perform well, without modification, on multiple machines with different cache sizes, or for a memory hierarchy with different levels of cache having different sizes.
Cache-oblivious algorithms are contrasted with explicit blocking, as in loop nest optimization, which explicitly breaks a problem into blocks that are optimally sized for a given cache.

Optimal cache-oblivious algorithms are known for matrix multiplication, matrix transposition, sorting, and several other problems.
Some more general algorithms, such as Cooley–Tukey FFT, are optimally cache-oblivious under certain choices of parameters.

Because these algorithms are only optimal in an asymptotic sense (ignoring constant factors), further machine-specific tuning may be required to obtain nearly optimal performance in an absolute sense.
The goal of cache-oblivious algorithms is to reduce the amount of such tuning that is required.

Typically, a cache-oblivious algorithm works by a recursive divide-and-conquer algorithm, where the problem is divided into smaller and smaller subproblems.

Eventually, one reaches a subproblem size that fits into cache, regardless of the cache size.

For example, an optimal cache-oblivious matrix multiplication is obtained by recursively dividing each matrix into four sub-matrices to be multiplied, multiplying the submatrices in a depth-first fashion.
In tuning for a specific machine, one may use a hybrid algorithm which uses blocking tuned for the specific cache sizes at the bottom level, but otherwise uses the cache-oblivious algorithm.

==
History ==
The idea (and name) for cache-oblivious algorithms was conceived by Charles E. Leiserson as early as 1996 and first published by Harald Prokop in his master's thesis at the Massachusetts Institute of Technology in 1999.
There were many predecessors, typically analyzing specific problems; these are discussed in detail in Frigo et al.
1999.
Early examples cited include Singleton 1969 for a recursive Fast Fourier Transform, similar ideas in Aggarwal et al.
1987, Frigo 1996 for matrix multiplication and LU decomposition, and Todd Veldhuizen 1996 for matrix algorithms in the Blitz++ library.

== Idealized cache model ==
Cache-oblivious algorithms are typically analyzed using an idealized model of the cache, sometimes called the cache-oblivious model.

This model is much easier to analyze than a real cache's characteristics (which have complicated associativity, replacement policies, etc.),
but in many cases is provably within a constant factor of a more realistic cache's performance.
It is different than the external memory model because cache-oblivious algorithms do not know the block size or the cache size.

In particular, the cache-oblivious model is an abstract machine (i.e. a theoretical model of computation).
It is similar to the RAM machine model which replaces the Turing machine's infinite tape with an infinite array.
Each location within the array can be accessed in 
  
    
      
        O
(
        1
)
{\displaystyle O(1)}
   time, similar to the random-access memory on a real computer.
Unlike the RAM machine model, it also introduces a cache: a second level of storage between the RAM and the CPU.
The other differences between the two models are listed below.
In the cache-oblivious model:
Memory is broken into blocks of 
  
    
      
        B
      
    
    {\displaystyle B}
   objects each.

A load or a store between main memory and a CPU register may now be serviced from the cache.

If a load or a store cannot be serviced from the cache, it is called a cache miss.

A cache miss results in one block being loaded from main memory into the cache.
Namely, if the CPU tries to access word
w
      
    
    {\displaystyle w}
   and
x
      
    
    {\displaystyle x}
   is the line containing
w
      
    
    {\displaystyle w}
  , then 
  
    
      
        x
      
    
    {\displaystyle x}
   is loaded into the cache.
If the cache was previously full, then a line will be evicted as well (see replacement policy below).

The cache holds 
  
    
      
        M
      
    
    {\displaystyle M}
   objects, where 
  
    
      
        M
=
        Ω
(
B
          
            2
          
        
        )
      
    
    {\displaystyle M=\Omega (B^{2})}
  .
This is also known as the tall cache assumption.

The cache is fully associative: each line can be loaded into any location in the cache.

The replacement policy is optimal.
In other words, the cache is assumed to be given the entire sequence of memory accesses during algorithm execution.
If it needs to evict a line at time
t
      
    
    {\displaystyle t}
  , it will look into its sequence of future requests and evict the line that is accessed furthest in the future.
This can be emulated in practice with the Least Recently Used policy, which is shown to be within a small constant factor of the offline optimal replacement strategyTo measure the complexity of an algorithm that executes within the cache-oblivious model
, we measure the number of cache misses that the algorithm experiences.
Because the model captures the fact that accessing elements in the cache is much faster than accessing things in main memory, the running time of the algorithm is defined only by the number of memory transfers between the cache and main memory.
This is similar to the external memory model, which all of the features above, but cache-oblivious algorithms are independent of cache parameters
(
  
    
      
        B
      
    
    {\displaystyle B}
   and 
  
    
      
        M
      
    
    {\displaystyle M}
  ).
The benefit of such an algorithm is that what is efficient on a cache-oblivious machine is likely to be efficient across many real machines without fine tuning for particular real machine parameters.
For many problems, an optimal cache-oblivious algorithm will also be optimal for a machine with more than two memory hierarchy levels.

== Examples ==
The simplest cache-oblivious algorithm presented in Frigo et al.
is an out-of-place matrix transpose operation (in-place algorithms have also been devised for transposition, but are much more complicated for non-square matrices).
Given m×n array A and n×m array B, we would like to store the transpose of A in B.
The naive solution traverses one array in row-major order and another in column-major.
The result is that when the matrices are large, we get a cache miss on every step of the column-wise traversal.
The total number of cache misses is 
  
    
      
        Θ
(
        m
        n
)
{\displaystyle \Theta (mn)}
  .

The cache-oblivious algorithm has optimal work complexity
O
(
        m
        n
)
      
    
    {
\displaystyle O(mn)}
and optimal cache complexity
O
(
        1
+
        m
        n
        
          /
        
        B
        )
{\displaystyle O(1+mn/B)}
  .
The basic idea is to reduce the transpose of two large matrices into the transpose of small (sub)matrices.
We do this by dividing the matrices in half along their larger dimension until we just have to perform the transpose of a matrix that will fit into the cache.

Because the cache size is not known to the algorithm, the matrices will continue to be divided recursively even after this point, but these further subdivisions will be in cache.

Once the dimensions m and n are small enough so an input array of size 
  
    
      
        m
×
        n
      
    
    {\displaystyle m\times n}
   and an output array of size 
  
    
      
        n
×
m
      
    
    {\displaystyle n\times m}
   fit into the cache,
both row-major and column-major traversals result in 
  
    
      
        O
(
        m
        n
)
{\displaystyle O(mn)}
   work and
O
(
        m
        n
        
          /
        
        B
)
{\displaystyle O(mn/B)}
cache misses.
By using this divide and conquer approach we can achieve the same level of complexity for the overall matrix.

(In principle, one could continue dividing the matrices until a base case of size 1×1 is reached, but in practice one uses a larger base case (e.g. 16×16) in order to amortize the overhead of the recursive subroutine calls.)

Most cache-oblivious algorithms rely on a divide-and-conquer approach.
They reduce the problem, so that it eventually fits in cache no matter how small the cache is, and end the recursion at some small size determined by the function-call overhead and similar cache-unrelated optimizations, and then use some cache-efficient access pattern to merge the results of these small, solved problems.

Like external sorting in the external memory model, cache-oblivious sorting is possible in two variants: funnelsort, which resembles mergesort, and cache-oblivious distribution sort, which resembles quicksort.
Like their external memory counterparts, both achieve a running time of 
  
    
      
        O
(
          
            
              
                
                  N
                  B
                
              
            
            
              log
              
                
                  
                    M
                    B
⁡
N
                  B
                
              
            
          
          )
        
      
    
    {\displaystyle O\left({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}\right)}
  , which matches a lower bound and is thus asymptotically optimal.

== See also ==
External memory algorithm
Funnelsort
Cache-oblivious distribution sort


==
References ==
The painter’s algorithm (also depth-sort algorithm and priority fill) is an algorithm for visible surface determination in 3D computer graphics that works on a polygon-by-polygon basis rather than a pixel-by-pixel, row by row, or area by area basis of other Hidden Surface Removal algorithms.
The painter’s algorithm creates images by sorting the polygons within the image by their depth and placing each polygon in order from the farthest to the closest object.

The painter's algorithm was initially proposed as a basic method to address the Hidden-surface determination problem by Martin Newell, Richard Newell, and Tom Sancha in 1972, while all three were working at CADCentre.
The name "painter's algorithm" refers to the technique employed by many painters where they begin by painting distant parts of a scene before parts that are nearer, thereby covering some areas of distant parts.
Similarly, the painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest.
It will paint over the parts that are normally not visible — thus solving the visibility problem — at the cost of having painted invisible areas of distant objects.
The ordering used by the algorithm is called a 'depth order' and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another, then the first object is painted after the object that it obscures.
Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.

==
Algorithm ==
Conceptually Painter’s Algorithm works as follows:
Sort each polygon by depth
Place each polygon from the furthest polygon to the closest polygon


===
Pseudo-code ===
1
sort polygons by depth
2
for each polygon p:
3      for each pixel that p covers:
4          paint
p.color on pixel


===
Time-Complexity ===
The painter's algorithm's time-complexity is heavily dependent on the sorting algorithm used to order the polygons.
Assuming the use of the most optimal sorting algorithm, painter's algorithm has a worst-case complexity of O(n log n + m*n), where n is the number of polygons and m is the number of pixels to be filled.

=== Space-Complexity ===
The painter's algorithm's worst-case space-complexity is O(n+m),
where n is the number of polygons and m is the number of pixels to be filled.

==
Advantages ==
There are two primary technical requisites that favor the use of the painter’s algorithm.

===
Basic Graphical Structure ===
The painter’s algorithm is not as complex in structure as its other depth sorting algorithm counterparts.
Components such as the depth-based rendering order, as employed by the painter’s algorithm, are one of the simplest ways to designate the order of graphical production.
This simplicity makes it useful in basic computer graphics output scenarios where an unsophisticated render will need to be made with little struggle.

===
Memory Efficiency ===
In the early 70s, when the painter’s algorithm was developed, physical memory was relatively small.
This required programs to manage memory as efficiently as possible to conduct large tasks without crashing.
The painter’s algorithm prioritizes the efficient use of memory but at the expense of higher processing power since all parts of all images must be rendered.

==
Limitations ==
The algorithm can fail in some cases, including cyclic overlap or piercing polygons.

===
Cyclical Overlapping ===
In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others.
In this case, the offending polygons must be cut to allow sorting.

===
Piercing Polygons ===
The case of piercing polygons arises when one polygon intersects another.
Similar to cyclic overlap, this problem may be resolved by cutting the offending polygons.

===
Efficiency ===
In basic implementations, the painter's algorithm can be inefficient.
It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene.
This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.

==
Variants ==


===
Extended painter's algorithm ===
Newell's algorithm, proposed as the extended algorithm to painter's algorithm, provides a method for cutting cyclical and piercing polygons.

=== Reverse painter's algorithm ===
Another variant of painter's algorithm includes reverse painter's algorithm.
Reverse painter's algorithm paints objects nearest to the viewer first — with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent).

In a computer graphic system, this can be very efficient since it is not necessary to calculate the colors (using lighting, texturing, and such) for parts of a distant scene that are hidden by nearby objects.

However, the reverse algorithm suffers from many of the same problems as the standard version.

==
Other computer graphics algorithms ==
The flaws of painter's algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order.
Even in such systems, a variant of the painter's algorithm is sometimes employed.
As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error.
These are overlaps or gaps at joints between polygons.

To avoid this, some graphics engine implementations "overrender", drawing the affected edges of both polygons in the order given by the painter's algorithm.

This means that some pixels are actually drawn twice (as in the full painter's algorithm), but this happens on only small parts of the image and has a negligible performance effect.

==
References ==
Foley, James; Feiner, Steven K.; Hughes, John F. (1990).
Computer Graphics:
Principles and Practice.
Reading, MA, USA: Addison-Wesley.
p. 1174.
ISBN 0-201-12110-7.
==
External links ==
Painter’s & Z-Buffer Algorithms and Polygon Rendering
https://www.clear.rice.edu/comp360/lectures/old/
HiddenSurfText.pdf
https://www.cs.princeton.edu/courses/archive/spring01/cs598b/papers/greene93.pdf
Social heuristics are simple decision making strategies that guide behavior and decisions in the social environment when time, information, or cognitive resources are scarce.
Social environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the decision making process through ignoring some information or relying on simple rules of thumb to make decisions.
The class of phenomena described by social heuristics overlap with those typically investigated by social psychology and game theory.
At the intersection of these fields, social heuristics have been applied to explain cooperation in economic games used in experimental research, based on the argument that cooperation is typically advantageous in daily life, and therefore people develop a cooperation heuristic that gets applied even to one-shot anonymous interactions (the so-called "social heuristics hypothesis" of human cooperation).

==
Overview ==
=== Bounded Rationality ===
In the decision-making process, optimisation is almost always intractable in any implementation, whether machine or neural..
Because of this, defined parameters or boundaries must be implemented in the process in order to achieve an acceptable outcome.
This method is known as applying bounded rationality, where an individual makes a collective and rational choice that considers “the limits of human capability to calculate, the severe deficiencies of human knowledge about the consequences of choice, and the limits of human ability to adjudicate among multiple goals”.
They are essentially incorporating a series of criteria, referred to as alternatives for choice.
These alternatives are often not initially given to the decision maker, so a theory of search is also incorporated.

=== Heuristics ===
Heuristics are a common alternative, which can be defined as simple strategies for decision making where the actor only pays attention to key pieces of information, allowing the decision to be made quickly and with less cognitive effort.

Daniel Kahneman and Shane Frederick have advanced the view that heuristics are decision making processes that employ attribute substitution, where the decision maker substitutes the "target attribute" of the thing he is trying to judge with a "heuristic attribute" that more easily comes to mind.
Shah and Daniel M. Oppenheimer have framed heuristics in terms of effort reduction, where the decision maker makes use of techniques that make decisions less effortful, such as only paying attention to some cues or only considering a subset of the available alternatives.
Another view of heuristics comes from Gerd Gigerenzer and colleagues, who conceptualize heuristics as "fast and frugal" techniques for decision making that simplify complex calculations and make up part of the "adaptive toolbox" of human capacities for reasoning and inference.
Under this framework, heuristics are ecologically rational, meaning a heuristic may be successful if the way it works matches the demands of the environment it is being used in.
Researchers in this vein also argue that heuristics may be just as or even more accurate when compared to more complex strategies such as multiple regression.

===
Social heuristics ===
Social heuristics can include heuristics that use social information, operate in social contexts, or both.
Examples of social information include information about the behavior of a social entity or the properties of a social system, while nonsocial information is information about something physical.
Contexts in which an organism may use social heuristics can include "games against nature" and "social games".
In games against nature, the organism strives to predict natural occurrences (such as the weather) or competes against other natural forces to accomplish something.
In social games, the organism is making decisions in a situation that involves other social beings.
Importantly, in social games, the most adaptive course of action also depends on the decisions and behavior of the other actors.
For instance, the follow-the-majority heuristic uses social information as inputs but is not necessarily applied in a social context, while the equity-heuristic uses non-social information but can be applied in a social context such as the allocation of parental resources amongst offspring.
Within social psychology, some researchers have viewed heuristics as closely linked to cognitive biases.
Others have argued that these biases result from the application of social heuristics depending on the structure of the environment that they operate in.
Researchers in the latter approach treat the study of social heuristics as closely linked to social rationality, a field of research that applies the ideas of bounded rationality and heuristics to the realm of social environments.
Under this view, social heuristics are seen as ecologically rational.
In the context of evolution, research utilizing evolutionary simulation models has found support for the evolution of social heuristics and cooperation when the outcomes of social interactions are uncertain.

== Examples ==
Examples of social heuristics include:
Imitate-the-majority heuristic, also referred to follow-the-majority heuristic.
An agent using the heuristic would imitate the behavior of the majority of agents in his reference group.
For instance, in deciding which restaurant to choose, people tend to choose the one with the longer waiting queue.

Imitate-the-successful heuristic, also referred to follow-the-best heuristic.
An agent using the heuristic would imitate the behavior of the most successful person in her reference group.

Equity heuristic, also referred to 1/N heuristic.
Using the heuristic means equally distributing resources among the available options.
The heuristic was found to be successful in the stock market and also been found to describe parental resource allocation decisions: parents typically allocate their time and effort equally amongst their children.

Social-circle heuristic.
The heuristic is used to infer which of two alternatives has the higher value.
An agent using the heuristic would search through her social circles in order of their proximity to the self (self, family, friends, and acquaintances), stopping the search as soon as the number of instances of one alternative within a circle exceeds that of the other, choosing the alternative with the higher tally.
For example, a person might decide which of two sports is more popular by thinking through how many members of each circle play each sport.

Tit-for-Tat heuristic.
In deciding whether to cooperate or defect, an agent using the heuristic would cooperate in the first round and in subsequent rounds, reciprocate his partner's action of cooperation or defection in the previous round.
The heuristic is typically investigated using a prisoner's dilemma in game theory, where there is substantial evidence that people use such a heuristic, leading to intuitive reciprocation.

Regret matching heuristic.
An agent using this heuristic will persist with a course of action in a cooperative game as long as she is not experiencing regret.
Once she experiences regret, this heuristic predicts a probability that the actor will switch her behavior that is proportional to the amount of regret she feels about missing out on a past payout.

Group recognition heuristic, which extends principles related to the recognition heuristic into a group decision making setting.
In individual decision making, the recognition heuristic is used when an individual asked which of two options has a higher value on a given criterion judges that the option he recognizes has a higher value than the option he does not recognize.
This is applied in group decision making settings when a group's choice of which of two options has a higher value is influenced by use of the recognition heuristic by some members of the group.

Majority heuristic (rule).
This is a decision rule used in group decision making by both humans and animals, where each member of the group votes for an alternative and a decision is reached based on the option with the most votes.
Researchers investigating majority rule (where the option with more than half of the votes is chosen) and plurality rule (where the option with the most votes in chosen) strategies for group decisions found such strategies to be both high-performing and computationally efficient for situations where there is a correct answer.

Base-Rate heuristic.
The process that involves using common mental shortcuts that help a decision to be made based on known probabilities.
For example, if an animal is heard howling in a large city, it is usually assumed to be a dog because the probability that a wolf is in a large city is very low.

Peak-and-end heuristic.
When past experiences are practically exclusively judged on how the agent was affected at the peak (both unpleasant and pleasant) and the end of event, creating a natural bias in the decision-making process as the whole experience is not analysed.

Familiarity heuristic.
The agent’s approach to solve a social decision in which they have experienced a similar event before involves them reflecting on comparable past situations, and often acting the same way they acted in the past.

==
Relation to other concepts ==


===
Dual-process approach ===
A dual-process approach to human cognition specifies two types of thought processes: one that is fast and happens unconsciously or automatically, and another that is slower and involves more conscious deliberation.
In the dominant dual-systems approach in social psychology, heuristics are believed to be automatically and unconsciously applied.
The study of social heuristics as a tool of bounded rationality asserts that heuristics may be used consciously or unconsciously.

===
Social heuristics hypothesis ===
The social heuristics hypothesis is a theory put forth by Rand and colleagues that explains the link between intuition and cooperation.
Under this theory, cooperating in everyday social situations tends to be successful, and as a result, cooperation is an internalized heuristic that is applied in unfamiliar social contexts, even those in which such behavior may not lead to the most personally advantageous result for the actor (such as a lab experiment).

Methods used by researchers to study cooperative behavior in the laboratory include economic games such as:
Prisoner's dilemma game: two players each decide whether to cooperate or defect; a player who defects when the other cooperates maximizes his payout, if both cooperate the payout is higher than if both defect.

Public goods game :
multiple players each choose how much money to put towards a public project; the amount in the public pot is increased by a given factor and distributed equally to those who contributed.

Trust game: one player transfers money to another player and the money is increased by a given factor; the other then decides whether and how much to transfer back.

Ultimatum game
: one player makes an offer for how to split a resource with the other player; the other player can accept the offer
(so that both players get the amount proposed by the split) or reject the offer (so that neither player gets anything).These economic games
all share the condition that, when played in a single round, an individual's payout is maximized if he acts selfishly and chooses not to cooperate.
However, over the course of repeated rounds, cooperation can be payout maximizing and thus be a self-interested strategy.
Following a dual-process framework, the social heuristics hypothesis contends that cooperation, which is automatic and intuitive, may be overridden by reflection.
The theory is supported by evidence from laboratory and online experiments suggesting that time pressure increases cooperation, though some evidence suggests this may be only among individuals who are not as familiar with the types of economic games typically used in this field of research.
Meta-analytic evidence based on 67 studies that looked at cooperation in the types of economic games described above suggests that cognitive-processing manipulations that encourage intuitive decision-making (such as time pressure or increased cognitive load) increase pure cooperation, where a one-shot action has no future consequences for the actor to consider and not cooperating is the most advantageous option.
However, such manipulations do not have an effect on strategic cooperation in situations in which cooperation may be the pay-off maximizing option because of a possibility of future interactions where the actor may be rewarded for cooperation.

== See also ==
Heuristics in judgment and decision-making
Bounded rationality


=
= References ==
In the United States, Advanced Placement Computer Science is a suite of Advanced Placement courses and examinations covering areas of computer science.
They are offered by the College Board to high school students as an opportunity to earn college credit for college-level courses.

The suite consists of two current classes and one discontinued class.

AP Computer Science was taught in Pascal for the 1984–1998 exams, in C++ for 1999–2003, and in Java since 2004.

==
AP Computer Science A ==
AP Computer Science A is a programming class.

The course emphasizes object-oriented programming methodology, especially problem solving and algorithm development, plus an overview of data structures and abstraction.
The AP Computer Science A exam tests students on their knowledge of Java.

It is meant to be the equivalent of a first-semester course in computer science.

The Microsoft-sponsored program Technology Education and Literacy in Schools (TEALS) aims to increase the number of students taking AP Computer Science classes.

==
AP Computer Science AB (discontinued) ==
AP Computer Science AB included all the topics of AP Computer Science A, as well as a more formal and a more in-depth study of algorithms, data structures, and data abstraction.
For example, binary trees were studied in AP Computer Science AB but not in AP Computer Science A.
The use of recursive data structures and dynamically allocated structures were fundamental to AP Computer Science AB.

AP Computer Science AB was equivalent to a full-year college course.
Due to low numbers of students taking the exam, AP Computer Science AB was discontinued following the May 2009 exam administration.

==
AP Computer Science Principles ==
AP Computer Science Principles is an introductory course to computer science, "with a focus on how computing powers the world".
It is designed as a parallel to AP Computer Science A, to emphasize computational thinking and fluency.
It is meant to be the equivalent of a first-semester course in computing.

== See also ==
AP Computer Science
A
Computer science
Glossary of computer science
Scope (computer science)
Computer graphics (computer science)


==
References ==
This is a glossary of terms relating to computer graphics.

For more general computer hardware terms, see glossary of computer hardware terms.

== 0–9 ==
7e3 format
A packed pixel format supported by some graphics processing units (GPUs) where a single 32-bit word encodes three 10-bit floating point color channels, each with seven bits of mantissa and three bits of exponent.

2D convolution
Operation that applies linear filtering to image with a given two-dimensional kernel, able to achieve e.g. edge detection, blurring, etc.

2D image
2D texture map
A texture map with two dimensions, typically indexed by UV coordinates.

2D vector
a two-dimensional vector, a common data type in rasterization algorithms, 2D computer graphics, graphical user interface libraries.

2.5D
Also pseudo 3D. Rendering whose result looks 3D while actually not being 3D or having great limitations, e.g. in camera degrees of freedom.

3D graphics pipeline
A graphics pipeline taking 3D models and producing a 2D bitmap image result.

3D scene
A collection of 3D models and lightsources in world space, into which a camera may be placed, describing a scene for 3D rendering.

3D paint tool
A 3D graphics application for digital painting of multiple texture map image channels directly onto a rotated 3D model, such as zbrush or mudbox, also sometimes able to modify vertex attributes

3D unit vector
A unit vector in 3D space.

4D vector
a common datatype in graphics code, holding homogeneous coordinates or RGBA data, or simply a 3D vector with unused W to benefit from alignment, naturally handled by machines with 4-element SIMD registers.

4×4 matrix
A matrix commonly used as a transformation of homogeneous coordinates in 3D graphics pipelines.

==
A ==
AABB
Axis aligned bounding box (sometimes called "axis oriented"),
a bounding box stored in world coordinates; one of the simplest bounding volumes.

Additive blending
A compositing operation where 
  
    
      
        d
        s
t
        =
d
        s
t
+
s
        r
c
        ,
      
    
    {\displaystyle dst=dst+src,}
   without the use of an alpha channel, used for various effects.
Also known as linear dodge in some applications.

Affine texture mapping
Linear interpolation of texture coordinates in screen space without taking perspective into account, causing texture distortion.

Aliasing
Unwanted effect arising when sampling high-frequency signals, in computer graphics appearing e.g. when downscaling images.
Antialiasing methods can prevent it.

Alpha channel
An additional image channel (e.g. extending an RGB image) or standalone channel controlling alpha blending.

Ambient lighting
An approximation to the light entering a region from a wide range of directions, used to avoid needing an exact solution to the rendering equation.

Ambient occlusion (AO)
Effect approximating, in an inexpensive way, one aspect of global illumination by taking into account how much ambient light is blocked by nearby geometry, adding visual clues about the shape.

Analytic model
A mathematical model for a phenomenon to be simulated, e.g. some approximation to surface shading.
Contrasts with Empirical models based purely on recorded data.

Anisotropic filtering
Advanced texture filtering improving on Mipmapping, preventing aliasing while reducing blur in textured polygons at oblique angles to the camera.

anti aliasing
methods for filtering and sampling to avoid visual artefacts associated with the uniform pixel grid in 3D rendering.

Array texture
A form of texture map containing an array of 2D texture slices selectable by a 3rd 'W' texture coordinate; used to reduce state changes in 3D rendering.

Augmented reality
Computer-rendered content inserted into the user's view of the real world.

AZDO
Approaching zero driver overhead, a set of techniques aimed at reducing the CPU overhead in preparing and submitting rendering commands in the OpenGL pipeline.
A compromise between the traditional GL API and other high-performance low-level rendering APIs.

==
B ==
Back-face culling
Culling (discarding) of polygons that are facing backwards from the camera.

Baking
Performing an expensive calculation offline,  and caching the results in a Texture map or Vertex attributes.
Typically used for generating lightmaps, normal maps, or low level of detail models.

Barycentric coordinates
Three-element coordinates of a point inside a triangle.

Beam tracing
Modification of ray tracing which instead of lines uses pyramid-shaped beams to address some of the shortcomings of traditional ray tracing, such as aliasing.

Bicubic interpolation
Extension of cubic interpolation to 2D, commonly used when scaling textures.

Bilinear interpolation
Linear interpolation extended to 2D, commonly used when scaling textures.

Binding
(edit required)

Billboard
A textured rectangle that keeps itself oriented towards the camera, typically used e.g. for vegetation or particle effects.

Binary space partitioning (BSP)
A data structure that can be used to accelerate visibility determination, used e.g. in Doom engine.

Bit depth
The number of bits per pixel, sample, or texel in a bitmap image (holding one or mode image channels, typical values being 4, 8, 16, 24, 32)
Bitmap
Image stored by pixels.

Bit plane
A format for bitmap images storing 1 bit per pixel in a contiguous 2D array; Several such parallel arrays combine to produce the a higher bit depth image.
Opposite of packed pixel format.

Blend operation
A render state controlling alpha blending, describing a formula for combining source and destination pixels.

Bone
Coordinate systems used to control surface deformation (via Weight maps) during Skeletal animation.
Typically stored in a hierarchy, controlled by keyframes, and other procedural constraints.

Bounding box
One of the simplest type of bounding volume, consisting of axis aligned or object aligned extents.

Bounding volume
A mathematically simple volume, such as a sphere or a box, containing 3D objects, used to simplify and accelerate spatial tests (e.g. for visibility or collisions).

Bump mapping
Technique similar to normal mapping that instead of normal maps uses so called bump maps (height maps).

BRDF
Bidirectional reflectance distribution functions (BRDFs), empirical models defining 4D functions for surface shading indexed by a view vector and light vector relative to a surface.

BVH
Bounding volume hierarchy


==
C ==
Camera
A virtual camera from which rendering is performed, also sometimes referred to as 'eye'.

Camera space
A space with the camera at the origin, aligned with the viewer's direction, after the application of the world transformation and view transformation.

Cel shading
Cartoon-like shading effect.

Clipping
Limiting specific operations to a specific region, usually the view frustum.

Clip plane
A plane used to clip rendering primitives in a graphics pipeline.
These may define the View frustum or be used for other effects.

Clip space
Coordinate space in which clipping is performed.

Clip window
A rectangular region in screen space, used during clipping.
A clip window may be used to enclose a region around a portal in portal rendering.

CLUT
A table of RGB color values to be indexed by a lower bit depth image (typically 4-8bits), a form of vector quantisation.

Color bleeding
Unwanted effect in texture mapping.
A color from a border of unmapped region of the texture may appear (bleed) in the mapped result due to interpolation.

Color channels
The set of channels in a bitmap image representing the visible color components, i.e. distinct from the alpha channel or other information.

Color resolution
Command buffer
A region of memory holding a set of instructions for a graphics processing unit for rendering a scene or portion of a scene.
These may be generated manually in bare metal programming, or managed by low level rendering APIs, or handled internally by high level rendering APIs.

Command list
a group of rendering commands ready for submission to a graphics processing unit, see also Command buffer.

Compute API
an API for efficiently processing large amounts of data.

Compute shader
a compute kernel managed by a rendering API, with easy access to rendering resources.

Cone tracing
Modification of ray tracing which instead of lines uses cones as rays in order to achieve e.g. antialiasing or soft shadows.

Connectivity information
Indices defining [rendering primitive]s between vertices, possibly held in index buffers.
describes geometry as a graph or hypergraph.

CSG
Constructive solid geometry, a method for generating complex solid models from boolean operations combining simpler modelling primitives.

Cube mapping
A form of environment reflection mapping in which the environment is captured on a surface of a cube (cube map).

Culling
Before rendering begins, culling removes objects that don't significantly contribute to the rendered result (e.g. being obscured or outside camera view).

== D ==
Decal
A "sticker" picture applied onto a surface (e.g. a crack on the wall).

Detail texture
texture maps repeated at high frequency combined with a main texture on a surface to prevent a blurred appearance close to the camera.

Deferred shading
A technique by which computation of shading is deferred to later stage by rendering in two passes, potentially increasing performance by not discarding expensively shaded pixels.
The first pass only captures surface parameters (such as depth, normals and material parameters), the second one performs the actual shading and computes the final colors.

Deformation lattice
A means of controlling free-form deformation via a regular 3D grid of control points moved to arbitrary positions, with polynomial interpolation of the space between them.

Degenerate triangles
Zero area triangle primitives placed in a triangle strip between actual primitives, to allow many parts of a triangle mesh to be rendered in a single drawcall.
These are trivially rejected by the triangle setup unit.

Delaunay triangulation
A method for generating an efficient triangulating between a set of vertices in a plane.

Depth buffer
a bitmap image holding depth values (either a Z buffer or a W buffer), used for visible surface determination, during rasterization of 3D scenes
Depth map
A bitmap image or texture map holding depth values.
Similar to a height map or displacement map, but usually associated with a projection.

Depth value
a value in a depth map representing a distance perpendicular to the space of an image.

Diffuse lighting
In shading, a diffuse component of light is the light reflected from the surface uniformly into all directions.
This component depends on the surface normal and direction to the light source but not on the viewer's position.

Direct3D
Microsoft Windows 3D API, with similar architecture to OpenGL.

Displacement mapping
a method for adding detail to surfaces by subdivision and displacement of the resulting vertices form a height map.

Distributed ray tracing
Modification of ray tracing that casts multiple rays through each pixel in order to model soft phenomena such as soft shadows, depth of field etc.

Double buffering
Using a dedicated buffer for rendering and copying the result to the screen buffer when finished.
This prevents stutter on the screen and the user seeing rendering in progress.

Drawcall
A single rendering command submitted to a rendering API, referring to a single set of render states.

== E ==
Edge vector
a vector between 2 position vertices in a polygon or polygon mesh, along an edge
Environment mapping
Also reflection mapping, a technique of approximating reflections of environment on complex surfces of 3D models in real time.
A complete 360 degree view of environment needs to be prerendered and stored in a texture using a specific mapping (e.g. cube mapping, sphere mapping etc.)

Extents
The minimum and maximum values of an object or primitive along a coordinate axis or set of axes.

==
F ==
Flat shading
Shading that assigns a uniform color to each face of a 3D model, giving it a "sharp-edge" look.

Forward rendering
A term for traditional 3D rendering pipelines which sort lightsources applicable to 3D models in world space prior to rasterization.
Contrasts with Deferred shading.

Forward-plus rendering
an extension of forward rendering using compute shaders to place lightsources into screen space tiles, to accelerate the use of many lightsources, bypassing some of the disadvantages of deferred shading.

Fractal
A complex, self-similar shape described by a simple equation.
Fractals can be used e.g. in procedural terrain generation.

Fragment (pixel) shader
Shader processing individual pixels or fragments (values that may potentially become pixels).

Frustum culling
A stage in a rendering pipeline, filtering out 3D models whose bounding volumes fail an intersection test with the view frustum, allowing trivial rejection.

Fixed-function pipeline
A hardware rendering pipeline without shaders, composed entirely of fixed function units.
A limited number of functions may be controlled by render states.

Fixed function unit
A piece of hardware in a graphics processing unit implementing a specific function (such as triangle setup or texture sampling), without programmable control by shaders.

Fresnel
According to Fresnel equations, surfaces show more specular reflections when viewed at near-grazing incidence.
This effect is often simulated in computer graphics.

FXAA
an approximate antialiasing method performed in a post processing step which smooths the image in screen space, guided by edge detection (contrasting with the usual supersampling approaches that require larger frame-buffers).

==
G ==
Geometry
typically used to refer to vertex & rendering primitive connectivity information (distinct from materials and textures).

Geometry shader
In APIs such as OpenGL and Direct3D, geometry shader is an optional stage able to process 3D model geometry in more advanced ways than a vertex or tessellation shaders (e.g. turn primitives into other primitives).

G-buffer
A screen space representation of geometry and material information, generated by an intermediate rendering pass in deferred shading rendering pipelines.

Global illumination
Computing the global interactions of light within the scene, e.g. reflections of light from one object to another, by which realism is added.

Goraud shading
Shading technique that computes values at triangle vertices and interpolates them across the surface.
This is more realistic and computationally expensive than flat shading, and less than Phong shading.

Graphics processing unit
Hardware used to accelerate graphical computations.

Graphical shader
a shader associated with the rendering pipeline; not a compute shader.

Grid cell index
Integer coordinates in a multidimensional array.
== H ==
HDR
High dynamic range imaging, an image format using floating-point values.
Allows additional realism with post processing.

Heightmap
a 2D array or texture map holding height values; typically used for defining landscapes, or for displacement mapping
Homogeneous coordinates
Coordinates of form
(x,y,z,w) used during matrix transforms of vertices, allowing to perform non-linear transforms such as the perspective transform.

==
I ==
Image channel
A single component (referred to as a channel) of a bitmap image
; one of multiple components per pixel, e.g. for RGB or YUV color space, or additional channels for alpha blending

Image format
A specific way of representing a bitmap image in memory, also refers to image file formats.

Image generation
synonymous with rendering; taking a 3D scene (or other form of encoded data) and producing a bitmap image result.

Image generator
A hardware accelerator for image generation, almost synonymous with a graphics processing unit, but historically used to refer to devices aimed at realtime rendering for simulation (e.g. Evans & Sutherland ESIG line).

Image order rendering
Rendering methods that iterate over pixels of the screen in order to draw the image (e.g. raytracing).

Image plane
The plane in the world which is identified with the plane of the display monitor used to view the image that is being rendered.

Immediate mode rendering
the submission of rendering commands and rendering primitive data without the extensive use of managed resources; rendering primitive vertex attribute data may be embedded directly into a command list, rather than referenced indirectly from resources.

Impostor
A dynamically rendered Billboard texture map used to stand in for geometry in the distance.
A form of level of detail optimization.

Incremental error algorithm
A set of rasterization algorithms which use simple integer arithmetic to update an error term that determines if another quantity is incremented, avoiding the need for expensive division or multiplication operations; e.g. bresenham's line algorithm, or rasterizing heightmap landscapes.

Index buffer
A rendering resource used to define rendering primitive connectivity information between vertices.

Indirect illumination
Another term for global illumination.

Instancing
Rendering multiple objects (instances) using the same geometry data.

Intersection test
determining
if two pieces of geometry intersect, commonly required in simulation, rendering pipelines, and 3D modelling applications.

==
K ==
K-DOP
A type of bounding volume used for fast intersection tests; a discrete oriented polytope (DOP).
These generalise bounding boxes with extents additional discrete planes (e.g. diagonals formed by each pair of coordinate axes, etc.).

==
L ==
Level of detail (LOD)
If an object contributes less to the rendered result, e.g. by being far away from the camera, LOD chooses to use a simpler version of the object (e.g. with fewer polygons or textures).

Light probe
Object used to capture light parameters at a specific point in space in order to help compute scene lighting.

Low level rendering API
A library providing a minimal abstraction layer over a graphics processing unit's raw command lists, such as Vulkan, LibGCM, or Metal (API).
The user typically has more control over (and responsibility for) resource management, command buffers, synchronisation issues.

Lumels
A term for texels in the texture map representing a lightmap.

Lighting
Computations simulating the behavior of light.

Light vector
In shading calculations, a 3D unit vector representing the direction of incident light onto a model's surface.

Light field
a data structure approximating the 4D flux  of light rays through a space (or in the general case, 5D); it may be captured using multiple cameras (e.g. light stage), or rendered from a 3D model by ray tracing.

Line primitive
A rendering primitive or modelling primitive representing a line segment, used for wireframes.

==
M ==
Manhattan distance
Measure of distance between two points, different from Euclidean distance,
that sums the distances along principal axes.

Marching cubes
A method for triangulating implicit surfaces.

MegaTexturing
Texturing technique that works with extremely large textures which are not loaded into memory all at once, but rather streamed from the hard disk depending on the camera view.

Modelling primitive
Basic elements from which 3D models and 3D scenes are composed.
Also known as a Geometric primitive.

Model space
Coordinate system in which a 3D model is created and stored.

Model transformation matrix
a transformation matrix producing world coordinates from a 3D model's local coordinates.

Microtexture
An alternative term sometimes used for Detail textures.

Mipmap
Method of preventing aliasing by storing differently scaled versions of the same image and using the correct one during rendering.

Multiply blend
A blending operation used for lightmaps, 
  
    
      
        d
        s
t
        =
d
        s
t
        ∗
        s
        r
c
        .
{\displaystyle dst=dst*src.}

==
N ==
Near clipping
The clipping of 3D rendering primitives against the near clip plane.
Necessary to correctly display rendering primitives that partially pass behind the camera.

Nearest-neighbor interpolation
Simplest form of interpolation that for given position outputs the color of the nearest sample.

Noise
In real world data a noise is an unwanted distortion of the captured signal, e.g. in photography.
In rendering, artificial noise, such as white noise or Perlin noise, is often generated and added on purpose to add realism.

Normal mapping
Method of adding detail to the surface of 3D models, without increasing geometry complexity, by using a texture with precomputed normals that are used during shading.

==
O ==
OBJ format
A common 3D file format.

Object order rendering
Rendering methods that iterate over objects in the scene and draws then one by one (e.g. rasterization).

Occlusion culling
Culling (discarding) of objects before rendering that are completely obscured by other objects.

Occlusion query
A command passed to a graphics processing unit requesting the testing of bounding volume geometry against the depth buffer to determine if any contents in the potentially visible set; used for hardware accelerated occlusion culling.

Offline rendering
Non-real-time rendering.

OOBB
An object oriented bounding box (sometimes called object aligned); a bounding box stored in some object's local coordinate system

OpenGL
Commonly used 2D and 3D graphics rendering API.

Outcode
A small integer holding a bit for the result of every plane test (or clip window edge test) failed in clipping.
Primitives may be trivially rejected if the bitwise AND of all its vertices outcodes is non zero


==
P ==
Packed pixel format
an image format where the image channels are interleaved contiguously in memory, possibly containing multiple channels within single machine words, equivalent to an array of structures for bitmap data.
Contrasts with planar image formats.

Parallax mapping
Shader effect that adds detail with a sense of depth to a 3D surface, in a more realistic way than normal mapping.

Parameter gradient
The derivative of a vertex attribute with respect to screen space coordinates during rasterization, used for interpolation across a rendering primitive surface.

Particle effect
Effects consisting of a number of particles that behave by certain rules, typically used to simulate fire, smoke etc.

Path tracing
Photorealistic iterative rendering method based on tracing light paths.

Perspective correct texturing
Non-linear texture coordinate interpolation that takes into account perspective, eliminating distortion seen in affine texture mapping.

Phong lighting
A commonly used model of local illumination that computes the result as a sum of ambient, diffuse and specular elements of light.

Phong shading
Shading technique that uses interpolated normals.

Photogrammetry
Science and technology of making measurement from photographs, e.g. automatically creating 3D models of environment.

Photometry
Science of measuring light in terms of human perception.

Photon mapping
Photorealistic rendering algorithm based on tracing rays from the camera as well as light sources, able to simulate effects such as caustics.

Physically based rendering (PBR)
rendering algorithms based on physics simulation of light, including conservation of energy, empirical models of surfaces.

Pixel
Smallest element of a raster image.

Planar image format
an image format where the image channels (or even bits) for a single pixel is separated into several parallel arrays, equivalent to a structure of arrays for bitmap data.

Point cloud
a surface defined by a collection of vertices without connectivity information.

Point sprite
A
rendering primitive in 3D graphics pipelines, allowing one vertex plus radius to define a billboard; corner vertices are automatically generated.
Typically used for particle systems
Polygon mesh
A 3D model consisting of vertices connected by polygon primitives.

Polygon primitive
A rendering or modelling primitive defining a flat surface connecting 3 or more vertices.

Portal
A means of occlusion culling, defining a visible window between adjacent bounding volumes, used in portal rendering.

Post processing
Effects applied to a bitmap image in screen space after 3D rendering pipeline, for example tone mapping, some approximations to motion blur, and blooms.

Predicated rendering
A feature facilitating occlusion culling within a graphics pipeline, performed by a command list asynchronously form the CPU, where a group of rendering commands are flagged to be conditional on the result of an earlier occlusion query.

Premultiplied alpha
A variation of a bitmap image or alpha blending calculation in which the RGB color values are assumed to be already multiplied by an alpha channel, to reduce computations during Alpha blending; uses the blend operation:
dst *= (1 - alpha)
+ src;
capable of mixing alpha blending with additive blending effects

Primitive
A basic unit of geometry for rendering or modelling
Procedural generation
Generating data, such as textures, 3D geometry or whole scenes by algorithms (as opposed to manually).

Procedural texture
A texture (very often a volume texture) generated procedurally by a mathematical function and with the use of noise functions.
==
Q ==
Quaternion
a means of representing rotations in a 4D vector, useful for skeletal animation, with advantages for interpolation compared to euler angles (i.e. not suffering from gimbal lock).

== R ==
Radiometry
Measurement of electromagnetic radiation such as light, defining measures such as flux or radiance.

Raster graphics
Graphics represented as a rectangular grid of pixels.

Rasterisation
Converting vector graphics to raster graphics.
This terms also denotes a common method of rendering 3D models in real time.

Ray casting
Rendering by casting non-recursive rays from the camera into the scene.
2D ray casting is a 2.5D rendering method.

Ray marching
Sampling 3D space at multiple points along a ray, typically used when analytical methods cannot be used.

Ray tracing
Recursively tracing paths of light rays through a 3D scene, may be used for 3D rendering (more commonly for offline rendering), or other tests.

Recursive subdivision
The process of subdividing an object (either geometric object, or a data structure)
recursively until some criteria is met.

Rendering equation
Mathematical equation used as a model of light behavior in photorealistic rendering.

RGB888 an RGB color value encoded as 8 bits per channel
RGBA
An RGB color value together with an alpha channel, typically held in bitmap images or intermediates in shading calculations.

RGBA888 an RGBA color value encoded as 8 bits per channel

RGB image
A bitmap image holding RGB color values in 3 image channels
RGB color value
a 3D vector describing a color using the RGB color model; may use fixed point or floating-point representations.

Rendering API
A software library for submitting rendering commands, and managing render states and rendering resources;examples include OpenGL, Direct3D, Vulkan.
Provides an abstraction layer for a graphics processing unit.

Render mapping
The baking of a rendering of a 3D model surface into a texture map to capture surface properties.
Also known as 'render surface map'.

Render pass
A stage in a rendering pipeline generating some (possibly incomplete) representation of the scene.

Render states
Information controlling a graphics pipeline, composed of modes and parameters, including resource identifiers, and shader bindings.

Render target
A graphics resource into which rendering primitives are rasterized by a graphics pipeline.
Render targets may be frame buffers or texture maps.

Render to texture
The process of rasterizing into a texture map (or texture buffer) for further use as a resource in subsequent render passes.
Used for environment mapping, impostor rendering, shadow mapping and post processing filters.
Requires the ability to use a texture map as a render target

Rendering command
An instruction for rasterizing geometry in a 3D graphics pipeline, typically held in a command buffer, or submitted programatically through a rendering API

Rendering primitive
geometry that can be drawn by a rasterizer or graphics processing unit, connecting vertices, e.g. points, lines, triangles, quadrilaterals
Rendering resources
Data managed by a graphics API, typically held in device memory, including vertex buffers, index buffers, texture maps and framebuffers
Repeating texture
A texture map applied with wrap-round UV coordinates extending between the 0-1 range (representing one unit of the image), exhibiting periodicity.
Contrasts with clamped, mirrored modes or unique mappings.

Resource
Data (often held in a buffer managed by a rendering API) read by a graphics pipeline, e.g. texture maps, vertex buffers, shaders, index buffers, or other pieces of 3D model data.

Rounding radius
a value used in smoothing the corners of a geometric figure such as a 2D polygon or 3D polygon
mesh


==
S ==
Scene graph
Data structure commonly used to represent a 3D scene to be rendered as a directed acyclic graph.

Screen space
The coordinate space of the resulting 2D image during 3D rendering.
The result of 3D projection on geometry in camera space.

Screen space ambient occlusion (SSAO)
Technique of approximating ambient occlusion in screen space.

Screen space directional occlusion
An enhancement of Screen space ambient occlusion (SSAO) taking direction into account to sample the ambient light, to better approximate global illumination.

Shader
a subroutine written in a shading language describing: vertex transformations, skinning, and possibly vertex lighting (in vertex shaders); shading calculations (in pixel shaders); control over tessellation(tessellation shaders); or general purpose computation.

Shading calculation
surface lighting and texturing blending operations, e.g. including specularity, bump mapping etc.

Shadow buffer
A synonym for shadow map.

Shadow map
A texture buffer holding depth values rendered in a separate render pass from the perspective of a lightsource, used in Shadow mapping; it is typically rendered onto other geometry in the main rendering pass.

Shadow volume
One of the techniques of adding shadows to 3D scenes.

Signed triangle area
Found using half the Z component of cross product of a pair of screen-space triangle edge vectors, useful for backface culling and computing parameter gradients in triangle rasterization.

Skybox
Method of creating background for a 3D scene by enclosing it in a textured cuboid (or another environment map).

Sliverous triangle
Sliver triangle
A triangle with one or two extremely acute angles, hence a long/thin shape, which has undesirable properties during some interpolation or rasterization processes.

Software renderer
Rendering software that doesn't use specialized hardware (a GPU) for its computations, i.e. only uses CPU for rendering.

Sparse texture
A texture that can partially reside in the video memory to reduce video memory usage and loading time.

Spatial hashing
A form of hashing to accelerate spatial testing e.g. for AI, collision detection, typically using a grid cell index as a key.

Specular exponent
Controls the glossiness in the phong shading model.

Specular higlights
In shading, specular highlight is a bright higlight caused by specular reflections, more prominent on metallic surfaces.
These highlights depend on the viewer's position as well as the position of the light source and surface normal.

Spline
A curve defined by polynomial interpolation through control points.

Sprite
2D image moving on the screen, with potential partial transparency and/or animation.

State changes
the passing of changes in render states in a graphics pipeline, incurring a performance overhead.
This overhead is typically minimised by scene sorting.

Stencil buffer
A buffer storing an integer value for each according screen pixel, used e.g. to mask out specific operations and achieve specific effects.

Stereo rendering
Rendering the view twice separately for each eye in order to present depth.

Surface normal vector
In shading calculations, the normal to a 3D model surface, typically compared with the light and view vectors to compute the resulting visible colour.
Also used for displacement mapping.

Swizzled texture
A texture map stored out of the natural pixel order; see Swizzling (computer graphics).
For example, it may be stored in morton order, giving improved cache coherency for 2D memory access patterns.

==
T ==
Terrain rendering
Rendering of landscapes, typically using heightmaps or voxels.

Tessellation
Converting a general 3D surface into polygonal representation, important because of HW being optimized for rendering polygons.

Texel
Texture element, a pixel of a texture.

Texture cache
A specialised read-only cache in a graphics processing unit for buffering texture map reads, accelerating texture sampling operations.

Texture sampling
The process of texture lookup with texture filtering.
Performed by a texture sampling unit in a graphics processing unit

Texture sampling unit
A fixed function unit performing texture sampling; also known as a texture mapping unit.

Texture buffer
A region of memory (or resource) used as both a render target and a texture map.

Texture map
A bitmap image/rendering resource used in texture mapping, applied to 3D models and indexed by UV mapping for 3D rendering.

Texture space
The coordinate space of a texture map, usually corresponding to UV coordinates in a 3D model.
Used for some rendering algorithms such as texture space diffusion
Transform feedback
A feature of a rendering pipeline where transformed vertices may be written back to a buffer for later use (e.g. for re-use in additional render passes or subsequent rendering commands), e.g. caching the result of skeletal animation for use in shadow rendering.

Triangulation
the process of turning arbitrary geometric models into triangle primitives, suitable for algorithms requiring triangle meshes

Triangle primitive
The most common rendering primitive defining triangle meshes, rendered by graphics processing units

Triangle setup
the process of ordering triangle primitive vertices, calculating signed triangle area and parameter gradients between vertex attributes as a prerequisite for rasterization.

Triangle setup unit
A fixed function unit in a GPU performing triangle setup (and may perform backface culling), prior to actual rasterization.

Trilinear filtering
Extension of bilinear filtering that additionally linearly interpolates between different Mipmap levels of the texture, eliminating sharp transitions.

Triple buffering
Improvement of double buffering for extra performance by adding another back buffer.

Tristrip
A common rendering primitive defining a sequence of adjacent triangle primitives, where each triangle re-uses 2 vertices from the previous one.

Trivial accept
The process of accepting an entire rendering primitive, 3D model, or bounding volume contents without further tests for clipping or occlusion culling.
The opposite of trivial rejection.

Trivial rejection
Rejecting a rendering primitive or 3D model based on a cheap calculation performed early in a graphics pipeline, (e.g. using outcodes in clipping).
The opposite of trivial accept.

==
U ==
UV unwrapping
The process of flattening a 3D model's surface into a flat 2D plane in a contiguous, spatially coherent manner for texture mapping.

Unified memory
A memory architecture where the CPU and GPU share the same address space, and often the same physical memory.
More common in SoCs and video game consoles.
Supported on some discrete GPUs with the use of an MMU.

UV coordinates
Coordinates in texture space, assigned as vertex attributes and/or
calculated in vertex shaders, used for texture lookup, defining the mapping from texture space to a 3D model surface or any rendering primitive.

==
V ==
Vector graphics
Graphics represented as a set of geometrical primitives.

Vector maths library
A library defining mathematical operations on vector spaces used in 3D graphics, concentrating on 3D and 4D vectors, and 4x4 matrices, often with optimised SIMD implementations.

Vertex buffer
A rendering resource managed by a rendering API holding vertex data.
May be connected by primitive indices to assemble rendering primitives such as triangle strips.
Also known as a Vertex buffer object in OpenGL.

Vertex cache
A specialised read-only cache in a graphics processing unit for buffering indexed vertex buffer reads.

Vertex shader
Shader processing vertices of a 3D model.

View transformation
A matrix transforming world space coordinates into camera space.

View vector
In shading calculations, a 3D unit vector between the camera and the point of interest on a surface.

View frustum
A truncated pyramid enclosing the subset of 3D space that projects onto a 'viewport' (a rectangular region in screen space, usually the entire screen).

Virtual reality
Computer-rendered content
that (unlike augmented reality) completely replaces the user's view of the real world.

Volume texture
A type of texture map with 3 dimensions.

Voxel
An extension of pixels into 3 dimensions.

Vulkan
High performance, low level graphics API by Khronos Group.

VSync
Vertical synchronization, synchronizes the rendering rate with the monitor refresh rate in order to prevent displaying only partially updated frame buffer, which is disturbing especially with horizontal camera movement.

==
W ==
W buffering
A depth buffer storing inverse depth values, which has some advantages for interpolation and precision scaling.

Weight map
A set of Vertex attributes controlling deformation of a 3D model during skeletal animation.
Per-vertex weights are assigned to control the influence of multiple bones (achieved by interpolating the transformations from each).

Window
A rectangular region of a screen or bitmap image.

Wireframe
may refer to Wireframe models or wireframe rendering.

Wireframe rendering
a rendering of a 3D model displaying only edge connectivity; used in 3D modelling applications for greater interactive speed, and clarity for mesh editing.

World space
The global coordinate system in a 3D scene, reached by applying a model transformation matrix from the objects' local coordinates


==
Z ==
Z buffer
a 2D array holding depth values in screen space; a component of a framebuffer; used for hidden surface determination.

Z test culling
a form of occlusion culling by testing bounding volumes against a Z buffer; may be performed by a graphics processing unit using occlusion querys.

Z order
A morton order space filling curve, useful for increasing cache coherency of spatial traversals.

==
References ==
In computing, a distributed cache is an extension of the traditional concept of cache used in a single locale.
A distributed cache may span multiple servers so that it can grow in size and in transactional capacity.
It is mainly used to store application data residing in database and web session data.
The idea of distributed caching has become feasible now because main memory has become very cheap and network cards have become very fast, with 1 Gbit now standard everywhere and 10 Gbit gaining traction.
Also, a distributed cache works well on lower cost machines usually employed for web servers as opposed to database servers which require expensive hardware.

An emerging internet architecture known as Information-centric networking (ICN) is one of the best examples of a distributed cache network.
The ICN is a network level solution hence the existing distributed network cache management schemes are not well suited for ICN.
In the supercomputer environment, distributed cache is typically implemented in the form of burst buffer.

==
Examples ==
Aerospike
Apache Ignite
Couchbase
Ehcache
GigaSpaces
GridGain
Systems
Hazelcast
Infinispan
Memcached
Oracle Coherence
Riak
Redis
SafePeak
Tarantool
Velocity/AppFabric


==
See also ==
Cache algorithms
Cache coherence
Cache-oblivious algorithm
Cache stampede
Cache language model
Database cache
Cache manifest in HTML5
==
References ==
A stacking window manager (also called floating window manager) is a window manager that draws all windows in a specific order, allowing them to overlap, using a technique called painter's algorithm.
All window managers that allow the overlapping of windows but are not compositing window managers are considered stacking window managers, although it is possible that not all use exactly the same methods.
Other window managers that are not considered stacking window managers are those that do not allow the overlapping of windows, which are called tiling window managers.
Stacking window managers allow windows to overlap by drawing them one at a time.
Stacking, or repainting (in reference to painter's algorithm) refers to the rendering of each window as an image, painted directly over the desktop, and over any other windows that might already have been drawn, effectively erasing the areas that are covered.
The process usually starts with the desktop, and proceeds by drawing each window and any child windows from back to front, until finally the foreground window is drawn.
The order in which windows are to be stacked is called their z-order.

==
Limitations ==
Stacking is a relatively slow process, requiring the redrawing of every window one-by-one, from the rear-most and outer-most to the front most and inner-most.
Many stacking window managers don't always redraw background windows.
Others can detect when a redraw of all windows is required, as some applications request stacking when their output has changed.
Re-stacking is usually done through a function call to the window manager, which selectively redraws windows as needed.
For example, if a background window is brought to the front, only that window should need to be redrawn.

A well-known disadvantage of stacking is that when windows are painted over each other, they actually end up erasing the previous contents of whatever part of the screen they are covering.
Those windows must be redrawn when they are brought to the foreground, or when visible parts of them change.
When a window has changed or when its position on the screen has changed, the window manager will detect this and may re-stack all windows, requiring that each window redraw itself, and pass its new appearance along to the window manager before it is drawn.
When an application stops responding, it may be unable to redraw itself, which sometimes causes the area within the window frame to retain images of other windows when it is brought to the foreground.
This problem is commonly seen on Windows XP and earlier, as well as some X window managers.

Another serious limitation that affects almost all stacking window managers is that they are often severely limited in the degree to which the interface can be accelerated by a graphics processing unit (GPU), and very little can be done about this.

===
Avoiding limitations ===
Some technological advances have been able to reduce or remove some of the disadvantages of stacking.
One possible solution to the limited availability of hardware acceleration is to treat a single foreground window as a special case, rendering it differently from other windows.

This does not always require a redesign of the window manager because a foreground window is drawn last, in a known location on the screen, and is not covered by any other windows.
Therefore, it can be easily isolated on the screen after it has been drawn.
For one, since we know where the foreground window is, when the screen raster reaches the graphics hardware, the area occupied by the foreground window can be easily replaced with an accelerated texture.

However, if the window manager is also able to supply an application with an updated image of what the screen looked like before the foreground window was drawn but after all other windows were already drawn more possibilities open up.
This would allow the one window in the foreground to appear semi-transparent, by using the before image as a texture filter on the final output.
This was possible in Windows XP with software included with many NVidia GeForce video cards as well as from third party sources, using a hardware texture overlay.
Another method of lessening the limitations of stacking is through the use of a hardware overlay and chroma keying.
Since the video hardware can draw on the outgoing screen, a window is drawn containing a known colour, which allows the video hardware to detect which parts of the window are showing and should be drawn on.
3D and 2D accelerated video and animation may be added to windows using this method.

Full screen video may also be considered a way of avoiding limitations imposed by stacking.
Full screen mode temporarily suspends the need for any window management, allowing applications to have full access to the video card.
Accelerated 3D games under Windows XP and earlier relied totally on this method, as these games would not have been possible to play in windowed mode.
However technically this method has nothing to do with the window manager, and is simply a means of superseding it.

===
Hybrid window managers ===
Some window managers may be able to treat the foreground window in an entirely different way, by rendering it indirectly, and sending its output to the video card to be added to the outgoing raster.
While this technique may be possible to accomplish within some stacking window managers, it is technically compositing, with the foreground window and
the screen raster being treated the same way two windows would be in a compositing window manager.

As described earlier, we might have access to a slightly earlier stage of stacking where the foreground window has not been drawn yet.
Even if it is later drawn and set to the video card, it is still possible to simply overwrite it entirely at the hardware level with the slightly out of date version, and then create the composite without even having to draw in the original location of the window.
This allows the foreground window to be transparent, or even three dimensional.

Unfortunately interacting with objects outside the original area of the foreground window might also be impossible, since the window manager would not be able to determine what the user is seeing, and would pass such mouse clicks to whatever programs occupied those areas of the screen during the last stacking event.

==
X Window System ==
Many windows managers under the X Window System provide stacking window functionality:


==
Microsoft Windows ==
Microsoft Windows 1.0 displayed windows using a tiling window manager.
In Windows 2.0, it was replaced with a stacking window manager, which allowed windows to overlap.
Microsoft kept the stacking window manager up through Windows XP, which presented severe limitations to its ability to display hardware-accelerated content inside normal windows.
Although it was technically possible to produce some visual effects using third-party software.
From Windows Vista onward, a new compositing window manager is the default on compatible systems.

==
History ==
1970s:
The Xerox Alto which contained the first working commercial GUI used a stacking window manager.

Early 1980s:
The Xerox Star, successor to the Alto, used tiling for most main application windows, and used overlapping only for dialogue windows removing the need for full stacking.

The Classic Mac OS was one of the earliest commercially successful examples of a GUI which used stacking windows.

GEM 1.1 predated Microsoft Windows and used stacking, allowing all windows to overlap.
As a result of a lawsuit by Apple, GEM was forced to remove the stacking capabilities.

Amiga OS contains an early example of a highly advanced stacking window manager.

== See also ==
Window manager
Tiling window manager
Compositing window manager


==
References ==


==
External links ==
Graphical User Interface Gallery
The history of the personal computer as a mass-market consumer electronic device began with the microcomputer revolution of the 1970s.

A personal computer is one intended for interactive individual use, as opposed to a mainframe computer where the end user's requests are filtered through operating staff, or a time-sharing system in which one large processor is shared by many individuals.
After the development of the microprocessor, individual personal computers were low enough in cost that they eventually became affordable consumer goods.
Early personal computers – generally called microcomputers – were sold often in electronic kit form and in limited numbers, and were of interest mostly to hobbyists and technicians.

==
Etymology ==
An early use of the term "personal computer" appeared in a 3 November 1962, New York Times article reporting John W. Mauchly's vision of future computing as detailed at a recent meeting of the Institute of Industrial Engineers.
Mauchly stated, "There is no reason to suppose the average boy or girl cannot be master of a personal computer".
In 1968, a manufacturer took the risk of referring to their product this way, when Hewlett-Packard advertised their "Powerful Computing Genie" as "The New Hewlett-Packard 9100A personal computer".
This advertisement was deemed too extreme for the target audience and replaced with a much drier ad for the HP 9100A programmable calculator.
Over the next seven years, the phrase had gained enough recognition that Byte magazine referred to its readers in its first edition as "[in] the personal computing field", and Creative Computing defined the personal computer as a "non-(time)shared system containing sufficient processing power and storage capabilities to satisfy the needs of an individual user.
"
In 1977, three new pre-assembled small computers hit the markets which Byte would refer to as the "1977 Trinity" of personal computing.
The Apple II and the PET 2001 were advertised as personal computers, while the TRS-80 was described as a microcomputer used for household tasks including "personal financial management".
By 1979, over half a million microcomputers were sold and the youth of the day had a new concept of the personal computer.

==
Overview ==
The history of the personal computer as mass-market consumer electronic devices effectively began in 1977 with the introduction of microcomputers, although some mainframe and minicomputers had been applied as single-user systems much earlier.
A personal computer is one intended for interactive individual use, as opposed to a mainframe computer where the end user's requests are filtered through operating staff, or a time sharing system in which one large processor is shared by many individuals.
After the development of the microprocessor, individual personal computers were low enough in cost that they eventually became affordable consumer goods.
Early personal computers – generally called microcomputers– were sold often in electronic kit form and in limited numbers, and were of interest mostly to hobbyists and technicians.

===
Mainframes, minicomputers, and microcomputers ===
Computer terminals were used for time sharing access to central computers.
Before the introduction of the microprocessor in the early 1970s, computers were generally large, costly systems owned by large corporations, universities, government agencies, and similar-sized institutions.
End users generally did not directly interact with the machine, but instead would prepare tasks for the computer on off-line equipment, such as card punches.
A number of assignments for the computer would be gathered up and processed in batch mode.
After the job had completed, users could collect the results.
In some cases, it could take hours or days between submitting a job to the computing center and receiving the output.

A more interactive form of computer use developed commercially by the middle 1960s.
In a time-sharing system, multiple computer terminals let many people share the use of one mainframe computer processor.
This was common in business applications and in science and engineering.

A different model of computer use was foreshadowed by the way in which early, pre-commercial, experimental computers were used, where one user had exclusive use of a processor.
In places such as Carnegie Mellon University and MIT, students with access to some of the first computers experimented with applications that would today be typical of a personal computer; for example, computer aided drafting was foreshadowed by T-square, a program written in 1961, and an ancestor of today's computer games was found in Spacewar!
in 1962.
Some of the first computers that might be called "personal" were early minicomputers such as the LINC and PDP-8, and later on VAX and larger minicomputers from Digital Equipment Corporation (DEC), Data General, Prime Computer, and others.
By today's standards, they were very large (about the size of a refrigerator) and cost prohibitive (typically tens of thousands of US dollars).
However, they were much smaller, less expensive, and generally simpler to operate than many of the mainframe computers of the time.
Therefore, they were accessible for individual laboratories and research projects.
Minicomputers largely freed these organizations from the batch processing and bureaucracy of a commercial or university computing center.

In addition, minicomputers were relatively interactive and soon had their own operating systems.
The minicomputer Xerox Alto (1973) was a landmark step in the development of personal computers because of its graphical user interface, bit-mapped high resolution screen, large internal and external memory storage, mouse, and special software.
In 1945, Vannevar Bush published an essay called "As We May Think" in which he outlined a possible solution to the growing problem of information storage and retrieval.
In 1968, SRI researcher Douglas Engelbart gave what was later called The Mother of All Demos, in which he offered a preview of things that have become the staples of daily working life in the 21st century: e-mail, hypertext, word processing, video conferencing, and the mouse.
The demo was the culmination of research in Engelbart's Augmentation Research Center laboratory, which concentrated on applying computer technology to facilitate creative human thought.

===
Microprocessor and cost reduction ===
The minicomputer ancestors of the modern personal computer used early integrated circuit (microchip) technology, which reduced size and cost, but they contained no microprocessor.
This meant that they were still large and difficult to manufacture just like their mainframe predecessors.
After the "computer-on-a-chip" was commercialized, the cost to manufacture a computer system dropped dramatically.
The arithmetic, logic, and control functions that previously occupied several costly circuit boards were now available in one integrated circuit, making it possible to produce them in high volume.
Concurrently, advances in the development of solid state memory eliminated the bulky, costly, and power-hungry magnetic core memory used in prior generations of computers.

The basic building block of every microprocessor and memory chip is the metal-oxide-semiconductor field-effect transistor (MOSFET, or MOS transistor), which was originally invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.
The MOSFET made it possible to build high-density integrated circuits, which led to the development of the first microprocessors.
The single-chip microprocessor was made possible by an improvement in MOS technology, the silicon-gate MOS chip, developed in 1968 by Federico Faggin, who later used silicon-gate MOS technology to develop the first single-chip microprocessor, the Intel 4004, in 1971.A few researchers at places such as SRI and Xerox PARC were working on computers that a single person could use and that could be connected by fast, versatile networks: not home computers, but personal ones.
At RCA, Joseph Weisbecker designed and built a true home computer known as FRED, but this saw mixed interest from management.
The CPU design was released as the COSMAC in 1974 and several experimental machines using it were built in 1975, but RCA declined to market any of these until introducing the COSMAC ELF in 1976, in kit form.
By this time a number of other machines had entered the market.

After the 1972 introduction of the Intel 4004, microprocessor costs declined rapidly.
In 1974 the American electronics magazine Radio-Electronics described the Mark-8 computer kit, based on the Intel 8008 processor.
In January of the following year, Popular Electronics magazine published an article describing a kit based on the Intel 8080, a somewhat more powerful and easier to use processor.
The Altair 8800 sold remarkably well even though initial memory size was limited to a few hundred bytes and there was no software available.
However, the Altair kit was much less costly than an Intel development system of the time
and so was purchased by companies interested in developing microprocessor control for their own products.

Expansion memory boards and peripherals were soon listed by the original manufacturer, and later by plug compatible manufacturers.
The very first Microsoft product was a 4 kilobyte paper tape BASIC interpreter, which allowed users to develop programs in a higher-level language.
The alternative was to hand-assemble machine code that could be directly loaded into the microcomputer's memory using a front panel of toggle switches, pushbuttons and LED displays.
While the hardware front panel emulated those used by early mainframe and minicomputers, after a very short time I/O through a terminal was the preferred human/machine interface, and front panels became extinct.

==
The beginnings of the personal computer industry ==
The "brain" [computer]
may one day come down to our level [of the common people] and help with our income-tax and book-keeping calculations.
But this is speculation and there is no sign of it so far.

===
Simon ===
Simon   was a project developed by Edmund Berkeley and presented in a thirteen articles series issued in Radio-Electronics magazine, from October 1950.
Although there were far more advanced machines at the time of its construction, the Simon represented the first experience of building an automatic simple digital computer, for educational purposes.
In fact, its ALU had only 2 bits, and the total memory was 12 bits (
2bits x6).
In 1950, it was sold for US$600.
===
IBM 610 ===
The IBM 610 was designed between 1948 and 1957 by  John Lentz at the Watson Lab at Columbia University as the Personal Automatic Computer (PAC) and announced by IBM as the 610 Auto-Point in 1957.
Although it was faulted for its speed, the IBM 610 handled floating-point arithmetic naturally.
With a price tag of $55,000, only 180 units were produced.

===
Elea Olivetti ===
The Elea 9003 is one of a series of mainframe computers Olivetti developed starting in the late 1950s.
The first prototype was created in 1957.
The system, made entirely with transistors for high performance, was conceived, designed and developed by a small group of researchers led by Mario Tchou (1924–1961).
It was the first solid-state computer designed (it was fully manufactured in Italy).
The knowledge obtained was applied a few years later in the development of the successful Programma 101 electronic calculator.

===
LINC ===
Designed in 1962, the LINC was an early laboratory computer especially designed for interactive use with laboratory instruments.
Some of the early LINC computers were assembled from kits of parts by the end users.

===
Olivetti Programma 101 ===
First produced in 1965, the Programma 101 was one of the first printing programmable calculators.
It was designed and produced by the Italian company Olivetti with Pier Giorgio Perotto being the lead developer.
The Olivetti Programma 101 was presented at the 1965 New York World's Fair after 2 years work (1962- 1964).
Over 44,000 units were sold worldwide; in the US its cost at launch was $3,200.
It was targeted to offices and scientific entities for their daily work because of its high computing capabilities in a small space with a relatively low cost; NASA was amongst its first owners.
Built without integrated circuits or microprocessors, it used only transistors, resistors and condensers for its processing, the Programma 101 had features found in modern personal computers, such as memory, keyboard, printing unit, magnetic card reader/recorder, control and arithmetic unit.
HP later copied the Programma 101 architecture for its HP9100 series.

===
Datapoint 2200 ===
Released in June 1970, the programmable terminal called the Datapoint 2200 is among the earliest known devices that bears significant resemblance to the modern personal computer, with a CRT screen, keyboard, programmability, and program storage.
It was made by CTC (now known as Datapoint) and was a complete system in a case with the approximate footprint of an IBM Selectric typewriter.

The system's CPU was constructed from roughly a hundred (mostly) TTL logic components, which are groups of gates, latches, counters, etc.
The company had commissioned Intel, and also Texas Instruments, to develop a single-chip CPU with that same functionality.
TI designed a chip rather quickly, based on Intel's early drawings.
But their attempt had several bugs and so did not work very well.
Intel's version was delayed and both were a little too slow for CTC's needs.
A deal was made that in return for not charging CTC for the development work, Intel could instead sell the processor as their own product, along with the supporting ICs they had developed.
The first customer was Seiko, which approached Intel early on with this idea, based on what they had seen Busicom do with the 4004.

This became the Intel 8008.
Although it demanded several additional ICs, it is generally known as the first 8-bit microprocessor.
The requirements of the Datapoint 2200 determined the 8008 architecture, which was later expanded into the 8080 and the Z80 upon which CP/M was designed.
These CPUs in turn influenced the 8086, which defined the whole line of "x86" processors used in all IBM-compatible PCs to this day (2020).

Although the design of the Datapoint 2200's TTL based bit serial CPU and the Intel 8008 were technically very different, they were largely software-compatible.
From a software perspective, the Datapoint 2200 therefore functioned as if it were using an 8008.

=== Kenbak-1 ===
The Kenbak-1, released in early 1971, is considered by the Computer History Museum to be the world's first personal computer.
It was designed and invented by John Blankenbaker of Kenbak Corporation in 1970, and was first sold in early 1971.
Unlike a modern personal computer, the Kenbak-1 was built of small-scale integrated circuits, and did not use a microprocessor.
The system first sold for US$750.
Only around 40 machines were ever built and sold.
In 1973, production of the Kenbak-1 stopped as Kenbak Corporation folded.

With only 256 bytes of memory, an 8-bit word size, and input and output restricted to lights and switches, and no apparent way to extend its power, the Kenbak-1 was most useful for learning the principles of programming but not capable of running application programs.
Interestingly, 256 bytes of memory, 8 bit word size, and
I/O limited to switches and lights on the front panel, are also the characteristics of the 1975 Altair 8800, whose fate was diametrically opposed to that of the Kenbak.
The differentiating factor might have been the extensibility of the Altair, without which it was practically useless.

===
Micral N ===
The French company R2E was formed by two former engineers of the Intertechnique company to sell their Intel 8008-based microcomputer design.
The system was developed at the Institut national de la recherche agronomique to automate hygrometric measurements.
The system ran at 500 kHz and included 16 kB of memory, and sold for 8500 Francs, about $1300US.

A bus, called Pluribus, was introduced that allowed connection of up to 14 boards.
Boards for digital I/O, analog I/O, memory, floppy disk were available from R2E.
The Micral operating system was initially called Sysmic, and was later renamed Prologue.

R2E was absorbed by Groupe Bull in 1978.
Although Groupe Bull continued the production of Micral computers, it was not interested in the personal computer market, and Micral computers were mostly confined to highway toll gates (where they remained in service until 1992) and similar niche markets.

===
Xerox Alto and Star ===
The Xerox Alto, developed at Xerox PARC in 1973, was the first computer to use a mouse, the desktop metaphor, and a graphical user interface (GUI),
concepts first introduced by Douglas Engelbart while at International.
It was the first example of what would today be recognized as a complete personal computer.
The first machines were introduced on 1 March 1973.In 1981, Xerox Corporation introduced the Xerox Star workstation, officially known as the "8010 Star Information System".
Drawing upon its predecessor, the Xerox Alto, it was the first commercial system to incorporate various technologies that today have become commonplace in personal computers, including a bit-mapped display, a windows-based graphical user interface, icons, folders, mouse, Ethernet networking, file servers, print servers and e-mail.

While its use was limited to the engineers at Xerox PARC, the Alto had features years ahead of its time.

Both the Xerox Alto and the Xerox Star would inspire the Apple Lisa and the Apple Macintosh.

===
IBM SCAMP ===
In 1972-1973 a team led by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard.
SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130.
In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC.
Because it was the first to emulate APL\1130 performance on a portable, single-user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer".
The prototype is in the Smithsonian Institution.

===
IBM 5100
===
IBM 5100 was a desktop computer introduced in September 1975, six years before the IBM PC.
It was the evolution of SCAMP (Special Computer APL Machine Portable) that IBM demonstrated in 1973.
In January 1978 IBM announced the IBM 5110, its larger cousin.
The 5100 was withdrawn in March 1982.

When the PC was introduced in 1981, it was originally designated as the IBM 5150, putting it in the "5100" series, though its architecture wasn't directly descended from the IBM 5100.

===
Altair 8800 ===
Development of the single-chip microprocessor was the gateway to the popularization of cheap, easy to use, and truly personal computers.
It was only a matter of time before one such design was able to hit a sweet spot in terms of pricing and performance, and that machine is generally considered to be the Altair 8800, from MITS, a small company that produced electronics kits for hobbyists.

The Altair was introduced in a Popular Electronics magazine article in the January 1975 issue.
In keeping with MITS's earlier projects, the Altair was sold in kit form, although a relatively complex one consisting of four circuit boards and many parts.
Priced at only $400, the Altair tapped into pent-up demand and surprised its creators when it generated thousands of orders in the first month.
Unable to keep up with demand, MITS sold the design after about 10,000 kits had shipped.

The introduction of the Altair spawned an entire industry based on the basic layout and internal design.
New companies like Cromemco started up to supply add-on kits, while Microsoft was founded to supply a BASIC interpreter for the systems.
Soon after a number of complete "clone" designs, typified by the IMSAI 8080, appeared on the market.
This led to a wide variety of systems based on the S-100 bus introduced with the Altair, machines of generally improved performance, quality and ease-of-use.

The Altair, and early clones, were relatively difficult to use.
The machines contained no operating system in ROM, so starting it up required a machine language program to be entered by hand via front-panel switches, one location at a time.
The program was typically a small driver for an attached cassette tape reader, which would then be used to read in another "real" program.
Later systems added bootstrapping code to improve this process, and the machines became almost universally associated with the CP/M operating system, loaded from floppy disk.

The Altair created a new industry of microcomputers and computer kits, with many others following, such as a wave of small business computers in the late 1970s based on the Intel 8080, Zilog Z80 and Intel 8085 microprocessor chips.
Most ran the CP/M-80 operating system developed by Gary Kildall at Digital Research.
CP/M-80 was the first popular microcomputer operating system to be used by many different hardware vendors, and many software packages were written for it, such as WordStar and dBase II.
===
Homebrew Computer Club ===
Although the Altair spawned an entire business, another side effect it had was to demonstrate that the microprocessor had so reduced the cost and complexity of building a microcomputer that anyone with an interest could build their own.
Many such hobbyists met and traded notes at the meetings of the Homebrew Computer Club (HCC) in Silicon Valley.
Although the HCC was relatively short-lived, its influence on the development of the modern PC was enormous.

Members of the group complained that microcomputers would never become commonplace if they still had to be built up, from parts like the original Altair, or even in terms of assembling the various add-ons that turned the machine into a useful system.
What they felt was needed was an all-in-one system.
Out of this desire came the Sol-20 computer, which placed an entire S-100 system – QWERTY keyboard, CPU, display card, memory and ports – into an attractive single box.
The systems were packaged with a cassette tape interface for storage and a 12" monochrome monitor.
Complete with a copy of BASIC, the system sold for US$2,100.
About 10,000 Sol-20 systems were sold.

Although the Sol-20 was the first all-in-one system that we would recognize today, the basic concept was already rippling through other members of the group, and interested external companies.

===
Other machines of the era ===
Other 1977 machines that were important within the hobbyist community at the time included the Exidy Sorcerer, the NorthStar Horizon, the Cromemco Z-2, and the Heathkit H8.

== 1977
and the emergence of the "Trinity" ==
By 1976, there were several firms racing to introduce the first truly successful commercial personal computers.
Three machines, the Apple II, PET 2001 and TRS-80 were all released in 1977, becoming the most popular by late 1978.
Byte magazine later referred to them as the "1977 Trinity".
Also in 1977, Sord Computer Corporation released the Sord M200 Smart Home Computer in Japan.

===
Apple II ===
Steve Wozniak (known as "Woz"), a regular visitor to Homebrew Computer Club meetings, designed the single-board Apple I computer and first demonstrated it there.
With specifications in hand and an order for 100 machines at US$500 each from the Byte Shop, Woz and his friend Steve Jobs founded Apple Computer.

About 200 of the machines sold before the company announced the Apple II as a complete computer.
It had color graphics, a full QWERTY keyboard, and internal slots for expansion, which were mounted in a high quality streamlined plastic case.
The monitor and I/O devices were sold separately.
The original Apple II operating system was only the built-in BASIC interpreter contained in ROM.
Apple DOS was added to support the diskette drive; the last version was "Apple DOS 3.3".

Its higher price and lack of floating point BASIC, along with a lack of retail distribution sites, caused it to lag in sales behind the other Trinity machines until 1979, when it surpassed the PET.
It was again pushed into 4th place when Atari introduced its popular Atari 8-bit systems.
Despite slow initial sales, the Apple II's lifetime was about eight years longer than other machines, and so accumulated the highest total sales.

By 1985 2.1 million had sold and more than 4 million Apple II's were shipped by the end of its production in 1993.

===
PET ===
Chuck Peddle designed the Commodore PET (short for Personal Electronic Transactor) around his MOS 6502 processor.
It was essentially a single-board computer with a simple TTL-based CRT driver circuit driving a small built-in monochrome monitor with 40×25 character graphics.
The processor card, keyboard, monitor and cassette drive were all mounted in a single metal case.
In 1982, Byte referred to the PET design as "the world's first personal computer".
The PET shipped in two models; the 2001–4 with 4 kB of RAM, or the 2001–8 with 8 kB.
The machine also included a built-in Datassette for data storage located on the front of the case, which left little room for the keyboard.
The 2001 was announced in June 1977 and the first 100 units were shipped in mid October 1977.
However they remained back-ordered for months, and to ease deliveries they eventually canceled the 4 kB version early the next year.

Although the machine was fairly successful, there were frequent complaints about the tiny calculator-like keyboard, often referred to as a "Chiclet keyboard" due to the keys' resemblance to the popular gum candy.
This was addressed in the upgraded "dash N" and "dash B" versions of the 2001, which put the cassette outside the case, and included a much larger keyboard with a full stroke non-click motion.
Internally a newer and simpler motherboard was used, along with an upgrade in memory to 8, 16, or 32 KB, known as the 2001-N-8, 2001-N-16 or 2001-N-32, respectively.

The PET was the least successful of the 1977 Trinity machines, with under 1 million sales.

=== TRS-80 ===
Tandy Corporation (Radio Shack) introduced the TRS-80, retroactively known as the Model
I as improved models were introduced.
The Model I combined the motherboard and keyboard into one unit with a separate monitor and power supply.
Although the PET and the Apple II offered certain features that were greatly advanced in comparison, Tandy's 3000
+
Radio Shack storefronts ensured that it would have widespread distribution that neither Apple nor Commodore could touch.

The Model I used a Zilog Z80 processor clocked at 1.77 MHz (the later models were shipped with a Z80A processor).
The basic model originally shipped with 4 kB of RAM, and later 16 kB, in the main computer.
The expansion unit allowed for RAM expansion for a total of 48K.
Its other strong features were its full stroke QWERTY keyboard, small size, well written Microsoft floating-point BASIC and inclusion of a monitor and tape deck for approximately half the cost of the Apple II.
Eventually, 5.25 inch floppy drives were made available by Tandy and several third party manufacturers.
The expansion unit allowed up to four floppy drives to be connected, provided a slot for the RS-232 option and a parallel port for printers.

The Model I could not meet FCC regulations on radio interference due to its plastic case and exterior cables.
Apple resolved the issue with an interior metallic foil but the solution would not work for Tandy with the Model I. Since the Model II and Model III were already in
production Tandy decided to stop manufacturing the Model I. Radio Shack had sold 1.5 million Model
I's by the cancellation in 1981.

==
Home computers ==
Byte in January 1980 announced in an editorial that "the era of off-the-shelf personal computers has arrived".
The magazine stated that "a desirable contemporary personal computer has 64 K of memory, about 500 K bytes of mass storage on line, any old competently designed computer architecture, upper and lowercase video terminal, printer, and high-level languages".
The author reported that when he needed to purchase such a computer quickly he did so at a local store for $6000 in cash, and cited it as an example of "what the state of the art is at present ...
as a mass-produced product".
By early that year Radio Shack, Commodore, and Apple manufactured the vast majority of the one half-million microcomputers that existed.
As component prices continued to fall, many companies entered the computer business.
This led to an explosion of low-cost machines known as home computers that sold millions of units before the market imploded in a price war in the early 1980s.

===
Atari 400/800 ===
Atari, Inc. was a well-known brand in the late 1970s, both due to their hit arcade games like Pong, as well as the hugely successful Atari VCS game console.
Realizing that the VCS would have a limited lifetime in the market before a technically advanced competitor came along, Atari decided they would be that competitor, and started work on a new console design that was much more advanced.

While these designs were being developed, the Trinity machines hit the market with considerable fanfare.
Atari's management decided to change their work to a home computer system instead.
Their knowledge of the home market through the VCS resulted in machines that were almost indestructible and just as easy to use as a games machine—simply plug in a cartridge and go.
The new machines were first introduced as the Atari 400 and 800 in 1978, but production problems prevented widespread sales until the next year.

With a trio of custom graphics and sound co-processors and a 6502 CPU clocked ~80% faster than most competitors, the Atari machines had capabilities that no other microcomputer could match.
In spite of a promising start with about 600,000 sold by 1981, they were unable to compete effectively with Commodore's introduction of the Commodore 64 in 1982, and only about 2 million machines were produced by the end of their production run.
The 400 and 800 were tweaked into superficially improved models—the 1200XL, 600XL, 800XL, 65XE—as well as the 130XE with 128K of bank-switched RAM.

===
Sinclair ===
Sinclair Research Ltd is a British consumer electronics company founded by Sir Clive Sinclair in Cambridge.
It was  incorporated in 1973 as Ablesdeal Ltd. and renamed "Westminster Mail Order Ltd" and then
"Sinclair Instrument Ltd." in 1975.
The company remained dormant until 1976, when it was activated with the intention of continuing Sinclair's commercial work from his earlier company Sinclair Radionics; it adopted the name Sinclair Research in 1981.
In 1980, Clive Sinclair entered the home computer market with the ZX80 at £99.95, at the time the cheapest personal computer for sale in the UK.
In 1982 the ZX Spectrum was released, later becoming Britain's best selling computer, competing aggressively against Commodore and British Amstrad.
At the height of its success, and largely inspired by the Japanese Fifth Generation Computer programme, the company established the "MetaLab" research centre at Milton Hall (near Cambridge), in order to pursue artificial intelligence, wafer-scale integration, formal verification and other advanced projects.
The combination of the failures of the Sinclair QL computer and the TV80 led to financial difficulties in 1985, and a year later Sinclair sold the rights to their computer products and brand name to Amstrad.
Sinclair Research Ltd exists today as a one-man company, continuing to market Sir Clive Sinclair's newest inventions.

ZX80
The ZX80 home computer was launched in February 1980 at £79.95 in kit form and £99.95 ready-built.
In November of the same year Science of Cambridge was renamed Sinclair Computers Ltd.

ZX81
The ZX81 (known as the TS 1000 in the United States) was priced at £49.95 in kit form and £69.95 ready-built, by mail order.

ZX Spectrum
The ZX Spectrum was launched on 23 April 1982, priced at £125 for the 16 KB RAM version and £175 for the 48 KB version.

Sinclair QL
The Sinclair QL was announced in January 1984, priced at £399.
Marketed as a more sophisticated 32-bit microcomputer for professional users, it used a Motorola 68008 processor.
Production was delayed by several months, due to unfinished development of hardware and software at the time of the QL's launch.

ZX Spectrum+The ZX Spectrum+ was a repackaged ZX Spectrum 48K launched in October 1984.

ZX Spectrum 128The ZX Spectrum 128, with RAM expanded to 128 kB, a sound chip and other enhancements, was launched in Spain in September 1985 and the UK in January 1986, priced at £179.95.
=== TI-99 ===
Texas Instruments (TI), at the time the world's largest chip manufacturer, decided to enter the home computer market with
the Texas Instruments TI-99/4A. Announced long before its arrival, most industry observers expected the machine to wipe out all competition – on paper its performance was untouchable, and TI had enormous cash reserves and development capability.

When it was released in late 1979, TI took a somewhat slow approach to introducing it, initially focusing on schools.
Contrary to earlier predictions, the TI-99's limitations meant it was not the giant-killer everyone expected, and a number of its design features were highly controversial.
A total of 2.8 million units were shipped before the TI-99/4A was discontinued in March 1984.

=== VIC-20 and Commodore 64 ===
Realizing that the PET could not easily compete with color machines like the Apple II and Atari, Commodore introduced the VIC-20 in 1980 to address the home market.
The tiny 5 kB memory and its relatively limited display in comparison to those machines was offset by a low and ever falling price.
Millions of VIC-20s were sold.

The best-selling personal computer of all time was released by Commodore International in 1982.
The Commodore 64 sold over 17 million units before its end.
The C64 name derived from its 64kb of RAM.
It used the 6510 microprocessor, a variant of the 6502.
MOS Technology, Inc. was then owned by Commodore.

===
BBC Micro ===
The BBC became interested in running a computer literacy series, and sent out a tender for a standardized small computer to be used with the show.
After examining several entrants, they selected what was then known as the Acorn Proton and made a number of minor changes to produce the BBC Micro.
The Micro was relatively expensive, which limited its commercial appeal, but with widespread marketing, BBC support and wide variety of programs, the system eventually sold as many as 1.5 million units.
Acorn was rescued from obscurity, and went on to develop the ARM processor (Acorn RISC Machine) to power follow-on designs.
The ARM is widely used to this day, powering a wide variety of products like the iPhone.
ARM processors also run the world's fastest supercomputer, record set in June 2020 Fugaku ARM Super computer.
The Micro is not to be confused with the BBC Micro Bit, another BBC microcomputer released in March 2016.

===
Commodore/Atari price war and crash ===
The current personal computer market is about the same size as the total potato-chip market.
Next year it will be about half the size of the pet-food market, and is fast approaching the total worldwide sales of panty hose.

In 1982, the TI 99/4A and Atari 400 were both $349, Radio Shack's Color Computer sold at $379, and Commodore had reduced the price of the VIC-20 to $199 and the Commodore 64 to $499.
TI had forced Commodore from the calculator market by dropping the price of its own-brand calculators to less than the cost of the chipsets it sold to third parties to make the same design.
Commodore's CEO, Jack Tramiel, vowed that this would not happen again, and purchased MOS Technology to ensure a supply of chips.
With his supply guaranteed, and good control over the component pricing, Tramiel launched a war against TI soon after the introduction of the Commodore 64.

Now vertically integrated, Commodore lowered the retail price of the 64 to $300 at the June 1983 Consumer Electronics Show, and stores sold it for as little as $199.
At one point the company was selling as many computers as the rest of the industry combined.
Commodore—which even discontinued list prices—could make a profit when selling the 64 for a retail price of $200 because of vertical integration.
Competitors also reduced prices; the Atari 800's price in July was $165, and by the time TI was ready in 1983 to introduce the 99/2 computer—designed to sell for $
99—the TI-99/4A sold for $99 in June.
The 99/4A had sold for $400 in the fall of 1982, causing a loss for TI of hundreds of millions of dollars.
A Service Merchandise executive stated "I've been in retailing 30 years
and I have never seen any category of goods get on a self-destruct pattern like this".
Such low prices probably hurt home computers' reputation; one retail executive said of the 99/4A, '"When they went to $99, people started asking 'What's wrong with it?'"
The founder of Compute!
stated in 1986 that "our market dropped from 300 percent growth per year to 20 percent".
While Tramiel's target was TI, everyone in the home computer market was hurt by the process; many companies went bankrupt or exited the business.
In the end, even Commodore's own finances were crippled by the demands of financing the massive building expansion needed to deliver the machines, and Tramiel was forced from the company.

===
Japanese computers ===
From the late 1970s to the early 1990s, Japan's personal computer market was largely dominated by domestic computer products.
NEC's PC-88 and PC-98 was the market leader, though with some competition from the Sharp X1 and X68000, the FM-7 and FM Towns, and the MSX and MSX2, the latter also gaining some popularity in Europe.
A key difference between Western and Japanese systems at the time was the latter's higher display resolutions (640x400)
in order to accommodate Japanese text.
Japanese computers also employed Yamaha FM synthesis sound boards since the early 1980s which produce higher quality sound.
Japanese computers were widely used to produce video games, though only a small portion of Japanese PC games were released outside of the country.
The most successful Japanese personal computer was NEC's PC-98, which sold more than 18 million units by 1999.

==
The IBM PC ==
IBM responded to the success of the Apple II with the IBM PC, released in August 1981.
Like the Apple II and S-100 systems, it was based on an open, card-based architecture, which allowed third parties to develop for it.
It used the Intel 8088 CPU running at 4.77 MHz, containing 29,000 transistors.
The first model used an audio cassette for external storage, though there was an expensive floppy disk option.
The cassette option was never popular and was removed in the PC XT of 1983.
The XT added a 10MB hard drive in place of one of the two floppy disks and increased the number of expansion slots from 5 to 8.
While the original PC design could accommodate only up to 64k on the main board, the architecture was able to accommodate up to 640KB of RAM, with the rest on cards.
Later revisions of the design increased the limit to 256K on the main board.

The IBM PC typically came with PC DOS, an operating system based upon Gary Kildall's CP/M-80 operating system.
In 1980, IBM approached Digital Research, Kildall's company, for a version of CP/M for its upcoming IBM PC.
Kildall's wife and business partner, Dorothy McEwen, met with the IBM representatives who were unable to negotiate a standard non-disclosure agreement with her.
IBM turned to Bill Gates, who was already providing the ROM BASIC interpreter for the PC.
Gates offered to provide 86-DOS, developed by Tim Paterson of Seattle Computer Products.
IBM rebranded it as PC DOS, while Microsoft sold variations and upgrades as MS-DOS.

The impact of the Apple II and the IBM PC was fully demonstrated when Time named the home computer the "Machine of the Year", or Person of the Year for 1982 (3 January 1983, "The Computer Moves In").
It was the first time in the history of the magazine that an inanimate object was given this award.

===
IBM PC clones ===
The original PC design was followed up in 1983 by the IBM PC XT, which was an incrementally improved design; it omitted support for the cassette, had more card slots, and was available with a 10MB hard drive.
Although mandatory at first, the hard drive was later made an option and a two floppy disk XT was sold.
While the architectural memory limit of 640K was the same, later versions were more readily expandable.

Although the PC and XT included a version of the BASIC language in read-only memory, most were purchased with disk drives and run with an operating system; three operating systems were initially announced with the PC.
One was CP/M-86 from Digital Research, the second was PC DOS from IBM, and the third was the UCSD p-System (from the University of California at San Diego).
PC DOS was the IBM branded version of an operating system from Microsoft, previously best known for supplying BASIC language systems to computer hardware companies.
When sold by Microsoft, PC DOS was called MS-DOS.
The UCSD p-System OS was built around the Pascal programming language and was not marketed to the same niche as IBM's customers.
Neither the p-System nor CP/M-86 was a commercial success.

Because MS-DOS was available as a separate product, some companies attempted to make computers available which could run MS-DOS and programs.
These early machines, including the ACT Apricot, the DEC Rainbow 100, the Hewlett-Packard HP-150, the Seequa Chameleon and many others were not especially successful, as they required a customized version of MS-DOS, and could not run programs designed specifically for IBM's hardware.
(
See List of early non-IBM-PC-compatible PCs.)

The first truly IBM PC compatible machines came from Compaq, although others soon followed.

Because the IBM PC was based on relatively standard integrated circuits, and the basic card-slot design was not patented, the key portion of that hardware was actually the BIOS software embedded in read-only memory.
This critical element got reverse engineered, and that opened the floodgates to the market for IBM PC imitators, which were dubbed "PC clones".
At the time that IBM had decided to enter the personal computer market in response to Apple's early success, IBM was the giant of the computer industry and was expected to crush Apple's market share.
But because of these shortcuts that IBM took to enter the market quickly, they ended up releasing a product that was easily copied by other manufacturers using off the shelf, non-proprietary parts.
So in the long run, IBM's biggest role in the evolution of the personal computer was to establish the de facto standard for hardware architecture amongst a wide range of manufacturers.
IBM's pricing was undercut to the point where IBM was no longer the significant force in development, leaving only the PC standard they had established.
Emerging as the dominant force from this battle amongst hardware manufacturers who were vying for market share was the software company Microsoft that provided the operating system and utilities to all PCs across the board, whether authentic IBM machines or the PC clones.

In 1984, IBM introduced the IBM Personal Computer/AT (more often called the PC/AT or AT) built around the Intel 80286 microprocessor.
This chip was much faster, and could address up to 16MB of RAM but only in a mode that largely broke compatibility with the earlier 8086 and 8088.
In particular, the MS-DOS operating system was not able to take advantage of this capability.

The bus in the PC/
AT was given the name Industry Standard Architecture (ISA).
Peripheral Component Interconnect (PCI) was released in 1992, and was supposed to replace ISA.

VESA
Local Bus (VLB) and Extended ISA were also displaced by PCI, but a majority of later (post-1992) 486-based systems were featuring a VESA Local Bus video card.
VLB importantly offered a less costly high speed interface for consumer systems, as only by 1994 was PCI commonly available outside of the server market.

PCI is later replaced by PCI-E (see below).

==
Apple Lisa and Macintosh ==
In 1983 Apple Computer introduced the first mass-marketed microcomputer with a graphical user interface, the Lisa.
The Lisa ran on a Motorola 68000 microprocessor and came equipped with 1 megabyte of RAM, a 12-inch (300 mm) black-and-white monitor, dual 5¼-inch floppy disk drives and a 5 megabyte Profile hard drive.
The Lisa's slow operating speed and high price (US$10,000), however, led to its commercial failure.

Drawing upon its experience with the Lisa, Apple launched the Macintosh in 1984, with an advertisement during the Super Bowl.
The Macintosh was the first successful mass-market mouse-driven computer with a graphical user interface or 'WIMP' (Windows, Icons, Menus, and Pointers).
Based on the Motorola 68000 microprocessor, the Macintosh included many of the Lisa's features at a price of US$2,495.
The Macintosh was introduced with 128 kb of RAM and later that year a 512 kb RAM model became available.
To reduce costs compared the Lisa, the year-younger Macintosh had a simplified motherboard design, no internal hard drive, and a single 3.5" floppy drive.

Applications that came with the Macintosh included MacPaint, a bit-mapped graphics program, and MacWrite, which demonstrated WYSIWYG word processing.

While not a success upon its release, the Macintosh was a successful personal computer for years to come.
This is particularly due to the introduction of desktop publishing in 1985 through Apple's partnership with Adobe.
This partnership introduced the LaserWriter printer and Aldus PageMaker (now Adobe PageMaker) to users of the personal computer.
During Steve Jobs' hiatus from Apple, a number of different models of Macintosh, including the Macintosh Plus and Macintosh II, were released to a great degree of success.
The entire Macintosh line of computers was IBM's major competition up until the early 1990s.

===
GUIs spread ===
In the Commodore world, GEOS was available on the Commodore 64 and Commodore 128.
Later, a version was available for PCs running DOS.
It could be used with a mouse or a joystick as a pointing device, and came with a suite of GUI applications.
Commodore's later product line, the Amiga platform, ran a GUI operating system by default.
The Amiga laid the blueprint for future development of personal computers with its groundbreaking graphics and sound capabilities.
Byte called it "the first multimedia computer...
so far ahead of its time
that almost nobody could fully articulate what it was all about."

In 1985, the Atari ST, also based on the Motorola 68000 microprocessor, was introduced with the first color GUI: Digital Research's GEM.

In 1987, Acorn launched the Archimedes range of high-performance home computers in Europe and Australasia.
Based on their own 32-bit ARM RISC processor, the systems were shipped with a GUI OS called Arthur.
In 1989, Arthur was superseded by a multi-tasking GUI-based operating system called RISC OS.
By default, the mice used on these computers had three buttons.

== PC clones dominate ==
The transition from a PC-compatible market being driven by IBM to one driven primarily by a broader market began to become clear in 1986 and 1987; in 1986, the 32-bit Intel 80386 microprocessor was released, and the first '386-based PC-compatible was the Compaq Deskpro 386.
IBM's response came nearly a year later with the initial release of the IBM Personal System/2 series of computers, which had a closed architecture and were a significant departure from the emerging "standard PC".
These models were largely unsuccessful, and the PC Clone style machines outpaced sales of all other machines through the rest of this period.
Toward the end of the 1980s PC XT clones began to take over the home computer market segment from the specialty manufacturers such as Commodore International and Atari that had previously dominated.
These systems typically sold for just under the "magic" $1000 price point (typically $999) and were sold via mail order rather than a traditional dealer network.
This price was achieved by using the older 8/16 bit technology, such as the 8088 CPU, instead of the 32-bits of the latest Intel CPUs.
These CPUs were usually made by a third party such as Cyrix or AMD.
Dell started out as one of these manufacturers, under its original name PC Limited.

== 1990s onward ==
===
NeXT ===
In 1990, the NeXTstation workstation computer went on sale, for "interpersonal" computing as Steve Jobs described it.
The NeXTstation was meant to be a new computer for the 1990s, and was a cheaper version of the previous NeXT Computer.
Despite its pioneering use of Object-oriented programming concepts, the NeXTstation was somewhat a commercial failure, and NeXT shut down hardware operations in 1993.

=== CD-ROM ===
In the early 1990s, the CD-ROM became an industry standard, and by the mid-1990s one was built into almost all desktop computers, and towards the end of the 1990s, in laptops as well.
Although introduced in 1982, the CD ROM was mostly used for audio during the 1980s, and then for computer data such as operating systems and applications into the 1990s.
Another popular use of CD ROMs in the 1990s was multimedia, as many desktop computers started to come with built-in stereo speakers capable of playing CD quality music and sounds with the Sound Blaster sound card on PCs.

===
ThinkPad ===
IBM introduced its successful ThinkPad range at COMDEX 1992 using the series designators 300, 500 and 700 (allegedly analogous to the BMW car range and used to indicate market), the 300 series being the "budget", the 500 series "midrange" and the 700 series "high end".
This designation continued until the late 1990s when IBM introduced the "T" series as 600/700 series replacements, and the 3, 5 and 7 series model designations were phased out for A (3&7) & X (5) series.
The A series was later partially replaced by the R series.

===
Dell ===
By the mid-1990s, Amiga, Commodore and Atari systems were no longer on the market, pushed out by strong IBM PC clone competition and low prices.
Other previous competition such as Sinclair and Amstrad were no longer in the computer market.
With less competition than ever before, Dell rose to high profits and success, introducing low cost systems targeted at consumers and business markets using a direct-sales model.
Dell surpassed Compaq as the world's largest computer manufacturer, and held that position until October 2006.

===
Power Macintosh, PowerPC ===
In 1994, Apple introduced the Power Macintosh series of high-end professional desktop computers for desktop publishing and graphic designers.
These new computers made use of new Motorola PowerPC processors  as part of the AIM alliance, to replace the previous Motorola 68k architecture used for the Macintosh line.
During the 1990s, the Macintosh remained with a low market share, but as the primary choice for creative professionals, particularly those in the graphics and publishing industries.

=== ARM ===
In 1994, Acorn Computers launched its Risc PC series of high-end desktop computers.
The Risc PC (codenamed Medusa) was Acorn's next generation ARM-based RISC OS computer, which superseded the Acorn Archimedes.
In 2005, the ARM Cortex-A8 was released, the first Cortex design to be adopted on a large scale for use in consumer devices.
An ARM-based processor is used in the Raspberry Pi, an inexpensive single-board computer.

===
IBM clones, Apple back into profitability ===
Due to the sales growth of IBM clones in the '90s, they became the industry standard for business and home use.
This growth was augmented by the introduction of Microsoft's Windows 3.0 operating environment in 1990, and followed by Windows 3.1 in 1992 and the Windows 95 operating system in 1995.
The Macintosh was sent into a period of decline by these developments coupled with Apple's own inability to come up with a successor to the Macintosh operating system, and by 1996 Apple was almost bankrupt.
In December 1996 Apple bought NeXT and in what has been described as a "reverse takeover", Steve Jobs returned to Apple in 1997.
The NeXT purchase and Jobs' return brought Apple back to profitability, first with the release of Mac OS 8, a major new version of the operating system for Macintosh computers, and then with the PowerMac G3 and iMac computers for the professional and home markets.
The iMac was notable for its transparent bondi blue casing in an ergonomic shape, as well as its discarding of legacy devices such as a floppy drive and serial ports in favor of Ethernet and USB connectivity.
The iMac sold several million units and a subsequent model using a different form factor remains in production as of August 2017.
In 2001 Mac OS X, the long-awaited "next generation" Mac OS based on the NeXT technologies was finally introduced by Apple, cementing its comeback.

===
Writable CDs, MP3, P2P file sharing ===
The ROM in CD-ROM stands for Read Only Memory.
In the late 1990s CD-R and later, rewritable CD-RW drives were included instead of standard CD ROM drives.
This gave the personal computer user the capability to copy and "burn" standard Audio CDs which were playable in any CD player.
As computer hardware grew more powerful and the MP3 format became pervasive, "ripping" CDs into small, compressed files on a computer's hard drive became popular.
"
Peer to peer" file sharing networks such as Napster, Kazaa and Gnutella arose to be used almost exclusively for sharing music files and became a primary computer activity for many individuals.

=== USB, DVD player ===
Since the late 1990s, many more personal computers started shipping that included USB (Universal Serial Bus) ports for easy plug and play connectivity to devices such as digital cameras, video cameras, personal digital assistants, printers, scanners, USB flash drives and other peripheral devices.
By the early 21st century, all shipping computers for the consumer market included at least two USB ports.
Also during the late 1990s DVD players started appearing on high-end, usually more expensive, desktop and laptop computers, and eventually on consumer computers into the first decade of the 21st century.

===
Hewlett-Packard ===
In 2002, Hewlett-Packard (HP) purchased Compaq.
Compaq itself had bought Tandem Computers in 1997 (which had been started by ex-HP employees), and Digital Equipment Corporation in 1998.
Following this strategy HP became a major player in desktops, laptops, and servers for many different markets.
The buyout made HP the world's largest manufacturer of personal computers, until Dell later surpassed HP.

=== 64 bits ===
In 2003, AMD shipped its 64-bit based microprocessor line for desktop computers, Opteron and Athlon 64.
Also in 2003, IBM released the 64-bit based PowerPC 970 for Apple's high-end Power Mac G5 systems.
Intel, in 2004, reacted to AMD's success with 64-bit based processors, releasing updated versions of their Xeon and Pentium 4 lines.
64-bit processors were first common in high end systems, servers and workstations, and then gradually replaced 32-bit processors in consumer desktop and laptop systems since about 2005.
===
Lenovo ===
In 2004, IBM announced the proposed sale of its PC business to Chinese computer maker Lenovo Group, which is partially owned by the Chinese government, for US$650 million in cash and $600 million US in Lenovo stock.
The deal was approved by the Committee on Foreign Investment in the United States in March 2005, and completed in May 2005.
IBM will have a 19% stake in Lenovo, which will move its headquarters to New York State and appoint an IBM executive as its chief executive officer.
The company will retain the right to use certain IBM brand names for an initial period of five years.
As a result of the purchase, Lenovo inherited a product line that featured the ThinkPad, a line of laptops that had been one of IBM's most successful products.

===
Wi-Fi, LCD monitor, flash memory ===
In the early 21st century, Wi-Fi began to become increasingly popular as many consumers started installing their own wireless home networks.
Many of today's laptops and desktop computers are sold pre-installed with wireless cards and antennas.
Also in the early 21st century, LCD monitors became the most popular technology for computer monitors, with CRT production being slowed down.
LCD monitors are typically sharper, brighter, and more economical than CRT monitors.
The first decade of the 21st century also saw the rise of multi-core processors (see following section) and flash memory.
Once limited to high-end industrial use due to expense, these technologies are now mainstream and available to consumers.
In 2008 the MacBook Air and Asus Eee PC were released, laptops that dispense with an optical drive and hard drive entirely relying on flash memory for storage.
===
Local area networks ===
The invention in the late 1970s of local area networks (LANs), notably Ethernet, allowed PCs to communicate with each other (peer-to-peer) and with shared printers.

As the microcomputer revolution continued, more robust versions of the same technology were used to produce microprocessor based servers that could also be linked to the LAN.
This was facilitated by the development of server operating systems to run on the Intel architecture, including several versions of both Unix and Microsoft Windows.
===
Multiprocessing ===
In May 2005, AMD and Intel released their first dual-core 64-bit processors, the Pentium D and the Athlon 64 X2 respectively.
Multi-core processors can be programmed and reasoned about using symmetric multiprocessing (SMP) techniques known since the 60s (see the SMP article for details).

Apple switches to Intel in 2006, also thereby gaining multiprocessing.

In 2013, a Xeon Phi extension card is released with 57 x86 cores, at a price of $1695, equalling circa 30 dollars per core.

===
PCI-E ===
PCI Express is released in 2003.
It becomes the most commonly used bus in PC-compatible desktop computers.

===
Cheap 3D graphics ===
The rise of cheap 3D accelerators displaced low-end products of Silicon Graphics (SGI), which went bankrupt in 2009.

Silicon Graphics was a major 3D business that had grown annual revenues of $5.4 million to $3.7 billion from 1984 to 1997.The addition of 3D graphic capabilities to PCs, and the ability of clusters of Linux- and BSD-based PCs to take on many of the tasks of larger SGI servers, ate into SGI's core markets.

Three former SGI employees had founded 3dfx in 1994.
Their Voodoo Graphics extension card relied on PCI to provide cheap 3D graphics for PC's.
Towards the end of 1996, the cost of EDO DRAM dropped significantly.
A card consisted of a DAC, a frame buffer processor and a texture mapping unit, along with 4 MB of EDO DRAM.
The RAM and graphics processors operated at 50 MHz.
It provided only 3D acceleration and as such the computer also needed a traditional video controller for conventional 2D software.

NVIDIA bought 3dfx in 2000.
In 2000, NVIDIA grew revenues 96%.SGI had made OpenGL.
Control of the specification was passed to the Khronos Group in 2006.

===
SDRAM ===
In 1993, Samsung introduced its KM48SL2000 synchronous DRAM, and by 2000, SDRAM had replaced virtually all other types of DRAM in modern computers, because of its greater performance.
For more information see Synchronous dynamic random-access memory#SDRAM history.

Double data rate synchronous dynamic random-access memory (DDR SDRAM) is introduced in 2000.

Compared to its predecessor in PC-clones, single data rate (SDR) SDRAM, the DDR SDRAM interface makes higher transfer rates possible by more strict control of the timing of the electrical data and clock signals.

===
ACPI ===
Released in December 1996, ACPI replaced Advanced Power Management (APM), the MultiProcessor Specification, and the Plug and Play BIOS (PnP) Specification.
Internally, ACPI advertises the available components and their functions to the operating system kernel using instruction lists ("methods") provided through the system firmware (Unified Extensible Firmware Interface (UEFI) or BIOS), which the kernel parses.
ACPI then executes the desired operations (such as the initialization of hardware components) using an embedded minimal virtual machine.

First-generation ACPI hardware had issues.
Windows 98 first edition disabled ACPI by default except on a whitelist of systems.
==
2010s ==
===
Semiconductor fabrication ===
In 2011, Intel announces the commercialisation of Tri-gate transistor.
The Tri-Gate design is a variant of the FinFET 3D structure.
FinFET was developed in the 1990s by Chenming Hu and his colleagues at UC Berkeley.
Through-silicon via is used in High Bandwidth Memory (HBM), a successor of DDR-SDRAM.
HBM was released in 2013.

In 2016 and 2017, Intel, TSMC and Samsung begin releasing 10 nanometer chips.
At the ≈10 nm scale, quantum tunneling (especially through gaps) becomes a significant phenomenon.

==
Market size ==
In 2001, 125 million personal computers were shipped in comparison to 48,000 in 1977.
More than 500 million PCs were in use in 2002 and one billion personal computers had been sold worldwide since mid-1970s till this time.
Of the latter figure, 75 percent were professional or work related, while the rest sold for personal or home use.
About 81.5 percent of PCs shipped had been desktop computers, 16.4 percent laptops and 2.1 percent servers.
United States had received 38.8 percent (394 million) of the computers shipped, Europe 25 percent and 11.7 percent had gone to Asia-Pacific region, the fastest-growing market as of 2002.
Almost half of all the households in Western Europe had a personal computer and a computer could be found in 40 percent of homes in United Kingdom, compared with only 13 percent in 1985.
The third quarter of 2008 marked the first time laptops outsold desktop PCs in the United States.
As of June 2008, the number of personal computers worldwide in use hit one billion.
Mature markets like the United States, Western Europe and Japan accounted for 58 percent of the worldwide installed PCs.
About 180 million PCs (16 percent of the existing installed base) were expected to be replaced and 35 million to be dumped into landfill in 2008.
The whole installed base grew 12 percent annually.

== See also ==
Timeline of electrical and electronic engineering
Computer museum and Personal Computer Museum
Expensive Desk Calculator
MIT Computer Science and Artificial Intelligence Laboratory
Educ-8
a 1974 pre-microprocessor "micro-computer"
Mark-8, a 1974 microprocessor-based microcomputer
Programma 101, a 1965 programmable calculator with some attributes of a personal computer
SCELBI, another 1974 microcomputer
Simon (computer), a 1949 demonstration of computing principles
List of pioneers in computer science


==
References ==


==
Further reading ==
Veit, Stan (1993).
Stan Veit's History of the Personal Computer.
WorldComm.
p. 304.
ISBN 978-1-56664-030-5.

Douglas K. Smith; Douglas K. Smith; Robert C. Alexander (1999).
Fumbling the Future: How Xerox Invented, then Ignored, the First Personal Computer.
Authors Choice Press.
pp.
276.
ISBN 978-1-58348-266-7.

Freiberger, Paul; Swaine, Michael (2000).
Fire in the Valley:
The Making of The Personal Computer.
McGraw-Hill Companies.
pp.
463.
ISBN 978-0-07-135892-7.

Allan, Roy A. (2001).
A History of the Personal Computer: The People and the Technology.
Allan Publishing.
p. 528.
ISBN 978-0-9689108-0-1.

Sherman, Josepha (2003).
The History of the Personal Computer.
Franklin Watts.
p. 64.
ISBN 978-0-531-16213-2.

Laing, Gordon (2004).
Digital Retro:
The Evolution and Design of the Personal Computer.
Sybex.
p. 192.
ISBN 978-0-7821-4330-0.
==
External links ==
A history of the personal computer:
Archived 2 July 2006 at the Wayback Machine
the people and the technology (PDF)
BlinkenLights Archaeological Institute – Personal Computer Milestones
Personal Computer Museum – A publicly viewable museum in Brantford, Ontario, Canada
Old Computers Museum –
Displaying over 100 historic machines.

Chronology of Personal Computers – a chronology of computers from 1947 on
"Total share: 30 years of personal computer market share figures"
Obsolete Technology – Old Computers
Intel Software Guard Extensions (SGX) is a set of security-related instruction codes that are built into some modern Intel central processing units (CPUs).
They allow user-level as well as operating system code to define private regions of memory, called enclaves, whose contents are protected and unable to be either read or saved by any process outside the enclave itself, including processes running at higher privilege levels.
SGX involves encryption by the CPU of a portion of memory.
The enclave is decrypted on the fly only within the CPU itself, and even then, only for code and data running from within the enclave itself.
The processor thus protects the code from being "spied on" or examined by other code.
The code and data in the enclave utilize a threat model in which the enclave is trusted but no process outside it can be trusted (including the operating system itself and any hypervisor), and therefore all of these are treated as potentially hostile.
The enclave contents are unable to be read by any code outside the enclave, other than in its encrypted form.
Applications running inside of SGX must be written to be side channel resistant as SGX does not protect against side channel measurement or observation.
SGX is designed to be useful for implementing secure remote computation, secure web browsing, and digital rights management (DRM).
Other applications include concealment of proprietary algorithms and of encryption keys.
As of Intel's 11th-generation Tiger Lake and Rocket Lake CPUs, Intel CPUs no longer include SGX.

==
Details ==
SGX was first introduced in 2015 with the sixth generation Intel Core microprocessors based on the Skylake microarchitecture.

Support for SGX in the CPU is indicated in CPUID "Structured Extended feature Leaf", EBX bit 02, but its availability to applications requires BIOS/UEFI support and opt-in enabling which is not reflected in CPUID bits.
This complicates the feature detection logic for applications.
Emulation of SGX was added to an experimental version of the QEMU system emulator in 2014.
In 2015, researchers at the Georgia Institute of Technology released an open-source simulator named "OpenSGX".
One example of SGX used in security was a demo application from wolfSSL using it for cryptography algorithms.

Intel Goldmont
Plus (Gemini Lake) microarchitecture also contains support for Intel SGX.

==
Attacks ==


===
Prime+Probe attack ===
On 27 March 2017 researchers at Austria's Graz University of Technology developed a proof-of-concept that can grab RSA keys from SGX enclaves running on the same system within five minutes by using certain CPU instructions in lieu of a fine-grained timer to exploit cache DRAM side-channels.

One countermeasure for this type of attack was presented and published by Daniel Gruss et al.
at the USENIX Security Symposium in 2017.
Among other published countermeasures, one countermeasure to this type of attack was published on September 28, 2017, a compiler-based tool, DR.SGX, that claims to have superior performance with the elimination of the implementation complexity of other proposed solutions.

M.M.MM
===
Spectre-like attack ===
The LSDS group at Imperial College London showed a proof of concept that the Spectre speculative execution security vulnerability can be adapted to attack the secure enclave.

The Foreshadow attack, disclosed in August 2018, combines speculative execution and buffer overflow to bypass the SGX.

===
Enclave attack ===
On 8 February 2019, researchers at Austria's Graz University of Technology published findings, which showed that in some cases it is possible to run malicious code from within the enclave itself.
The exploit involves scanning through process memory, in order to reconstruct a payload, which can then run code on the system.
The paper claims that due to the confidential and protected nature of the enclave, it is impossible for Antivirus software to detect and remove malware residing within it.
However, since modern anti-malware and antivirus solutions monitor system calls, and the interaction of the application with the operating system, it should be possible to identify malicious enclaves by their behavior, and this issue is unlikely to be a concern for state-of-the-art antiviruses.
Intel issued a statement, stating that this attack was outside the threat model of SGX, that they cannot guarantee that code run by the user comes from trusted sources, and urged consumers to only run trusted code.

===
MicroScope replay attack ===
There is a proliferation of Side-channel attack plaguing modern computer architecture.
Many of these attacks measure slight, nondeterministic variations in the execution of some code, so the attacker needs many, possibly tens of thousands, of measurements to learn secrets.
However, the Microscope attack allows a malicious OS to replay code an arbitrary number of times regardless of the programs actual structure, enabling dozens of side-channel attacks.

===
Plundervolt ===
Security researchers were able to inject timing specific faults into execution within the enclave,
resulting in leakage of information.

The attack can be executed remotely, but requires
access to the privileged control of the processor's voltage and frequency.
===
LVI ===
Load Value Injection injects data into a program aiming to replace the value loaded from memory which is then used for a short time before the mistake is spotted and rolled back, during which LVI controls data and control flow.

===
SGAxe ===
SGAxe, a SGX vulnerability, extends a speculative execution attack on cache, leaking content of the enclave.
This allows an attacker to access private CPU keys used for remote attestation.
In other words, a threat actor can bypass Intel's countermeasures to breach SGX's enclaves confidentiality.
The SGAxe attack is carried out by extracting attestation keys from SGX's private quoting enclave, that are signed by Intel.
The attacker can then masquerade as legitimate Intel machines by signing arbitrary SGX attestation quotes.

== See also ==
Intel MPX
Spectre-NG
Trusted execution environment (TEE)


==
References ==


==
External links ==
Intel Software Guard Extensions (Intel SGX) /
ISA Extensions, Intel
Intel Software Guard Extensions (Intel SGX)
Programming Reference, Intel, October 2014
IDF
2015 - Tech Chat:
A Primer on Intel Software Guard Extensions, Intel (poster)
ISCA
2015 tutorial slides for Intel SGX, Intel, June 2015
McKeen, Frank, et al.
(
Intel), Innovative Instructions and Software Model for Isolated Execution
// Proceedings of the 2nd International Workshop on Hardware and Architectural Support for Security and Privacy.
ACM, 2013.

Jackson, Alon, (PhD dissertation).
Trust is in the Keys of the Beholder:
Extending SGX Autonomy and Anonymity, May 2017.

Joanna Rutkowska, Thoughts on Intel's upcoming Software Guard Extensions (Part 1), August 2013
SGX: the good, the bad and the downright ugly / Shaun Davenport, Richard Ford (Florida Institute of Technology) /
Virus Bulletin, 2014-01-07
Victor Costan and Srinivas Devadas
, Intel SGX Explained, January 2016.

wolfSSL, October 2016.

The Security of Intel SGX for Key Protection and Data Privacy Applications /
Professor Yehuda Lindell (Bar Ilan University & Unbound Tech), January 2018
Intel SGX Technology and the Impact of Processor Side-Channel Attacks, March 2020
How Confidential Computing Delivers A Personalised Shopping Experience, January 2021
A heuristic technique, or a heuristic (; Ancient Greek: εὑρίσκω, heurískō, 'I find, discover')
, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation.
Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.
Heuristics can be mental shortcuts that ease the cognitive load of making a decision.
Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.

==
Overview ==
Heuristics are the strategies derived from previous experiences with similar problems.
These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues.
When an individual applies heuristics in practice, generally performs as expected however
it can alternatively it could create systematic errors.

The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems.
In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification.
Here are a few commonly used heuristics from George Pólya's 1945 book, How to Solve It:
If you are having difficulty understanding a problem, try drawing a picture.

If you can't find a solution, try assuming that you have a solution and seeing what you can derive from that ("working backward").

If the problem is abstract, try examining a concrete example.

Try solving a more general problem first (the "inventor's paradox": the more ambitious plan may have more chances of success).

In psychology, heuristics are simple, efficient rules, learned or inculcated by evolutionary processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex problems or incomplete information.
Researchers test if people use those rules with various methods.
These rules work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.

==
History ==
The study of heuristics in human decision-making was developed in the 1970s and the 1980s by the psychologists Amos Tversky and Daniel Kahneman although the concept had been originally introduced by the Nobel laureate Herbert A. Simon, whose original, primary object of research was problem solving that showed that we operate within what he calls bounded rationality.
He coined the term satisficing, which denotes a situation in which people seek solutions, or accept choices or judgments, that are "good enough" for their purposes although they could be optimized.
Rudolf Groner analyzed the history of heuristics from its roots in ancient Greece up to contemporary work in cognitive psychology and artificial intelligence, proposing a cognitive style "heuristic versus algorithmic thinking," which can be assessed by means of a validated questionnaire.

===
Adaptive toolbox ===
Gerd Gigerenzer and his research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
They study the fast and frugal heuristics in the "adaptive toolbox" of individuals or institutions, and the ecological rationality of these heuristics; that is, the conditions under which a given heuristic is likely to be successful.
The descriptive study of the "adaptive toolbox" is done by observation and experiment, the prescriptive study of the ecological rationality requires mathematical analysis and computer simulation.
Heuristics – such as the recognition heuristic, the take-the-best heuristic, and fast-and-frugal trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
It is often said that heuristics trade accuracy for effort
but this is only the case in situations of risk.
Risk refers to situations where all possible actions, their outcomes and probabilities are known.
In the absence of this information, that is under uncertainty, heuristics can achieve higher accuracy with lower effort.
This finding, known as a less-is-more effect, would not have been found without formal models.
The valuable insight of this program is that heuristics are effective not despite of their simplicity — but because of it.
Furthermore, Gigerenzer and Wolfgang Gaissmaier found that both individuals and organizations rely on heuristics in an adaptive way.

=== Cognitive-experiential self-theory ===
Heuristics, through greater refinement and research, have begun to be applied to other theories, or be explained by them.
For example, the cognitive-experiential self-theory (CEST) also is an adaptive view of heuristic processing.
CEST breaks down two systems that process information.
At some times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
On other occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
From this perspective, heuristics are part of a larger experiential processing system that is often adaptive, but vulnerable to error in situations that require logical analysis.

===
Attribute substitution ===
In 2002, Daniel Kahneman and Shane Frederick proposed that cognitive heuristics work by a process called attribute substitution, which happens without conscious awareness.
According to this theory, when somebody makes a judgment (of a "target attribute") that is computationally complex, a more easily calculated "heuristic attribute" is substituted.
In effect, a cognitively difficult problem is dealt with by answering a rather simpler problem, without being aware of this happening.
This theory explains cases where judgments fail to show regression toward the mean.
Heuristics can be considered to reduce the complexity of clinical judgments in health care.

==
Psychology ==


===
Informal models of heuristics ===
Affect heuristic —
Mental shortcut which uses emotion to influence the decision.
Emotion is the effect that plays the lead role that makes the decision or solves the problem quickly or efficiently.
Is used while judging the risks and benefits of something, depending on the positive or negative feelings that people associate with a stimulus.
Can also be considered the gut decision since if the gut feeling is right, then the benefits are high and the risks are low.
Anchoring and adjustment — Describes the common human tendency to rely more heavily on the first piece of information offered (the "anchor") when making decisions.

For example, in a study done with children, the children were told to estimate the number of jellybeans in a jar.

Groups of children were given either a high or low "base" number (anchor).

Children estimated the number of jellybeans to be closer to the anchor number that they were given.

Availability heuristic — A mental shortcut that occurs when people make judgments about the probability of events by the ease with which examples come to mind.

For example, in a 1973 Tversky & Kahneman experiment, the majority of participants reported that there were more words in the English language that start with the letter K than for which K was the third letter.

There are actually twice as many words in the English Language that have K as the third letter as those that start with K, but words that start with K are much easier to recall and bring to mind.

Balance Heuristic —
Applies to when an individual balances the negative and positive effects from a decision which makes the choice obvious.

Base Rate Heuristic —
When a decision involves probability this is a mental shortcut that uses relevant data to determine the probability of an outcome occurring.
When using this Heuristic there is a common issue where individuals misjudge the likelihood of a situation.
For example, if there is a test for a disease which has an accuracy of 90%, people may think it’s a 90%
they have the disease even though the disease only affects 1 in 500 people.

Common Sense Heuristic --- Used frequently by individuals when the potential outcomes of a decision appear obvious.
For example, when your television remote goes flat, you would change the batteries.
Contagion heuristic — follows the Law of Contagion or Similarity.
This leads people to avoid others that are viewed as “contaminated” to the observer.
This happens due to the fact of the observer viewing something that is seen as bad or to seek objects that have been associated with what seems good.
Somethings one can view as harmful can tend not to really be.
This sometimes leads to irrational thinking on behalf of the observer.

Default Heuristic —
In real world models it is common for consumers to apply this heuristic when selecting the default option regardless of whether the option was their preference.

Educated Guess Heuristic —
When an individual responds to a decision using relevant information they have stored relating to the problem.

Effort heuristic —
the worth of an object is determined by the amount of effort put into the production of the object.
Objects that took longer to produce are more valuable while the objects that took less time are deemed not as valuable.
Also applies to how much effort is put into achieving the product.
This can be seen as the difference of working and earning the object versus finding the object on the side of the street.
It can be the same object but the one found will not be deemed as valuable as the one that we earned.

Escalation of commitment — Describes the phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the cost, starting today, of continuing the decision outweighs the expected benefit.
This is related to the sunk cost fallacy.

Fairness Heuristic —
Applies to the reaction of an individual to a decision from an authoritative figure.
If the decision is enacted in a fair manner the likelihood of the individual to comply voluntarily is higher than if it is unfair.

Familiarity heuristic —
A mental shortcut applied to various situations in which individuals assume that the circumstances underlying the past behavior still hold true for the present situation and that the past behavior thus can be correctly applied to the new situation.
Especially prevalent when the individual experiences a high cognitive load.

Naïve diversification —
When asked to make several choices at once, people tend to diversify more than when making the same type of decision sequentially.

Peak–end rule — experience of an event is judged by the feelings of the peak of the event and nothing more.
Usually not every event is seen as complete but what was felt at the climax whether the event was pleasant or unpleasant to the observer.

All other feelings is not lost but is not used.
This can also include how long the event happened.

Representativeness heuristic —
A mental shortcut used when making judgments about the probability of an event under uncertainty.
Or, judging a situation based on how similar the prospects are to the prototypes the person holds in his or her mind.
For example, in a 1982 Tversky and Kahneman experiment, participants were given a description of a woman named Linda.
Based on the description, it was likely that Linda was a feminist.

Eighty to ninety percent of participants, choosing from two options, chose that it was more likely for Linda to be a feminist and a bank teller than only a bank teller.

The likelihood of two events cannot be greater than that of either of the two events individually.
For this reason, the representativeness heuristic is exemplary of the conjunction fallacy.

Scarcity heuristic — works as the same as the economy.
The scarcer an object or event is, the more value that thing holds.
The abundance is the indicator of the value and is a mental shortcut that places a value on an item based on how easily it might be lost, especially to competitors.
The scarcity heuristic stems from the idea that the more difficult it is to acquire an item the more value that item has.
In many situations we use an item’s availability, its perceived abundance, to quickly estimate quality and/or utility.
This can lead to systemic errors or cognitive bias.

Simulation heuristic — simplified mental strategy in which people determine the likelihood of an event happening based on how easy it is to mentally picture the event happening.
People regret the events that are easier to image over the ones that would be harder to.
It is also thought that people will use this heuristic to predict the likelihood of another's behavior happening.
This shows that people are constantly simulating everything around them in order to be able to predict the likelihood of events around them.
It is believe that people do this by mentally undoing events that they have experienced and then running mental simulations of the events with the corresponding input values of the altered model.

Social proof — also known as the informational social influence which was given its name by Robert Cialdini in his book called Influence written in 1984.
It is where people copy the actions of others in order to attempt to undertake the behavior in a given situation.
It is more prominent in situations were people are unable to determine the appropriate mode of behavior and are driven to the assumption that the surrounding people have more knowledge about the current situation.
This can be see more dominantly in ambiguous social situations.

Working Backward Heuristic — When an individual assumes, they have already solved a problem they work backwards in order to find how to achieve the solution they originally figured out.

===
Formal models of heuristics ===
Elimination by Aspects heuristic
Fast-and-frugal trees
Fluency heuristic
Gaze heuristic
Recognition heuristic
Satisficing
Similarity heuristic
Take-the-best
heuristic


===
Cognitive maps ===
Heuristics were also found to be used in the manipulation and creation of cognitive maps.
Cognitive maps are internal representations of our physical environment, particularly associated with spatial relationships.
These internal representations are used by our memory as a guide in our external environment.
It was found that when questioned about maps imaging, distancing, etc.,
people commonly made distortions to images.
These distortions took shape in the regularization of images (i.e., images are represented as more like pure abstract geometric images, though they are irregular in shape).

There are several ways that humans form and use cognitive maps, with visual intake being an especially key part of mapping: the first is by using landmarks, whereby a person uses a mental image to estimate a relationship, usually distance, between two objects.
The second is route-road knowledge, and is generally developed after a person has performed a task and is relaying the information of that task to another person.
The third is a survey, whereby a person estimates a distance based on a mental image that, to them, might appear like an actual map.
This image is generally created when a person's brain begins making image corrections.
These are presented in five ways:
Right-angle bias:
when a person straightens out an image, like mapping an intersection, and begins to give everything 90-degree angles, when in reality it may not be that way.

Symmetry heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than they really are.

Rotation heuristic: when a person takes a naturally (realistically) distorted image and straightens it out for their mental image.

Alignment heuristic:
similar to the previous, where people align objects mentally to make them straighter than they really are.

Relative-position heuristic: people do not accurately distance landmarks in their mental image based on how well they remember that particular item.
Another method of creating cognitive maps is by means of auditory intake based on verbal descriptions.
Using the mapping based from a person's visual intake, another person can create a mental image, such as directions to a certain location.

==
Philosophy ==
A heuristic device is used when an entity X exists to enable understanding of, or knowledge concerning, some other entity Y.
A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models.
Stories, metaphors, etc.,
can also be termed heuristic in this sense.
A classic example is the notion of utopia as described in Plato's best-known work, The Republic.
This means that the "ideal city" as depicted in The Republic is not given as something to be pursued, or to present an orientation-point for development.
Rather, it shows how things would have to be connected, and how one thing would lead to another (often with highly problematic results), if one opted for certain principles and carried them through rigorously.

Heuristic is also often used as a noun to describe a rule-of-thumb, procedure, or method.
Philosophers of science have emphasized the importance of heuristics in creative thought and the construction of scientific theories.
(
See The Logic of Scientific Discovery by Karl Popper; and philosophers such as Imre Lakatos, Lindley Darden, William C. Wimsatt, and others.)
== Law ==
In legal theory, especially in the theory of law and economics, heuristics are used in the law when case-by-case analysis would be impractical, insofar as "practicality" is defined by the interests of a governing body.
The present securities regulation regime largely assumes that all investors act as perfectly rational persons.

In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.
For instance, in all states in the United States the legal drinking age for unsupervised persons is 21 years, because it is argued that people need to be mature enough to make decisions involving the risks of alcohol consumption.
However, assuming people mature at different rates, the specific age of 21 would be too late for some and too early for others.
In this case, the somewhat arbitrary deadline is used because it is impossible or impractical to tell whether an individual is sufficiently mature for society to trust them with that kind of responsibility.
Some proposed changes, however, have included the completion of an alcohol education course rather than the attainment of 21 years of age as the criterion for legal alcohol possession.
This would put youth alcohol policy more on a case-by-case basis and less on a heuristic one, since the completion of such a course would presumably be voluntary and not uniform across the population.

The same reasoning applies to patent law.
Patents are justified on the grounds that inventors must be protected so they have incentive to invent.
It is therefore argued that it is in society's best interest that inventors receive a temporary government-granted monopoly on their idea, so that they can recoup investment costs and make economic profit for a limited period.
In the United States, the length of this temporary monopoly is 20 years from the date the patent application was filed, though the monopoly does not actually begin until the application has matured into a patent.
However, like the drinking-age problem above, the specific length of time would need to be different for every product to be efficient.
A 20-year term is used because it is difficult to tell what the number should be for any individual patent.
More recently, some, including University of North Dakota law professor Eric E. Johnson, have argued that patents in different kinds of industries – such as software patents – should be protected for different lengths of time.

==
Stereotyping ==
Stereotyping is a type of heuristic that people use to form opinions or make judgments about things they have never seen or experienced.
They work as a mental shortcut to assess everything from the social status of a person (based on their actions), to whether a plant is a tree based on the assumption that it is tall, has a trunk, and has leaves (even though the person making the evaluation might never have seen that particular type of tree before).

Stereotypes, as first described by journalist Walter Lippmann in his book Public Opinion (1922), are the pictures we have in our heads that are built around experiences as well as what we are told about the world.

==
Artificial intelligence ==
A heuristic can be used in artificial intelligence systems while searching a solution space.
The heuristic is derived by using some function that is put into the system by the designer, or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.

==
Critiques and controversies ==
The concept of heuristics has critiques and controversies.
The popular "We Cannot Be That Dumb" critique argues that people would be doomed if it weren't for their ability to make sound and effective judgments.

== See also ==
Algorithm
Behavioral economics
Erudition
Failure mode and effects analysis
Heuristics in judgment and decision-making
List of biases in judgment and decision making
Neuroheuristics
Priority heuristic
Social heuristics


==
References ==


==
Further reading ==
How To Solve It:
Modern Heuristics, Zbigniew Michalewicz and David B. Fogel, Springer Verlag, 2000.
ISBN 3-540-66061-5
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence:
A Modern Approach
(2nd ed.),
Upper Saddle River, New Jersey:
Prentice Hall, ISBN 0-13-790395-2
The Problem of Thinking Too Much, 2002-12-11, Persi Diaconis
In behavioural sciences, social rationality is a type of decision strategy used in social contexts, in which a set of simple rules is applied in complex and uncertain situations.

==
Definition ==
Social rationality is a form of bounded rationality applied to social contexts, where individuals make choices and predictions under uncertainty.
While game theory deals with well-defined situations, social rationality explicitly deals with situations in which not all alternatives, consequences, and  event probabilities can be foreseen.
The idea is that, similar to non-social environments, individuals rely, and should rely, on fast and frugal heuristics in order to deal with complex and  genuinely uncertain social environments.
This emphasis on simple rules in an uncertain world contrasts with the view that the complexity of social situations requires highly sophisticated mental strategies, as has been assumed in primate research and neuroscience, among others.

==
A descriptive and normative program ==
Social rationality is both a descriptive program and a normative program.
The descriptive program studies the repertoire of heuristics an individual or organization uses, that is, their adaptive toolbox.
The normative program studies the environmental conditions to which a heuristic is adapted
, that is, where it performs better than other decision strategies.
This approach is called the study of the ecological rationality of social heuristics.
It assumes that social heuristics are domain- and problem-specific.

==
Applications ==
Heuristics can be applied to social and non-social decision tasks (also called social games and games against nature), judgments, or categorizations.
They can use social or non-social input.
Social rationality is thus about three of the four possible combinations, excluding the case of heuristics using non-social input for non-social tasks. '
Games against nature' comprise situations where individuals face environmental uncertainty, and need to predict or outwit nature, e.g., harvest food or master hard-to-predict or unpredictable hazards. '
Social games' include situations, where the decision outcome depends on the choices of others, e.g., in cooperation, competition, mate search and even in morally significant situations.
Social rationality has been studied in a number of other fields than human decision-making, e.g. in evolutionary social learning, and social learning in animals.

===
Examples ===


====
Imitate-the-majority heuristic ====
An example for a heuristic that is not necessarily social but that requires social input is the imitate-the-majority heuristic, where in a situation of uncertainty, individuals follow the actions or choices of the majority of their peers regardless of their social status.
The domain of pro-environmental behavior provides numerous illustrations for this strategy, such as littering behavior in public places, the reuse of towels in hotel rooms, and changes in private energy consumption in response to information about the consumption of the majority of neighbors.

====
1/N (Equality heuristic) ====
Following the equality heuristic (sometimes called 1/N rule)
people divide and invest their resources equally in a number of N different options.
These options can be both social (e.g., time spent with children) and nonsocial entities (e.g., financial investments or natural resources).
For example, many parents invest their limited resources, such as affection, time, and money (e.g., for education) equally into their offspring.
In highly uncertain environments with large numbers of assets and only few possibilities to learn, the equality heuristic can outperform optimizing strategies and yield better performance on various measures of success than optimal asset allocation strategies.

==
Social heuristics ==
Adapted from Hertwig & Herzog, 2009.

Imitate-the-majority
heuristic
Social circle heuristic
Averaging heuristic
Tit-for-tat
Generous tit-for-tat (or tit-for-two-tat)
Status tree
Regret matching heuristic
Mirror
heuristic
1/N (Equality heuristic)
Group recognition heuristic
White coat heuristic/ Trust your doctor heuristic
Imitate-the-successful heuristic
Plurality
vote-based lexicographic heuristic


== See also ==
Social heuristics
Ecological rationality
Optimization
Risk
Uncertainty
Max Planck Institute for Human Development


==
Notes ==


==
References ==
Cialdini, R. B., Reno, R. R., & Kallgren, C. A. (1990).
A focus theory of normative conduct: Recycling the concept of norms to reduce littering in public places.
Journal of Personality and Social Psychology, 58(6), 1015–1026.

DeMiguel, V., Garlappi, L., & Uppal, R. (2009).
Optimal versus naive diversification: How inefficient ist the 1/N portfolio strategy?
The Review of Financial Studies, 22(5), 1915-1953.

Gigerenzer, G. (2010).
Moral satisficing: Rethinking moral behavior as bounded rationality.
Topics in Cognitive Science, 2(3), 528–554.
doi:10.1111/j.1756-8765.2010.01094.x
Gigerenzer, G., Todd, P., & the ABC Research Group (1999).
Simple heuristics that make us smart.
New York:
Oxford University Press.

Hertwig, R., & Herzog, S. M. (2009).
Fast and frugal heuristics: tools of social rationality.
Social Cognition, 27(5), 661–698.
Retrieved from http://guilfordjournals.com/doi/abs/10.1521/soco.2009.27.5.661
Hertwig, R. Hoffrage, U. & the ABC Research Group (2012).
Simple heuristics in a social world.
New York:
Oxford University Press.

Hertwig, R. & Hoffrage, U. (2012). "
Simple heuristics:
The foundations of adaptive social behavior".

In R. Hertwig, U. Hoffrage, & the ABC Research Group (ed.).
Simple heuristics in a social world (PDF).
New York:
Oxford University Press.
pp.
3–33.
doi:10.1093
/acprof:oso/9780195388435.001.0001.CS1
maint:
multiple names: authors list (link)
Morgan, T. J. H.; Rendell, L. E.; Ehn, M.; Hoppitt, W.; Laland, K. N. (2011-07-27).
"
The evolutionary basis of human social learning".
Proceedings of the Royal Society B: Biological Sciences.
The Royal Society.
279 (1729)
: 653–662.
doi:10.1098/rspb.2011.1172.
ISSN 0962-8452.
PMC 3248730.
PMID 21795267.

Rieucau, G., & Giraldeau, L.-A. (2011).
Exploring the costs and benefits of social information use: An appraisal of current experimental evidence.
Philosophical Transactions of the Royal Society B, 366(1567), 949–957.
doi:10.1098/
rstb.2010.0325
Seymour, B., & Dolan, R. (2008).
Emotion, decision making, and the amygdala.
Neuron, 58, 662–671.

Schultz, P. W., Nolan, J. M., Cialdini, R. B., Goldstein, N. J., & Griskevicius, V. (2007).
The constructive, destructive, and reconstructive power of social norms.
Psychological Science, 18(5), 429–434.

Simon, Herbert A. (1956).
Rational choice and the structure of the environment.
Psychological Review, 63(2), 129–138.
Avram Joel Spolsky (born 1965) is a software engineer and writer.
He is the author of Joel on Software, a blog on software development, and the creator of the project management software Trello.
He was a Program Manager on the Microsoft Excel team between 1991 and 1994.
He later founded Fog Creek Software in 2000 and launched the Joel on Software blog.
In 2008, he launched the Stack Overflow programmer Q&A site in collaboration with Jeff Atwood.
Using the Stack Exchange software product which powers Stack Overflow, the Stack Exchange Network now hosts over 170 Q&A sites.

==
Biography ==
Spolsky was born to Jewish parents and grew up in Albuquerque, New Mexico, and lived there until he was 15.
He then moved with his family to Israel, where he attended high school and completed his military service in the Paratroopers Brigade.
He was one of the founders of the kibbutz Hanaton in Lower Galilee.
In 1987, he returned to the United States to attend college.
He studied at the University of Pennsylvania for a year before transferring to Yale University, where he was a member of Pierson College and graduated in 1991 with a BS summa cum laude in Computer Science.
Spolsky started working at Microsoft in 1991 as a Program Manager on the Microsoft Excel team, where he designed Excel Basic and drove Microsoft's Visual Basic for Applications strategy.
He moved to New York City in 1995 where he worked for Viacom and Juno Online Services.
In 2000, he founded Fog Creek Software and created the Joel on Software blog.
Joel on Software was "one of the first blogs set up by a business owner".
In 2005, Spolsky co-produced and appeared in Aardvark'd:
12 Weeks with Geeks, a documentary documenting Fog Creek's development of Project Aardvark, a remote assistance tool.
In 2008, Spolsky co-founded Stack Overflow, a question and answer community website for software developers, with Jeff Atwood.
He served as CEO of the company until Prashanth Chandrasekar succeeded him in the role on October 1, 2019.
Spolsky remains the company's Chairman.
In 2011, Spolsky launched Trello, an online project management tool inspired by Kanban methodology.
In 2016, Spolsky announced the appointment of Anil Dash as Fog Creek Software's new CEO, with Spolsky continuing as Stack Overflow's CEO and as a Fog Creek Software board member.
The company has since been renamed Glitch.
He is the author of five books, including User Interface Design for Programmers and Smart and Gets Things Done.
He is also the creator of "The Joel Test".
Spolsky coined the term fix it twice for a process improvement method.
It implies a quick, immediate solution for fixing an incident and a second, slower fix for preventing the same problem from occurring again by targeting the root cause.
His use of the term Shlemiel the painter's algorithm, referring to an algorithm that is not scalable due to performing too many redundant actions, was described by salon.com's Scott Rosenberg as an example of good writing "about their insular world in a way that wins the respect of their colleagues and the attention of outsiders.
"Spolsky made an appearance at the WeAreDevelopers Conference 2017, stating how developers are writing the script for the future.
In his speech, Spolsky talks about how software is eating the world, how it is becoming more evident in everyday life as people interact with more software on a day-to-day basis, and how developers are helping to shape how the world will work as technology keeps evolving.
He uses the metaphor "
we are just little vegetables floating in software soup", referring to our constant use of software for the most mundane activities, including work, social networking, and even taking a cab.

In December 2019, Spolsky revealed he was the Chairman of an open-source simulation startup called HASH.

===
Personal life ===
In 2015, Spolsky announced his marriage to his husband, Jared, on social media and his blog.
He lives on the Upper West Side of Manhattan.

==
Schlemiel the Painter's algorithm ==
In software development, a Shlemiel the painter's algorithm (sometimes, Shlemiel the painter algorithm, not to be confused with "Painter's algorithm") is a method that is inefficient because the programmer has overlooked some fundamental issues at the very lowest levels of software design.
The term was coined in 2001 by Spolsky, who used a Yiddish joke to illustrate a certain poor programming practice: Schlemiel (also rendered Shlemiel) is to paint the dotted lines down the middle of a road.
Each day, Schlemiel paints less than he painted the day before, and complains that it is because each day he gets farther away from the paint can, and it takes him longer to go back and put paint on his brush.
The inefficiency to which Spolsky was drawing an analogy was the poor programming practice of repeated concatenation of C-style null-terminated strings.
The first step in every implementation of the C standard library function for concatenating strings is determining the length of the first string by checking each character to see whether it is the terminating null character.
Next, the second string is copied to the end of the first.

In Spolsky's example, the "Schlemiels" occur when multiple strings are concatenated together:
After "Paul" has been appended to "John", the length of "JohnPaul" (or, more precisely, the position of the terminating null character) is known within the scope of strcat() but is discarded upon the end of function.
Afterwards, when strcat() is told to append "George" to "JohnPaul", strcat() starts at the very first character of "JohnPaul" (which is "J") all over again just to find the terminating null character.
Each subsequent call to strcat() has to compute the length again before concatenating another name to the buffer.
Analogous to Schlemiel not carrying the paint bucket (or the string's length) with him, all the subsequent strcat()s have to "walk" the length of the string again to determine where the second string should be copied.
As more data is added to buffer with each call to strcat(), that terminating null character also gets farther away from the beginning, meaning that subsequent calls are increasingly slow.

The problems illustrated by Spolsky's example are not noticed by a programmer who is using a high-level language and has little or no understanding of how the language implementation works, including some basic knowledge of its underlying principles and functions.

==
Publications ==
Spolsky, Joel (2001).
User Interface Design for Programmers.
Apress.
ISBN 1-893115-94-1.

Spolsky, Joel (2004).
Joel on Software:
And on Diverse and Occasionally Related Matters That Will Prove of Interest to Software Developers, Designers, and Managers, and to Those
Who, Whether by Good Fortune or Ill Luck, Work with Them in Some Capacity.
Apress.
ISBN 1-59059-389-8.

Spolsky, Joel (2005).
The Best Software Writing
I:
Selected and Introduced by Joel Spolsky.
Apress.
ISBN 1-59059-500-9.

Spolsky, Joel (2007).
Smart and Gets Things Done:
Joel Spolsky's Concise Guide to Finding the Best Technical Talent.
Apress.
ISBN 978-1-59059-838-2.

Spolsky, Joel (2008).
More Joel on Software:
Further Thoughts on Diverse and Occasionally Related Matters
That Will Prove of Interest to Software Developers, Designers, and to Those
Who, Whether by Good Fortune or Ill Luck, Work with Them in Some Capacity.
Apress.
ISBN 978-1-4302-0987-4.
==
See also ==
LGBT culture in New York City
List of self-identified LGBTQ New Yorkers
Tech companies in the New York metropolitan area
Leaky abstraction


==
References ==


==
External links ==
Joel on Software
Links to essays in 'Best Software Writing
I'
Choice-supportive bias or post-purchase rationalization is the tendency to retroactively ascribe positive attributes to an option one has selected and/or to demote the forgone options.
It is part of cognitive science, and is a distinct cognitive bias that occurs once a decision is made.
For example, if a person chooses option A instead of option B, they are likely to ignore or downplay the faults of option A while amplifying or ascribing new negative faults to option B. Conversely, they are also likely to notice and amplify the advantages of option A and not notice or de-emphasize those of option B.
What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences.
Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways.
In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options.
Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased.
It is believed this may influence our future decision-making.
These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context.
True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference where they came from.

==
General definition ==
The tendency to remember one's choices as better than they actually were.
In this respect, people tend to over attribute positive features to options they chose and negative features to options not chosen.

==
Theory ==
Experiments in cognitive science and social psychology have revealed a wide variety of biases in areas such as statistical reasoning, social attribution, and memory.
Choice-supportive memory distortion is thought to occur during the time of memory retrieval and was the result of the belief that, "I chose this option, therefore it must have been the better option."

Essentially,  after a choice is made people tend to adjust their attitudes to be consistent with, the decision they have already made.
It is also possible that choice-supportive memories arise because an individual is only paying attention to certain pieces of information when making a decision or to post-choice cognitive dissonance.
In addition, biases can also arise because they are closely related to the high level cognitive operations and complex social interactions.
Memory distortions may sometimes serve a purpose because it may be in our interest to not remember some details of an event or to forget others altogether.

==
Making a selection ==
The objective of a choice is generally to pick the best option.
Thus, after making a choice, a person is likely to maintain the belief that the chosen option was better than the options rejected.
Every choice has an upside and a downside.
The process of making a decision mostly relies upon previous experiences.
Therefore, a person will remember not only the decision made but also the reasoning behind making that decision.

===
Motivation ===
Motivation may also play a role in this process because when a person remembers the option that they chose as being the best option, it should help reduce regret about their choice.
This may represent a positive illusion that promotes well-being.
===
Cases when the individual is not in control ===
There are cases where an individual is not always in control of which options are received.
People often end up with options that were not chosen but, instead were assigned by others, such as job assignments made by bosses,
course instructors assigned by a registrar, or vacation spots selected by other family members.
However, being assigned (random or not) to an option leads to a different set of cognitions and memory attributions that tend to favor the alternative (non-received) option and may emphasize regret and disappointment.

Assigned options:
Making a choice or having a choice made for you by other people in your best interest can prompt memory attributions that support that choice.

Current experiments show no choice-supportive memory bias for assigned options.
However, choices which are made on a person's behalf in their best interest do show a tendency for choice-supportive memory bias.

Random selection: People do not show choice-supportive biases when choices are made randomly for them.

This is because choice-supportive memory bias tends to arise during the act of making the decision.

==
How different forms of misremembering create different types of choice-supportive biases ==


===
Misattribution ===
Misattribution is a well-known commission error supported by researchers.
It results in a type of choice-supportive bias when information is attributed to the wrong source.
Consequently, the positive attributes of the forgone option are remembered as the positive attributes of the chosen option, or the negative attributes of the chosen option are remembered as the negative attributes of the foregone option.
For example, if one had to choose between two pairs of trainers and the chosen pair fitted slightly tighter and the forgone option fitted perfectly, the chosen pair would be remembered as fitting perfectly whereas the forgone pair would be remembered as being slightly tighter (although this was not the case in reality)
While misattribution presupposes correct encoding and recall of the information in relation to a person’s decision, the source of the information remains unclear or incorrect.
Therefore it is not to be mistaken with completely false information.

===
Fact distortion ===
Fact distortion results in a type of choice-supportive bias when the facts belonging to the chosen option are remembered in a distorted manner.
The distortion refers to the objective values and/or features of the chosen option being misremembered in a more preferential way to their actual values.
Alternatively, the forgone option can be misremembered as being significantly less preferential then their actual values.
An example of fact distortion would be if you have to choose between buying one out of two cars which can both drive at a maximum speed of 130 mph, the foregone car would be remembered with a maximum speed of 100 mph, whereas the chosen car will be remembered with a maximum speed of 160 mph.
Consequently, the facts concerning both cars have been distorted.

There are varieties of fact distortion that remain contested throughout the literature; as a result, there is no consensus on how it works.
Some authors argue that the distortion mainly occurs when the favoured option is misremembered more preferentially, whereas other researchers argue that memory distortion does not occur or is only likely to take place during the post-decision stage.
Overall, some studies have argued that holding the belief that distortion cannot take place (as soon as the decision is made) means that facts cannot be distorted.

===
Selective forgetting ===
Selective forgetting results in a form of choice-supportive bias when information is selectively forgotten.
In this respect, the positive attributes of the chosen option and the negative attributes of the forgone option are retained in memory at a higher rate and the alternatives are displaced from memory at a faster rate.
An example of selective forgetting would be correctly remembering that your chosen pair of trainers were aesthetically pleasing, but forgetting that they were slightly tight.
This omission error may be related to the limited capacity of memory overtime, otherwise known as transience.

===
False memory ===
False memory in the context of choice-supportive biases is when items that were not part of the original decision are remembered as being presented.
If these entirely new items are positive, they will be remembered as belonging to the chosen option and if they are negative, they will be remembered as belonging to the forgone option.
For example, a chosen pair of shoes might be remembered as good for running, although there was no information presented in respect to the shoes running capabilities.

This type of error is fundamentally different to the other types of misremembering in choice-supportive bias because it is not due to correct encoding and later confusion, but it is due to a completely false memory.
This type of bias means that falsely remembered events can affect future attitudes and possibly decision-making.

==
Potential influential factors ==


===
Alignability ===
Alignability in the context of choice-supportive bias refers to whether both options have directly comparable features.
For example, an alignable characteristic could be whether both options can be measured in centimetres.
In the context of decision making, alignability can influence choices or the ability to make a similarity judgement of a potential option.
The alignment process enables a person to draw similarities and difference which impact their choice-supportive biases.
Research to support this can be displayed by the following example: when given a choice between two brands of popcorn, participants were more likely to choose the one with the superior alignable differences, such as “pops in its own bag” compared with “requires a microwaveable bowl” than the one with superior non-alignable differences, such as “not likely to burn” compared with those containing “some citric acid"


===
Delay ===
The extent of the delay between encoding is a potential factor that could affect choice-supportive bias.
If there is a larger delay between encoding (i.e. viewing the information about the options) and retrieval (i.e. memory tests) it is likely to result in more biased choices rather than the impact of the actual choice on choice-supportive bias.
Some studies have found that the extent of false memories increases over time.
Whereas other researchers have shown that a 2-day delay between making choices and assessment of memory resulted in reasonably high (86%) recognition accuracy.
Therefore, these findings indicate that the influence of delays on choice-supportive bias remain varied, and the influence of delays could affect different types of memory distortions differently.

===
Beliefs about the choices one has made ===
This factor refers to a persons perceived decisions concerning the choices they made, more specifically this includes memories that have been falsified to reflect a selected choice that the person did not actually make.
Research illustrates that people favour the options they think they have chosen and remember the attributes of their "chosen choice" more vividly and favourably.
Essentially, this influences assesses how one believes the choices they made affects their subsequent memories.
As a result, peoples memories are biased in favour of the option they thought they had selected rather than their actual choices.

===
Individual differences ===
Individual differences in choice-supportive bias affect the way a person remembers their options and the way they make decisions and therefore it may influence the degree to which a person engages in choice-supportive bias.
Factors such as age and individual characteristics can influence an individuals cognitive abilities, personality and thus their overall choice-supportive biases.
For example, it has been observed by correlations that people with better performance in tests of frontal or executive functioning were less prone to choice-supportive memory.

==
Relation to self ==
People's conception of who they are, can be shaped by the memories of the choices they make;
the college favored over the one renounced, the job chosen over the one rejected, the candidate elected instead of another one not selected.
Memories of chosen as well as forgone alternatives can affect one's sense of well-being.
Regret for options not taken can cast a shadow, whereas satisfaction at having made the right choice can make a good outcome seem even better.
===
Positive illusions ===
Choice-supportive bias often results in memories that depict the self in an overly favorable light.
In general, cognitive biases loosen our grasp on reality because the line between reality and fantasy can become fuzzy if one's brain has failed to remember a particular event.

Positive illusions are generally mild and are important contributors to our sense of well being.
However, we all need to be aware that they do exist as part of human nature.

==
Memory storage ==
Human beings are blessed with having an intelligent and complex mind, which allows us to remember our past, be able to optimize the present, and plan for the future.
Remembering involves a complex interaction between the current environment, what one expects to remember, and what is retained from the past.
The mechanisms of the brain that allow memory storage and retrieval serves us well most of the time, but occasionally gets us into trouble.

===
Memories change over time ===
There is now abundant evidence that memory content can undergo systematic changes.
After some period of time and if the memory is not used often, it may become forgotten.

Memory retention
: It is recognized that retention is best for experiences that are pleasant, intermediate for experiences that are unpleasant, and worst for experiences that are neutral.
Generic memories provide the basis for inferences that can bring about distortions.
These distortions in memory do not displace an individual's specific memories, but they supplement and fill in the gaps when the memories are lost.
It has been shown that a wide variety of strategic and systematic processes are used to activate different areas of the brain in order to retrieve information.

Credibility of a memory: People have a way to self-check memories, in which a person may consider the plausibility of the retrieved memory by asking themselves is this event even possible.
For example, if a person remembers seeing a pig fly, they must conclude that it was from a dream because pigs cannot fly in the real world.
Memory does not provide people with perfect reproductions of what happened, it only consists of constructions and reconstructions of what happened.

==
Brain areas of interest ==
There is extensive evidence that the amygdala is involved in effectively influencing memory.

Emotional arousal, usually fear based, activates the amygdala and results in the modulation of memory storage occurring in other brain regions.

The forebrain is one of the targets of the amygdala.
The forebrain receives input from amygdala and calculates the emotional significance of the stimulus, generates an emotional response, and transmits it to cerebral cortex.
This can alter the way neurons respond to future input, and therefore cognitive biases, such as choice-supportive bias can influence future decisions.

===
Stress hormones affect memory ===
Effects of stress-related hormones, such as epinephrine and glucocorticoids are mediated by influences involving the amygdala.
It has been shown in experiments with rats that when they are given systemic injections of epinephrine while being trained to perform a task, they show an enhanced memory of performing the task.
In effect the stronger the emotion that is tied to the memory, the more likely the individual is to remember.
Therefore, if the memory is stored and retrieved properly it is less likely to be distorted.

==
Brain mapping ==
A PET scan or fMRI can be used to identify different regions of the brain that are activated during specific memory retrieval.

===
fMRI study ===
True versus false memories
: One study asked subjects to remember a series of events while being monitored by an fMRI to see which areas "light up".
When an individual remembered a greater number of true memories than false memories, it showed a cluster spanning the right superior temporal gyrus and lateral occipital cortex.
However, when the reverse occurred (when an individual remembered a greater number of false memories than true) the brain area that showed activation was the left insula.

These findings may provide some insight as to which areas of the brain are involved with storing memories and later retrieving them.

==
Influence of age ==
Studies now show that as people age, their process of memory retrieval changes.
Although general memory problems are common to everyone because no memory is perfectly accurate, older adults are more likely than younger adults to show choice-supportive biases.

===
Aging of the brain ===
Normal aging may be accompanied by neuropathy in the frontal brain regions.
Frontal regions help people encode or use specific memorial attributes to make source judgments, controls personality and the ability to plan for events.
These areas can attribute to memory distortions and regulating emotion.

===
Regulation of emotion ===
In general, older adults are more likely to remember emotional aspects of situations than are younger adults.
For example, on a memory characteristic questionnaire, older adults rated remembered events as having more associated thoughts and feelings than did younger adults.
As a person ages, regulating personal emotion becomes a higher priority, whereas knowledge acquisition becomes less of a powerful motive.
Therefore, choice-supportive bias would arise because their focus was on how they felt about the choice rather than on the factual details of the options.
Studies have shown that when younger adults are encouraged to remember the emotional aspect of a choice, they are more likely to show choice-supportive bias.

This may be related to older adults' greater tendency to show a positivity effect in memory.

===
Rely on familiarity ===
Older adults rely more than younger adults on categorical or general knowledge about an event to recognize particular elements from the event.
Older adults are also less likely to correctly remember contextual features of events, such as their color or location.
This may be because older adults remember (or rely on) fewer source identifying characteristics than the young.
Consequently, older adults must more often guess or base a response on less specific information, such as familiarity.
As a result, if they can't remember something, they are more likely to fill in the missing gaps with things that are familiar to them.

===
Getting the 'gist' ===
Older adults are more reliant on gist-based retrieval.
A number of studies suggest that using stereotypes or general knowledge to help remember an event is less cognitively demanding than relying on other types of memorial information and thus might require less reflective activity.
This shift towards gist-based processes might occur as a compensation for age decrements in verbatim memory.

===
Inhibition ===
The episodic memory and inhibition accounts of age-related increases in false memories.

Inhibition of a memory may be related to an individual's hearing capacity and attention span.
If the person cannot hear what is going on around them or is not paying much attention, the memory cannot be properly stored and therefore cannot be accurately retrieved.
==
Examples
==
===
Deciding between two used cars ===
Henkel and Mather tested the role of beliefs at the time of retrieval about which option was chosen by giving participants several hypothetical choices like deciding between two used cars.

After making several choices, participants left and were asked to return a week later.
At that point, Henkel and Mather reminded them which option they had chosen for each choice and gave them a list of the features of the two options; some new positive and negative features were mixed in with the old features.
Next, participants were asked to indicate whether each option was new, had been associated with the option they chose, or had been associated with the option they rejected.
Participants favored whichever option Henkel and Mather had told them they had chosen in their memories.
These findings show that beliefs at the time of retrieval about which option was chosen shape both which features are attributed to the options and how vividly they are remembered.

===
Remembering high school grades ===
One study looked at the accuracy and distortion in memory for high school grades.
The relation between accuracy and distortion of autobiographical memory content was examined by verifying 3,220 high school grades recalled by 99 freshman college students.

It was shown that most errors inflated the actual high school grade, meaning that these distortions are attributed to memory reconstructions in a positive and emotionally gratifying direction.
In addition, their findings indicate that the process of distortion does not cause the actual unpleasant memory loss of getting the bad grade.
This is because there was no correlation found between the percentage of accuracy recall and the degree of asymmetry, or distortion.
This shows that the distortion in memories of high school grades arises after the content has been forgotten by another mechanism.

===
A 50 year study of college grades ===
Many similar studies have been performed, such as a fifty-year study of remembering college grades.
In this study one to 54 years after graduating, 276 alumni correctly recalled 3,025 of 3,967 college grades.
The number of omission errors increased with the retention interval and better students made fewer errors.
The accuracy of recall increased with confidence in recall.
Eighty-one percent of errors of commission inflated the actual grade.
These data suggested that distortions occur soon after graduation, remain constant during the retention interval, and are greater for better students and for courses students enjoyed most.
Therefore, sometime in between when the memory is stored and when it is retrieved some time later, the distortion may arise.

== Methods for testing ==


===
Written scenario memory tests ===
Researchers have used written scenarios in which participants are asked to make a choice between two options.
Later, on a memory test, participants are given a list of positive and negative features, some of which were in the scenario and some of which are new.
A choice-supportive bias is seen when both correct and incorrect attributions tend to favor the chosen option, with positive features more likely to be attributed to the chosen option and negative features to the rejected option.

Deception: Henkel and Mather (2007) found that giving people false reminders about which option they chose in a previous experiment session led people to remember the option they were told they had chosen as being better than the other option.
This reveals that choice-supportive biases arise in large part when remembering past choices, rather than being the result of biased processing at the time of the choice.

===
Deese/Roediger–McDermott paradigm ===
The Deese–Roediger–
McDermott paradigm (DRM) consists of a participant listening to an experimenter read lists of thematically related words (e.g. table, couch, lamp, desk); then after some period of time the experimenter will ask if a word was presented in the list.

Participants often report that related but non-presented words (e.g. chair) were included in the encoding series, essentially suggesting that they 'heard' the experimenter say these non-presented words (or critical lures).
Incorrect 'yes' responses to critical lures, often referred to as false memories, are remarkably high under standard DRM conditions.

==
Relation to cognitive dissonance ==
The theory of cognitive dissonance proposes that people have a motivational drive to reduce dissonance.
Choice-supportive bias is potentially related to the aspect of cognitive dissonance explored by Jack Brehm (1956) as postdecisional dissonance.
Within the context of cognitive dissonance, choice-supportive bias would be seen as reducing the conflict between "I prefer X" and "I have committed to Y".

==
Debiasing ==
A study of the Lady
Macbeth effect showed reduced choice-supportive bias by having participants engage in washing.
However, the underlying general effect has not replicated in larger studies


==
See also ==
Attribution (psychology)
Choice
Cognitive dissonance
Confabulation
Confirmation bias
Decision making
Escalation of commitment
List of cognitive biases
List of memory biases
Wishful thinking


==
References ==
In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out, sometimes called swap out, or write to disk, when a page of memory needs to be allocated.
Page replacement happens when a requested page is not in memory (page fault) and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.

When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion.
This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm.
A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.

The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known.

==
History ==
Page replacement algorithms were a hot topic of research and debate in the 1960s and 1970s.

That mostly ended with the development of sophisticated LRU (least recently used) approximations and working set algorithms.
Since then, some basic assumptions made by the traditional page replacement algorithms were invalidated, resulting in a revival of research.
In particular, the following trends in the behavior of underlying hardware and user-level software have affected the performance of page replacement algorithms:
Size of primary storage has increased by multiple orders of magnitude.
With several gigabytes of primary memory, algorithms that require a periodic check of each and every memory frame are becoming less and less practical.

Memory hierarchies have grown taller.
The cost of a CPU cache miss is far more expensive.
This exacerbates the previous problem.

Locality of reference of user software has weakened.
This is mostly attributed to the spread of object-oriented programming techniques that favor large numbers of small functions, use of sophisticated data structures like trees and hash tables that tend to result in chaotic memory reference patterns, and the advent of garbage collection that drastically changed memory access behavior of applications.
Requirements for page replacement algorithms have changed due to differences in operating system kernel architectures.
In particular, most modern OS kernels have unified virtual memory and file system caches, requiring the page replacement algorithm to select a page from among the pages of both user program virtual address spaces and cached files.
The latter pages have specific properties.
For example, they can be locked, or can have write ordering requirements imposed by journaling.
Moreover, as the goal of page replacement is to minimize total time waiting for memory, it has to take into account memory requirements imposed by other kernel sub-systems that allocate memory.
As a result, page replacement in modern kernels (Linux, FreeBSD, and Solaris) tends to work at the level of a general purpose kernel memory allocator, rather than at the higher level of a virtual memory subsystem.

==
Local vs. global replacement ==
Replacement algorithms can be local or global.

When a process incurs a page fault, a local page replacement algorithm selects for replacement some page that belongs to that same process (or a group of processes sharing a memory partition).

A global replacement algorithm is free to select any page in memory.

Local page replacement assumes some form of memory partitioning that determines how many pages are to be assigned to a given process or a group of processes.
Most popular forms of partitioning are fixed partitioning and balanced set algorithms based on the working set model.
The advantage of local page replacement is its scalability: each process can handle its page faults independently, leading to more consistent performance for that process.
However global page replacement is more efficient on an overall system basis.

==
Detecting which pages are referenced and modified ==
Modern general purpose computers and some embedded processors have support for virtual memory.
Each process has its own virtual address space.
A page table maps a subset of the process virtual addresses to physical addresses.
In addition, in most architectures the page table holds an "access" bit and a "dirty" bit for each page in the page table.
The CPU sets the access bit when the process reads or writes memory in that page.
The CPU sets the dirty bit when the process writes memory in that page.
The operating system can modify the access and dirty bits.
The operating system can detect accesses to memory and files through the following means:
By clearing the access bit in pages present in the process' page table.
After some time, the OS scans the page table looking for pages that had the access bit set by the CPU.
This is fast because the access bit it set automatically by the CPU and inaccurate because the OS does not immediately receives notice of the access nor does it have information about the order in which the process accessed these pages.

By removing pages from the process' page table without necessarily removing them from physical memory.
The next access to that page is detected immediately because it causes a page fault.
This is slow because a page fault involves a context switch to the OS, software lookup for the corresponding physical address, modification of the page table and a context switch back to the process and accurate because the access is detected immediately after it occurs.

Directly when the process makes system calls that potentially access the page cache like read and write in POSIX.

==
Precleaning ==
Most replacement algorithms simply return the target page as their result.
This means that if target page is dirty (that is, contains data that have to be written to the stable storage before page can be reclaimed), I/O has to be initiated to send that page to the stable storage (to clean the page).
In the early days of virtual memory, time spent on cleaning was not of much concern, because virtual memory was first implemented on systems with full duplex channels to the stable storage, and cleaning was customarily overlapped with paging.
Contemporary commodity hardware, on the other hand, does not support full duplex transfers, and cleaning of target pages becomes an issue.

To deal with this situation, various precleaning policies are implemented.
Precleaning is the mechanism that starts
I/O on dirty pages that are (likely) to be replaced soon.
The idea is that by the time the precleaned page is actually selected for the replacement, the I/O will complete and the page will be clean.
Precleaning assumes that it is possible to identify pages that will be replaced next.
Precleaning that is too eager can waste I/O bandwidth by writing pages that manage to get re-dirtied before being selected for replacement.
==
Anticipatory paging ==
Some systems use demand paging—waiting until a page is actually requested before loading it into RAM.

Other systems attempt to reduce latency by guessing which pages not in RAM are likely to be needed soon, and pre-loading such pages into RAM, before that page is requested.
(
This is often in combination with pre-cleaning, which guesses which pages currently in RAM are not likely to be needed soon, and pre-writing them out to storage).

When a page fault occurs, "anticipatory paging" systems will not only bring in the referenced page, but also the next few consecutive pages (analogous to a prefetch input queue in a CPU).

The swap prefetch mechanism goes even further in loading pages (even if they are not consecutive) that are likely to be needed soon.

==
The (h,k)-paging problem ==
The (h,k)-paging problem is a generalization of the model of paging problem: Let h,k be positive integers such that 
  
    
      
        h
        ≤
k
      
    
    {\displaystyle h\leq k}
  .
We measure the performance of an algorithm with cache of size
h
        ≤
k
      
    
    {\displaystyle h\leq k}
   relative to the theoretically optimal page replacement algorithm.
If 
  
    
      
        h
        <
k
      
    
    {\displaystyle h<k}
  , we provide the optimal page replacement algorithm with strictly less resource.

The (h,k)-paging problem is a way to measure how an online algorithm performs by comparing it with the performance of the optimal algorithm, specifically, separately parameterizing the cache size of the online algorithm and optimal algorithm.

==
Marking algorithms ==
Marking algorithms is a general class of paging algorithms.
For each page, we associate it with a bit called its mark.
Initially, we set all pages as unmarked.
During a stage of page requests, we mark a page when it is first requested in this stage.
A marking algorithm is such an algorithm that never pages out a marked page.

If ALG is a marking algorithm with a cache of size k, and OPT is the optimal algorithm with a cache of size h, where 
  
    
      
        h
        ≤
        k
      
    
    {\displaystyle h\leq k}
  , then ALG is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
-competitive.
So every marking algorithm attains the 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive ratio.

LRU is a marking algorithm while FIFO is not a marking algorithm.

==
Conservative algorithms ==
An algorithm is conservative, if on any consecutive request sequence containing k or fewer distinct page references, the algorithm will incur k or fewer page faults.

If ALG is a conservative algorithm with a cache of size k, and OPT is the optimal algorithm with a cache of 
  
    
      
        h
        ≤
k
      
    
    {\displaystyle h\leq k}
  , then ALG is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
-competitive.
So every conservative algorithm attains the 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive ratio.

LRU, FIFO and CLOCK are conservative algorithms.

==
Page replacement algorithms ==
There are a variety of page replacement algorithms:


===
The theoretically optimal page replacement algorithm ===
The theoretically optimal page replacement algorithm (also known as OPT, clairvoyant replacement algorithm, or Bélády's optimal page replacement policy) is an algorithm that works as follows: when a page needs to be swapped in, the operating system swaps out the page whose next use will occur farthest in the future.
For example, a page that is not going to be used for the next 6 seconds will be swapped out over a page that is going to be used within the next 0.4 seconds.

This algorithm cannot be implemented in a general purpose operating system because it is impossible to compute reliably how long it will be before a page is going to be used, except when all software that will run on a system is either known beforehand and is amenable to static analysis of its memory reference patterns, or only a class of applications allowing run-time analysis.
Despite this limitation, algorithms exist that can offer near-optimal performance — the operating system keeps track of all pages referenced by the program, and it uses those data to decide which pages to swap in and out on subsequent runs.
This algorithm can offer near-optimal performance, but not on the first run of a program, and only if the program's memory reference pattern is relatively consistent each time it runs.

Analysis of the paging problem has also been done in the field of online algorithms.
Efficiency of randomized online algorithms for the paging problem is measured using amortized analysis.

=== Not recently used ===
The not recently used (NRU) page replacement algorithm is an algorithm that favours keeping pages in memory that have been recently used.
This algorithm works on the following principle: when a page is referenced, a referenced bit is set for that page, marking it as referenced.
Similarly, when a page is modified (written to), a modified bit is set.
The setting of the bits is usually done by the hardware, although it is possible to do so on the software level as well.

At a certain fixed time interval, a timer interrupt triggers and clears the referenced bit of all the pages, so only pages referenced within the current timer interval are marked with a referenced bit.
When a page needs to be replaced, the operating system divides the pages into four classes:

3.
referenced, modified
2.
referenced, not modified
1.
not referenced, modified
0.
not referenced, not modifiedAlthough
it does not seem possible for a page to be modified yet not referenced, this happens when a class 3 page has its referenced bit cleared by the timer interrupt.
The NRU algorithm picks a random page from the lowest category for removal.
So out of the above four page categories, the NRU algorithm will replace a not-referenced, not-modified page if such a page exists.
Note that this algorithm implies that a modified but not-referenced (within the last timer interval) page is less important than a not-modified page that is intensely referenced.

NRU is a marking algorithm, so it is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive.
===
First-in, first-out ===
The simplest page-replacement algorithm is a FIFO algorithm.
The first-in, first-out (FIFO)
page replacement algorithm is a low-overhead algorithm that requires little bookkeeping on the part of the operating system.
The idea is obvious from the name – the operating system keeps track of all the pages in memory in a queue, with the most recent arrival at the back, and the oldest arrival in front.
When a page needs to be replaced, the page at the front of the queue (the oldest page) is selected.
While FIFO is cheap and intuitive, it performs poorly in practical application.
Thus, it is rarely used in its unmodified form.
This algorithm experiences Bélády's anomaly.

In simple words, on a page fault, the frame that has been in memory the longest is replaced.

FIFO page replacement algorithm is used by the VAX/VMS operating system, with some modifications.
Partial second chance is provided by skipping a limited number of entries with valid translation table references, and additionally, pages are displaced from process working set to a systemwide pool from which they can be recovered if not already re-used.

FIFO is a conservative algorithm, so it is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive.
===
Second-chance ===
A modified form of the FIFO page replacement algorithm, known as the Second-chance page replacement algorithm, fares relatively better than FIFO at little cost for the improvement.
It works by looking at the front of the queue as FIFO does, but instead of immediately paging out that page, it checks to see if its referenced bit is set.
If it is not set, the page is swapped out.
Otherwise, the referenced bit is cleared, the page is inserted at the back of the queue (as if it were a new page) and this process is repeated.
This can also be thought of as a circular queue.
If all the pages have their referenced bit set, on the second encounter of the first page in the list, that page will be swapped out, as it now has its referenced bit cleared.
If all the pages have their reference bit cleared, then second chance algorithm degenerates into pure FIFO.

As its name suggests, Second-chance gives every page a "second-chance" – an old page that has been referenced is probably in use, and should not be swapped out over a new page that has not been referenced.
===
Clock ===
Clock is a more efficient version of FIFO than Second-chance because pages don't have to be constantly pushed to the back of the list, but it performs the same general function as Second-Chance.
The clock algorithm keeps a circular list of pages in memory, with the "hand" (iterator) pointing to the last examined page frame in the list.
When a page fault occurs and no empty frames exist, then the R (referenced) bit is inspected at the hand's location.
If R is 0, the new page is put in place of the page the "hand" points to, and the hand is advanced one position.
Otherwise, the R bit is cleared, then the clock hand is incremented and the process is repeated until a page is replaced.
This algorithm was first described in 1969 by F. J. Corbató.

====
Variants of clock ====
GCLOCK:
Generalized clock page replacement algorithm.

Clock-Pro keeps a circular list of information about recently referenced pages, including all M pages in memory as well as the most recent M pages that have been paged out.
This extra information on paged-out pages, like the similar information maintained by ARC, helps it work better than LRU on large loops and one-time scans.

WSclock.
By combining the Clock algorithm with the concept of a working set (i.e., the set of pages expected to be used by that process during some time interval), the performance of the algorithm can be improved.
In practice, the "aging" algorithm and the "WSClock" algorithm are probably the most important page replacement algorithms.

Clock with Adaptive Replacement (CAR) is a page replacement algorithm that has performance comparable to ARC, and substantially outperforms both LRU and CLOCK.
The algorithm CAR is self-tuning and requires no user-specified magic parameters.
CLOCK is a conservative algorithm, so it is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive.
===
Least recently used ===
The least recently used (LRU) page replacement algorithm, though similar in name to NRU, differs in the fact that LRU keeps track of page usage over a short period of time, while NRU just looks at the usage in the last clock interval.
LRU works on the idea that pages that have been most heavily used in the past few instructions are most likely to be used heavily in the next few instructions too.
While LRU can provide near-optimal performance in theory (almost as good as adaptive replacement cache), it is rather expensive to implement in practice.
There are a few implementation methods for this algorithm that try to reduce the cost yet keep as much of the performance as possible.

The most expensive method is the linked list method, which uses a linked list containing all the pages in memory.
At the back of this list is the least recently used page, and at the front is the most recently used page.
The cost of this implementation lies in the fact that items in the list will have to be moved about every memory reference, which is a very time-consuming process.

Another method that requires hardware support is as follows: suppose the hardware has a 64-bit counter that is incremented at every instruction.
Whenever a page is accessed, it acquires the value equal to the counter at the time of page access.
Whenever a page needs to be replaced, the operating system selects the page with the lowest counter and swaps it out.

Because of implementation costs, one may consider algorithms (like those that follow) that are similar to LRU, but which offer cheaper implementations.

One important advantage of the LRU algorithm is that it is amenable to full statistical analysis.
It has been proven, for example, that LRU can never result in more than N-times more page faults than OPT algorithm, where N is proportional to the number of pages in the managed pool.

On the other hand, LRU's weakness is that its performance tends to degenerate under many quite common reference patterns.
For example, if there are N pages in the LRU pool, an application executing a loop over array of N + 1 pages will cause a page fault on each and every access.
As loops over large arrays are common, much effort has been put into modifying LRU to work better in such situations.
Many of the proposed LRU modifications try to detect looping reference patterns and to switch into suitable replacement algorithm, like Most Recently Used (MRU).

====
Variants on LRU ====
LRU-K evicts the page whose K-th most recent access is furthest in the past.
For example, LRU-1 is simply LRU whereas LRU-2 evicts pages according to the time of their penultimate access.
LRU-K improves greatly on LRU with regards to locality in time.

The ARC algorithm extends LRU by maintaining a history of recently evicted pages and uses this to change preference to recent or frequent access.
It is particularly resistant to sequential scans.
A comparison of ARC with other algorithms (LRU, MQ, 2Q, LRU-2, LRFU, LIRS) can be found in Megiddo & Modha
2004.LRU is a marking algorithm, so it is 
  
    
      
        
          
            
              k
              
                k
                −
h
+
1
              
            
          
        
      
    
    {\displaystyle {\dfrac {k}{k-h+1}}}
  -competitive.

===
Random ===
Random replacement algorithm replaces a random page in memory.
This eliminates the overhead cost of tracking page references.
Usually it fares better than FIFO, and for looping memory references it is better than LRU, although generally LRU performs better in practice.
OS/390 uses global LRU approximation and falls back to random replacement when LRU performance degenerates, and the Intel i860 processor used a random replacement policy (Rhodehamel 1989).
===
Not frequently used (NFU) ===
The not frequently used (NFU) page replacement algorithm requires a counter, and every page has one counter of its own which is initially set to 0.
At each clock interval, all pages that have been referenced within that interval will have their counter incremented by 1.
In effect, the counters keep track of how frequently a page has been used.
Thus, the page with the lowest counter can be swapped out when necessary.

The main problem with NFU is that it keeps track of the frequency of use without regard to the time span of use.
Thus, in a multi-pass compiler, pages which were heavily used during the first pass, but are not needed in the second pass will be favoured over pages which are comparably lightly used in the second pass, as they have higher frequency counters.
This results in poor performance.
Other common scenarios exist where NFU will perform similarly, such as an OS boot-up.
Thankfully, a similar and better algorithm exists, and its description follows.

The not frequently used page-replacement algorithm generates fewer page faults than the least recently used page replacement algorithm when the page table contains null pointer values.

===
Aging ===
The aging algorithm is a descendant of the NFU algorithm, with modifications to make it aware of the time span of use.
Instead of just incrementing the counters of pages referenced, putting equal emphasis on page references regardless of the time, the reference counter on a page is first shifted right (divided by 2), before adding the referenced bit to the left of that binary number.
For instance, if a page has referenced bits 1,0,0,1,1,0 in the past 6 clock ticks, its referenced counter will look like this: 10000000, 01000000, 00100000, 10010000, 11001000, 01100100.
Page references closer to the present time
have more impact than page references long ago.
This ensures that pages referenced more recently, though less frequently referenced, will have higher priority over pages more frequently referenced in the past.
Thus, when a page needs to be swapped out, the page with the lowest counter will be chosen.

The following Python code simulates the aging algorithm.

Counters
V
i
          
        
      
    
    {\displaystyle V_{i}}
   are initialized with 
  
    
      
        0
      
    
    {\displaystyle 0}
   and updated as described above via 
  
    
      
        
          V
i
←
(
        
          R
i
≪
(
        k
        −
1
        )
        )
        
          |
(
        
          V
          
            i
≫
1
        )
{\displaystyle V_{i}\leftarrow
(R_{i}\ll (k-1))|(V_{i}\gg 1)}
  , using arithmetic shift operators.

In the given example of R-bits for 6 pages over 5 clock ticks, the function prints the following output, which lists the R-bits for each clock tick
t
      
    
    {\displaystyle t}
   and the individual counter values
V
i
          
        
      
    
    {\displaystyle V_{i}}
   for each page in binary representation.

t  |
R-bits (0-5)        |
Counters for pages 0-5
00
|  [1, 0, 1, 0, 1, 1]  |
[10000000, 00000000,
10000000, 00000000,
10000000, 10000000]
01
|  [1, 1, 0, 0, 1, 0]  |  [11000000, 10000000, 01000000, 00000000, 11000000, 01000000]
02  |  [1, 1, 0, 1, 0, 1]  |  [11100000, 11000000, 00100000, 10000000, 01100000, 10100000]
03  |  [1, 0, 0, 0, 1, 0]  |  [11110000, 01100000, 00010000, 01000000, 10110000, 01010000]
04  |  [0, 1, 1, 0, 0, 0]  |  [01111000, 10110000, 10001000, 00100000, 01011000, 00101000]
Note that aging differs from LRU in the sense that aging can only keep track of the references in the latest 16/32 (depending on the bit size of the processor's integers) time intervals.
Consequently, two pages may have referenced counters of 00000000, even though one page was referenced 9 intervals ago and the other 1000 intervals ago.
Generally speaking, knowing the usage within the past 16 intervals is sufficient for making a good decision as to which page to swap out.
Thus, aging can offer near-optimal performance for a moderate price.
===
Longest distance first (LDF)
page replacement algorithm ===
The basic idea behind this algorithm is Locality of Reference as used in LRU
but the difference is that in LDF, locality is based on distance not on the used references.
In the LDF, replace the page which is on longest distance from the current page.
If two pages are on same distance then the page which is next to current page in anti-clock rotation will get replaced.

==
Implementation details ==


===
Techniques for hardware with no reference bit ===
Many of the techniques discussed above assume the presence of a reference bit associated with each page.

Some hardware has no such bit, so its efficient use requires techniques that operate well without one.

One notable example is VAX hardware running OpenVMS.
This system knows if a page has been modified, but not necessarily if a page has been read.
Its approach is known as Secondary Page Caching.
Pages removed from working sets (process-private memory, generally) are placed on special-purpose lists while remaining in physical memory for some time.
Removing a page from a working set is not technically a page-replacement operation, but effectively identifies that page as a candidate.
A page whose backing store is still valid (whose contents are not dirty, or otherwise do not need to be preserved) is placed on the tail of the Free Page List.
A page that requires writing to backing store will be placed on the Modified Page List.
These actions are typically triggered when the size of the Free Page List falls below an adjustable threshold.

Pages may be selected for working set removal in an essentially random fashion, with the expectation that if a poor choice is made, a future reference may retrieve that page from the Free or Modified list before it is removed from physical memory.
A page referenced this way will be removed from the Free or Modified list and placed back into a process working set.
The Modified Page List additionally provides an opportunity to write pages out to backing store in groups of more than one page, increasing efficiency.
These pages can then be placed on the Free Page List.
The sequence of pages that works its way to the head of the Free Page List resembles the results of a LRU or NRU mechanism and the overall effect has similarities to the Second-Chance algorithm described earlier.

Another example is used by the Linux kernel on ARM.
The lack of hardware functionality is made up for by providing two page tables – the processor-native page tables, with neither referenced bits nor dirty bits, and software-maintained page tables with the required bits present.
The emulated bits in the software-maintained table are set by page faults.
In order to get the page faults, clearing emulated bits in the second table revokes some of the access rights to the corresponding page, which is implemented by altering the native table.

===
Page cache in Linux ===
Linux uses a unified page cache for

brk and anonymous mmaped-regions.
This includes the heap and stack of user-space programs.
It is written to swap when paged out.

Non-anonymous (file-backed) mmaped regions.
If present in memory and not privately modified the physical page is shared with file cache or buffer.

Shared memory acquired through shm_open.

The tmpfs in-memory filesystem; written to swap when paged out.

The file cache including; written to the underlying block storage (possibly going through the buffer, see below) when paged out.

The cache of block devices, called the "buffer" by Linux (not to be confused with other structures
also called buffers like those use for pipes and buffers used internally in Linux);
written to the underlying storage when paged out.
The unified page cache operates on units of the smallest page size supported by the CPU (4 KiB in ARMv8, x86 and x86-64) with some pages of the next larger size (2 MiB in x86-64) called "huge pages" by Linux.
The pages in the page cache are divided in an "active" set and an "inactive" set.
Both sets keep a LRU list of pages.
In the basic case, when a page is accessed by a user-space program it is put in the head of the inactive set.
When it is accessed repeatedly, it is moved to the active list.
Linux moves the pages from the active set to the inactive set as needed so that the active set is smaller than the inactive set.
When a page is moved to the inactive set it is removed from the page table of any process address space, without being paged out of physical memory.
When a page is removed from the inactive set, it is paged out of physical memory.
The size of the "active" and "inactive" list can be queried from /proc/meminfo in the fields "Active", "Inactive", "Active(anon)", "Inactive(anon)", "Active(file)" and "Inactive(anon)".

==
Working set ==
The working set of a process is the set of pages expected to be used by that process during some time interval.

The "working set model" isn't a page replacement algorithm in the strict sense
(it's actually a kind of medium-term scheduler)


==
References ==


==
Further reading ==
Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal.
It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.

In the depth sorting phase of hidden surface removal, if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted.
If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.

In that case Newell's algorithm tests the following:

Test for Z overlap; implied in the selection of the face Q from the sort list
The extreme coordinate values in X of the two faces do not overlap (minimax test in X)
The extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)
All vertices of P lie deeper than the plane of Q
All vertices of Q lie closer to the viewpoint than the plane of P
The rasterisation of P and Q do not overlapThe tests are given in order of increasing computational difficulty.

The polygons must be planar.

If the tests are all false, then switch the order of P and Q in the sort, record having done so, and try again.
If there is an attempt to switch the order of a polygon a second time, there is a visibility cycle, and the polygons must be split.
Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon.
The above tests are again performed, and the algorithm continues until all polygons pass the above tests.

==
References ==
Sutherland, Ivan E.; Sproull, Robert F.; Schumacker, Robert A. (1974), "A characterization of ten hidden-surface algorithms", Computing Surveys, 6 (1): 1–55, CiteSeerX 10.1.1.132.8222, doi:10.1145/356625.356626.

Newell, M. E.; Newell, R. G.; Sancha, T. L. (1972), "A new approach to the shaded picture problem", Proc.
ACM National Conference, pp.
443–450.

== See also ==
Painter's algorithm
Boolean operations on polygons
John Edward Warnock (born October 6, 1940) is an American computer scientist and businessman best known for co-founding Adobe Systems Inc., the graphics and publishing software company, with Charles Geschke.
Warnock was President of Adobe for his first two years and chairman and CEO for his remaining sixteen years at the company.
Although he retired as CEO in 2000, he still co-chaired the board with Geschke.
Warnock has pioneered the development of graphics, publishing, Web and electronic document technologies that have revolutionized the field of publishing and visual communications.

==
Life ==
Warnock was born and raised in Salt Lake City, Utah.
Although he failed mathematics in ninth grade while graduating from Olympus High School in 1958,   Warnock went on to earn a Bachelor of Science degree in mathematics and philosophy, a Doctor of Philosophy degree in electrical engineering (computer science), and an honorary degree in science, all from the University of Utah.
At the University of Utah he was a member of the Gamma Beta Chapter of the Beta Theta Pi fraternity.
He also has an honorary degree from the American Film Institute.
He currently lives in the San Francisco Bay Area with his wife Marva E. Warnock, an illustrator.
They have three children.

==
Career ==
Warnock's earliest publication and subject of his master's thesis, was his 1964 proof of a theorem solving the Jacobson radical for row-finite matrices, which was originally posed by the American mathematician Nathan Jacobson in 1956.

In his 1969 doctoral thesis, Warnock invented the Warnock algorithm for hidden surface determination in computer graphics.

It works by recursive subdivision of a scene until areas are obtained that are trivial to compute.
It solves the problem of rendering a complicated image by avoiding the problem.
If the scene is simple enough to compute then it is rendered; otherwise it is divided into smaller parts and the process is repeated.
Warnock notes that for this work he received "the dubious distinction of having written the shortest doctoral thesis in University of Utah history".
In 1976, while Warnock worked at Evans & Sutherland, a Salt Lake City-based computer graphics company, the concepts of the PostScript language were seeded.
Prior to co-founding Adobe, with Geschke and Putman, Warnock worked with Geschke at Xerox's Palo Alto Research Center (Xerox PARC), where he had started in 1978.
Unable to convince Xerox management of the approach to commercialize the InterPress graphics language for controlling printing, he, together with Geschke and Putman, left Xerox to start Adobe in 1982.
At their new company, they developed an equivalent technology, PostScript, from scratch, and brought it to market for Apple's LaserWriter in 1985.

In the spring of 1991, Warnock outlined a system called "Camelot", that evolved into the Portable Document Format (PDF) file-format.
The goal of Camelot was to "effectively capture documents from any application, send electronic versions of these documents anywhere, and view and print these documents on any machines".
Warnock's document contemplated:
Imagine if the IPS (Interchange PostScript) viewer is also equipped with text searching capabilities.
In this case the user could find all documents that contain a certain word or phrase, and then view that word or phrase in context within the document.
Entire libraries could be archived in electronic form...
One of Adobe's popular typefaces, Warnock, is named after him.

Adobe's PostScript technology made it easier to print text and images from a computer, revolutionizing media and publishing in the 1980s.

In 2003, Warnock and his wife donated 200,000 shares of Adobe Systems (valued at over $5.7 million) to the University of Utah as the main gift for a new engineering building.
The John E. and Marva M. Warnock Engineering Building was completed in 2007 and houses the Scientific Computing and Imaging Institute and the Dean of the University of Utah College of Engineering.

Dr. Warnock holds seven patents.
In addition to Adobe Systems, he serves or has served on the board of directors at ebrary, Knight-Ridder, MongoNet, Netscape Communications and Salon Media Group.
Warnock is a past Chairman of the Tech Museum of Innovation in San Jose.
He also serves on the Board of Trustees of the American Film Institute and the Sundance Institute.

His hobbies include photography, skiing, Web development, painting, hiking, curation of rare scientific books and historical Native American objects.
A strong supporter of higher education, Warnock and his wife, Marva, have supported three presidential endowed chairs in computer science, mathematics and fine arts at the University of Utah and also an endowed chair in medical research at Stanford University.

==
Recognition ==
The recipient of numerous scientific and technical awards, Warnock won the Software Systems Award from the Association for Computing Machinery in 1989.

In 1995 Warnock received the University of Utah Distinguished Alumnus Award and in 1999 he was inducted as a Fellow of the Association for Computing Machinery.

Warnock was awarded the Edwin H. Land Medal from the Optical Society of America in 2000.

In 2002, he was made a Fellow of the Computer History Museum for "his accomplishments in the commercialization of desktop publishing with John Warnock and for innovations in scalable type, computer graphics and printing.
"Oxford
University's Bodleian Library bestowed the Bodley Medal on Warnock in November 2003.

In 2004, Warnock received the Lovelace Medal from the British Computer Society in London.

In October 2006, Warnock—along with Adobe co-founder Charles Geschke—received the American Electronics Association's Annual Medal of Achievement Award, being the first software executives to receive this award.

In 2008, Warnock and Geschke received the Computer Entrepreneur Award from the IEEE Computer Society "for inventing PostScript and PDF and helping to launch the desktop publishing revolution and change the way people engage with information and entertainment".

In September 2009, Warnock and Geschke were chosen to receive the National Medal of Technology and Innovation, one of the nation's highest honors bestowed on scientists, engineers and inventors.

In 2010, Warnock and Geschke received the Marconi Prize, considered the highest honor specifically for contributions to information science and communications.
Warnock is a member of the National Academy of Engineering, the American Academy of Arts and Sciences, and the American Philosophical Society, the latter being America's oldest learned society.

He has received honorary degrees from the University of Utah, the American Film Institute, and The University of Nottingham in the UK.

== See also ==
Warnock algorithm
==
References ==


==
External links ==
Interview in Knowledge@Wharton published
January 20, 2010
Biography at Computer History Museum
Biography on Adobe Web site
Warnock's Utah Bed and Breakfast-The Blue Boar Inn
Warnock's Rare Book Room educational site which allows visitors to examine and read some of the great books of the world
Warnock's Splendid Heritage website which documents rare American Indian objects from private collections
Appearances on C-SPAN
Warnock is a surname.
It originated with the Mac-Gille-Warnocks (MacIlvernocks clan) in Scotland prior to 1066; its motto is "Ne oublie" (Do not forget).

Notable people with the surname include: Jed Warnock, US Army Officer.
Veteran of Multiple Combat tours who is a recipient of the Bronze Star and Purple Heart and who has been recognized for valor in combat.

==
List of people with the surname ==
Barbara Mills née Warnock (1940–2011), British barrister
Barton H. Warnock (1911–1998), a botanist and taxonomist
Bob Warnock, sailor on the deck of the submarine, USS Cachalot, when the attack on Pearl Harbor began
Bryan Warnock, originator of Warnock's dilemma
Ceri Warnock, British-born New Zealand environmental legal scholar
Dave Warnock (1910–1976), Scottish footballer (Aberdeen)
David Warnock (1865–1932), Scottish-Canadian politician and veterinarian
Diana Warnock (born 1940), Australian radio broadcaster and politician
Geoffrey Warnock (died 1995), British philosopher and former Vice-Chancellor of Oxford University
James Warnock, American engineer
Jimmy Warnock (fl.
1930s), Belfast boxer who fought and beat Benny Lynch when World Champion in 1936 and again in 1937
John Warnock (born 1940), American co-founder of Adobe Systems software company and inventor of Warnock algorithm
Mary Warnock, Baroness Warnock (1924–2019), British philosopher and chair of committees that produced reports about education and medicine
Matthew Warnock (born 3 April 1984), Australian rules footballer
Neil Warnock (born 1948), English football manager
Raphael Warnock (born 1969), American pastor and US Senator in Georgia
Robert Warnock (born 1987), Australian rules footballer
Stephen Warnock (born 1981), English footballer (Liverpool, Blackburn Rovers, Aston Villa, Leeds United, Derby County)Jed Warnock (
born 1975)
United States Army, Bronze Star, Purple Heart , and awarded for combat valor on October 8 2004.
Later, he was recommended for a subsequent Valor Device for combat actions on October 27, 2012.

== See also ==
Warnock (disambiguation)


==
References ==
In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.

==
Search algorithms ==
An admissible heuristic is used to estimate the cost of reaching the goal state in an informed search algorithm.
In order for a heuristic
to be admissible to the search problem, the estimated cost must always be lower than or equal to the actual cost of reaching the goal state.

The search algorithm uses the admissible heuristic to find an estimated 
optimal path to the goal state from the current node.

For example, in A* search the evaluation function (where 

  
    
      
        n
      
    
    {\displaystyle n}
   is the current node) is:
f
(
        n
        )
        =
g
(
        n
        )
        +
h
        (
n
        )
      
    
    {\displaystyle f(n)=g(n)+h(n)}
  
where

  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
=
the evaluation function.

g
(
        n
        )
      
    
    {\displaystyle g(n)}
=
the cost from the start node to the current node
h
(
        n
        )
      
    
    {\displaystyle h(n)
}
   = estimated cost from current node to goal.

h
(
        n
        )
      
    
    {\displaystyle h(n)}
   is calculated using the heuristic 
function.
With a non-admissible heuristic, the A* algorithm could 
overlook the optimal solution to a search problem due to an 
overestimation in 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
  .
==
Formulation ==
n
      
    
    {\displaystyle n}
   is a node
h
{\displaystyle h}
   is a heuristic

  
    
      
        h
(
n
        )
      
    
    {\displaystyle h(n)
}
   is cost indicated by 
  
    
      
        h
      
    
    {\displaystyle h}
   to reach a goal from 
  
    
      
        n
      
    
    {\displaystyle n}
h
          
            ∗
(
        n
        )
      
    
    {\displaystyle h^{*}(n)}
   is the optimal cost to reach a goal from 
  
    
      
        n
      
    
    {\displaystyle n}
h
(
        n
        )
      
    
    {\displaystyle h(n)}
   is admissible if,
∀
        n
      
    
    {\displaystyle \forall n}
  
  
    
      
        h
(
        n
        )
        ≤
h
          
            ∗
(
        n
        )
      
    
    {\displaystyle h(n)\leq h^{*}(n)}
== Construction ==
An admissible heuristic can be derived from a relaxed
version of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods.

== Examples ==
Two different examples of admissible heuristics apply to the fifteen puzzle problem:
Hamming distance
Manhattan distanceThe Hamming distance is the total number of misplaced tiles.
It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles (each tile not in place must be moved at least once).
The cost (number of moves) to the goal (an ordered puzzle) is at least the Hamming distance of the puzzle.

The Manhattan distance of a puzzle is defined as:

  
    
      
        h
(
        n
        )
        =
∑
all tiles
d
i
            s
            t
            a
n
            c
            e
(
        
          tile, correct position
        
        )
      
    
    {\displaystyle
h(n)=\sum _{
\text{all tiles}}{\mathit {distance}}({\text{tile, correct position}})}
  Consider the puzzle below in which the player wishes to move each tile such that the numbers are ordered.
The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the number of spots in between itself and its correct position.

The subscripts show the Manhattan distance for each tile.
The total Manhattan distance for the shown puzzle is:

  
    
      
        h
(
        n
        )
=
3
+
        1
+
        0
+
        1
+
        2
+
3
+
3
+
        4
+
3
+
        2
+
        4
+
        4
+
        4
+
        1
+
        1
=
36
      
    
    {\displaystyle h(n)=3+1+0+1+2+3+3+4+3+2+4+4+4+1+1=36}
  


==
Optimality guarantee ==
If an admissible heuristic is used in an algorithm that, per iteration, progresses only the one path that has lowest total expected cost of several candidate paths and terminates the moment any path reaches the goal accepting that path as shortest (for example in A* search algorithm), then this algorithm will terminate on the shortest path.
To see why, simply consider that any path that the algorithm terminates on was only progressed because its total expected cost was lowest of the candidates.
For an admissible heuristic, none of the candidates overestimate their costs so their true costs can only be greater to or equal to that of the accepted path.
Finally, the total expected cost is the true cost for a path that reaches goal because the only admissible heuristic on reaching goal is zero.

As an example of why admissibility can guarantee optimality, let us say we have costs as follows:(the cost above/below a node is the heuristic, the cost at an edge is the actual cost)
0     10   0
100   0
START ----
O  -----
GOAL
|                   |
0|                   |100
|                   |
O -------
O
------
O
100   1
100   1   100
So clearly we would start off visiting the top middle node, since the expected total cost, i.e. 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
, is 
  
    
      
        10
+
        0
=
10
      
    
    {
\displaystyle 10+0=10}
  .
Then the goal would be a candidate, with 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
   equal to 
  
    
      
        10
+
100
+
        0
=
110
      
    
    {\displaystyle 10+100+0=110}
  .
Then we would clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
lower than the 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
   of the current goal, i.e. their 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)
}
   is 
  
    
      
        100
        ,
101
        ,
102
        ,
102
      
    
    {\displaystyle 100,101,102,102}
  .
So even though the goal was a candidate, we could not pick it because there were still better paths out there.
This way, an admissible heuristic can ensure optimality.

However, note that although an admissible heuristic can guarantee final optimality, it is not necessarily efficient.
==
Notes ==
While all consistent heuristics are admissible, not all admissible heuristics are consistent.

For tree search problems, if an admissible heuristic is used, the A* search algorithm will never return a suboptimal goal node.

==
References ==


==
See also ==
Consistent heuristic
Heuristic function
Search algorithm
Gerd Gigerenzer (born September 3, 1947, Wallersdorf, Germany) is a German psychologist who has studied the use of bounded rationality and heuristics in decision making.
Gigerenzer is director emeritus of the Center for Adaptive Behavior and Cognition (ABC) at the Max Planck Institute for Human Development and director of the Harding Center for Risk Literacy, both in Berlin, Germany.

Gigerenzer investigates how humans make inferences about their world with limited time and knowledge.

He proposes that, in an uncertain world, probability theory is not sufficient; people also use smart heuristics, that is, rules of thumb.

He conceptualizes rational decisions in terms of the adaptive toolbox (the repertoire of heuristics an individual or institution has) and the ability to choose a good heuristics for the task at hand.

A heuristic is called ecologically rational to the degree that it is adapted to the structure of an environment.

Gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the accuracy-effort trade-off view assumes, in which heuristics are seen as short-cuts that trade less effort for less accuracy.

In contrast, his and associated researchers' studies have identified situations in which "less is more", that is, where heuristics make more accurate decisions with less effort.

This contradicts the traditional view that more information is always better or at least can never hurt if it is free.
Less-is-more effects have been shown experimentally, analytically, and by computer simulations.

==
Biography ==


===
Academic career ===
Gigerenzer received his PhD from the University of Munich in 1977 and became a professor of psychology there the same year.
In 1984 he moved to the University of Konstanz and in 1990 to the University of Salzburg.
From 1992 to 1995 he was Professor of Psychology at the University of Chicago and has been the John M. Olin
Distinguished Visiting Professor, School of Law at the University of Virginia.
In 1995 he became director of the Max Planck Institute for Psychological Research in Munich.
Since 2009 he has been director of the Harding Center for Risk Literacy in Berlin.

Gigerenzer was awarded honorary doctorates from the University of Basel and the Open University of the Netherlands.
He is also Batten Fellow at the Darden Business School, University of Virginia, Fellow of the Berlin-Brandenburg Academy of Sciences and the German Academy of Sciences Leopoldina, and Honorary Member of the American Academy of Arts and Sciences and the American Philosophical Society.

=== Heuristics ===
With Daniel Goldstein he first theorized the recognition heuristic and the take-the-best heuristic.
They proved analytically conditions under which semi-ignorance (lack of recognition) can lead to better inferences than with more knowledge.
These results were experimentally confirmed in many experiments, e.g., by showing that semi-ignorant people who rely on recognition are as good as or better than the Association of Tennis Professionals (ATP) Rankings and experts at predicting the outcomes of the Wimbledon tennis tournaments.
Similarly, decisions by experienced experts (e.g., police, professional burglars, airport security) were found to follow the take-the-best heuristic rather than weight and add all information, while inexperienced students tend to do the latter.
A third class of heuristics, Fast-And-Frugal trees, are designed for categorization and are used for instance in emergency units to predict heart attacks, and model bail decisions made by magistrates in London courts.
In such cases, the risks are not knowable and professionals hence face uncertainty.
To better understand the logic of Fast-And-Frugal trees and other heuristics, Gigerenzer and his colleagues use the strategy of mapping its concepts onto those of well-understood optimization theories, such as signal-detection theory.

A critic of the work of Daniel Kahneman and Amos Tversky, Gigerenzer argues that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases, but rather to conceive rationality as an adaptive tool that is not identical to the rules of formal logic or the probability calculus.
He and his collaborators have theoretically and experimentally shown that many cognitive fallacies are better understood as adaptive responses to a world of uncertainty—such as the conjunction fallacy, the base rate fallacy, and overconfidence.

===
The adaptive toolbox ===
The basic idea of the adaptive toolbox is that different domains of thought require different specialized cognitive mechanisms instead of one universal strategy.
The analysis of the adaptive toolbox and its evolution is descriptive research with the goal of specifying the core cognitive capacities (such as recognition memory) and the heuristics that exploit these (such as the recognition heuristic).

===
Risk communication ===
Alongside his research on heuristics, Gigerenzer investigates risk communication in situations where risks can actually be calculated or precisely estimated.
He has developed an ecological approach to risk communication where the key is the match between cognition and the presentation of the information in the environment.
For instance, lay people as well as professionals often have problems making Bayesian inferences, typically committing what has been called the base-rate fallacy in the cognitive illusions literature.
Gigerenzer and Ulrich Hoffrage were the first to develop and test a representation called natural frequencies, which helps people make Bayesian inferences correctly without any outside help.
Later it was shown that with this method, even 4th graders were able to make correct inferences.
Once again, the problem is not simply in the human mind, but in the representation of the information.
Gigerenzer has taught risk literacy to some 1,000 doctors in their CMU and some 50 US federal judges, and natural frequencies has now entered the vocabulary of evidence-based medicine.
In recent years, medical schools around the world have begun to teach tools such as natural frequencies to help young doctors understand test results.

==
Intellectual background ==
Intellectually, Gigerenzer's work is rooted in Herbert Simon's work on satisficing (as opposed to maximizing) and on ecological and evolutionary views of cognition, where adaptive function and success is central, as opposed to logical structure and consistency, although the latter can be means towards function.

Gigerenzer and colleagues write of the mid-17th century "probabilistic revolution", "the demise of the dream of certainty and the rise of a calculus of uncertainty – probability theory".
Gigerenzer calls for a second revolution, "replacing the image of an omniscient mind computing intricate probabilities and utilities with that of a bounded mind reaching into an adaptive toolbox filled with fast and frugal heuristics".
These heuristics would equip humans to deal more specifically with the many situations they face in which not all alternatives and probabilities are known, and surprises can happen.

==
Personal ==
Gigerenzer is a jazz and Dixieland musician.
He was part of The Munich Beefeaters Dixieland Band which performed in a TV ad for the VW Golf around the time it came out in 1974.
The ad can be viewed on YouTube, with Gigerenzer at the steering wheel and on the banjo.

He is married to Lorraine Daston, director at the Max Planck Institute for the History of Science and has one daughter, Thalia Gigerenzer.

==
Awards ==
Gigerenzer is recipient of the AAAS Prize for Behavioral Science Research for the best article in the behavioral sciences, the Association of American Publishers Prize for the best book in the social and behavioral sciences, the German Psychology Prize, and the Communicator Award of the German Research Association (DFG), among others.
(
See the German Wikipedia entry, Gerd Gigerenzer, for an extensive list of honors and awards.)
==
Publications ==
===
Books ===
Cognition as intuitive statistics (1987) with David Murray
The Empire of Chance:
How Probability Changed Science and Everyday Life (1989)
Simple Heuristics That Make Us Smart (1999)
Bounded Rationality:
The Adaptive Toolbox (2001) with Reinhard Selten
Reckoning with Risk: Learning to Live with Uncertainty (2002, published in the U.S. as Calculated Risks: How to Know When Numbers Deceive You)
Gut Feelings:
The Intelligence of the Unconscious (2007)
Rationality for Mortals (2008)
Heuristics:
The Foundation of Adaptive Behavior (2011) with Ralph Hertwig & Torsten Pachur
Risk Savvy:
How to Make Good Decisions (2014)
Simply Rational:
Decision Making in the Real World
(2015)


== Video ==
Video on Gerd Gigerenzer's research (Latest Thinking)


==
See also ==


==
References ==


==
External links ==
Resume
Books
Edge.org
bio
Article:
Simple tools for understanding risks: from innumeracy to insight
Harding Center for Risk Literacy
Gerd Gigerenzer in the German National Library catalogue
The history of computer science began long before our modern discipline of computer science, usually appearing in forms like mathematics or physics.
Developments in previous centuries alluded to the discipline that we now know as computer science.
This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.

==
Prehistory ==
The earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer.
The Sumerians' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.
Its original style of usage was by lines drawn in sand with pebbles.
Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.
In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical.
Panini used metarules, transformations and recursions.
The Antikythera mechanism is believed to be an early mechanical analog computer.

It was designed to calculate astronomical positions.
It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah.
According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus.
Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari's programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer.
Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.
When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools.
In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624.
Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria.
Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer.
The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching.
Although never built, the design has been studied extensively and is understood to be Turing equivalent.
The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.

==
Binary logic ==
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system.
In his system, the ones and zeros also represent true and false values or on and off states.
But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.
By this time, the first mechanical devices driven by a binary pattern had been invented.
The industrial revolution had driven forward the mechanization of many tasks, and this included weaving.
Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero.
Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.

==
Emergence of a discipline ==


===
Charles Babbage and Ada Lovelace ===
Charles Babbage is often regarded as one of the first pioneers of computing.
Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables.
Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long.
Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places.
By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations.
The machine would store numbers in memory units, and there would be a form of sequential control.

This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail.
This machine was to be known as the “Analytical Engine”, which was the first true representation of what is the modern computer.
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius.
Lovelace began working with Charles Babbage as an assistant while Babbage was working on his “Analytical Engine”, the first mechanical computer.
During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers.
Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not.
While she was never able to see the results of her work, as the “Analytical Engine” was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.

===
Charles Sanders Peirce and electrical switching circuits ===
In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.
During 1880–81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933.
The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow.
Consequently, these gates are sometimes called universal logic gates.
Eventually, vacuum tubes replaced relays for logic operations.
Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate.
Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921).
Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924.
Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).

Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.

This changed with NEC engineer Akira Nakashima's switching circuit theory in the 1930s.
From 1934 to 1936, Nakashima published a series of papers showing that the two-valued Boolean algebra, which he discovered independently (he was unaware of George Boole's work until 1938), can describe the operation of switching circuits.
This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers.
Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.

Nakashima's work was later cited and elaborated on in Claude Elwood Shannon's seminal 1937 master's thesis "A Symbolic Analysis of Relay and Switching Circuits".
While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems.
His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.

===
Alan Turing and the Turing machine ===
Before the 1920s, computers (sometimes computors) were human clerks that performed computations.
They were usually under the lead of a physicist.
Many thousands of computers were employed in commerce, government, and research establishments.
Many of these clerks who served as human computers were women.
Some performed astronomical calculations for calendars, others ballistic tables for the military.
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis.
The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind.
They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit.
Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common.
These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described "purely mechanical.
"
The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.
The mathematical foundations of modern computer science began to be laid by Kurt Gödel with his incompleteness theorem (1931).
In this theorem, he showed that there were limits to what could be proved and disproved within a formal system.
This led to work by Gödel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.
In 1936  Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a "purely mechanical" model for computing.
This became the Church–Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers.
The thesis states that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine.
This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use.
These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability.
If a Turing machine can complete the task, it is considered Turing computable.
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:
I know that in or about 1943 or ‘44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936…
Von Neumann introduced me to that paper and at his urging I studied it with care.
Many people have acclaimed von Neumann as the "father of the computer" (in a modern sense of the term) but I am sure that he would never have made that mistake himself.
He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...


===
Early computer hardware ===
The world's first electronic digital computer, the Atanasoff–Berry computer, was built on the Iowa State campus from 1939 through 1942 by John V. Atanasoff, a professor of physics and mathematics, and Clifford Berry, an engineering graduate student.

In 1941, Konrad Zuse developed the world's first functional program-controlled computer, the Z3.
In 1998, it was shown to be Turing-complete in principle.
Zuse also developed the S2 computing machine, considered the first process control computer.
He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer.

In 1946, he designed the first high-level programming language, Plankalkül.
In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers.
The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.
In 1950, Britain's National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing's philosophy.
With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world.
Turing's design for ACE had much in common with today's RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day.
Had Turing's ACE been built as planned and in full, it would have been in a different league from the other early computers.

The first actual computer bug was a moth.
It was stuck in between the relays on the Harvard Mark II.

While the invention of the term 'bug' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the "bug" on September 9, 1945, most other accounts conflict at least with these details.
According to these accounts, the actual date was September 9, 1947 when operators filed this 'incident' — along with the insect and the notation "First actual case of bug being found" (see software bug for details).
===
Shannon and information theory ===
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.

This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.

===
Wiener and cybernetics ===
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for "steersman."
He published "Cybernetics" in 1948, which influenced artificial intelligence.
Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.

===
John von Neumann and the von Neumann architecture ===
In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture.
Since 1950, the von Neumann model provided uniformity in subsequent computer designs.
The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.

The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU).
In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.
Von Neumann's machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks.
(
This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.)

With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed.
Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops.
The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa.
Von Neumann architecture accepts fractions and instructions as data types.
Finally, as the von Neumann architecture is a simple one, its register management is also simple.
The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions.
These registers include the "IR" (instruction register), "IBR" (instruction buffer register), "MQ" (multiplier quotient register), "MAR" (memory address register), and "MDR" (memory data register)."

The architecture also uses a program counter ("PC") to keep track of where in the program the machine is.
===
John McCarthy, Marvin Minsky and artificial intelligence ===
The term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research.
The naming of artificial intelligence also led to the birth of a new field in computer science.
On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon.
The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence's makeup.
McCarthy and his colleagues' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results.
They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program.
The knowledge to produce a program that sophisticated was not there yet.
The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process.
The way computers can understand is at a hardware level.
This language is written in binary (1s and 0's).
This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.
Minsky's process determined how these artificial neural networks could be arranged to have similar qualities to the human brain.
However, he could only produce partial results and needed to further the research into this idea.
However, they were only to receive partial test resultsMcCarthy and Shannon's idea behind this theory was to develop a way to use complex problems to determine and measure the machine's efficiency through mathematical theory and computations.
However, they were only to receive partial test results.
The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter.
This would allow for a machine to grow in intelligence and increase calculation speeds.
The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.
The group thought that research in this category could be broken down into smaller groups.
This would consist of sensory and other forms of information about artificial intelligence.
Abstractions in computer science can refer to mathematics and programing language.
Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking.
They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do.
If this machine could do this; they needed to think of how did the machine determine the outcome.

== See also ==
Computer Museum
List of computer term etymologies, the origins of computer science words
List of pioneers in computer science
History of computing
History of computing hardware
History of software
History of personal computers
Timeline of algorithms
Timeline of women in computing
Timeline of computing 2020–2029


==
References ==


===
Sources ===
Evans, Claire L. (2018).
Broad Band:
The Untold Story of the Women Who Made the Internet.
New York: Portfolio/Penguin.
ISBN 9780735211759.

Grier, David Alan (2013).
When Computers Were Human.
Princeton:
Princeton University Press.
ISBN 9781400849369 – via Project MUSE.

==
Further reading ==
Tedre, Matti (2014).
The Science of Computing:
Shaping a Discipline.
Taylor and Francis / CRC Press.
ISBN 978-1-4822-1769-8.

Kak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt.
Ltd (2001)
The Development of Computer Science:
A Sociocultural Perspective Matti Tedre's Ph.D. Thesis, University of Joensuu (2006)
Ceruzzi, Paul E. (1998).
A History of a Modern Computing.
The MIT Press.
ISBN 978-0-262-03255-1.

Copeland, B. Jack. "
The Modern History of Computing".

In Zalta, Edward N. (ed.).
Stanford Encyclopedia of Philosophy.

==
External links ==
Computer History Museum
Computers:
From the Past to the Present
The First "Computer Bug" at the Naval History and Heritage Command Photo Archives.

Bitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s
Oral history interviews
In software engineering and computer science, abstraction is:

the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems to focus attention on details of greater importance; it is similar in nature to the process of generalization;
the creation of abstract concept-objects by mirroring common features or attributes of various non-abstract objects or systems of study – the result of the process of abstraction.
Abstraction, in general, is a fundamental concept in computer science and software development.
The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design.
Models can also be considered types of abstractions per their generalization of aspects of reality.

Abstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects, but is also related to other notions of abstraction used in other fields such as art.
Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:

the usage of data types to perform data abstraction to separate usage from working representations of data structures within programs;
the concept of procedures, functions, or subroutines which represent a specific of implementing control flow in programs;
the rules commonly named "abstraction" that generalize expressions using free and bound variables in the various versions of lambda calculus;
the usage of S-expressions as an abstraction of data structures and programs in the Lisp programming language;
the process of reorganizing common behavior from non-abstract classes into "abstract classes" using inheritance to abstract over sub-classes as seen in the object-oriented C++ and Java programming languages.

==
Rationale ==
Computing mostly operates independently of the concrete world.
The hardware implements a model of computation that is interchangeable with others.
The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time.
These architectures are made of specific choices of abstractions.
Greenspun's Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.

A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system.
Modeling languages help in planning.
Computer languages can be processed with a computer.
An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language.
Each stage can be used as a stepping stone for the next stage.
The language abstraction continues for example in scripting languages and domain-specific programming languages.

Within a programming language, some features let the programmer create new abstractions.
These include subroutines, modules, polymorphism, and software components.
Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.

Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on.
The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky – that they can never completely hide the details below; however, this does not negate the usefulness of abstraction.

Some abstractions are designed to inter-operate with other abstractions – for example, a programming language may contain a foreign function interface for making calls to the lower-level language.

==
Abstraction features ==


===
Programming languages ===
Different programming languages provide different types of abstraction, depending on the intended applications for the language.
For example:

In object-oriented programming languages such as C++, Object Pascal, or Java, the concept of abstraction has itself become a declarative statement – using the syntax function(parameters) = 0; (in C++) or the keywords abstract and interface (in Java).
After such a declaration, it is the responsibility of the programmer to implement a class to instantiate the object of the declaration.

Functional programming languages commonly exhibit abstractions related to functions, such as lambda abstractions (making a term into a function of some variable) and higher-order functions (parameters are functions).

Modern members of the Lisp programming language family such as Clojure, Scheme and Common Lisp support macro systems to allow syntactic abstraction.
Other programming languages such as Scala also have macros, or very similar metaprogramming features (for example, Haskell has Template Haskell, and OCaml has MetaOCaml).
These can allow a programmer to eliminate boilerplate code, abstract away tedious function call sequences, implement new control flow structures, and implement Domain Specific Languages (DSLs), which allow domain-specific concepts to be expressed in concise and elegant ways.
All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit.
A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to "more traditional" programming languages such as Python, C or Java.
===
Specification methods ===
Analysts have developed various methods to formally specify software systems.

Some known methods include:

Abstract-model based method (VDM, Z);
Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);
Process-based techniques (LOTOS, SDL, Estelle);
Trace-based techniques (SPECIAL, TAM);
Knowledge-based techniques (Refine, Gist).
===
Specification languages ===
Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation.
The UML specification language, for example, allows the definition of abstract classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.

==
Control abstraction ==
Programming languages offer control abstraction as one of the main purposes of their use.
Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits.
Programming languages allow this to be done in the higher level.
For example, consider this statement written in a Pascal-like fashion:

a := (1 + 2)
*
5To a human, this seems a fairly simple and obvious calculation ("one plus two is three,
times five is fifteen").
However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex.
The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication).
Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.

Without control abstraction, a programmer would need to specify all the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable.
Such duplication of effort has two serious negative consequences:
it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed
it forces the programmer to program for the particular hardware and instruction set


===
Structured programming ===
Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with a reduction of the complexity potential for side-effects.

In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.

In a larger system, it may involve breaking down complex tasks into many different modules.
Consider a system which handles payroll on ships and at shore offices:
The uppermost level may feature a menu of typical end-user operations.

Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.

Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program.
A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).

Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.
These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others.
Object-oriented programming embraces and extends this concept.

==
Data abstraction ==
Data abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation.
The abstract properties are those that are visible to client code that makes use of the data type—the interface to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time.
The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.

For example, one could define an abstract data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys.
Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs.
As far as client code is concerned, the abstract properties of the type are the same in each case.

Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code.
As one way to look at this: the interface forms a contract on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.

==
Manual data abstraction ==
While much of data abstraction occurs through computer science and automation, there are times when this process is done manually and without programming intervention.
One way this can be understood is through data abstraction within the process of conducting a systematic review of the literature.
In this methodology, data is abstracted by one or several abstractors when conducting a meta-analysis, with errors reduced through dual data abstraction followed by independent checking, known as adjudication.

==
Abstraction in object oriented programming ==
In object-oriented programming theory, abstraction involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system.
The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction.

When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism.
When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.

Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role.
Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime.
This would leave only a minimum of such bindings to change at run-time.

Common Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism.
Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.

C++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.

Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code – all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.

Consider for example a sample Java fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding.
It defines an Animal class to represent both the state of the animal and its functions:

With the above definition, one could create objects of type Animal and call their methods like this:
In the above example, the class Animal is an abstraction used in place of an actual animal, LivingThing is a further abstraction (in this case a generalisation) of Animal.

If one requires a more differentiated hierarchy of animals – to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives – that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.

Such an abstraction could remove the need for the application coder to specify the type of food, so they could concentrate instead on the feeding schedule.
The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types.
These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others.
A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or
other means to achieve polymorphism.
The class notation is simply a coder's convenience.

=== Object-oriented design ===
Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysis—actually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.

In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design.
In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions.
A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and it is called an abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.

==
Considerations ==
When discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors.
For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions.
Abstraction is defined to a concrete (more precise) model of execution.

Abstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model.
For instance, if one wishes to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth modulo n, then one needs only perform all operations modulo n (a familiar form of this abstraction is casting out nines).

Abstractions, however, though not necessarily exact, should be sound.
That is, it should be possible to get sound answers from them—even though the abstraction may simply yield a result of undecidability.
For instance, students in a class may be abstracted by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "
I don't know".

The level of abstraction included in a programming language can influence its overall usability.
The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism.
This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.

Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice's theorem).
As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision
(they may answer "I don't know" to some questions).

Abstraction is the core concept of abstract interpretation.
Model checking generally takes place on abstract versions of the studied systems.

==
Levels of abstraction ==
Computer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail.
Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.

Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation.
For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages.
Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.
===
Database systems ===
Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:
Physical level: The lowest level of abstraction describes how a system actually stores data.
The physical level describes complex low-level data structures in detail.

Logical level: The next higher level of abstraction describes what data the database stores, and what relationships exist among those data.
The logical level thus describes an entire database in terms of a small number of relatively simple structures.
Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity.
This is referred to as physical data independence.
Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.

View level
: The highest level of abstraction describes only part of the entire database.
Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database.
Many users of a database system do not need all this information; instead, they need to access only a part of the database.
The view level of abstraction exists to simplify their interaction with the system.
The system may provide many views for the same database.

===
Layered architecture ===
The ability to provide a design of different levels of abstraction can

simplify the design considerably
enable different role players to effectively work at various levels of abstraction
support the portability of software artifacts (model-based ideally)Systems design and business process design can both use this.
Some design processes specifically generate designs that contain various levels of abstraction.

Layered architecture partitions the concerns of the application into stacked groups (layers).

It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.

== See also ==
Abstraction principle (computer programming)
Abstraction inversion for an anti-pattern of one danger in abstraction
Abstract data type for an abstract description of a set of data
Algorithm for an abstract description of a computational procedure
Bracket abstraction for making a term into a function of a variable
Data modeling for structuring data independent of the processes that use it
Encapsulation for abstractions that hide implementation details
Greenspun's Tenth Rule for an aphorism about an (the?)
optimum point in the space of abstractions
Higher-order function for abstraction where functions produce or consume other functions
Lambda abstraction for making a term into a function of some variable
List of abstractions (computer science)
Refinement for the opposite of abstraction in computing
Integer (computer science)
Heuristic (computer science)


==
References ==


==
Further reading ==


==
External links ==
SimArch example of layered architecture for distributed simulation systems.
Computer Science & Engineering (CSE) is an academic program at many universities which comprises scientific and engineering aspects of computing.
CSE is also a term often used in Europe to translate the name of engineering informatics academic programs.

==
Academic courses ==
Academic programs vary between colleges.
Courses usually include introduction to programming, introduction to algorithms and data structures, computer architecture, operating systems, computer networks, parallel computing, embedded systems, algorithms design, circuit analysis and electronics, digital logic and processor design, computer graphics, scientific computing, software engineering, database systems, digital signal processing, virtualization, computer simulations and games programming.
CSE programs also include core subjects of theoretical computer science such as theory of computation, numerical methods, machine learning, programming theory and paradigms.
Modern academic programs also cover emerging computing fields like image processing, data science, robotics, bio-inspired computing, computational biology, autonomic computing and artificial intelligence.
Most of the above CSE areas require initial mathematical knowledge, hence the first year of study is dominated by mathematical courses, primarily discrete mathematics, mathematical analysis, linear algebra and statistics, as well as the basics of physics - field theory and electromagnetism.

==
Example universities with CSE majors ==
Massachusetts Institute of Technology
University of Oxford
California Institute of Technology
Stanford University
North South University
North Western University,
Khulna
Dhaka University
University of Barisal
University of Chittagong
American University of Beirut
Santa Clara
University
University of Michigan
University of New South Wales
University of Washington
Bucknell University
Indian Institute of Technology Kanpur
Indian Institute of Technology Bombay
Indian Institute of Technology Delhi
Indian Institute of Technology Madras
National University of Singapore
Amrita Vishwa Vidyapeetham
University of Nevada
University of Notre Dame
Delft University of Technology


==
See also ==
Computer science
Computer graphics (computer science)


==
References ==
Adaptive Replacement Cache (ARC) is a page replacement algorithm with 
better performance than LRU (least recently used).
This is accomplished by keeping track of both frequently used and recently used pages plus a recent eviction history for both.
The algorithm was developed at the IBM Almaden Research Center.
In 2006, IBM was granted a patent for the adaptive replacement cache policy.

==
Summary ==
Basic LRU maintains an ordered list (the cache directory) of resource entries in the cache, with the sort order based on the time of most recent access.

New entries are added at the top of the list, after the bottom entry has been evicted.

Cache hits move to the top, pushing all other entries down.

ARC improves the basic LRU strategy by splitting the cache directory into two lists, T1 and T2, for recently and frequently referenced entries.

In turn, each of these is extended with a ghost list (B1 or B2), which is attached to the bottom of the two lists.

These ghost lists act as scorecards by keeping track of the history of recently evicted cache entries, and the algorithm uses ghost hits to adapt to recent change in resource usage.

Note that the ghost lists only contain metadata (keys for the entries) and not the resource data itself, i.e. as an entry is evicted into a ghost list its data is discarded.
The combined cache directory is organised in four LRU lists:

T1, for recent cache entries.

T2, for frequent entries, referenced at least twice.

B1, ghost entries recently evicted from the T1 cache, but are still tracked.

B2, similar ghost entries, but evicted from T2.T1 and B1 together are referred to as L1, a combined history of recent single references.

Similarly, L2 is the combination of T2 and B2.

The whole cache directory can be visualised in a single line:

. . .
[
B1  <-[     T1
<-!->
T2   ]->  B2   ] . .

[ . . . .
[ . . . . . . ! .
.^. . . . ] .
. . .
]
[   fixed cache size (c)    ]
The inner [ ] brackets indicate actual cache, which although fixed in size, can move freely across the B1 and B2 history.

L1 is now displayed from right to left, starting at the top, indicated by the !
marker.
^ indicates the target size for T1, and may be equal to, smaller than, or larger than the actual size (as indicated by !).

New entries enter T1, to the left of !,
and are gradually pushed to the left, eventually being evicted from T1 into B1, and finally dropped out altogether.

Any entry in L1 that gets referenced once more, gets another chance, and enters L2, just to the right of the central !
marker.

From there, it is again pushed outward, from T2 into B2.

Entries in L2 that get another hit can repeat this indefinitely, until they finally drop out on the far right of B2.

===
Replacement ===
Entries (re-)entering the cache (T1, T2) will cause !
to move towards the target marker ^.  If no free space exists in the cache, this marker also determines whether either T1 or T2 will evict an entry.

Hits in B1 will increase the size of T1, pushing ^ to the right.
The last entry in T2 is evicted into B2.

Hits in B2 will shrink T1, pushing ^ back to the left.

The last entry in T1 is now evicted into B1.

A cache miss will not affect ^, but the !
boundary will move closer to ^.


==
Deployment ==
ARC is currently deployed in IBM's DS6000/DS8000 storage controllers.

Sun Microsystems's scalable file system ZFS uses a variant of ARC as an alternative to the traditional Solaris filesystem page cache in virtual memory.
It has been modified to allow for locked pages that are currently in use and cannot be vacated.

PostgreSQL used ARC in its buffer manager for a brief time (version 8.0.0), but quickly replaced it with another algorithm,
citing concerns over an IBM patent on ARC.VMware's vSAN (formerly Virtual SAN) is a hyper-converged, software-defined storage (SDS) product developed by VMware.
It uses A variant of ARC in its Caching Algorithm.

== See also ==
Clock with Adaptive Replacement
LIRS caching algorithm


==
References ==


==
External links ==
ARC:
A Self-Tuning, Low Overhead Replacement Cache (2003) by Nimrod Megiddo,  Dharmendra Modha
Linux Memory Management Wiki
Bourbonnais, Roch.
ZFS Dynamics
Python implementation, recipe 576532
Comparison of LRU, ARC and others
In 3D computer graphics and computer vision, a depth map is an image or image channel that contains information relating to the distance of the surfaces of scene objects from a viewpoint.
The term is related to and may be analogous to depth buffer, Z-buffer, Z-buffering and Z-depth.
The "Z" in these latter terms relates to a convention that the central axis of view of a camera is in the direction of the camera's Z axis, and not to the absolute Z axis of a scene.

== Examples ==
Two different depth maps can be seen here, together with the original model from which they are derived.

The first depth map shows luminance in proportion to the distance from the camera.
Nearer surfaces are darker; further surfaces are lighter.
The second depth map shows luminance in relation to the distances from a nominal focal plane.
Surfaces closer to the focal plane are darker; surfaces further from the focal plane are lighter, (both closer to and also further away from the viewpoint).

== Uses ==
Depth maps have a number of uses, including:

Simulating the effect of uniformly dense semi-transparent media within a scene - such as fog, smoke or large volumes of water.

Simulating shallow depths of field - where some parts of a scene appear to be out of focus.
Depth maps can be used to selectively blur an image to varying degrees.
A shallow depth of field can be a characteristic of macro photography
and so the technique may form a part of the process of miniature faking.

Z-buffering and z-culling, techniques which can be used to make the rendering of 3D scenes more efficient.
They can be used to identify objects hidden from view and which may therefore be ignored for some rendering purposes.
This is particularly important in real time applications such as computer games, where a fast succession of completed renders must be available in time to be displayed at a regular and fixed rate.

Shadow mapping - part of one process used to create shadows cast by illumination in 3D computer graphics.
In this use, the depth maps are calculated from the perspective of the lights, not the viewer.

To provide the distance information needed to create and generate autostereograms and in other related applications intended to create the illusion of 3D viewing through stereoscopy .

Subsurface scattering - can be used as part of a process for adding realism by simulating the semi-transparent properties of translucent materials such as human skin.

In computer vision single-view or multi-view images depth maps, or other types of images, are used to model 3D shapes or reconstruct them.
Depth maps can be generated by 3D scanners or reconstructed from multiple images.

In Machine vision and computer vision, to allow 3D images to be processed by 2D image tools.

Making depth image datasets.

==
Limitations ==
Single channel depth maps record the first surface seen, and so cannot display information about those surfaces seen or refracted through transparent objects, or reflected in mirrors.
This can limit their use in accurately simulating depth of field or fog effects.

Single channel depth maps cannot convey multiple distances where they occur within the view of a single pixel.
This may occur where more than one object occupies the location of that pixel.
This could be the case - for example - with models featuring hair, fur or grass.
More generally, edges of objects may be ambiguously described where they partially cover a pixel.

Depending on the intended use of a depth map, it may be useful or necessary to encode the map at higher bit depths.
For example, an 8 bit depth map can only represent a range of up to 256 different distances.

Depending on how they are generated, depth maps may represent the perpendicular distance between an object and the plane of the scene camera.
For example, a scene camera pointing directly at - and perpendicular to - a flat surface may record a uniform distance for the whole surface.
In this case, geometrically, the actual distances from the camera to the areas of the plane surface seen in the corners of the image are greater than the distances to the central area.
For many applications, however, this discrepancy is not a significant issue.

==
References ==


==
See also ==
2D-plus-depth
Painter's algorithm
Range imaging
Structured light
WOWvx
Cache hierarchy, or multi-level caches, refers to a memory architecture that uses a hierarchy of memory stores based on varying access speeds to cache data.
Highly-requested data is cached in high-speed access memory stores, allowing swifter access by central processing unit (CPU) cores.

Cache hierarchy is a form and part of memory hierarchy and can be considered a form of tiered storage.
This design was intended to allow CPU cores to process faster despite the memory latency of main memory access.
Accessing main memory can act as a bottleneck for CPU core performance as the CPU waits for data, while making all of main memory high-speed may be prohibitively expensive.
High-speed caches are a compromise allowing high-speed access to the data most-used by the CPU, permitting a faster CPU clock.

==
Background ==
In the history of computer and electronic chip development, there was a period when increases in CPU speed outpaced the improvements in memory access speed.
The gap between the speed of CPUs and memory meant that the CPU would often be idle.
CPUs were increasingly capable of running and executing larger amounts of instructions in a given time, but the time needed to access data from main memory prevented programs from fully benefiting from this capability.
This issue motivated the creation of memory models with higher access rates in order to realize the potential of faster processors.
This resulted in the concept of cache memory, first proposed by Maurice Wilkes, a British computer scientist at the University of Cambridge in 1965.
He called such memory models "slave memory".
Between roughly 1970 and 1990, papers and articles by Anant Agarwal, Alan Jay Smith, Mark D. Hill, Thomas R. Puzak, and others discussed better cache memory designs.
The first cache memory models were implemented at the time, but even as researchers were investigating and proposing better designs, the need for faster memory models continued.
This need resulted from the fact that although early cache models improved data access latency, with respect to cost and technical limitations it was not feasible for a computer system's cache to approach the size of main memory.
From 1990 onward, ideas such as adding another cache level (second-level), as a backup for the first-level cache were proposed.
Jean-Loup Baer, Wen-Hann Wang, Andrew W. Wilson, and others have conducted research on this model.
When several simulations and implementations demonstrated the advantages of two-level cache models, the concept of multi-level caches caught on as a new and generally better model of cache memories.
Since 2000, multi-level cache models have received widespread attention and are currently implemented in many systems, such as the three-level caches that are present in Intel's Core i7 products.

== Multi-level cache ==
Accessing main memory for each instruction execution may result in slow processing, with the clock speed depending on the time required to find and fetch the data.
In order to hide this memory latency from the processor, data caching is used.
Whenever the data is required by the processor, it is fetched from the main memory and stored in the smaller memory structure called a cache.
If there is any further need of that data, the cache is searched first before going to the main memory.
This structure resides closer to the processor in terms of the time taken to search and fetch data with respect to the main memory.
The advantages of using cache can be proven by calculating the average access time (AAT) for the memory hierarchy with and without the cache.
===
Average access time (AAT) ===
Caches, being small in size, may result in frequent misses – when a search of the cache does not provide the sought-after information – resulting in a call to main memory to fetch data.
Hence, the AAT is affected by the miss rate of each structure from which it searches for the data.

AAT
        
        =
        
          hit time
+
(
        (
        
          miss rate
        
        )
        ×
(
        
          miss penalty
        
        )
        )
      
    
    {\displaystyle {
\text{AAT}}={\text{hit time}}+(({\text{miss rate}})\times ({\text{miss penalty}}))}
AAT for main memory is given by Hit time main memory.
AAT for caches can be given by

Hit timecache
+
(Miss ratecache × Miss Penaltytime taken to go to main memory after missing cache).The hit time for caches is less than the hit time for the main memory, so the AAT for data retrieval is significantly lower when accessing data through the cache rather than main memory.

=== Trade-offs ===
While using the cache may improve memory latency, it may not always result in the required improvement for the time taken to fetch data due to the way caches are organized and traversed.
For example, direct-mapped caches that are the same size usually have a higher miss rate than fully associative caches.
This may also depend on the benchmark of the computer testing the processor and on the pattern of instructions.
But using a fully associative cache may result in more power consumption, as it has to search the whole cache every time.
Due to this, the trade-off between power consumption (and associated heat) and the size of the cache becomes critical in the cache design.

===
Evolution ===
In the case of a cache miss, the purpose of using such a structure will be rendered useless and the computer will have to go to the main memory to fetch the required data.
However, with a multiple-level cache, if the computer misses the cache closest to the processor (level-one cache or L1)
it will then search through the next-closest level(s) of cache and go to main memory only if these methods fail.
The general trend is to keep the L1 cache small and at a distance of 1–2 CPU clock cycles from the processor, with the lower levels of caches increasing in size to store more data than L1, hence being more distant but with a lower miss rate.
This results in a better AAT.
The number of cache levels can be designed by architects according to their requirements after checking for trade-offs between cost, AATs, and size.

===
Performance gains ===
With the technology-scaling that allowed memory systems able to be accommodated on a single chip, most modern day processors have up to three or four cache levels.
The reduction in the AAT can be understood by this example, where the computer checks AAT for different configurations up to L3 caches.

Example:
main memory = 50 ns, L1 = 1 ns with 10% miss rate, L2 = 5 ns
with1% miss rate)
, L3 = 10 ns with 0.2% miss rate.

No cache, AAT = 50
ns
L1 cache,
AAT = 1 ns
+ (0.1 × 50 ns) =
6 ns
L1–2 caches, AAT = 1 ns
+ (0.1 × [5 ns
+ (0.01 × 50 ns)]) =
1.55 ns
L1–3 caches, AAT = 1 ns
+ (0.1 × [5 ns
+ (0.01 × [10 ns + (0.002 × 50 ns)])]) =
1.5101 ns
===
Disadvantages ===
Cache memory comes at an increased marginal cost than main memory and thus can increase the cost of the overall system.

Cached data is stored only so long as power is provided to the cache.

Increased on-chip area required for memory system.

Benefits may be minimized or eliminated in the case of a large programs with poor temporal locality, which frequently access the main memory.

==
Properties ==


=== Banked versus unified ===
In a banked cache, the cache is divided into a cache dedicated to instruction storage and a cache dedicated to data.
In contrast, a unified cache contains both the instructions and data in the same cache.
During a process, the L1 cache (or most upper-level cache in relation to its connection to the processor) is accessed by the processor to retrieve both instructions and data.
Requiring both actions to be implemented at the same time requires multiple ports and more access time in a unified cache.
Having multiple ports requires additional hardware and wiring, leading to a significant structure between the caches and processing units.
To avoid this, the L1 cache is often organized as a banked cache which results in fewer ports, less hardware, and generally lower access times.
Modern processors have split caches, and in systems with multilevel caches higher level caches may be unified while lower levels split.
===
Inclusion policies ===
Whether a block present in the upper cache layer can also be present in the lower cache level is governed by the memory system's inclusion policy, which may be inclusive, exclusive or non-inclusive non-exclusive (NINE).With an inclusive policy, all the blocks present in the upper-level cache have to be present in the lower-level cache as well.
Each upper-level cache component is a subset of the lower-level cache component.
In this case, since there is a duplication of blocks, there is some wastage of memory.
However, checking is faster.
Under an exclusive policy, all the cache hierarchy components are completely exclusive, so that any element in the upper-level cache will not be present in any of the lower cache components.
This enables complete usage of the cache memory.
However, there is a high memory-access latency.
The above policies require a set of rules to be followed in order to implement them.
If none of these are forced, the resulting inclusion policy is called non-inclusive non-exclusive (NINE).
This means that the upper-level cache may or may not be present in the lower-level cache.
===
Write policies ===
There are two policies which define the way in which a modified cache block will be updated in the main memory: write through and write back.
In the case of write through policy, whenever the value of the cache block changes, it is further modified in the lower-level memory hierarchy as well.
This policy ensures that the data is stored safely as it is written throughout the hierarchy.

However, in the case of the write back policy, the changed cache block will be updated in the lower-level hierarchy only when the cache block is evicted.
A "dirty bit" is attached to each cache block and set whenever the cache block is modified.
During eviction, blocks with a set dirty bit will be written to the lower-level hierarchy.
Under this policy, there is a risk for data-loss as the most recently changed copy of a datum is only stored in the cache
and therefore some corrective techniques must be observed.

In case of a write where the byte is not present in the cache block, the byte may be brought to the cache as determined by a write allocate or write no-allocate policy.
Write allocate policy states that in case of a write miss, the block is fetched from the main memory and placed in the cache before writing.
In the write no-allocate policy, if the block is missed in the cache it will write in the lower-level memory hierarchy without fetching the block into the cache.
The common combinations of the policies are "write block", "write allocate", and "write through write no-allocate".
===
Shared versus private ===
A private cache is assigned to one particular core in a processor, and cannot be accessed by any other cores.
In some architectures, each core has its own private cache; this creates the risk of duplicate blocks in a system's cache architecture, which results in reduced capacity utilization.
However, this type of design choice in a multi-layer cache architecture can also be good for a lower data-access latency.
A shared cache is a cache which can be accessed by multiple cores.
Since it is shared, each block in the cache is unique and therefore has a larger hit rate as there will be no duplicate blocks.
However, data-access latency can increase as multiple cores try to access the same cache.
In multi-core processors, the design choice to make a cache shared or private impacts the performance of the processor.
In practice, the upper-level cache L1 (or sometimes L2) is implemented as private and lower-level caches are implemented as shared.
This design provides high access rates for the high-level caches and low miss rates for the lower-level caches.

==
Recent implementation models ==


===
Intel Broadwell microarchitecture (2014) ===
L1 cache (instruction and data) – 64 kB per core
L2 cache – 256 kB per core
L3 cache – 2 MB to 6 MB shared
L4 cache – 128 MB of eDRAM
(Iris Pro models only)


===
Intel Kaby Lake microarchitecture (2016) ===
L1 cache (instruction and data) –
64 kB per core
L2 cache – 256 kB per core
L3 cache –
2 MB to 8 MB shared


===
AMD Zen microarchitecture (2017) ===
L1 cache –
32 kB data & 64 kB instruction per core, 4-way
L2 cache – 512 kB per core, 4-way inclusive
L3 cache – 4 MB local & remote per 4-core CCX, 2 CCXs per chiplet, 16-way non-inclusive.
Up to 16 MB on desktop CPUs and 64 MB on server CPUs


===
AMD Zen 2 microarchitecture (2019) ===
L1 cache – 32 kB data & 32 kB instruction per core, 8-way
L2 cache – 512 kB per core, 8-way inclusive
L3 cache – 16 MB local per 4-core CCX, 2 CCXs per chiplet, 16-way non-inclusive.
Up to 64 MB on desktop CPUs and 256 MB on server
CPUs


===
IBM Power 7 ===
L1 cache (instruction and data) –
each 64-banked, each bank has 2rd+1wr ports 32 kB, 8-way associative, 128B block, write through
L2 cache – 256 kB, 8-way, 128B block, write back, inclusive of L1, 2 ns access latency
L3 cache – 8 regions of 4 MB (total 32 MB),
local region 6 ns,
remote 30 ns, each region 8-way associative, DRAM data array,
SRAM tag array


==
See also ==
Power7
Intel Broadwell Microarchitecture
Intel Kaby Lake Microarchitecture
CPU Cache
Memory hierarchy
CAS latency
Cache (computing)


==
References ==
This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields.
Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.

==
A ==
abductive logic programming (ALP)
A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning.
It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates.

abductive reasoning
Also abduction.

A form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation.
This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it.
abductive inference, or retroduction
abstract data type
A mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations.

abstraction
The process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest
accelerating change
A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.

action language
A language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world.
Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning.

action model learning
An area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment.
This knowledge is usually represented in logic-based action description language and used as the input for automated planners.

action selection
A way of characterizing the most basic problem of intelligent systems: what to do next.
In artificial intelligence and computational cognitive science, "the action selection problem" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment.

activation function
In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.

adaptive algorithm
An algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion.

adaptive neuro fuzzy inference system (ANFIS)
Also adaptive network-based fuzzy inference system.

A kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system.
The technique was developed in the early 1990s.
Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework.
Its inference system corresponds to a set of fuzzy
IF–THEN rules that have learning capability to approximate nonlinear functions.
Hence, ANFIS is considered to be a universal estimator.
For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm.

admissible heuristic
In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.

affective computing
Also artificial emotional intelligence or emotion AI.

The study and development of systems and devices that can recognize, interpret, process, and simulate human affects.
Affective computing is an interdisciplinary field spanning computer science, psychology, and cognitive science.

agent architecture
A blueprint for software agents and intelligent control systems, depicting the arrangement of components.
The architectures implemented by intelligent agents are referred to as cognitive architectures.

AI accelerator
A class of microprocessor or computer system designed as hardware acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning.

AI-complete
In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI.
To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm.

algorithm
An unambiguous specification of how to solve a class of problems.
Algorithms can perform calculation, data processing, and automated reasoning tasks.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm.
An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources.
Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

algorithmic probability
In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation.
It was invented by Ray Solomonoff in the 1960s.

AlphaGo
A computer program that plays the board game Go.
It was developed by Alphabet Inc.'s Google DeepMind in London.
AlphaGo has several versions including AlphaGo Zero, AlphaGo Master, AlphaGo Lee, etc.
In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player without handicaps on a full-sized 19×19 board.

ambient intelligence (AmI)
Electronic environments that are sensitive and responsive to the presence of people.

analysis of algorithms
The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them.
Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity).

analytics
The discovery, interpretation, and communication of meaningful patterns in data.

answer set programming (ASP)
A form of declarative programming oriented towards difficult (primarily NP-hard) search problems.
It is based on the stable model (answer set) semantics of logic programming.

In ASP, search problems are reduced to computing stable models, and answer set solvers—programs for generating stable models—are used to perform search.

anytime algorithm
An algorithm that can return a valid solution to a problem even if it is interrupted before it ends.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software.
In general terms, it is a set of clearly defined methods of communication among various components.
A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.
An API may be for a web-based system, operating system, database system, computer hardware, or software library.

approximate string matching
Also fuzzy string searching.

The technique of finding strings that match a pattern approximately (rather than exactly).
The problem of approximate string matching is typically divided into two sub-problems: finding approximate substring matches inside a given string and finding dictionary strings that match the pattern approximately.

approximation error
The discrepancy between an exact value and some approximation to it.

argumentation framework
Also argumentation system.

A way to deal with contentious information and draw conclusions from it.
In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition.
Conflicts between arguments are represented by a binary relation on the set of arguments.
In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.

There exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.

artificial general intelligence (AGI)

artificial immune system (AIS)
A class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system.
The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.

artificial intelligence (AI)
Also machine intelligence.

Any intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.
In computer science, AI research is defined as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.
Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".

Artificial Intelligence Markup Language
An XML dialect for creating natural language software agents.

artificial neural network (ANN)
Also connectionist system.

Any computing system vaguely inspired by the biological neural networks that constitute animal brains.

Association for the Advancement of Artificial Intelligence (AAAI)
An international, nonprofit, scientific society devoted to promote research in, and responsible use of, artificial intelligence.
AAAI also aims to increase public understanding of artificial intelligence (AI), improve the teaching and training of AI practitioners, and provide guidance for research planners and funders concerning the importance and potential of current AI developments and future directions.

asymptotic computational complexity
In computational complexity theory, asymptotic computational complexity is the usage of asymptotic analysis for the estimation of computational complexity of algorithms and computational problems, commonly associated with the usage of the big O notation.

attributional calculus
A logic and representation system defined by Ryszard S. Michalski.
It combines elements of predicate logic, propositional calculus, and multi-valued logic.
Attributional calculus provides a formal language for natural induction, an inductive learning process whose results are in forms natural to people.

augmented reality (AR)
An interactive experience of a real-world environment where the objects that reside in the real-world are "augmented" by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory, and olfactory.

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them.
It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated planning and scheduling
Also simply AI planning.

A branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles.
Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space.
Planning is also related to decision theory.

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning.
The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically.
Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.

autonomic computing (AC)
The self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users.
Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth.

autonomous car
Also self-driving car, robot car, and driverless car.

A vehicle that is capable of sensing its environment and moving with little or no human input.

autonomous robot
A robot that performs behaviors or tasks with a high degree of autonomy.
Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering.

==
B ==
backpropagation
A method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.
Backpropagation is shorthand for "the backward propagation of errors", since an error is computed at the output and distributed backwards throughout the network's layers.
It is commonly used to train deep neural networks, a term referring to neural networks with more than one hidden layer.

backpropagation through time (BPTT)
A gradient-based technique for training certain types of recurrent neural networks.
It can be used to train Elman networks.
The algorithm was independently derived by numerous researchers

backward chaining
Also backward reasoning.

An inference method described colloquially as working backward from the goal.
It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.

bag-of-words model
A simplifying representation used in natural language processing and information retrieval (IR).
In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.
The bag-of-words model has also been used for computer vision.
The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.

bag-of-words model in computer vision
In computer vision, the bag-of-words model (BoW model) can be applied to image classification, by treating image features as words.

In document classification, a bag of words is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary.
In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features.

batch normalization
A technique for improving the performance and stability of artificial neural networks.
It is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance.
Batch normalization was introduced in a 2015 paper.
It is used to normalize the input layer by adjusting and scaling the activations.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

bees algorithm
A population-based search algorithm which was developed by Pham, Ghanbarzadeh and et al.
in 2005.
It mimics the food foraging behaviour of honey bee colonies.
In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization.
The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined.
The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.

behavior informatics (BI)
The informatics of behaviors so as to obtain behavior intelligence and behavior insights.

behavior tree (BT)
A mathematical model of plan execution used in computer science, robotics, control systems and video games.
They describe switchings between a finite set of tasks in a modular fashion.
Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented.
BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state.
Its ease of human understanding make BTs less error-prone and very popular in the game developer community.
BTs have shown to generalize several other control architectures.

belief-desire-intention software model (BDI)
A software model developed for programming intelligent agents.
Superficially characterized by the implementation of an agent's beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming.
In essence, it provides a mechanism for separating the activity of selecting a plan (from a plan library or an external planner application) from the execution of currently active plans.
Consequently, BDI agents are able to balance the time spent on deliberating about plans (choosing what to do) and executing those plans (doing it).
A third activity, creating the plans in the first place (planning), is not within the scope of the model, and is left to the system designer and programmer.

bias–variance tradeoff
In statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.
Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

Big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.

It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child.
A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set.
Some authors allow the binary tree to be the empty set as well.

blackboard system
An artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the "blackboard", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution.

Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state.

In this way, the specialists work together to solve the problem.

Boltzmann machine
Also stochastic Hopfield network with hidden units.

A type of stochastic recurrent neural network and Markov random field.

Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield networks.

Boolean satisfiability problem
Also propositional satisfiability problem; abbreviated SATISFIABILITY or SAT.

{{{content}}}

brain technology
Also self-learning know
-how system.

A technology that employs the latest findings in neuroscience.
The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the ROBOY project.
Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities.
In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as "know-how maps".

branching factor
In computing, tree data structures, and game theory, the number of children at each node, the outdegree.
If this value is not uniform, an average branching factor can be calculated.

brute-force search
Also exhaustive search or generate and test.

A very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.
==
C ==
capsule neural network (CapsNet)
A machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships.
The approach is an attempt to more closely mimic biological neural organization.

case-based reasoning (CBR)
Broadly construed, the process of solving new problems based on the solutions of similar past problems.

chatbot
Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity.

A computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.

cloud robotics
A field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics.
When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.).
Humans can also delegate tasks to robots remotely through networks.
Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies.
Thus, it is possible to build lightweight, low cost, smarter robots have intelligent "brain" in the cloud.
The "brain" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support, etc.

cluster analysis
Also clustering.

The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).
It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.

Cobweb
An incremental system for hierarchical conceptual clustering.
COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.
COBWEB incrementally organizes observations into a classification tree.
Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node.
This classification tree can be used to predict missing attributes or the class of a new object.

cognitive architecture
The Institute of Creative Technologies defines cognitive architecture as: "hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together – in conjunction with knowledge and skills embodied within the architecture – to yield intelligent behavior in a diversity of complex environments."
cognitive computing
In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain and helps to improve human decision-making.
In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus.

cognitive science
The interdisciplinary scientific study of the mind and its processes.

combinatorial optimization
In Operations Research, applied mathematics and theoretical computer science, combinatorial optimization  is a topic that consists of finding an optimal object from a finite set of objects.

committee machine
A type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response.

The combined response of the committee machine is supposed to be superior to those of its constituent experts.
Compare ensembles of classifiers.

commonsense knowledge
In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as "Lemons are sour", that all humans are expected to know.

The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy.

commonsense reasoning
A branch of artificial intelligence concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems.

computational complexity theory
Focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other.
A computational problem is a task solved by a computer.
A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational creativity
Also artificial creativity, mechanical creativity, creative computing, or creative computation.

A multidisciplinary endeavour that includes the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.

computational cybernetics
The integration of cybernetics and computational intelligence techniques.

computational humor
A branch of computational linguistics and artificial intelligence which uses computers in humor research.

computational intelligence (CI)
Usually refers to the ability of a computer to learn a specific task from data or experimental observation.

computational learning theory
In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.

computational linguistics
An interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions.

computational mathematics
The mathematical research in areas of science where computing plays an essential role.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience.

A branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational number theory
Also algorithmic number theory.

The study of algorithms for performing number theoretic computations.

computational problem
In theoretical computer science, a computational problem is a mathematical object representing a collection of questions that computers might be able to solve.

computational statistics
Also statistical computing.

The interface between statistics and computer science.

computer-automated design (CAutoD)
Design automation usually refers to electronic design automation, or Design Automation which is a Product Configurator.
Extending Computer-Aided Design (CAD), automated design and computer-automated design are concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification and optimization, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.
More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically inspired machine learning, including heuristic search techniques such as evolutionary computation, and swarm intelligence algorithms.

computer audition (CA)
See machine listening.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers.
It involves the study of algorithms that process, store, and communicate digital information.
A computer scientist specializes in the theory of computation and the design of computational systems.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos.
From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

concept drift
In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways.
This causes problems because the predictions become less accurate as time passes.

connectionism
An approach in the fields of cognitive science, that hopes to explain mental phenomena using artificial neural networks.

consistent heuristic
In the study of path-finding problems in artificial intelligence, a heuristic function is said to be consistent, or monotone, if its estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor.

constrained conditional model (CCM)
A machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints.

constraint logic programming
A form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction.
A constraint logic program is a logic program that contains constraints in the body of clauses.
An example of a clause including a constraint is A(X,Y) :- X+Y>0, B(X), C(Y).
In this clause, X+Y>0 is a constraint; A(X,Y), B(X), and C(Y) are literals as in regular logic programming.
This clause states one condition under which the statement A(X,Y) holds: X+Y is greater than zero and both B(X) and C(Y) are true.

constraint programming
A programming paradigm wherein relations between variables are stated in the form of constraints.
Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found.

constructed language
Also conlang.

A language whose phonology, grammar, and vocabulary are consciously devised, instead of having developed naturally.
Constructed languages may also be referred to as artificial, planned, or invented languages.

control theory
In control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines.
The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability.

convolutional neural network
In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.
CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing.
They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.

crossover
Also recombination.

In genetic algorithms and evolutionary computation, a genetic operator used to combine the genetic information of two parents to generate new offspring.
It is one way to stochastically generate new solutions from an existing population, and analogous to the crossover that happens during sexual reproduction in biological organisms.
Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction.
Newly generated solutions are typically mutated before being added to the population.

== D ==
Darkforest
A computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network.
Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search.
The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them.
With the update, the system is known as Darkfmcts3.

Dartmouth workshop
The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many (though not all) to be the seminal event for artificial intelligence as a field.

data augmentation
Data augmentation in data analysis are techniques used to increase the amount of data.
It helps reduce overfitting when training a machine learning.

data fusion
The process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source.

data integration
The process of combining data residing in different sources and providing users with a unified view of them.
This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains.
Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes.
It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

data mining
The process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.

data science
An interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.
Data science is a "concept to unify statistics, data analysis, machine learning and their related methods" in order to "understand and analyze actual phenomena" with data.
It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data set
Also
dataset.

A collection of data.
Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question.
The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set.

Each value is known as a datum.
The data set may comprise data for one or more members, corresponding to the number of rows.

data warehouse (DW or DWH)
Also enterprise data warehouse (EDW).

A system used for reporting and data analysis.
DWs are central repositories of integrated data from one or more disparate sources.
They store current and historical data in one single place
Datalog
A declarative logic programming language that syntactically is a subset of Prolog.
It is often used as a query language for deductive databases.
In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, and cloud computing.

decision boundary
In the case of backpropagation-based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has.
If it has no hidden layers, then it can only learn linear problems.
If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary.

decision support system (DSS)
Aan information system that supports business or organizational decision-making activities.
DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e.
unstructured and semi-structured decision problems.
Decision support systems can be either fully computerized or human-powered, or a combination of both.

decision theory
Also theory of choice.

The study of the reasoning underlying an agent's choices.
Decision theory can be broken into two branches: normative decision theory, which gives advice on how to make the best decisions given a set of uncertain beliefs and a set of values, and descriptive decision theory which analyzes how existing, possibly irrational agents actually make decisions.

decision tree learning
Uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).
It is one of the predictive modeling approaches used in statistics, data mining and machine learning.

declarative programming
A programming paradigm—a style of building the structure and elements of computer programs—that expresses the logic of a computation without describing its control flow.

deductive classifier
A type of artificial intelligence inference engine.
It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology.
For example, the names of classes, sub-classes, properties, and restrictions on allowable values.

Deep Blue
was a chess-playing computer developed by IBM.
It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls.

deep learning
Also deep structured learning or hierarchical learning.

Part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms.
Learning can be supervised, semi-supervised, or unsupervised.

DeepMind Technologies
A British artificial intelligence company founded in September 2010, currently owned by
Alphabet Inc. The company is based in London, with research centres in Canada, France, and the United States.
Acquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.
The company made headlines in 2016 after its AlphaGo program beat human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film.
A more general program, AlphaZero, beat the most powerful programs playing Go, chess, and shogi (Japanese chess)
after a few days of play against itself using reinforcement learning.

default logic
A non-monotonic logic proposed by Raymond Reiter to formalize reasoning with default assumptions.

description logic (DL)
A family of formal knowledge representation languages.
Many DLs are more expressive than propositional logic but less expressive than first-order logic.
In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems.
There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors.

developmental robotics (DevRob)
Also epigenetic robotics.

A scientific field which aims at studying the developmental mechanisms, architectures, and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines.

diagnosis
Concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct.
If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing.
The computation is based on observations, which provide information on the current behaviour.

dialogue system
Also conversational agent (CA).

A computer system intended to converse with a human with a coherent structure.
Dialogue systems have employed text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.

dimensionality reduction
Also dimension reduction.

The process of reducing the number of random variables under consideration by obtaining a set of principal variables.
It can be divided into feature selection and feature extraction.

discrete system
Any system with a countable number of states.
Discrete systems may be contrasted with continuous systems, which may also be called analog systems.
A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory.
Because discrete systems have a countable number of states, they may be described in precise mathematical models.
A computer is a finite state machine that may be viewed as a discrete system.
Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems.
One such method involves sampling a continuous signal at discrete time intervals.

distributed artificial intelligence (DAI)
Also decentralized artificial intelligence.

A subfield of artificial intelligence research dedicated to the development of distributed solutions for problems.
DAI is closely related to and a predecessor of the field of multi-agent systems.

dynamic epistemic logic (DEL)
A logical framework dealing with knowledge and information change.
Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur.

== E ==
eager learning
A learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system.

Ebert test
A test which gauges whether a computer-based synthesized voice can tell a joke with sufficient skill to cause people to laugh.
It was proposed by film critic Roger Ebert at the 2011 TED conference as a challenge to software developers to have a computerized voice master the inflections, delivery, timing, and intonations of a speaking human.
The test is similar to the Turing test proposed by Alan Turing in 1950 as a way to gauge a computer's ability to exhibit intelligent behavior by generating performance indistinguishable from a human being.

echo state network (ESN)
A recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity).
The connectivity and weights of hidden neurons are fixed and randomly assigned.
The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns.
The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons.
Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.

embodied agent
Also interface agent.

An intelligent agent that interacts with the environment through a physical body within that environment.
Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment.

embodied cognitive science
An interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior.
It comprises three main methodologies: 1)
the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2)
the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.

error-driven learning
A sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback.
It is a type of reinforcement learning.

ensemble averaging
In machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model.

ethics of artificial intelligence
The part of the ethics of technology specific to artificial intelligence.

evolutionary algorithm (EA)
A subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm.
An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.
Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function).
Evolution of the population then takes place after the repeated application of the above operators.

evolutionary computation
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms.
In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.

evolving classification function (ECF)
Evolving classifier functions or evolving classifiers are used for classifying and clustering in the field of machine learning and artificial intelligence, typically employed for data stream mining tasks in dynamic and changing environments.

existential risk
The hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe.

expert system
A computer system that emulates the decision-making ability of a human expert.
Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.

==
F ==
fast-and-frugal trees
A type of classification tree.
Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category.

feature extraction
In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations.

feature learning
In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.
This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.

feature selection
In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.

federated learning
A type of machine learning that allows for training on multiple devices with decentralized data, thus helping preserve the privacy of individual users and their data.

first-order logic
Also known as first-order predicate calculus and predicate logic.

A collection of formal systems used in mathematics, philosophy, linguistics, and computer science.
First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form
"there exists X such that X is
Socrates and X is a man" and there exists
is a quantifier while X is a variable.
This distinguishes it from propositional logic, which does not use quantifiers or relations.

fluent
A condition that can change over time.
In logical approaches to reasoning about actions, fluents can be represented in first-order logic by predicates having an argument that depends on time.

formal language
A set of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.

forward chaining
Also forward reasoning.

One of the two main methods of reasoning when using an inference engine and can be described logically as repeated application of modus ponens.
Forward chaining is a popular implementation strategy for expert systems, business and production rule systems.
The opposite of forward chaining is backward chaining.

Forward
chaining starts with the available data and uses inference rules to extract more data (from an end user, for example) until a goal is reached.
An inference engine using forward chaining searches the inference rules until it finds one where the antecedent (If clause) is known to be true.
When such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data.

frame
An artificial intelligence data structure used to divide knowledge into substructures by representing "stereotyped situations".
Frames are the primary data structure used in artificial intelligence frame language.

frame language
A technology used for knowledge representation in artificial intelligence.
Frames are stored as ontologies of sets and subsets of the frame concepts.
They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different.
Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding.
Frames originated in AI research and objects primarily in software engineering.
However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

frame problem
The problem of finding adequate collections of axioms for a viable description of a robot environment.

friendly artificial intelligence
Also friendly AI or FAI.

A hypothetical artificial general intelligence (AGI) that would have a positive effect on humanity.
It is a part of the ethics of artificial intelligence and is closely related to machine ethics.
While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.

futures studies
The study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them.

fuzzy control system
A control system based on fuzzy logic—a mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively).

fuzzy logic
A simple form for the many-valued logic, in which the truth values of variables may have any degree of "Truthfulness" that can be represented by any real number in the range between 0 (as in Completely False) and 1 (as in Completely True) inclusive.
Consequently, It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.
In contrast to Boolean logic, where the truth values of variables may have the integer values 0 or 1 only.

fuzzy rule
A rule used within fuzzy logic systems to infer an output based on input variables.

fuzzy set
In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition — an element either belongs or does not belong to the set.
By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1].
Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1.
In fuzzy set theory, classical bivalent sets are usually called crisp sets.
The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics.

==
G ==
game theory
The study of mathematical models of strategic interaction between rational decision-makers.

general game playing (GGP)
General game playing is the design of artificial intelligence programs to be able to run and play more than one game successfully.

generative adversarial network (GAN)
A class of machine learning systems.
Two neural networks contest with each other in a zero-sum game framework.

genetic algorithm (GA)
A metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA).
Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection.

genetic operator
An operator used in genetic algorithms to guide the algorithm towards a solution to a given problem.
There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful.

glowworm swarm optimization
A swarm intelligence optimization algorithm based on the behaviour of glowworms (also known as fireflies or lightning bugs).

graph (abstract data type)
In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from mathematics; specifically, the field of graph theory.

graph (discrete mathematics)
In mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense "related".
The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called an arc or line).

graph database (GDB)
A database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data.
A key concept of the system is the graph (or edge or relationship), which directly relates data items in the store a collection of nodes of data and edges representing the relationships between the nodes.
The relationships allow data in the store to be linked together directly, and in many cases retrieved with one operation.
Graph databases hold the relationships between data as a priority.
Querying relationships within a graph database is fast because they are perpetually stored within the database itself.
Relationships can be intuitively visualized using graph databases, making it useful for heavily inter-connected data.

graph theory
The study of graphs, which are mathematical structures used to model pairwise relations between objects.

graph traversal
Also graph search.

The process of visiting (checking and/or updating)
each vertex in a graph.
Such traversals are classified by the order in which the vertices are visited.
Tree traversal is a special case of graph traversal.

== H ==
halting problem
heuristic
A technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.

This is achieved by trading optimality, completeness, accuracy, or precision for speed.

In a way, it can be considered a shortcut.

A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow.
For example, it may approximate the exact solution.

hidden layer
An internal layer of neurons in an artificial neural network, not dedicated to input or output.

hidden unit
A neuron in a hidden layer in an artificial neural network.

hyper-heuristic
A heuristic search method that seeks to automate the process of selecting, combining, generating, or adapting several simpler heuristics (or components of such heuristics) to efficiently solve computational search problems, often by the incorporation of machine learning techniques.
One of the motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just one problem.

==
I ==
IEEE Computational Intelligence Society
A professional society of the Institute of Electrical and Electronics Engineers (IEEE) focussing on "the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained".

incremental learning
A method of machine learning, in which input data is continuously used to extend the existing model's knowledge i.e. to further train the model.
It represents a dynamic technique of supervised learning and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits.
Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms.

inference engine
A component of the system that applies logical rules to the knowledge base to deduce new information.

information integration (II)
The merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations.
It is used in data mining and consolidation of data from unstructured or semi-structured resources.
Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content.
Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.

Information Processing Language (IPL)
A programming language that includes features intended to help with programs that perform simple problem solving actions such as lists, dynamic memory allocation, data types, recursion, functions as arguments, generators, and cooperative multitasking.

IPL invented the concept of list processing, albeit in an assembly-language style.

intelligence amplification (IA)
Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence.

The effective use of information technology in augmenting human intelligence.

intelligence explosion
A possible outcome of humanity building artificial general intelligence (AGI).
AGI would be capable of recursive self-improvement leading to rapid emergence of ASI (artificial superintelligence), the limits of which are unknown, at the time of the technological singularity.

intelligent agent (IA)
An autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).
Intelligent agents may also learn or use knowledge to achieve their goals.
They may be very simple or very complex.

intelligent control
A class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms.

intelligent personal assistant
Also virtual assistant or personal digital assistant.

A software agent that can perform tasks or services for an individual based on verbal commands.
Sometimes the term "chatbot" is used to refer to virtual assistants generally or specifically accessed by online chat (or in some cases online chat programs that are exclusively for entertainment purposes).

Some virtual assistants are able to interpret human speech and respond via synthesized voices.
Users can ask their assistants questions, control home automation devices and media playback via voice, and manage other basic tasks such as email, to-do lists, and calendars with verbal commands.

interpretation
An assignment of meaning to the symbols of a formal language.
Many formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation.
The general study of interpretations of formal languages is called formal semantics.

intrinsic motivation
An intelligent agent is intrinsically motivated to act if the information content alone, of the experience resulting from the action, is the motivating factor.
Information content in this context is measured in the information theory sense as quantifying uncertainty.
A typical intrinsic motivation is to search for unusual (surprising) situations, in contrast to a typical extrinsic motivation such as the search for food.
Intrinsically motivated artificial agents display behaviours akin to exploration and curiosity.

issue tree
Also logic tree.

A graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.

Issue trees are useful in problem solving to identify the root causes of a problem as well as to identify its potential solutions.
They also provide a reference point to see how each piece fits into the whole picture of a problem.

==
J ==
junction tree algorithm
Also Clique Tree.

A method used in machine learning to extract marginalization in general graphs.
In essence, it entails performing belief propagation on a modified graph called a junction tree.
The graph is called a tree because it branches into different sections of data; nodes of variables are the branches.

==
K ==
kernel method
In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM).
The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets.

KL-ONE
A well-known knowledge representation system in the tradition of semantic networks and frames
; that is, it is a frame language.
The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.

knowledge acquisition
The process used to define the rules and ontologies required for a knowledge-based system.
The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies.

knowledge-based system (KBS)
A computer program that reasons and uses a knowledge base to solve complex problems.
The term is broad and refers to many different kinds of systems.
The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge.
Thus, a knowledge-based system has two distinguishing features: a knowledge base and an inference engine.

knowledge engineering (KE)
All technical, scientific, and social aspects involved in building, maintaining, and using knowledge-based systems.

knowledge extraction
The creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources.
The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing.
Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criteria is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema.
It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.

knowledge Interchange Format (KIF)
A computer language designed to enable systems to share and re-use information from knowledge-based systems.
KIF is similar to frame languages such as KL-ONE and LOOM but
unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems.
The designers of KIF likened it to PostScript.
PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents.
In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc.

<dt class="glossary " id="knowledge representation and reasoning (kr2 or kr&r)
" style="margin-top: 0.4em;">knowledge
representation and reasoning (KR2 or KR&R)
The field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language.
Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.

Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.
Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies.
Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.

==
L ==
lazy learning
In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.

Lisp (programming language) (LISP)
A family of programming languages with a long history and a distinctive, fully parenthesized prefix notation.

logic programming
A type of programming paradigm which is largely based on formal logic.
Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.
Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.

long short-term memory (LSTM)
An artificial recurrent neural network architecture used in the field of deep learning.
Unlike standard feedforward neural networks, LSTM has feedback connections that make it a "general purpose computer"
(that is, it can compute anything that a Turing machine can).
It can not only process single data points (such as images), but also entire sequences of data (such as speech or video).

==
M ==
machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry.

Machine vision is a term encompassing a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise.
Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science.

It attempts to integrate existing technologies in new ways and apply them to solve real world problems.
The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

Markov chain
A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

Markov decision process (MDP)
A discrete time stochastic control process.
It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.
MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning.

mathematical optimization
Also mathematical programming.

In mathematics, computer science, and operations research, the selection of a best element (with regard to some criterion) from some set of available alternatives.

machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead.

machine listening
Also computer audition (CA).

A general field of study of algorithms and systems for audio understanding by machine.

machine perception
The capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.

mechanism design
A field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally.
Because it starts at the end of the game, then goes backwards, it is also called reverse game theory.
It has broad applications, from economics and politics (markets, auctions, voting procedures) to networked-systems (internet interdomain routing, sponsored search auctions).

mechatronics
Also mechatronic engineering.

A multidisciplinary branch of engineering that focuses on the engineering of both electrical and mechanical systems, and also includes a combination of robotics, electronics, computer, telecommunications, systems, control, and product engineering.

metabolic network reconstruction and simulation
Allows for an in-depth insight into the molecular mechanisms of a particular organism.
In particular, these models correlate the genome with molecular physiology.

metaheuristic
In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity.
Metaheuristics sample a set of solutions which is too large to be completely sampled.

model checking
In computer science, model checking or property checking is, for a given model of a system, exhaustively and automatically checking whether this model meets a given specification.
Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash.
Model checking is a technique for automatically verifying correctness properties of finite-state systems.

modus ponens
In propositional logic, modus ponens is a rule of inference.
It can be summarized as "P implies Q and P is asserted to be true, therefore Q must be true."

modus tollens
In propositional logic, modus tollens is a valid argument form and a rule of inference.

It is an application of the general truth that if a statement is true, then so is its contrapositive.

The inference rule modus tollens asserts that the inference from P implies Q to the negation of Q implies the negation of P is valid.

Monte Carlo tree search
In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes.
multi-agent system (MAS)
Also self-organized system.

A computerized system composed of multiple interacting intelligent agents.
Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.
Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.

multi-swarm optimization
A variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm.
The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms.
The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist.

mutation
A genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next.
It is analogous to biological mutation.
Mutation alters one or more gene values in a chromosome from its initial state.
In mutation, the solution may change entirely from the previous solution.
Hence GA can come to a better solution by using mutation.
Mutation occurs during evolution according to a user-definable mutation probability.
This probability should be set low.
If it is set too high, the search will turn into a primitive random search.

Mycin
An early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patient's body weight – the name derived from the antibiotics themselves, as many antibiotics have the suffix "-mycin".
The MYCIN system was also used for the diagnosis of blood clotting diseases.

==
N ==
naive Bayes classifier
In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.

naive semantics
An approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications.
In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas.

name binding
In programming languages, name binding is the association of entities (data and/or code) with identifiers.
An identifier bound to an object is said to reference that object.
Machine languages have no built-in notion of identifiers, but name-object bindings as a service and notation for the programmer is implemented by programming languages.
Binding is intimately connected with scoping, as scope determines which names bind to which objects – at which locations in the program code (lexically) and in which one of the possible execution paths (temporally).

Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence.
In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to; such occurrences are called applied occurrences.

named-entity recognition (NER)
Also entity identification, entity chunking, and entity extraction.

A subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.

named graph
A key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata.

Named graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large.

natural language generation (NLG)
A software process that transforms structured data into plain-English content.

It can be used to produce long-form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application.
It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out loud by a text-to-speech system.

natural language processing (NLP)
A subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

natural language programming
An ontology-assisted way of programming in terms of natural-language sentences, e.g. English.

network motif
All networks, including biological networks, social networks, technological networks (e.g., computer networks and electrical circuits) and more, can be represented as graphs, which include a wide variety of subgraphs.
One important local property of networks are so-called network motifs, which are defined as recurrent and statistically significant sub-graphs or patterns.

neural machine translation (NMT)
An approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.

neural Turing machine (NTM)
A recurrent neural network model.
NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers.
An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms.
The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent.
An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.

neuro-fuzzy
Combinations of artificial neural networks and fuzzy logic.

neurocybernetics
Also brain–computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain–machine interface (BMI).

A direct communication pathway between an enhanced or wired brain and an external device.
BCI differs from neuromodulation in that it allows for bidirectional information flow.
BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.

neuromorphic engineering
Also neuromorphic computing.

A concept describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system.
In recent times, the term neuromorphic has been used to describe
analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration).
The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, and transistors.

node
A basic unit of a data structure, such as a linked list or tree data structure.
Nodes contain data and also may link to other nodes.
Links between nodes are often implemented by pointers.

nondeterministic algorithm
An algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm.

nouvelle AI
Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects.
Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the "real world", instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.

NP
In computational complexity theory, NP (nondeterministic polynomial time) is a complexity class used to classify decision problems.

NP is the set of decision problems for which the problem instances, where the answer is "yes", have proofs verifiable in polynomial time.

NP-completeness
In computational complexity theory, a problem is NP-complete when it can be solved by a restricted class of brute force search algorithms and it can be used to simulate any other problem with a similar algorithm.
More precisely, each input to the problem should be associated with a set of solutions of polynomial length, whose validity can be tested quickly (in polynomial time), such that the output for any input is "yes" if the solution set is non-empty and "no" if it is empty.

NP-hardness
Also non-deterministic polynomial-time hardness.

In computational complexity theory, the defining property of a class of problems that are, informally, "at least as hard as the hardest problems in NP".
A simple example of an NP-hard problem is the subset sum problem.

==
O ==
Occam's razor
Also Ockham's razor or Ocham's razor.

The problem-solving principle that states that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions; the principle is not meant to filter out hypotheses that make different predictions.
The idea is attributed to the English Franciscan friar William of Ockham (c. 1287–1347), a scholastic philosopher and theologian.

offline learning

online machine learning
A method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once.
Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms.
It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time.

ontology learning
Also ontology extraction, ontology generation, or ontology acquisition.

The automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval.

OpenAI
The for-profit corporation OpenAI LP, whose parent organization is the non-profit organization OpenAI Inc that conducts research in the field of artificial intelligence (AI) with the stated aim to promote and develop friendly AI in such a way as to benefit humanity as a whole.

OpenCog
A project that aims to build an open-source artificial intelligence framework.
OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system.

Open Mind Common Sense
An artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web.
open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose.
Open-source software may be developed in a collaborative public manner.
Open-source software is a prominent example of open collaboration.

==
P ==
partial order reduction
A technique for reducing the size of the state-space to be searched by a model checking or automated planning and scheduling algorithm.
It exploits the commutativity of concurrently executed transitions, which result in the same state when executed in different orders.

partially observable Markov decision process (POMDP)
A generalization of a Markov decision process (MDP).
A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state.
Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.

particle swarm optimization (PSO)
A computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality.
It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity.
Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles.
This is expected to move the swarm toward the best solutions.

pathfinding
Also pathing.

The plotting, by a computer application, of the shortest route between two points.
It is a more practical variant on solving mazes.
This field of research is based heavily on Dijkstra's algorithm for finding a shortest path on a weighted graph.

pattern recognition
Concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.

predicate logic
Also first-order logic, predicate logic, and first-order predicate calculus.

A collection of formal systems used in mathematics, philosophy, linguistics, and computer science.
First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form
"there exists x such that x is Socrates and x is a man" and there exists is a quantifier while x is a variable.
This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic.

predictive analytics
A variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.

principal component analysis (PCA)
A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.
This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to the preceding components.
The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set.
PCA is sensitive to the relative scaling of the original variables.

principle of rationality
Also rationality principle.

A principle coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework.
It is related to what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism.
According to Popper's rationality principle, agents act in the most adequate way according to the objective situation.
It is an idealized conception of human behavior which he used to drive his model of situational analysis.

probabilistic programming (PP)
A programming paradigm in which probabilistic models are specified and inference for these models is performed automatically.
It represents an attempt to unify probabilistic modeling and traditional general-purpose programming in order to make the former easier and more widely applicable.
It can be used to create systems that help make decisions in the face of uncertainty.

Programming languages used for probabilistic programming are referred to as "Probabilistic programming languages" (PPLs).

production system

programming language
A formal language, which comprises a set of instructions that produce various kinds of output.
Programming languages are used in computer programming to implement algorithms.

Prolog
A logic programming language associated with artificial intelligence and computational linguistics.

Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.

A computation is initiated by running a query over these relations.

propositional calculus
Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic.

A branch of logic which deals with propositions (which can be true or false) and argument flow.
Compound propositions are formed by connecting propositions by logical connectives.
The propositions without logical connectives are called atomic propositions.
Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers.
However, all the machinery of propositional logic is included in first-order logic and higher-order logics.
In this sense, propositional logic is the foundation of first-order logic and higher-order logic.

Python
An interpreted, high-level, general-purpose programming language created by Guido van Rossum and first released in 1991.
Python's design philosophy emphasizes code readability with its notable use of significant whitespace.
Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.

==
Q ==
qualification problem
In philosophy and artificial intelligence (especially knowledge-based systems), the qualification problem is concerned with the impossibility of listing all of the preconditions required for a real-world action to have its intended effect.
It might be posed as how to deal with the things that prevent me from achieving my intended result.
It is strongly connected to, and opposite the ramification side of, the frame problem.

quantifier
In logic, quantification specifies the quantity of specimens in the domain of discourse that satisfy an open formula.
The two most common quantifiers mean "for all" and "there exists".
For example, in arithmetic, quantifiers allow one to say that the natural numbers go on forever, by writing that for all n (where n is a natural number), there is another number (say, the successor of n) which is one bigger than n.

quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation.
A quantum computer is used to perform such computation, which can be implemented theoretically or physically.

query language
Query languages or data query languages (DQLs) are computer languages used to make queries in databases and information systems.

Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages.
The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.

== R ==
R programming language
A programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.
The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radial basis function network
In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions.
The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters.
Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control.
They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.

random forest
Also random decision forest.

An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
Random decision forests correct for decision trees' habit of overfitting to their training set.

reasoning system
In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction.
Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.

recurrent neural network (RNN)
A class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence.
This allows it to exhibit temporal dynamic behavior.
Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs.
This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.

region connection calculus

reinforcement learning (RL)
An area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.
Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.

It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected.
Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).

reservoir computing
A framework for computation that may be viewed as an extension of neural networks.
Typically an input signal is fed into a fixed (random) dynamical system called a reservoir and the dynamics of the reservoir map the input to a higher dimension.
Then a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output.
The main benefit is that training is performed only at the readout stage and the reservoir is fixed.
Liquid-state machines and echo state networks are two major types of reservoir computing.

Resource Description Framework (RDF)
A family of World Wide Web Consortium (W3C) specifications originally designed as a metadata data model.
It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats.
It is also used in knowledge management applications.

restricted Boltzmann machine (RBM)
A generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.

Rete algorithm
A pattern matching algorithm for implementing rule-based systems.
The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base.
It is used to determine which of the system's rules should fire based on its data store, its facts.

robotics
An interdisciplinary branch of science and engineering that includes mechanical engineering, electronic engineering, information engineering, computer science, and others.
Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing.

rule-based system
In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way.
It is often used in artificial intelligence applications and research.

Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets.

Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type.

==
S ==
satisfiability
In mathematical logic, satisfiability and validity are elementary concepts of semantics.
A formula is satisfiable if it is possible to find an interpretation (model) that makes the formula true.

A formula is valid if all interpretations make the formula true.
The opposites of these concepts are unsatisfiability and invalidity, that is, a formula is unsatisfiable if none of the interpretations make the formula true, and invalid if some such interpretation makes the formula false.
These four concepts are related to each other in a manner exactly analogous to Aristotle's square of opposition.

search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

selection
The stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).

self-management
The process by which computer systems manage their own operation without human intervention.

semantic network
Also frame network.

A knowledge base that represents semantic relations between concepts in a network.
This is often used as a form of knowledge representation.
It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts, mapping or connecting semantic fields.

semantic reasoner
Also reasoning engine, rules engine, or simply reasoner.

A piece of software able to infer logical consequences from a set of asserted facts or axioms.
The notion of a semantic reasoner generalizes that of an inference engine, by providing a richer set of mechanisms to work with.
The inference rules are commonly specified by means of an ontology language, and often a description logic language.
Many reasoners use first-order predicate logic to perform reasoning; inference commonly proceeds by forward chaining and backward chaining.

semantic query
Allows for queries and analytics of associative and contextual nature.
Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data.
They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide-open questions through pattern matching and digital reasoning.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages.
It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved.
In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation.
Semantics describes the processes a computer follows when executing a program in that specific language.
This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sensor fusion
The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually.

separation logic
An extension of Hoare logic, a way of reasoning about programs.
The assertion language of separation logic is a special case of the logic of bunched implications (BI).

similarity learning
An area of supervised machine learning in artificial intelligence.
It is closely related to regression and classification, but the goal is to learn from a similarity function that measures how similar or related two objects are.
It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.

simulated annealing (SA)
A probabilistic technique for approximating the global optimum of a given function.
Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem.

situated approach
In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment.
This requires designing AI "from the bottom-up" by focussing on the basic perceptual and motor skills required to survive.
The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.

situation calculus
A logic formalism designed for representing and reasoning about dynamical domains.

Selective Linear Definite clause resolution
Also simply SLD resolution.

The basic inference rule used in logic programming.
It is a refinement of resolution, which is both sound and refutation complete for Horn clauses.

software
A collection of data or computer instructions that tell the computer how to work.
This is in contrast to physical hardware, from which the system is built and actually performs the work.
In computer science and software engineering, computer software is all information processed by computer systems, programs and data.
Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media.

software engineering
The application of engineering to the development of software in a systematic method.

spatial-temporal reasoning
An area of artificial intelligence which draws from the fields of computer science, cognitive science, and cognitive psychology.
The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind.
The applied goal—on the computing side—involves developing high-level control systems of automata for navigating and understanding time and space.

SPARQL
An RDF query language—that is, a semantic query language for databases—able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.

speech recognition
An interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers.
It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT).
It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.

spiking neural network (SNN)
An artificial neural network that more closely mimics a natural neural network.
In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their Operating Model.

state
In information technology and computer science, a program is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statistical classification
In machine learning and statistics, classification is the problem of identifying to which of a set of categories
(sub-populations)
a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.

Examples are assigning a given email to the "spam" or "non-spam" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).

Classification is an example of pattern recognition.

statistical relational learning (SRL)
A subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.
Note that SRL is sometimes called Relational Machine Learning (RML) in the literature.
Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming.

stochastic optimization
(SO)
Any optimization method that generates and uses random variables.
For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints.
Stochastic optimization methods also include methods with random iterates.
Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization.
Stochastic optimization methods generalize deterministic methods for deterministic problems.

stochastic semantic analysis
An approach used in computer science as a semantic component of natural language understanding.
Stochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach.

Stanford Research Institute Problem Solver (STRIPS)

subject-matter expert

superintelligence
A hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds.
Superintelligence may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act within the physical world.
A superintelligence may or may not be created by an intelligence explosion and be associated with a technological singularity.

supervised learning
The machine learning task of learning a function that maps an input to an output based on example input-output pairs.
It infers a function from labeled training data consisting of a set of training examples.

In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).

A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.
An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.
This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).

support-vector machines
In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.

swarm intelligence (SI)
The collective behavior of decentralized, self-organized systems, either natural or artificial.
The expression was introduced in the context of cellular robotic systems.

symbolic artificial intelligence
The term for the collection of all methods in artificial intelligence research that are based on high-level "symbolic" (human-readable) representations of problems, logic, and search.

synthetic intelligence (SI)
An alternative term for artificial intelligence which emphasizes that the intelligence of machines need not be an imitation or in any way artificial; it can be a genuine form of intelligence.

systems neuroscience
A subdiscipline of neuroscience and systems biology that studies the structure and function of neural circuits and systems.

It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks.

==
T ==
technological singularity
Also simply the singularity.

A hypothetical point in the future when technological growth becomes uncontrollable and irreversible, resulting in unfathomable changes to human civilization.

temporal difference learning
A class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function.
These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.

tensor network theory
A theory of brain function (particularly that of the cerebellum)
that provides a mathematical model of the transformation of sensory space-time coordinates into motor coordinates and vice versa by cerebellar neuronal networks.
The theory was developed as a geometrization of brain function (especially of the central nervous system) using tensors.

TensorFlow
A free and open-source software library for dataflow and differentiable programming across a range of tasks.
It is a symbolic math library, and is also used for machine learning applications such as neural networks.

theoretical computer science (TCS)
A subset of general computer science and mathematics that focuses on more mathematical topics of computing and includes the theory of computation.

theory of computation
In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm.
The field is divided into three major branches: automata theory and languages, computability theory, and computational complexity theory, which are linked by the question: "What are the fundamental capabilities and limitations of computers?".

Thompson sampling
A heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem.
It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.

time complexity
The computational complexity that describes the amount of time it takes to run an algorithm.
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform.
Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor.

transhumanism
Abbreviated H+ or h+.

An international philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology.

transition system
In theoretical computer science, a transition system is a concept used in the study of computation.
It is used to describe the potential behavior of discrete systems.
It consists of states and transitions between states, which may be labeled with labels chosen from a set; the same label may appear on more than one transition.
If the label set is a singleton, the system is essentially unlabeled, and a simpler definition that omits the labels is possible.

tree traversal
Also tree search.

A form of graph traversal and refers to the process of visiting (checking and/or updating)
each node in a tree data structure, exactly once.
Such traversals are classified by the order in which the nodes are visited.

true quantified Boolean formula
In computational complexity theory, the language TQBF is a formal language consisting of the true quantified Boolean formulas.

A (fully) quantified Boolean formula is a formula in quantified propositional logic where every variable is quantified (or bound), using either existential or universal quantifiers, at the beginning of the sentence.
Such a formula is equivalent to either true or false (since there are no free variables).
If such a formula evaluates to true, then that formula is in the language TQBF.
It is also known as QSAT (Quantified SAT).

Turing machine
Turing test
A test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human, developed by Alan Turing in 1950.
Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses.
The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another.
The conversation would be limited to a text-only channel such as a computer keyboard and screen
so the result would not depend on the machine's ability to render words as speech.
If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test.
The test results do not depend on the machine's ability to give correct answers to questions, only how closely its answers resemble those a human would give.

type system
In programming languages, a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions, or modules.
These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. "string",
"array of float", "function returning boolean").
The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way.
This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking.
Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc.

==
U ==
unsupervised learning
A type of self-organized Hebbian learning that helps find previously unknown patterns in data set without pre-existing labels.
It is also known as self-organization and allows modeling probability densities of given inputs.
It is one of the main three categories of machine learning, along with supervised and reinforcement learning.
Semi-supervised learning has also been described and is a hybridization of supervised and unsupervised techniques.

==
V ==
vision processing unit (VPU)
A type of microprocessor designed to accelerate machine vision tasks.
Value-alignment complete –
Analogous to an AI-complete problem, a value-alignment complete problem is a problem where the AI control problem needs to be fully solved to solve it.

==
W ==
Watson
A question-answering computer system capable of answering questions posed in natural language, developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci.
Watson was named after IBM's first CEO, industrialist Thomas J. Watson.

weak AI
Also narrow AI.

Artificial intelligence that is focused on one narrow task.

World Wide Web Consortium (W3C)
The main international standards organization for the World Wide Web (abbreviated WWW or W3).

== See also ==
Artificial intelligence


==
References ==


== Notes ==
In computer science, divide and conquer is an algorithm design paradigm.
A divide-and-conquer algorithm recursively breaks down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly.
The solutions to the sub-problems are then combined to give a solution to the original problem.

The divide-and-conquer technique is the basis of efficient algorithms for many problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g., the Karatsuba algorithm), finding the closest pair of points, syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFT).Designing efficient divide-and-conquer algorithms can be difficult.
As in mathematical induction, it is often necessary to generalize the problem to make it amenable to recursive solution.
The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.

== Divide and conquer ==
The divide-and-conquer paradigm is often used to find an optimal solution of a problem.
Its basic idea is to decompose a given problem into two or more similar, but simpler, subproblems, to solve them in turn, and to compose their solutions to solve the given problem.
Problems of sufficient simplicity are solved directly.

For example, to sort a given list of n natural numbers, split it into two lists of about n/2 numbers each, sort each of them in turn, and interleave both results appropriately to obtain the sorted version of the given list (see the picture).
This approach is known as the merge sort algorithm.

The name "divide and conquer" is sometimes applied to algorithms that reduce each problem to only one sub-problem, such as the binary search algorithm for finding a record in a sorted list (or its analog in numerical computing, the bisection algorithm for root finding).

These algorithms can be implemented more efficiently than general divide-and-conquer algorithms; in particular, if they use tail recursion, they can be converted into simple loops.

Under this broad definition, however, every algorithm that uses recursion or loops could be regarded as a "divide-and-conquer algorithm".

Therefore, some authors consider that the name "divide and conquer" should be used only when each problem may generate two or more subproblems.
The name decrease and conquer has been proposed instead for the single-subproblem class.
An important application of divide and conquer is in optimization, where if the search space is reduced ("pruned") by a constant factor at each step, the overall algorithm has the same asymptotic complexity as the pruning step, with the constant depending on the pruning factor (by summing the geometric series); this is known as prune and search.

==
Early historical examples ==
Early examples of these algorithms are primarily decrease and conquer – the original problem is successively broken down into single subproblems, and indeed can be solved iteratively.

Binary search, a decrease-and-conquer algorithm where the subproblems are of roughly half the original size, has a long history.
While a clear description of the algorithm on computers appeared in 1946 in an article by John Mauchly, the idea of using a sorted list of items to facilitate searching dates back at least as far as Babylonia in 200 BC.
Another ancient decrease-and-conquer algorithm is the Euclidean algorithm to compute the greatest common divisor of two numbers by reducing the numbers to smaller and smaller equivalent subproblems, which dates to several centuries BC.

An early example of a divide-and-conquer algorithm with multiple subproblems is Gauss's 1805 description of what is now called the Cooley–Tukey fast Fourier transform (FFT) algorithm, although he did not analyze its operation count quantitatively, and FFTs did not become widespread until they were rediscovered over a century later.

An early two-subproblem D&C algorithm that was specifically developed for computers and properly analyzed is the merge sort algorithm, invented by John von Neumann in 1945.Another notable example
is the algorithm invented by Anatolii A. Karatsuba in 1960 that could multiply two n-digit numbers in 
  
    
      
        O
(
        
          n
          
            
              log
              
                2
⁡
            3
          
        
        )
{\displaystyle O(n^{\log _{2}3})}
   operations (in Big O notation).
This algorithm disproved Andrey Kolmogorov's 1956 conjecture that 
  
    
      
        Ω
(
        
          n
          
            2
          
        
        )
{\displaystyle \Omega (n^{2})}
   operations would be required for that task.

As another example of a divide-and-conquer algorithm that did not originally involve computers, Donald Knuth gives the method a post office typically uses to route mail: letters are sorted into separate bags for different geographical areas, each of these bags is itself sorted into batches for smaller sub-regions, and so on until they are delivered.
This is related to a radix sort, described for punch-card sorting machines as early as 1929.

==
Advantages ==


===
Solving difficult problems ===
Divide and conquer is a powerful tool for solving conceptually difficult problems: all it requires is a way of breaking the problem into sub-problems, of solving the trivial cases and of combining sub-problems to the original problem.
Similarly, decrease and conquer only requires reducing the problem to a single smaller problem, such as the classic Tower of Hanoi puzzle, which reduces moving a tower of height
n
      
    
    {\displaystyle n}
   to moving a tower of height
n
−
1
      
    
    {\displaystyle n-1}
  .
===
Algorithm efficiency ===
The divide-and-conquer paradigm often helps in the discovery of efficient algorithms.

It was the key, for example, to Karatsuba's fast multiplication method, the quicksort and mergesort algorithms, the Strassen algorithm for matrix multiplication, and fast Fourier transforms.

In all these examples, the D&C approach led to an improvement in the asymptotic cost of the solution.
For example, if (a) the base cases have constant-bounded size, the work of splitting the problem and combining the partial solutions is proportional to the problem's size
n
      
    
    {\displaystyle n}
  , and
(b) there is a bounded number
p
{\displaystyle p}
   of sub-problems of size
~ 
  
    
      
        n
        
          /
        
        p
      
    
    {\displaystyle n
/p}
   at each stage, then the cost of the divide-and-conquer algorithm will be 
  
    
      
        O
(
        n
        
          log
          
            p
⁡
n
        )
{\displaystyle O(n\log _{p}n)}
  .
===
Parallelism ===
Divide-and-conquer algorithms are naturally adapted for execution in multi-processor machines, especially shared-memory systems where the communication of data between processors does not need to be planned in advance, because distinct sub-problems can be executed on different processors.

===
Memory access ===
Divide-and-conquer
algorithms naturally tend to make efficient use of memory caches.
The reason is that once a sub-problem is small enough, it and all its sub-problems can, in principle, be solved within the cache, without accessing the slower main memory.
An algorithm designed to exploit the cache in this way is called cache-oblivious, because it does not contain the cache size as an explicit parameter.
Moreover, D&C algorithms can be designed for important algorithms (e.g., sorting, FFTs, and matrix multiplication) to be optimal cache-oblivious algorithms–they use the cache in a probably optimal way, in an asymptotic sense, regardless of the cache size.
In contrast, the traditional approach to exploiting the cache is blocking, as in loop nest optimization, where the problem is explicitly divided into chunks of the appropriate size—this can also use the cache optimally, but only when the algorithm is tuned for the specific cache sizes of a particular machine.

The same advantage exists with regards to other hierarchical storage systems, such as NUMA or virtual memory, as well as for multiple levels of cache:
once a sub-problem is small enough, it can be solved within a given level of the hierarchy, without accessing the higher (slower) levels.

===
Roundoff control ===
In computations with rounded arithmetic, e.g. with floating-point numbers,
a divide-and-conquer algorithm may yield more accurate results than a superficially equivalent iterative method.
For example, one can add N numbers either by a simple loop that adds each datum to a single variable, or by a D&C algorithm called pairwise summation that breaks the data set into two halves, recursively computes the sum of each half, and then adds the two sums.

While the second method performs the same number of additions as the first, and pays the overhead of the recursive calls, it is usually more accurate.

==
Implementation issues ==


===
Recursion ===
Divide-and-conquer algorithms are naturally implemented as recursive procedures.
In that case, the partial sub-problems leading to the one currently being solved are automatically stored in the procedure call stack.
A recursive function is a function that calls itself within its definition.

===
Explicit stack ===
Divide-and-conquer algorithms can also be implemented by a non-recursive program that stores the partial sub-problems in some explicit data structure, such as a stack, queue, or priority queue.

This approach allows more freedom in the choice of the sub-problem that is to be solved next, a feature that is important in some applications — e.g. in breadth-first recursion and the branch-and-bound method for function optimization.
This approach is also the standard solution in programming languages that do not provide support for recursive procedures.

===
Stack size ===
In recursive implementations of D&C algorithms, one must make sure that there is sufficient memory allocated for the recursion stack, otherwise the execution may fail because of stack overflow.

D&C algorithms that are time-efficient often have relatively small recursion depth.

For example, the quicksort algorithm can be implemented so that it never requires more than
log
          
            2
          
        
        ⁡
n
      
    
    {\displaystyle \log _{
2}n}
   nested recursive calls to sort
n
      
    
    {\displaystyle n}
   items.

Stack overflow may be difficult to avoid when using recursive procedures, since many compilers assume that the recursion stack is a contiguous area of memory, and some allocate a fixed amount of space for it.

Compilers may also save more information in the recursion stack than is strictly necessary, such as return address, unchanging parameters, and the internal variables of the procedure.

Thus, the risk of stack overflow can be reduced by minimizing the parameters and internal variables of the recursive procedure or by using an explicit stack structure.

===
Choosing the base cases ===
In any recursive algorithm, there is considerable freedom in the choice of the base cases, the small subproblems that are solved directly in order to terminate the recursion.

Choosing the smallest or simplest possible base cases is more elegant and usually leads to simpler programs, because there are fewer cases to consider and they are easier to solve.
For example, an FFT algorithm could stop the recursion when the input is a single sample, and the quicksort list-sorting algorithm could stop when the input is the empty list; in both examples there is only one base case to consider, and it requires no processing.

On the other hand, efficiency often improves if the recursion is stopped at relatively large base cases, and these are solved non-recursively, resulting in a hybrid algorithm.
This strategy avoids the overhead of recursive calls that do little or no work, and may also allow the use of specialized non-recursive algorithms that, for those base cases, are more efficient than explicit recursion.
A general procedure for a simple hybrid recursive algorithm is short-circuiting the base case, also known as arm's-length recursion.
In this case whether the next step will result in the base case is checked before the function call, avoiding an unnecessary function call.
For example, in a tree, rather than recursing to a child node and then checking whether it is null, checking null before recursing; this avoids half the function calls in some algorithms on binary trees.
Since a D&C algorithm eventually reduces each problem or sub-problem instance to a large number of base instances, these often dominate the overall cost of the algorithm, especially when the splitting/joining overhead is low.
Note that these considerations do not depend on whether recursion is implemented by the compiler or by an explicit stack.

Thus, for example, many library implementations of quicksort will switch to a simple loop-based insertion sort (or similar) algorithm once the number of items to be sorted is sufficiently small.
Note that, if the empty list were the only base case, sorting a list with
n
      
    
    {\displaystyle n}
   entries would entail maximally
n
      
    
    {\displaystyle n}
   quicksort calls that would do nothing but return immediately.

Increasing the base cases to lists of size 2 or less will eliminate most of those do-nothing calls, and more generally a base case larger than 2 is typically used to reduce the fraction of time spent in function-call overhead or stack manipulation.

Alternatively, one can employ large base cases that still use a divide-and-conquer algorithm, but implement the algorithm for predetermined set of fixed sizes where the algorithm can be completely unrolled into code that has no recursion, loops, or conditionals (related to the technique of partial evaluation).

For example, this approach is used in some efficient FFT implementations, where the base cases are unrolled implementations of divide-and-conquer FFT algorithms for a set of fixed sizes.

Source-code generation methods may be used to produce the large number of separate base cases desirable to implement this strategy efficiently.
The generalized version of this idea is known as recursion "unrolling" or "coarsening", and various techniques have been proposed for automating the procedure of enlarging the base case.

===
Sharing repeated subproblems ===
For some problems, the branched recursion may end up evaluating the same sub-problem many times over.

In such cases it may be worth identifying and saving the solutions to these overlapping subproblems, a technique commonly known as memoization.

Followed to the limit, it leads to bottom-up divide-and-conquer algorithms such as dynamic programming and chart parsing.

== See also ==
Akra–Bazzi method
Decomposable aggregation function
Fork–join model
Master theorem (analysis of algorithms)
Mathematical induction
MapReduce
Heuristic (computer science)


==
References ==
Computer science is the study of algorithmic processes, computational machines and computation itself.
As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.
Its fields can be divided into theoretical and practical disciplines.
For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications.
Algorithms and data structures have been called the heart of computer science.
Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems.
Computer architecture describes construction of computer components and computer-operated equipment.
Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals.
A digital computer is capable of simulating various information processes.
The fundamental concern of computer science is determining what can and cannot be automated.
Computer scientists usually focus on academic research.
The Turing Award is generally recognized as the highest distinction in computer sciences.

==
History ==
The earliest foundations of what would become computer science predate the invention of the modern digital computer.
Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division.
Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.

Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.
In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.
Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system.
In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment.
Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.
He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".
"
A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable.
In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.
Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM.
Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published  the 2nd of the only two designs for mechanical analytical engines in history.
In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark
I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit.
When the machine was finished, some hailed it as "Babbage's dream come true".

During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.
As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general.
In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City.
The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science.
The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.
Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.
Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.
The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953.
The first computer science department in the United States was formed at Purdue University in 1962.
Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.

==
Etymology ==
Although first proposed in 1956, the term "computer science" appears in a 1959 article in Communications of the ACM,
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.

His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.
Despite its name, a significant amount of computer science does not involve the study of computers themselves.
Because of this, several alternative names have been proposed.
Certain departments of major universities prefer the term computing science, to emphasize precisely that difference.
Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers.
The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy.
The term is used mainly in the Scandinavian countries.
An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.

In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.
Three months later in the same journal, comptologist was suggested, followed next year by hypologist.
The term computics has also been suggested.
In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek.
Similar words have also been adopted in the UK (as in the School of Informatics of the University of Edinburgh).
"
In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.
"A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes.
"
The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science.
For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems.
However, there has been much cross-fertilization of ideas between the various computer-related disciplines.
Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.

Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.
Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church
and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.
David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis.
Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science.
Both types of departments tend to make efforts to bridge the field educationally if not across all research.

==
Philosophy ==
A number of computer scientists have argued for the distinction of three separate paradigms in computer science.
Peter Wegner argued that those paradigms are science, technology, and mathematics.
Peter Denning's working group argued that they are theory, abstraction (modeling), and design.
Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).

Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.

==
Fields ==
Computer science is no more about computers than astronomy is about telescopes.

As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.
CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society
(IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture.
In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.

===
Theoretical computer science ===
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation.
Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.

====
Theory of computation ====
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"
Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations.
In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation.
The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.

The famous P = NP?
problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.

====
Information and coding theory ====
Information theory, closely related to probability and statistics, is related to the quantification of information.
This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.

Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application.
Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding.
Codes are studied for the purpose of designing efficient and reliable data transmission methods.

====
Data structures and algorithms ====
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.

====
Programming language theory and formal methods ====
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features.
It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics.
It is an active research area, with numerous dedicated academic journals.

Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.
The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.
They form an important theoretical underpinning for software engineering, especially where safety or security is involved.
Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing.
For industrial use, tool support is required.
However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance.
Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
===
Computer systems and computational processes ===


====
Artificial intelligence ====
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals.
From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence.
AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding.
The starting point in the late 1940s was Alan Turing's question "Can computers think?",
and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence.
But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.

====
Computer architecture and organization ====
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system.
It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.
Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems.
The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.

====
Concurrent, parallel and distributed computing ====
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.
A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.
When multiple computers are connected in a network while using concurrency, this is known as a distributed system.
Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.

====
Computer networks ====
This branch of computer science aims to manage networks between computers worldwide.

====
Computer security and cryptography ====
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information.
Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.

====
Databases and data mining ====
A database is intended to organize, store, and retrieve large amounts of data easily.
Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages.
Data mining is a process of discovering patterns in large data sets.

====
Computer graphics and visualization ====
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data.
The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.

====
Image and sound processing ====
Information can take the form of images, sound, video or other multimedia.
Bits of information can be streamed via signals.
Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological.
This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others.
What is the lower bound on the complexity of fast Fourier transform algorithms?
is one of unsolved problems in theoretical computer science.

===
Applied computer science ===


====
Computational science, finance and engineering ====
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems.
A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others.
Modern computers enable optimization of such designs as complete aircraft.
Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs.
The latter includes essential design software for integrated circuits.

====
Social computing and human–computer interaction ====
Social computing is an area that is concerned with the intersection of social behavior and computational systems.
Human–computer interaction research develops theories, principles, and guidelines for user interface designers.

====
Software engineering ====
Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build.
It is a systematic approach to software design, involving the application of engineering practices to software.
Software engineering deals with the organizing and analyzing of software
—it doesn't just deal with the creation or manufacture of new software, but its internal arrangement and maintenance.
For example software testing, systems engineering, technical debt and software development processes.

==
Discoveries ==
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:
Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent "anything".
All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as "on/off", "magnetized/de-magnetized", "high-voltage/low-voltage", etc.).

Alan Turing's insight: there are only five actions that a computer has to perform in order to do "anything".
Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;
move right one location;
read symbol at current location;
print 0 at current location;
print 1 at current location.

Corrado Böhm and Giuseppe Jacopini's insight
: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do "anything".
Only three rules are needed to combine any set of basic instructions into more complex ones:
sequence: first do this, then do that;
selection:
IF such-and-such is the case, THEN do this, ELSE do that;
repetition: WHILE such-and-such is the case, DO this.

Note that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).

==
Programming paradigms ==
Programming languages can be used to accomplish different tasks in different ways.
Common programming paradigms include:

Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data.
It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.

Imperative programming, a programming paradigm that uses statements that change a program's state.
In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform.
Imperative programming focuses on describing how a program operates.

Object-oriented programming, a programming paradigm based on the concept of "objects", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods.
A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated.
Thus object-oriented computer programs are made out of objects that interact with one another.

Service-oriented programming, a programming paradigm that uses "services" as the unit of computer work, to design and implement integrated business applications and mission critical software
programsMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.

==
Academia ==
Conferences are important events for computer science research.
During these conferences, researchers from the public and private sectors present their recent work and meet.
Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.
One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.

==
Education ==
Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students.
In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students.
Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4.
In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured.
According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.

== See also ==


== Notes ==


==
References ==


==
Further reading ==


==
External links ==
Computer science at Curlie
Scholarly Societies in Computer Science
What is Computer Science?

Best Papers Awards in Computer Science since 1996
Photographs of computer scientists by Bertrand Meyer
EECS.berkeley.edu


===
Bibliography and academic search engines ===
CiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.

DBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universität Trier, in Germany.

The Collection of Computer Science Bibliographies (Collection of Computer Science Bibliographies)


===
Professional organizations ===
Association for Computing Machinery
IEEE Computer Society
Informatics Europe
AAAI
AAAS Computer Science


===
Misc ===
Computer Science—
Stack Exchange: a community-run question-and-answer site for computer science
What is computer science
Is computer science science?

Computer Science (Software) Must be Considered as an Independent Discipline.
Cognitive biases are systematic patterns of deviation from norm and/or rationality in judgment.
They are often studied in psychology and behavioral economics.
Although the reality of most of these biases is confirmed by reproducible research, there are often controversies about how to classify these biases or how to explain them.
Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing).
Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.
Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments.
Biases have a variety of forms and appear as cognitive ("cold") bias, such as mental noise, or motivational ("hot") bias, such as when beliefs are distorted by wishful thinking.
Both effects can be present at the same time.
There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior.
For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person.
However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.
Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well.
For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.

==
Belief, decision-making and behavioral ==
These biases affect belief formation, reasoning processes, business and economic decisions, and human behavior in general.

==
Social ==


== Memory ==
In psychology and cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory.
There are many types of memory bias, including:


==
See also ==


==
Footnotes ==


==
References ==
Heuristics are simple strategies or mental processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex problems.
This happens when an individual focuses on the most relevant aspects of a problem or situation to formulate a solution.
Some heuristics are more applicable and useful than others depending on the situation.
Heuristic processes are used to find the answers and solutions that are most likely to work or be correct.
However, heuristics are not always right or the most accurate.
While they can differ from answers given by logic and probability, judgments and decisions based on a heuristic can be good enough to satisfy a need.
They are meant to serve as quick mental references for everyday experiences and decisions.
In situations of uncertainty, where information is incomplete, heuristics allow for the less-is-more effect, in which less information leads to greater accuracy.

==
History ==
Herbert A. Simon formulated one of the first models of heuristics, known as satisficing.
His more general research program posed the question of how humans make decisions when the conditions for rational choice theory are not met, that is how people decide under uncertainty.
Simon is also known as the father of bounded rationality, which he understood as the study of the match (or mismatch) between heuristics and decision environments.
This program was later extended into the study of ecological rationality.

In the early 1970s, psychologists Amos Tversky and Daniel Kahneman took a different approach, linking heuristics to cognitive biases.
Their typical experimental setup consisted of a rule of logic or probability, embedded in a verbal description of a judgement problem, and demonstrated that people's intuitive judgement deviated from the rule.
The "Linda problem" below gives an example.
The deviation is then explained by a heuristic.
This research, called the heuristics-and-biases program, challenged the idea that human beings are rational actors and first gained worldwide attention in 1974 with the Science paper "Judgment Under Uncertainty:
Heuristics and Biases"  and although the originally proposed heuristics have been refined over time, this research program has changed the field by permanently setting the research questions.
The original ideas by Herbert Simon were taken up in the 1990s by Gerd Gigerenzer and others.
According to their perspective, the study of heuristics requires formal models that allow predictions of behavior to be made ex ante.
Their program has three aspects:
What are the heuristics humans use?
(
the descriptive study of the "adaptive toolbox")
Under what conditions should humans rely on a given heuristic?
(
the prescriptive study of ecological rationality)
How to design heuristic decision aids that are easy to understand and execute?
(
the engineering study of intuitive design)Among others, this program has shown that heuristics can lead to fast, frugal, and accurate decisions in many real-world situations that are characterized by uncertainty.
These two different research programs have led to two kinds of models of heuristics, formal models and informal ones.
Formal models describe the decision process in terms of an algorithm, which allows for mathematical proofs and computer simulations.
In contrast, informal models are verbal descriptions.

==
Formal models of heuristics ==


===
Simon's satisficing strategy ===
Herbert Simon's satisficing heuristic can be used to choose one alternative from a set of alternatives in situations of uncertainty.
Here, uncertainty means that the total set of alternatives and their consequences is not known or knowable.
For instance, professional real-estate entrepreneurs rely on satisficing to decide in which location to invest to develop new commercial areas: "If I believe I can get at least x return within y years, then I take the option.
"
In general, satisficing is defined as:

Step 1:
Set an aspiration level α
Step 2
: Choose the first alternative that satisfies αIf no alternative is found, then the aspiration level can be adapted.

Step 3:
If after time β no alternative has satisfied α, then decrease α by some amount δ and return to step 1.Satisficing has been reported across many domains, for instance as a heuristic car dealers use to price used BMWs.

===
Elimination by aspects ===
Unlike satisficing, Amos Tversky's elimination-by-aspect heuristic can be used when all alternatives are simultaneously available.
The decision-maker gradually reduces the number of alternatives by eliminating alternatives that do not meet the aspiration level of a specific attribute (or aspect).

===
Recognition heuristic ===
The recognition heuristic exploits the basic psychological capacity for recognition in order to make inferences about unknown quantities in the world.
For two alternatives, the heuristic is:If one of two alternatives is recognized and the other not, then infer that the recognized alternative has the higher value with respect to the criterion.
For example, in the 2003 Wimbledon tennis tournament, Andy Roddick played Tommy Robredo.
If one has heard of Roddick but not of Robredo, the recognition heuristic leads to the prediction that Roddick will win.
The recognition heuristic exploits partial ignorance, if one has heard of both or no player, a different strategy is needed.
Studies of Wimbledon 2003 and 2005 have shown that the recognition heuristic applied by semi-ignorant amateur players predicted the outcomes of all gentlemen single games as well and better than the seedings of the Wimbledon experts (who had heard of all players), as well as the ATP rankings.
The recognition heuristic is ecologically rational (that is, it predicts well) when the recognition validity is substantially above chance.
In the present case, recognition of players' names is highly correlated with their chances of winning.

===
Take-the-best ===
The take-the-best heuristic exploits the basic psychological capacity for retrieving cues from memory in the order of their validity.

Based on the cue values, it infers which of two alternatives has a higher value on a criterion.
Unlike the recognition heuristic, it requires that all alternatives are recognized, and it thus can be applied when the recognition heuristic cannot.
For binary cues (where 1 indicates the higher criterion value), the heuristic is defined as:
Search rule: Search cues in the order of their validity v.  
Stopping rule: Stop search on finding the first cue that discriminates between the two alternatives (i.e., one cue values are 0 and 1).

Decision rule:
Infer that the alternative with the positive cue value (1) has the higher criterion value).

The validity vi of a cue i is defined as the proportion of correct decisions
ci:
vi =
ci / tiwhere ti is the number of cases the values of the two alternatives differ on cue i.
The validity of each cue can be estimated from samples of observation.

Take-the-best has remarkable properties.
In comparison with complex machine learning models, it has been shown that it can often predict better than regression models, classification-and-regression trees, neural networks, and support vector machines.
[
Brighton & Gigerenzer, 2015]
Similarly, psychological studies have shown that in situations where take-the-best is ecologically rational, a large proportion of people tend to rely on it.
This includes decision making by airport custom officers, professional burglars and police officers  and student populations.
The conditions under which take-the-best is ecologically rational are mostly known.
Take-the-best shows that the previous view that ignoring part of the information would be generally irrational is incorrect.
Less can be more.

===
Fast-and-frugal trees ===
A fast-and-frugal tree is a heuristic that allows to make a classifications, such as whether a patient with severe chest pain is likely to have a heart attack or not, or whether a car approaching a checkpoint is likely to be a terrorist or a civilian.
It is called “fast and frugal” because, just like take-the-best, it allows for quick decisions with only few cues or attributes.
It is called a “tree” because it can be represented like a decision tree in which one asks a sequence of questions.
Unlike a full decision tree, however, it is an incomplete tree – to save time and reduce the danger of overfitting.

Figure 1 shows a fast-and-frugal tree used for screening for HIV (human immunodeficiency virus).
Just like take-the-best, the tree has a search rule, stopping rule, and decision rule:
Search rule:
Search through cues in a specified order.

Stopping rule: Stop search if an exit is reached.

Decision rule:
Classify the person according to the exit (here:
No HIV or HIV).

In the HIV tree, an ELISA (enzyme-linked immunosorbent assay) test is conducted first.
If the outcome is negative, then the testing procedure stops and the client is informed of the good news, that is, “no HIV.”
If, however, the result is positive, a second ELISA test is performed, preferably from a different manufacturer.
If the second ELISA is negative, then the procedure stops and the client is informed of having “no HIV.”
However, if the result is positive, a final test, the Western blot, is conducted.

In general, for n binary cues, a fast-and-frugal tree has exactly n + 1 exits – one for each cue and two for the final cue.
A full decision tree, in contrast, requires 2n exits.
The order of cues (tests) in a fast-and-frugal tree is determined by the sensitivity and specificity of the cues, or by other considerations such as the costs of the tests.
In the case of the HIV tree, the ELISA is ranked first because it produces fewer misses than the Western blot test, and also is less expensive.
The Western blot test, in contrast, produces fewer false alarms.
In a full tree, in contrast, order does not matter for the accuracy of the classifications.

Fast-and-frugal trees are descriptive or prescriptive models of decision making under uncertainty.
For instance, an analysis or court decisions reported that the best model of how London magistrates make bail decisions is a fast and frugal tree.
The HIV tree is both prescriptive– physicians are taught the procedure – and a descriptive model, that is, most physicians actually follow the procedure.

==
Informal models of heuristics ==
In their initial research, Tversky and Kahneman proposed three heuristics—availability, representativeness, and anchoring and adjustment.
Subsequent work has identified many more.
Heuristics that underlie judgment are called "judgment heuristics".
Another type, called "evaluation heuristics", are used to judge the desirability of possible choices.

===
Availability ===
In psychology, availability is the ease with which a particular idea can be brought to mind.
When people estimate how likely or how frequent an event is on the basis of its availability, they are using the availability heuristic.
When an infrequent event can be brought easily and vividly to mind, this heuristic overestimates its likelihood.
For example, people overestimate their likelihood of dying in a dramatic event such as a tornado or terrorism.
Dramatic, violent deaths are usually more highly publicised and therefore have a higher availability.
On the other hand, common but mundane events are hard to bring to mind, so their likelihoods tend to be underestimated.
These include deaths from suicides, strokes, and diabetes.
This heuristic is one of the reasons why people are more easily swayed by a single, vivid story than by a large body of statistical evidence.
It may also play a role in the appeal of lotteries: to someone buying a ticket, the well-publicised, jubilant winners are more available than the millions of people who have won nothing.
When people judge whether more English words begin with T or with K ,  the availability heuristic gives a quick way to answer the question.
Words that begin with T come more readily to mind, and so subjects give a correct answer without counting out large numbers of words.
However, this heuristic can also produce errors.
When people are asked whether there are more English words with K in the first position or with K in the third position, they use the same process.
It is easy to think of words that begin with K, such as kangaroo, kitchen, or kept.
It is harder to think of words with K as the third letter, such as lake, or acknowledge, although objectively these are three times more common.
This leads people to the incorrect conclusion that K is more common at the start of words.
In another experiment, subjects heard the names of many celebrities, roughly equal numbers of whom were male and female.
The subjects were then asked whether the list of names included more men or more women.
When the men in the list were more famous, a great majority of subjects incorrectly thought there were more of them, and vice versa for women.
Tversky and Kahneman's interpretation of these results is that judgments of proportion are based on availability, which is higher for the names of better-known people.
In one experiment that occurred before the 1976 U.S. Presidential election, some participants were asked to imagine Gerald Ford winning, while others did the same for a Jimmy Carter victory.
Each group subsequently viewed their allocated candidate as significantly more likely to win.
The researchers found a similar effect when students imagined a good or a bad season for a college football team.
The effect of imagination on subjective likelihood has been replicated by several other researchers.
A concept's availability can be affected by how recently and how frequently it has been brought to mind.
In one study, subjects were given partial sentences to complete.
The words were selected to activate the concept either of hostility or of kindness: a process known as priming.
They then had to interpret the behavior of a man described in a short, ambiguous story.
Their interpretation was biased towards the emotion they had been primed with: the more priming,
the greater the effect.
A greater interval between the initial task and the judgment decreased the effect.
Tversky and Kahneman offered the availability heuristic as an explanation for illusory correlations in which people wrongly judge two events to be associated with each other.
They explained that people judge correlation on the basis of the ease of imagining or recalling the two events together.

===
Representativeness ===
The representativeness heuristic is seen when people use categories, for example when deciding whether or not a person is a criminal.
An individual thing has a high representativeness for a category if it is very similar to a prototype of that category.
When people categorise things on the basis of representativeness, they are using the representativeness heuristic.
"
Representative" is here meant in two different senses: the prototype used for comparison is representative of its category, and representativeness is also a relation between that prototype and the thing being categorised.
While it is effective for some problems, this heuristic involves attending to the particular characteristics of the individual, ignoring how common those categories are in the population (called the base rates).
Thus, people can overestimate the likelihood that something has a very rare property, or underestimate the likelihood of a very common property.
This is called the base rate fallacy.
Representativeness explains this and several other ways in which human judgments break the laws of probability.
The representativeness heuristic is also an explanation of how people judge cause and effect: when they make these judgements on the basis of similarity, they are also said to be using the representativeness heuristic.
This can lead to a bias, incorrectly finding causal relationships between things that resemble one another and missing them when the cause and effect are very different.
Examples of this include both the belief that "emotionally relevant events ought to have emotionally relevant causes", and magical associative thinking.

====
Representativeness of base rates ====
A 1973 experiment used a psychological profile of Tom W., a fictional graduate student.
One group of subjects had to rate Tom's similarity to a typical student in each of nine academic areas (including Law, Engineering and Library Science).
Another group had to rate how likely it is that Tom specialised in each area.
If these ratings of likelihood are governed by probability, then they should resemble the base rates, i.e. the proportion of students in each of the nine areas (which had been separately estimated by a third group).
If people based their judgments on probability, they would say that Tom is more likely to study Humanities than Library Science, because there are many more Humanities students, and the additional information in the profile is vague and unreliable.
Instead, the ratings of likelihood matched the ratings of similarity almost perfectly, both in this study and a similar one where subjects judged the likelihood of a fictional woman taking different careers.
This suggests that rather than estimating probability using base rates, subjects had substituted the more accessible attribute of similarity.

====
Conjunction fallacy ====
When people rely on representativeness, they can fall into an error which breaks a fundamental law of probability.
Tversky and Kahneman gave subjects a short character sketch of a woman called Linda, describing her as, "31 years old, single, outspoken, and very bright.
She majored in philosophy.
As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations".
People reading this description then ranked the likelihood of different statements about Linda.
Amongst others, these included "Linda is a bank teller", and, "Linda is a bank teller and is active in the feminist movement".
People showed a strong tendency to rate the latter, more specific statement as more likely, even though a conjunction of the form "Linda is both X and Y" can never be more probable than the more general statement
"Linda is X".
The explanation in terms of heuristics is that the judgment was distorted because, for the readers, the character sketch was representative of the sort of person who might be an active feminist but not of someone who works in a bank.
A similar exercise concerned Bill, described as "intelligent but unimaginative".
A great majority of people reading this character sketch rated "Bill is an accountant who plays jazz for a hobby", as more likely than "Bill plays jazz for a hobby".
Without success, Tversky and Kahneman used what they described as "a series of increasingly desperate manipulations" to get their subjects to recognise the logical error.
In one variation, subjects had to choose between a logical explanation of why "Linda is a bank teller" is more likely, and a deliberately illogical argument which said that "Linda is a feminist bank teller" is more likely "because she resembles an active feminist more than she resembles a bank teller".
Sixty-five percent of subjects found the illogical argument more convincing.

Other researchers also carried out variations of this study, exploring the possibility that people had misunderstood the question.
They did not eliminate the error.
It has been shown that individuals with high CRT scores are significantly less likely to be subject to the conjunction fallacy.
The error disappears when the question is posed in terms of frequencies.
Everyone in these versions of the study recognised that out of 100 people fitting an outline description, the conjunction statement ("She is X and Y") cannot apply to more people than the general statement ("She is X").

====
Ignorance of sample size ====
Tversky and Kahneman asked subjects to consider a problem about random variation.
Imagining for simplicity that exactly half of the babies born in a hospital are male, the ratio will not be exactly half in every time period.
On some days, more girls will be born and on others, more boys.
The question was, does the likelihood of deviating from exactly half depend on whether there are many or few births per day?
It is a well-established consequence of sampling theory that proportions will vary much more day-to-day when the typical number of births per day is small.
However, people's answers to the problem do not reflect this fact.
They typically reply that the number of births in the hospital makes no difference to the likelihood of more than 60% male babies in one day.
The explanation in terms of the heuristic is that people consider only how representative the figure of 60% is of the previously given average of 50%.

====
Dilution effect ====
Richard E. Nisbett and colleagues suggest that representativeness explains the dilution effect, in which irrelevant information weakens the effect of a stereotype.
Subjects in one study were asked whether "Paul" or "Susan" was more likely to be assertive, given no other information than their first names.
They rated Paul as more assertive, apparently basing their judgment on a gender stereotype.
Another group, told that Paul's and Susan's mothers each commute to work in a bank, did not show this stereotype effect; they rated Paul and Susan as equally assertive.
The explanation is that the additional information about Paul and Susan made them less representative of men or women in general, and so the subjects' expectations about men and women had a weaker effect.
This means unrelated and non-diagnostic information about certain issue can make relative information less powerful to the issue when people understand the phenomenon.

====
Misperception of randomness ===
=
Representativeness explains systematic errors that people make when judging the probability of random events.
For example, in a sequence of coin tosses, each of which comes up heads (H) or tails (T), people reliably tend to judge a clearly patterned sequence such as HHHTTT as less likely than a less patterned sequence such as HTHTTH.
These sequences have exactly the same probability, but people tend to see the more clearly patterned sequences as less representative of randomness, and so less likely to result from a random process.
Tversky and Kahneman argued that this effect underlies the gambler's fallacy; a tendency to expect outcomes to even out over the short run, like expecting a roulette wheel to come up black because the last several throws came up red.
They emphasised that even experts in statistics were susceptible to this illusion: in a 1971 survey of professional psychologists, they found that respondents expected samples to be overly representative of the population they were drawn from.
As a result, the psychologists systematically overestimated the statistical power of their tests, and underestimated the sample size needed for a meaningful test of their hypotheses.

===
Anchoring and adjustment ===
Anchoring and adjustment is a heuristic used in many situations where people estimate a number.
According to Tversky and Kahneman's original description, it involves starting from a readily available number—the "anchor"—and shifting either up or down to reach an answer that seems plausible.
In Tversky and Kahneman's experiments, people did not shift far enough away from the anchor.
Hence the anchor contaminates the estimate, even if it is clearly irrelevant.
In one experiment, subjects watched a number being selected from a spinning "wheel of fortune".
They had to say whether a given quantity was larger or smaller than that number.
For instance, they might be asked, "Is the percentage of African countries which are members of the United Nations larger or smaller than 65%?"
They then tried to guess the true percentage.
Their answers correlated well with the arbitrary number they had been given.

Insufficient adjustment from an anchor is not the only explanation for this effect.
An alternative theory is that people form their estimates on evidence which is selectively brought to mind by the anchor.

The anchoring effect has been demonstrated by a wide variety of experiments both in laboratories and in the real world.
It remains when the subjects are offered money as an incentive to be accurate, or when they are explicitly told not to base their judgment on the anchor.
The effect is stronger when people have to make their judgments quickly.
Subjects in these experiments lack introspective awareness of the heuristic, denying that the anchor affected their estimates.
Even when the anchor value is obviously random or extreme, it can still contaminate estimates.
One experiment asked subjects to estimate the year of Albert Einstein's first visit to the United States.
Anchors of 1215 and 1992 contaminated the answers just as much as more sensible anchor years.
Other experiments asked subjects if the average temperature in San Francisco is more or less than 558 degrees, or whether there had been more or fewer than 100,025 top ten albums by The Beatles.
These deliberately absurd anchors still affected estimates of the true numbers.
Anchoring results in a particularly strong bias when estimates are stated in the form of a confidence interval.
An example is where people predict the value of a stock market index on a particular day by defining an upper and lower bound so that they are 98% confident the true value will fall in that range.
A reliable finding is that people anchor their upper and lower bounds too close to their best estimate.
This leads to an overconfidence effect.
One much-replicated finding is that when people are 98% certain that a number is in a particular range, they are wrong about thirty to forty percent of the time.
Anchoring also causes particular difficulty when many numbers are combined into a composite judgment.
Tversky and Kahneman demonstrated this by asking a group of people to rapidly estimate the product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
Another group had to estimate the same product in reverse order; 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8.
Both groups underestimated the answer by a wide margin, but the latter group's average estimate was significantly smaller.
The explanation in terms of anchoring is that people multiply the first few terms of each product and anchor on that figure.
A less abstract task is to estimate the probability that an aircraft will crash, given that there are numerous possible faults each with a likelihood of one in a million.
A common finding from studies of these tasks is that people anchor on the small component probabilities and so underestimate the total.
A corresponding effect happens when people estimate the probability of multiple events happening in sequence, such as an accumulator bet in horse racing.
For this kind of judgment, anchoring on the individual probabilities results in an overestimation of the combined probability.

====
Examples ====
People's valuation of goods, and the quantities they buy, respond to anchoring effects.
In one experiment, people wrote down the last two digits of their social security numbers.
They were then asked to consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate, and computer equipment.
They then entered an auction to bid for these items.
Those with the highest two-digit numbers submitted bids that were many times higher than those with the lowest numbers.
When a stack of soup cans in a supermarket was labelled, "Limit 12 per customer", the label influenced customers to buy more cans.
In another experiment, real estate agents appraised the value of houses on the basis of a tour and extensive documentation.
Different agents were shown different listing prices, and these affected their valuations.
For one house, the appraised value ranged from US$114,204 to $128,754.Anchoring and adjustment has also been shown to affect grades given to students.
In one experiment, 48 teachers were given bundles of student essays, each of which had to be graded and returned.
They were also given a fictional list of the students' previous grades.
The mean of these grades affected the grades that teachers awarded for the essay.
One study showed that anchoring affected the sentences in a fictional rape trial.
The subjects were trial judges with, on average, more than fifteen years of experience.
They read documents including witness testimony, expert statements, the relevant penal code, and the final pleas from the prosecution and defence.
The two conditions of this experiment differed in just one respect: the prosecutor demanded a 34-month sentence in one condition and 12 months in the other; there was an eight-month difference between the average sentences handed out in these two conditions.
In a similar mock trial, the subjects took the role of jurors in a civil case.
They were either asked to award damages "in the range from $15 million to $50 million" or "in the range from $50 million to $150 million".
Although the facts of the case were the same each time, jurors given the higher range decided on an award that was about three times higher.
This happened even though the subjects were explicitly warned not to treat the requests as evidence.
Assessments can also be influenced by the stimuli provided.

In one review, researchers found that if a stimulus is perceived to be important or carry "weight" to a situation, that people were more likely to attribute that stimulus as heavier physically.
===
Affect heuristic ===
"Affect", in this context, is a feeling such as fear, pleasure or surprise.
It is shorter in duration than a mood, occurring rapidly and involuntarily in response to a stimulus.
While reading the words "lung cancer" might generate an affect of dread, the words "mother's love" can create an affect of affection and comfort.
When people use affect ("gut responses") to judge benefits or risks, they are using the affect heuristic.
The affect heuristic has been used to explain why messages framed to activate emotions are more persuasive than those framed in a purely factual way.

===
Others ===


==
Theories ==
There are competing theories of human judgment, which differ on whether the use of heuristics is irrational.
A cognitive laziness approach argues that heuristics are inevitable shortcuts given the limitations of the human brain.
According to the natural assessments approach, some complex calculations are already done rapidly and automatically by the brain, and other judgments make use of these processes rather than calculating from scratch.
This has led to a theory called "attribute substitution", which says that people often handle a complicated question by answering a different, related question, without being aware that this is what they are doing.
A third approach argues that heuristics perform just as well as more complicated decision-making procedures, but more quickly and with less information.
This perspective emphasises the "fast and frugal" nature of heuristics.

===
Cognitive laziness ===
An effort-reduction framework proposed by Anuj K. Shah and Daniel M. Oppenheimer states that people use a variety of techniques to reduce the effort of making decisions.

===
Attribute substitution ===
In 2002 Daniel Kahneman and Shane Frederick proposed a process called attribute substitution which happens without conscious awareness.
According to this theory, when somebody makes a judgment (of a target attribute) which is computationally complex, a rather more easily calculated heuristic attribute is substituted.
In effect, a difficult problem is dealt with by answering a rather simpler problem, without the person being aware this is happening.

This explains why individuals can be unaware of their own biases, and why biases persist even when the subject is made aware of them.
It also explains why human judgments often fail to show regression toward the mean.
This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.
Hence, when someone tries to answer a difficult question, they may actually answer a related but different question, without realizing that a substitution has taken place.
In 1975, psychologist Stanley Smith Stevens proposed that the strength of a stimulus (e.g. the brightness of a light, the severity of a crime) is encoded by brain cells in a way that is independent of modality.
Kahneman and Frederick built on this idea, arguing that the target attribute and heuristic attribute could be very different in nature.

Kahneman and Frederick propose three conditions for attribute substitution:
The target attribute is relatively inaccessible.
Substitution is not expected to take place in answering factual questions that can be retrieved directly from memory ("What is your birthday?")
or about current experience ("Do you feel thirsty now?).

An associated attribute is highly accessible.
This might be because it is evaluated automatically in normal perception or because it has been primed.
For example, someone who has been thinking about their love life and is then asked how happy they are might substitute how happy they are with their love life rather than other areas.

The substitution is not detected and corrected by the reflective system.
For example, when asked "A bat and a ball together cost $1.10.
The bat costs $1 more than the ball.
How much does the ball cost?
"
many subjects incorrectly answer $0.10.
An explanation in terms of attribute substitution is that, rather than work out the sum, subjects parse the sum of $1.10 into a large amount and a small amount, which is easy to do.
Whether they feel that is the right answer will depend on whether they check the calculation with their reflective system.
Kahneman gives an example where some Americans were offered insurance against their own death in a terrorist attack while on a trip to Europe, while another group were offered insurance that would cover death of any kind on the trip.
Even though "death of any kind" includes "death in a terrorist attack", the former group were willing to pay more than the latter.
Kahneman suggests that the attribute of fear is being substituted for a calculation of the total risks of travel.
Fear of terrorism for these subjects was stronger than a general fear of dying on a foreign trip.

===
Fast and frugal ===
Gerd Gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
According to them, heuristics are "fast and frugal" alternatives to more complicated procedures, giving answers that are just as good.

==
Consequences ==


===
Efficient decision heuristics ===
Warren Thorngate, a social psychologist, implemented ten simple decision rules or heuristics in a computer program.
He determined how often each heuristic selected alternatives with highest-through-lowest expected value in a series of randomly-generated decision situations.
He found that most of the simulated heuristics selected alternatives with highest expected value and almost never selected alternatives with lowest expected value.

=== "Beautiful-is-familiar" effect ===
Psychologist Benoît Monin reports a series of experiments in which subjects, looking at photographs of faces, have to judge whether they have seen those faces before.
It is repeatedly found that attractive faces are more likely to be mistakenly labeled as familiar.
Monin interprets this result in terms of attribute substitution.
The heuristic attribute in this case is a "warm glow"; a positive feeling towards someone that might either be due to their being familiar or being attractive.
This interpretation has been criticised, because not all the variance in familiarity is accounted for by the attractiveness of the photograph.
===
Judgments of morality and fairness ===
Legal scholar Cass Sunstein has argued that attribute substitution is pervasive when people reason about moral, political or legal matters.
Given a difficult, novel problem in these areas, people search for a more familiar, related problem (a "prototypical case") and apply its solution as the solution to the harder problem.
According to Sunstein, the opinions of trusted political or religious authorities can serve as heuristic attributes when people are asked their own opinions on a matter.
Another source of heuristic attributes is emotion: people's moral opinions on sensitive subjects like sexuality and human cloning may be driven by reactions such as disgust, rather than by reasoned principles.
Sunstein has been challenged as not providing enough evidence that attribute substitution, rather than other processes, is at work in these cases.

===
Persuasion ===
An example of how persuasion plays a role in heuristic processing can be explained through the heuristic-systematic model.
This explains how there are often two ways we are able to process information from persuasive messages, one being heuristically and the other systematically.
A heuristic is when we make a quick short judgement into our decision making.
On the other hand, systematic processing involves more analytical and inquisitive cognitive thinking.
Individuals looks further than their own prior knowledge for the answers.
An example of this model could be used when watching an advertisement about a specific medication.
One without prior knowledge would see the person in the proper pharmaceutical attire and assume that they know what they are talking about.
Therefore, that person automatically has more credibility and is more likely to trust the content of the messages than they deliver.
While another who is also in that field of work or already has prior knowledge of the medication will not be persuaded by the ad because of their systematic way of thinking.
This was also formally demonstrated in an experiment conducted my Chaiken and Maheswaran (1994).
In addition to these examples, the fluency heuristic ties in perfectly with the topic of persuasion.
It is described as how we all easily make "the most of an automatic by-product of retrieval from memory".
An example would be a friend asking about good books to read.
Many could come to mind, but you name the first book recalled from your memory.
Since it was the first thought, therefore you value it as better than any other book one could suggest.
The effort heuristic is almost identical to fluency.
The one distinction would be that objects that take longer to produce are seen with more value.
One may conclude that a glass vase is more valuable than a drawing, merely because it may take longer for the vase.
These two varieties of heuristics confirms how we may be influenced easily our mental shortcuts, or what may come quickest to our mind.

== See also ==


==
Citations ==


==
References ==
Baron, Jonathan (2000), Thinking and deciding (
3rd ed.)
,
New York:
Cambridge University Press, ISBN 978-0521650304, OCLC
316403966
Gilovich, Thomas; Griffin, Dale W. (2002), "Introduction – Heuristics and Biases:
Then and Now",  in Gilovich, Thomas; Griffin, Dale W.; Kahneman, Daniel (eds.),
Heuristics and biases: the psychology of intuitive judgement, Cambridge University Press, pp.
1–18, ISBN 9780521796798
Hardman, David (2009), Judgment and decision making:
psychological perspectives, Wiley-Blackwell, ISBN
9781405123983
Hastie, Reid; Dawes, Robyn M. (29 September 2009), Rational Choice in an Uncertain World:
The Psychology of Judgment and Decision Making, SAGE, ISBN
9781412959032
Koehler, Derek J.; Harvey, Nigel (2004), Blackwell handbook of judgment and decision making, Wiley-Blackwell, ISBN 9781405107464
Kunda, Ziva (1999), Social Cognition:
Making Sense of People, MIT Press, ISBN 978-0-262-61143-5, OCLC 40618974
Mussweiler, Thomas; Englich, Birte; Strack, Fritz (2004), "Anchoring effect",  in Pohl, Rüdiger F. (ed.)
,
Cognitive Illusions:
A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
183–200, ISBN 9781841693514, OCLC 55124398
Plous, Scott (1993), The Psychology of Judgment and Decision Making, McGraw-Hill, ISBN 9780070504776, OCLC 26931106
Poundstone, William (2010), Priceless:
the myth of fair value (and how to take advantage of it), Hill and Wang, ISBN 9780809094691
Reber, Rolf (2004), "Availability",  in Pohl, Rüdiger F. (ed.),
Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
147–163, ISBN 9781841693514, OCLC 55124398
Sutherland, Stuart (2007), Irrationality (2nd ed.),
London: Pinter and Martin, ISBN 9781905177073, OCLC 72151566
Teigen, Karl Halvor (2004), "Judgements by representativeness",  in Pohl, Rüdiger F. (ed.),
Cognitive Illusions:
A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
165–182, ISBN 9781841693514, OCLC 55124398
Tversky, Amos; Kahneman, Daniel (1974), "Judgments Under Uncertainty: Heuristics and Biases" (PDF), Science, 185 (4157): 1124–1131,
Bibcode:1974Sci...
185.1124T, doi:10.1126/science.185.4157.1124, PMID 17835457, S2CID 143452957 reprinted in Daniel Kahneman; Paul Slovic; Amos Tversky, eds.
(
1982).
Judgment Under Uncertainty: Heuristics and Biases.
Cambridge:
Cambridge University Press.
pp.
3–20.
ISBN 9780521284141.

Yudkowsky, Eliezer (2011).
"
Cognitive biases potentially affecting judgment of global risks".

In Bostrom, Nick; Cirkovic, Milan M. (eds.).
Global Catastrophic Risks.
OUP Oxford.
pp.
91–119.
ISBN 978-0-19-960650-4.
==
Further reading ==
Slovic, Paul; Melissa Finucane; Ellen Peters; Donald G. MacGregor (2002).
"
The Affect Heuristic".

In Thomas Gilovich; Dale Griffin; Daniel Kahneman (eds.).
Heuristics and Biases: The Psychology of Intuitive Judgment.
Cambridge University Press.
pp.
397–420.
ISBN 9780521796798.

Gigerenzer, Gerd; Selten, Reinhard (2001).
Bounded rationality : the adaptive toolbox.
Cambridge, MA: MIT Press.
ISBN 0585388288.
OCLC 49569412.

Korteling, Johan E.; Brouwer, Anne-Marie; Toet, Alexander (3 September 2018). "
A Neural Network Framework for Cognitive Bias".
Frontiers in Psychology.
9:
1561.
doi:10.3389/fpsyg.2018.01561.
PMC 6129743.
PMID 30233451.

Chow, Sheldon (20 April 2011).
"
Heuristics, Concepts, and Cognitive Architecture:
Toward Understanding
How The Mind Works".
Electronic Thesis and Dissertation Repository.

Todd, P.M. (2001).
"
Heuristics for Decision and Choice".
International Encyclopedia of the Social & Behavioral Sciences.
pp.
6676–6679.
doi:10.1016
/B0-08-043076-7/00629-X. ISBN 978-0-08-043076-8.
==
External links ==
Test Yourself:
Decision Making and the Availability Heuristic
The following is a list of algorithms along with one-line descriptions for each.

==
Automated planning ==


==
Combinatorial algorithms ==


===
General combinatorial algorithms ===
Brent's algorithm: finds a cycle in function value iterations using only two iterators
Floyd's cycle-finding algorithm: finds a cycle in function value iterations
Gale–Shapley algorithm:
solves the stable marriage problem
Pseudorandom number generators (uniformly distributed—see also List of pseudorandom number generators for other PRNGs with varying degrees of convergence and varying statistical quality):
ACORN generator
Blum Blum Shub
Lagged Fibonacci generator
Linear congruential generator
Mersenne Twister


==
=
Graph algorithms ===
Coloring algorithm:
Graph coloring algorithm.

Hopcroft–
Karp algorithm:
convert a bipartite graph to a maximum cardinality matching
Hungarian algorithm:
algorithm for finding a perfect matching
Prüfer coding: conversion between a labeled tree and its Prüfer sequence
Tarjan's off-line lowest common ancestors algorithm:
compute lowest common ancestors for pairs of nodes in a tree
Topological sort: finds linear order of nodes (e.g. jobs) based on their dependencies.

====
Graph drawing ====
Force-based algorithms (also known as force-directed algorithms or spring-based algorithm)
Spectral layout


====
Network theory ====
Network analysis
Link analysis
Girvan–
Newman algorithm:
detect communities in complex systems
Web link analysis
Hyperlink-Induced Topic Search (HITS) (also known as Hubs and authorities)
PageRank
TrustRank
Flow networks
Dinic's algorithm: is a strongly polynomial algorithm for computing the maximum flow in a flow network.

Edmonds–Karp algorithm: implementation of Ford–Fulkerson
Ford–
Fulkerson algorithm:
computes the maximum flow in a graph
Karger's algorithm:
a Monte Carlo method to compute the minimum cut of a connected graph
Push–relabel algorithm: computes a maximum flow in a graph


====
Routing for graphs ====
Edmonds' algorithm (also known as Chu–Liu/Edmonds' algorithm): find maximum or minimum branchings
Euclidean minimum spanning tree:
algorithms for computing the minimum spanning tree of a set of points in the plane
Euclidean shortest path problem: find the shortest path between two points that does not intersect any obstacle
Longest path problem: find a simple path of maximum length in a given graph
Minimum spanning tree
Borůvka's algorithm
Kruskal's algorithm
Prim's algorithm
Reverse-delete algorithm
Nonblocking minimal spanning switch say, for a telephone exchange
Shortest path problem
Bellman–Ford algorithm: computes shortest paths in a weighted graph (where some of the edge weights may be negative)
Dijkstra's algorithm: computes shortest paths in a graph with non-negative edge weights
Floyd–
Warshall algorithm:
solves the all pairs shortest path problem in a weighted, directed graph
Johnson's algorithm: All pairs shortest path algorithm in sparse weighted directed graph
Transitive closure problem: find the transitive closure of a given binary relation
Traveling salesman problem
Christofides algorithm
Nearest neighbour algorithm
Warnsdorff's rule:
A heuristic method for solving the Knight's tour problem.
====
Graph search ====
A*: special case of best-first search that uses heuristics to improve speed
B
*: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)
Backtracking: abandons partial solutions when they are found not to satisfy a complete solution
Beam search: is a heuristic search algorithm that is an optimization of best-first search that reduces its memory requirement
Beam stack search:
integrates backtracking with beam search
Best-first search: traverses a graph in the order of likely importance using a priority queue
Bidirectional search: find the shortest path from an initial vertex to a goal vertex in a directed graph
Breadth-first search: traverses a graph level by level
Brute-force search: An exhaustive and reliable search method, but computationally inefficient in many applications.

D
*: an incremental heuristic search algorithm
Depth-first search: traverses a graph branch by branch
Dijkstra's algorithm: A special case of A* for which no heuristic function is used
General Problem Solver: a seminal theorem-proving algorithm intended to work as a universal problem solver machine.

Iterative deepening depth-first search (IDDFS):
a state space search strategy
Jump point search:
An optimization to A* which may reduce computation time by an order of magnitude using further heuristics.

Lexicographic breadth-first search (also known as Lex-BFS):
a linear time algorithm for ordering the vertices of a graph
Uniform-cost search: a tree search that finds the lowest-cost route where costs vary
SSS*: state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm
F
*: Special algorithm to merge the two arrays


====
Subgraphs ====
Cliques
Bron–
Kerbosch algorithm:
a technique for finding maximal cliques in an undirected graph
MaxCliqueDyn maximum clique algorithm: find a maximum clique in an undirected graph
Strongly connected components
Path-based strong component algorithm
Kosaraju's algorithm
Tarjan's strongly connected components algorithm
Subgraph isomorphism problem


===
Sequence algorithms ===


====
Approximate sequence matching ====
Bitap algorithm:
fuzzy algorithm that determines if strings are approximately equal.

Phonetic algorithms
Daitch–
Mokotoff Soundex: a Soundex refinement which allows matching of Slavic and Germanic surnames
Double Metaphone: an improvement on Metaphone
Match rating approach: a phonetic algorithm developed by Western Airlines
Metaphone: an algorithm for indexing words by their sound, when pronounced in English
NYSIIS: phonetic algorithm, improves on Soundex
Soundex: a phonetic algorithm for indexing names by sound, as pronounced in English
String metrics: compute a similarity or dissimilarity (distance) score between two pairs of text strings
Damerau–Levenshtein distance compute a distance measure between two strings, improves on Levenshtein distance
Dice's coefficient (also known as the Dice coefficient): a similarity measure related to the Jaccard index
Hamming distance:
sum number of positions which are different
Jaro–Winkler distance:  is a measure of similarity between two strings
Levenshtein edit distance
: compute a metric for the amount of difference between two sequences
Trigram search:
search for text when the exact syntax or spelling of the target object is not precisely known


====
Selection algorithms ====
Quickselect
Introselect


====
Sequence search ====
Linear search: locates an item in an unsorted sequence
Selection algorithm: finds the kth largest item in a sequence
Ternary search: a technique for finding the minimum or maximum of a function that is either strictly increasing and then strictly decreasing or vice versa
Sorted lists
Binary search algorithm: locates an item in a sorted sequence
Fibonacci search technique: search a sorted sequence using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers
Jump search (or block search):
linear search on a smaller subset of the sequence
Predictive search: binary-like search which factors in magnitude of search term versus the high and low values in the search.

Sometimes called dictionary search or interpolated search.

Uniform binary search: an optimization of the classic binary search algorithm


====
Sequence merging ====
Simple merge algorithm
k-way merge algorithm
Union (merge, with elements on the output not repeated)


====
Sequence permutations ====
Fisher–Yates shuffle (also known as the Knuth shuffle): randomly shuffle a finite set
Schensted algorithm: constructs a pair of Young tableaux from a permutation
Steinhaus–Johnson–Trotter algorithm (also known as the Johnson–Trotter algorithm): generate permutations by transposing elements
Heap's permutation generation algorithm:
interchange elements to generate next permutation


====
Sequence combinations ====


====
Sequence alignment ====
Dynamic time warping: measure similarity between two sequences which may vary in time or speed
Hirschberg's algorithm: finds the least cost sequence alignment between two sequences, as measured by their Levenshtein distance
Needleman–
Wunsch algorithm:
find global alignment between two sequences
Smith–
Waterman algorithm:
find local sequence alignment


====
Sequence sorting ====
Exchange sorts
Bubble sort: for each pair of indices, swap the items if out of order
Cocktail shaker sort or bidirectional bubble sort, a bubble sort traversing the list alternately from front to back and back to
front
Comb sort
Gnome sort
Odd
–even sort
Quicksort: divide list into two, with all items on the first list coming before all items on the second list.
;
then sort the two lists.
Often the method of choice
Humorous or ineffective
Bogosort
Stooge sort
Hybrid
Flashsort
Introsort: begin with quicksort and switch to heapsort when the recursion depth exceeds a certain level
Timsort: adaptative algorithm derived from merge sort and insertion sort.
Used in Python 2.3 and up, and Java SE 7.

Insertion sorts
Insertion sort: determine where the current item belongs in the list of sorted ones, and insert it there
Library sort
Patience sorting
Shell sort: an attempt to improve insertion sort
Tree sort (binary tree sort):
build binary tree, then traverse it to create sorted list
Cycle sort:
in-place with theoretically optimal number of writes
Merge sorts
Merge
sort: sort the first and second half of the list separately, then merge the sorted lists
Slowsort
Strand sort
Non-comparison sorts
Bead sort
Bucket sort
Burstsort: build a compact, cache efficient burst trie and then traverse it to create sorted output
Counting sort
Pigeonhole
sort
Postman sort:
variant of Bucket sort which takes advantage of hierarchical structure
Radix sort:
sorts strings letter by letter
Selection sorts
Heapsort: convert the list into a heap, keep removing the largest element from the heap and adding it to the end of the list
Selection sort: pick the smallest of the remaining elements, add it to the end of the sorted list
Smoothsort
Other
Bitonic sorter
Pancake sorting
Spaghetti sort
Topological sort
Unknown class
Samplesort


====
Subsequences ====
Kadane's algorithm: finds maximum sub-array of any size
Longest common subsequence problem: Find the longest subsequence common to all sequences in a set of sequences
Longest increasing subsequence problem: Find the longest increasing subsequence of a given sequence
Shortest common supersequence problem: Find the shortest supersequence that contains two or more sequences as subsequences


====
Substrings ====
Longest common substring problem:  find the longest string (or strings) that is a substring (or are substrings) of two or more strings
Substring search
Aho–Corasick string matching algorithm:
trie based algorithm for finding all substring matches to any of a finite set of strings
Boyer–
Moore string-search algorithm:
amortized linear (sublinear in most times) algorithm for substring search
Boyer–Moore–
Horspool algorithm:
Simplification of Boyer–Moore
Knuth–Morris–
Pratt algorithm:
substring search which bypasses reexamination of matched characters
Rabin–Karp string search algorithm: searches multiple patterns efficiently
Zhu–
Takaoka string matching algorithm: a variant of Boyer–Moore
Ukkonen's algorithm: a linear-time, online algorithm for constructing suffix trees
Matching wildcards
Rich Salz' wildmat: a widely used open-source recursive algorithm
Krauss matching wildcards algorithm:
an open-source non-recursive algorithm


==
Computational mathematics ==


===
Abstract algebra ===
Chien search: a recursive algorithm for determining roots of polynomials defined over a finite field
Schreier
–Sims algorithm:
computing a base and strong generating set (BSGS) of a permutation group
Todd–Coxeter algorithm:
Procedure for generating cosets.
===
Computer algebra ===
Buchberger's algorithm: finds a Gröbner basis
Cantor–Zassenhaus algorithm:
factor polynomials over finite fields
Faugère F4 algorithm:
finds a Gröbner basis (also mentions the F5 algorithm)
Gosper's algorithm: find sums of hypergeometric terms that are themselves hypergeometric terms
Knuth–Bendix completion algorithm: for rewriting rule systems
Multivariate division algorithm: for polynomials in several indeterminates
Pollard's kangaroo algorithm (also known as Pollard's lambda algorithm
):
an algorithm for solving the discrete logarithm problem
Polynomial long division: an algorithm for dividing a polynomial by another polynomial of the same or lower degree
Risch algorithm:
an algorithm for the calculus operation of indefinite integration (i.e. finding antiderivatives)


===
Geometry ===
Closest pair problem:
find the pair of points (from a set of points) with the smallest distance between them
Collision detection algorithms: check for the collision or intersection of two given solids
Cone algorithm:
identify surface points
Convex hull algorithms: determining the convex hull of a set of points
Graham scan
Quickhull
Gift wrapping algorithm or Jarvis march
Chan's algorithm
Kirkpatrick–
Seidel algorithm
Euclidean distance transform: computes the distance between every point in a grid and a discrete collection of points.

Geometric hashing: a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation
Gilbert–Johnson–Keerthi distance algorithm: determining the smallest distance between two convex shapes.

Jump-and-Walk algorithm: an algorithm for point location in triangulations
Laplacian smoothing: an algorithm to smooth a polygonal mesh
Line segment intersection: finding whether lines intersect, usually with a sweep line algorithm
Bentley–
Ottmann algorithm
Shamos–
Hoey algorithm
Minimum bounding box algorithms: find the oriented minimum bounding box enclosing a set of points
Nearest neighbor search:
find the nearest point or points to a query point
Point in polygon algorithms:
tests whether a given point lies within a given polygon
Point set registration algorithms: finds the transformation between two point sets to optimally align them.

Rotating calipers
: determine all antipodal pairs of points and vertices on a convex polygon or convex hull.

Shoelace algorithm:
determine the area of a polygon whose vertices are described by ordered pairs in the plane
Triangulation
Delaunay triangulation
Ruppert's algorithm (also known as Delaunay refinement): create quality Delaunay triangulations
Chew's second algorithm:
create quality constrained Delaunay triangulations
Marching triangles:
reconstruct two-dimensional surface geometry from an unstructured point cloud
Polygon triangulation algorithms: decompose a polygon into a set of triangles
Voronoi diagrams, geometric dual of Delaunay triangulation
Bowyer–Watson algorithm:
create voronoi diagram in any number of dimensions
Fortune's Algorithm: create voronoi diagram
Quasitriangulation


===
Number
theoretic algorithms ===
Binary GCD algorithm:
Efficient way of calculating GCD.

Booth's multiplication algorithm
Chakravala method: a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation
Discrete logarithm:
Baby-step giant-step
Index calculus algorithm
Pollard's rho algorithm for logarithms
Pohlig–Hellman algorithm
Euclidean algorithm:
computes the greatest common divisor
Extended Euclidean algorithm: Also solves the equation ax + by = c.
Integer factorization: breaking an integer into its prime factors
Congruence of squares
Dixon's algorithm
Fermat's factorization method
General number field sieve
Lenstra elliptic curve factorization
Pollard's p − 1 algorithm
Pollard's rho algorithm
prime factorization algorithm
Quadratic sieve
Shor's algorithm
Special number field sieve
Trial division
Multiplication algorithms:
fast multiplication of two numbers
Karatsuba algorithm
Schönhage
–Strassen algorithm
Toom–
Cook multiplication
Modular square root:
computing square roots modulo a prime number
Tonelli–Shanks algorithm
Cipolla's algorithm
Berlekamp's root finding algorithm
Odlyzko–Schönhage algorithm: calculates nontrivial zeroes of the Riemann zeta function
Lenstra–Lenstra–
Lovász algorithm (also known as LLL algorithm)
: find a short, nearly orthogonal lattice basis in polynomial time
Primality tests: determining whether a given number is prime
AKS primality test
Baillie–PSW primality test
Fermat primality test
Lucas primality test
Miller–Rabin primality test
Sieve of Atkin
Sieve of Eratosthenes
Sieve of Sundaram


===
Numerical algorithms ===


====
Differential equation solving ====
Euler method
Backward Euler method
Trapezoidal rule (differential equations)
Linear multistep methods
Runge–Kutta methods
Euler integration
Multigrid methods (MG methods), a group of algorithms for solving differential equations using a hierarchy of discretizations
Partial differential equation:
Finite difference method
Crank–Nicolson method for diffusion equations
Lax
–Wendroff for wave equations
Verlet integration
(French pronunciation: ​[vɛʁˈlɛ]):
integrate Newton's equations of motion


====
Elementary and special functions ====
Computation of π:
Borwein's algorithm: an algorithm to calculate the value of 1/π
Gauss–Legendre algorithm: computes the digits of
pi
Chudnovsky algorithm:
A fast method for calculating the digits of π
Bailey–Borwein–
Plouffe formula: (BBP formula) a spigot algorithm for the computation of the nth binary digit of π
Division algorithms: for computing quotient and/or remainder of two numbers
Long division
Restoring division
Non-restoring division
SRT division
Newton–Raphson division: uses Newton's method to find the reciprocal of D, and multiply that reciprocal by N to find the final quotient Q.
Goldschmidt division
Hyperbolic and Trigonometric Functions:
BKM algorithm:
compute elementary functions using a table of logarithms
CORDIC:
compute hyperbolic and trigonometric functions using a table of arctangents
Exponentiation:
Addition-chain exponentiation:
exponentiation by positive integer powers that requires a minimal number of multiplications
Exponentiating by squaring: an algorithm used for the fast computation of large integer powers of a number
Montgomery reduction: an algorithm that allows modular arithmetic to be performed efficiently when the modulus is large
Multiplication algorithms: fast multiplication of two numbers
Booth's multiplication algorithm: a multiplication algorithm that multiplies two signed binary numbers in two's complement notation
Fürer's algorithm:  an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity
Karatsuba algorithm:
an efficient procedure for multiplying large numbers
Schönhage–Strassen algorithm: an asymptotically fast multiplication algorithm for large integers
Toom–
Cook multiplication:
(Toom3) a multiplication algorithm for large integers
Multiplicative inverse Algorithms: for computing a number's multiplicative inverse (reciprocal).

Newton's method
Rounding functions: the classic ways to round numbers
Spigot algorithm: A way to compute the value of a mathematical constant without knowing preceding digits
Square and Nth root of a number:
Alpha max plus
beta min algorithm:
an approximation of the square-root of the sum of two squares
Methods of computing square roots
nth root algorithm
Shifting nth-root algorithm: digit by digit root extraction
Summation:
Binary splitting:  a divide and conquer technique which speeds up the numerical evaluation of many types of series with rational terms
Kahan summation algorithm: a more accurate method of summing floating-point numbers
Unrestricted algorithm
====
Geometric ====
Filtered back-projection
: efficiently compute the inverse 2-dimensional Radon transform.

Level set method (LSM):  a numerical technique for tracking interfaces and shapes


====
Interpolation and extrapolation ====
Birkhoff interpolation: an extension of polynomial interpolation
Cubic interpolation
Hermite interpolation
Lagrange interpolation: interpolation using Lagrange polynomials
Linear interpolation: a method of curve fitting using linear polynomials
Monotone cubic interpolation: a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.

Multivariate interpolation
Bicubic interpolation, a generalization of cubic interpolation to two dimensions
Bilinear interpolation: an extension of linear interpolation for interpolating functions of two variables on a regular grid
Lanczos resampling ("Lanzosh"): a multivariate interpolation method used to compute new values for any digitally sampled data
Nearest-neighbor interpolation
Tricubic interpolation, a generalization of cubic interpolation to three dimensions
Pareto interpolation: a method of estimating the median and other properties of a population that follows a Pareto distribution.

Polynomial interpolation
Neville's algorithm
Spline interpolation:
Reduces error with Runge's phenomenon.

De Boor algorithm: B-splines
De Casteljau's algorithm:
Bézier curves
Trigonometric interpolation


====
Linear algebra ====
Eigenvalue algorithms
Arnoldi iteration
Inverse iteration
Jacobi method
Lanczos iteration
Power iteration
QR algorithm
Rayleigh quotient iteration
Gram–Schmidt process:
orthogonalizes a set of vectors
Matrix multiplication algorithms
Cannon's algorithm: a distributed algorithm for matrix multiplication especially suitable for computers laid out in an N × N mesh
Coppersmith–
Winograd algorithm:
square matrix multiplication
Freivalds' algorithm: a randomized algorithm used to verify matrix multiplication
Strassen algorithm:
faster matrix multiplication
Solving systems of linear equations
Biconjugate gradient method:
solves systems of linear equations
Conjugate gradient: an algorithm for the numerical solution of particular systems of linear equations
Gaussian elimination
Gauss–Jordan elimination: solves systems of linear equations
Gauss
–Seidel method:
solves systems of linear equations iteratively
Levinson recursion: solves equation involving a Toeplitz matrix
Stone's method: also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations
Successive over-relaxation (SOR):
method used to speed up convergence of the Gauss–Seidel method
Tridiagonal matrix algorithm
(Thomas algorithm):
solves systems of tridiagonal equations
Sparse matrix algorithms
Cuthill–McKee algorithm:
reduce the bandwidth of a symmetric sparse matrix
Minimum degree algorithm: permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition
Symbolic Cholesky decomposition:
Efficient way of storing sparse matrix


====
Monte Carlo ====
Gibbs sampling: generate a sequence of samples from the joint probability distribution of two or more random variables
Hybrid Monte Carlo: generate a sequence of samples using Hamiltonian weighted Markov chain Monte Carlo, from a probability distribution which is difficult to sample directly.

Metropolis–
Hastings algorithm:
used to generate a sequence of samples from the probability distribution of one or more variables
Wang and Landau algorithm:
an extension of Metropolis–
Hastings algorithm
sampling


====
Numerical integration ====
MISER algorithm:
Monte Carlo simulation,
numerical integration


====
Root finding ====
Bisection method
False position method: approximates roots of a function
ITP method: minmax optimal and superlinar convergence simultaneously
Newton's method: finds zeros of functions with calculus
Halley's method: uses first and second derivatives
Secant method: 2-point, 1-sided
False position method and Illinois method: 2-point, bracketing
Ridder's method: 3-point, exponential scaling
Muller's method: 3-point, quadratic interpolation


===
Optimization algorithms ===
Alpha–beta pruning: search to reduce number of nodes in minimax algorithm
Branch and bound
Bruss algorithm:
see odds algorithm
Chain matrix multiplication
Combinatorial optimization: optimization problems where the set of feasible solutions is discrete
Greedy randomized adaptive search procedure (GRASP):
successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search
Hungarian method: a combinatorial optimization algorithm which solves the assignment problem in polynomial time
Constraint satisfaction
General algorithms for the constraint satisfaction
AC-3 algorithm
Difference map algorithm
Min conflicts algorithm
Chaff algorithm:
an algorithm for solving instances of the boolean satisfiability problem
Davis–
Putnam algorithm:
check the validity of a first-order logic formula
Davis–Putnam–Logemann–
Loveland algorithm (DPLL):
an algorithm for deciding the satisfiability of propositional logic formula in conjunctive normal form, i.e. for solving the CNF-SAT problem
Exact cover problem
Algorithm
X: a nondeterministic algorithm
Dancing Links: an efficient implementation of Algorithm X
Cross-entropy method: a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling
Differential evolution
Dynamic Programming: problems exhibiting the properties of overlapping subproblems and optimal substructure
Ellipsoid method: is an algorithm for solving convex optimization problems
Evolutionary computation: optimization inspired by biological mechanisms of evolution
Evolution strategy
Gene expression programming
Genetic algorithms
Fitness proportionate selection – also known as roulette-wheel selection
Stochastic
universal sampling
Truncation selection
Tournament selection
Memetic algorithm
Swarm intelligence
Ant colony optimization
Bees algorithm:
a search algorithm which mimics the food foraging behavior of swarms of honey bees
Particle swarm
golden-section search: an algorithm for finding the maximum of a real function
Gradient descent
Grid Search
Harmony search (HS):
a metaheuristic algorithm mimicking the improvisation process of musicians
Interior point method
Linear programming
Benson's algorithm: an algorithm for solving linear vector optimization problems
Dantzig–Wolfe decomposition: an algorithm for solving linear programming problems with special structure
Delayed column generation
Integer linear programming:
solve linear programming problems where some or all the unknowns are restricted to integer values
Branch and cut
Cutting-plane method
Karmarkar's algorithm: The first reasonably efficient algorithm that solves the linear programming problem in polynomial time.

Simplex algorithm:
An algorithm for solving linear programming problems
Line search
Local search: a metaheuristic for solving computationally hard optimization problems
Random-restart hill
climbing
Tabu search
Minimax used in game programming
Nearest neighbor search (NNS): find closest points in a metric space
Best Bin First: find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces
Newton's method in optimization
Nonlinear optimization
BFGS method:
A nonlinear optimization algorithm
Gauss–Newton algorithm:
An algorithm for solving nonlinear least squares problems.

Levenberg–Marquardt algorithm:
An algorithm for solving nonlinear least squares problems.

Nelder–Mead method (downhill simplex method):
A nonlinear optimization algorithm
Odds algorithm
(Bruss algorithm)
: Finds the optimal strategy to predict a last specific event in a random sequence event
Random Search
Simulated annealing
Stochastic tunneling
Subset sum algorithm
==
Computational science ==


===
Astronomy ===
Doomsday algorithm:
day of the week
Zeller's congruence is an algorithm to calculate the day of the week for any Julian or Gregorian calendar date
various Easter algorithms are used to calculate the day of Easter


===
Bioinformatics ===
Basic Local Alignment Search Tool also known as BLAST: an algorithm for comparing primary biological sequence information
Kabsch algorithm:
calculate the optimal alignment of two sets of points in order to compute the root mean squared deviation between two protein structures.

Velvet: a set of algorithms manipulating de Bruijn graphs for genomic sequence assembly
Sorting by signed reversals: an algorithm for understanding genomic evolution.

Maximum parsimony (phylogenetics): an algorithm for finding the simplest phylogenetic tree to explain a given character matrix.

UPGMA:
a distance-based phylogenetic tree construction algorithm.

===
Geoscience ===
Vincenty's formulae: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoid
Geohash: a public domain algorithm that encodes a decimal latitude/longitude pair as a hash string


===
Linguistics ===
Lesk algorithm: word sense disambiguation
Stemming algorithm:
a method of reducing words to their stem, base, or root form
Sukhotin's algorithm: a statistical classification algorithm for classifying characters in a text as vowels or consonants


===
Medicine ===
ESC algorithm for the diagnosis of heart failure
Manning Criteria for irritable bowel syndrome
Pulmonary embolism diagnostic algorithms
Texas Medication Algorithm Project


===
Physics ===
Constraint algorithm:
a class of algorithms for satisfying constraints for bodies that obey Newton's equations of motion
Demon algorithm:
a Monte Carlo method for efficiently sampling members of a microcanonical ensemble with a given energy
Featherstone's algorithm: compute the effects of forces applied to a structure of joints and links
Ground state approximation
Variational method
Ritz method
n-body problems
Barnes–Hut simulation:
Solves the n-body problem in an approximate way that has the order O(n log n) instead of O(n2) as in a direct-sum simulation.

Fast multipole method (FMM): speeds up the calculation of long-ranged forces
Rainflow-counting algorithm: Reduces a complex stress history to a count of elementary stress-reversals for use in fatigue analysis
Sweep and prune: a broad phase algorithm used during collision detection to limit the number of pairs of solids that need to be checked for collision
VEGAS algorithm:
a method for reducing error in Monte Carlo simulations


===
Statistics ===
Algorithms for calculating variance: avoiding instability and numerical overflow
Approximate
counting algorithm
: Allows counting large number of events in a small register
Bayesian statistics
Nested sampling algorithm: a computational approach to the problem of comparing models in Bayesian statistics
Clustering Algorithms
Average-linkage clustering: a simple agglomerative clustering algorithm
Canopy clustering algorithm:
an unsupervised pre-clustering algorithm related to the K-means algorithm
Complete-linkage clustering: a simple agglomerative clustering algorithm
DBSCAN:
a density based clustering algorithm
Expectation-maximization algorithm
Fuzzy clustering: a class of clustering algorithms where each point has a degree of belonging to clusters
Fuzzy c-means
FLAME clustering (
Fuzzy clustering by Local Approximation of MEmberships): define clusters in the dense parts of a dataset and perform cluster assignment solely based on the neighborhood relationships among objects
KHOPCA clustering algorithm: a local clustering algorithm, which produces hierarchical multi-hop clusters in static and mobile environments.

k-means clustering: cluster objects based on attributes into partitions
k-means++: a variation of this, using modified random seeds
k-medoids: similar to k-means, but chooses datapoints or medoids as centers
Linde–Buzo–
Gray algorithm:
a vector quantization algorithm to derive a good codebook
Lloyd's algorithm (Voronoi iteration or relaxation):
group data points into a given number of categories, a popular algorithm for k-means clustering
OPTICS:
a density based clustering algorithm with a visual evaluation method
Single-linkage clustering: a simple agglomerative clustering algorithm
SUBCLU:
a subspace clustering algorithm
Ward's method: an agglomerative clustering algorithm, extended to more general Lance–
Williams algorithms
WACA clustering algorithm: a local clustering algorithm with potentially multi-hop structures; for dynamic networks
Estimation Theory
Expectation-maximization algorithm A class of related algorithms for finding maximum likelihood estimates of parameters in probabilistic models
Ordered subset expectation maximization (OSEM): used in medical imaging for positron emission tomography, single-photon emission computed tomography and X-ray computed tomography.

Odds algorithm
(Bruss algorithm)
Optimal online search for distinguished value in sequential random input
Kalman filter
: estimate the state of a linear dynamic system from a series of noisy measurements
False nearest neighbor algorithm (FNN) estimates fractal dimension
Hidden Markov model
Baum–Welch algorithm: compute maximum likelihood estimates and posterior mode estimates for the parameters of a hidden Markov model
Forward-backward algorithm a dynamic programming algorithm for computing the probability of a particular observation sequence
Viterbi algorithm:
find the most likely sequence of hidden states in a hidden Markov model
Partial least squares regression:
finds a linear model describing some predicted variables in terms of other observable variables
Queuing theory
Buzen's algorithm: an algorithm for calculating the normalization constant G(K) in the Gordon–Newell theorem
RANSAC (an abbreviation for "RANdom SAmple Consensus"): an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers
Scoring algorithm: is a form of Newton's method used to solve maximum likelihood equations numerically
Yamartino method: calculate an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data
Ziggurat algorithm:
generate random numbers from a non-uniform distribution


==
Computer science ==


===
Computer architecture ===
Tomasulo algorithm: allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially


===
Computer graphics ===
Clipping
Line clipping
Cohen
–Sutherland
Cyrus–Beck
Fast-clipping
Liang–
Barsky
Nicholl–Lee–
Nicholl
Polygon clipping
Sutherland–
Hodgman
Vatti
Weiler–Atherton
Contour lines and Isosurfaces
Marching cubes:
extract a polygonal mesh of an isosurface from a three-dimensional scalar field (sometimes called voxels)
Marching squares: generate contour lines for a two-dimensional scalar field
Marching tetrahedrons: an alternative to Marching cubes
Discrete Green's Theorem: is an algorithm for computing double integral over a generalized rectangular domain in constant time.

It is a natural extension to the summed area table algorithm
Flood fill: fills a connected region of a multi-dimensional array with a specified symbol
Global illumination algorithms:
Considers direct illumination and reflection from other objects.

Ambient occlusion
Beam tracing
Cone tracing
Image-based lighting
Metropolis light transport
Path tracing
Photon mapping
Radiosity
Ray tracing
Hidden surface removal or Visual surface determination
Newell's algorithm: eliminate polygon cycles in the depth sorting required in hidden surface removal
Painter's algorithm: detects visible parts of a 3-dimensional scenery
Scanline rendering: constructs an image by moving an imaginary line over the image
Warnock algorithm
Line Drawing: graphical algorithm for approximating a line segment on discrete graphical media.

Bresenham's line algorithm: plots points of a 2-dimensional array to form a straight line between 2 specified points (uses decision variables)
DDA line algorithm:
plots points of a 2-dimensional array to form a straight line between 2 specified points (uses floating-point math)
Xiaolin Wu's line algorithm: algorithm for line antialiasing.

Midpoint circle algorithm:
an algorithm used to determine the points needed for drawing a circle
Ramer–Douglas–
Peucker algorithm:
Given a 'curve' composed of line segments to find a curve not too dissimilar but that has fewer points
Shading
Gouraud shading:
an algorithm to simulate the differing effects of light and colour across the surface of an object in 3D computer graphics
Phong shading: an algorithm to interpolate surface normal-vectors for surface shading in 3D computer graphics
Slerp (spherical linear interpolation): quaternion interpolation for the purpose of animating 3D rotation
Summed area table (also known as an integral image): an algorithm for computing the sum of values in a rectangular subset of a grid in constant time


===
Cryptography ===
Asymmetric (public key
) encryption:
ElGamal
Elliptic curve cryptography
MAE1
NTRUEncrypt
RSA
Digital signatures (asymmetric authentication):
DSA, and its variants:
ECDSA and Deterministic ECDSA
EdDSA
(Ed25519)
RSA
Cryptographic hash functions (see also the section on message authentication codes):
BLAKE
MD5
– Note that there is now a method of generating collisions for MD5
RIPEMD-160
SHA-1 –
Note that there is now a method of generating collisions for
SHA-1
SHA-2 (SHA-224, SHA-256, SHA-384, SHA-512)
SHA-3
(SHA3-224,
SHA3-256,
SHA3-384,
SHA3-512, SHAKE128, SHAKE256)
Tiger (TTH), usually used in Tiger tree hashes
WHIRLPOOL
Cryptographically secure pseudo-random number generators
Blum Blum Shub – based on the hardness of factorization
Fortuna, intended as an improvement on Yarrow algorithm
Linear-feedback shift register (note:
many LFSR-based algorithms are weak or have been broken)
Yarrow algorithm
Key exchange
Diffie–Hellman key exchange
Elliptic-curve Diffie–Hellman (ECDH)
Key derivation functions, often used for password hashing and key stretching
bcrypt
PBKDF2
scrypt
Argon2
Message authentication codes (symmetric authentication algorithms, which take a key as a parameter):
HMAC: keyed-hash message authentication
Poly1305
SipHash
Secret sharing, Secret Splitting, Key Splitting, M of N algorithms
Blakey's Scheme
Shamir's Scheme
Symmetric (secret key)
encryption:
Advanced Encryption Standard (AES), winner of NIST competition, also known as Rijndael
Blowfish
Twofish
Threefish
Data Encryption Standard (DES),
sometimes DE Algorithm, winner of NBS selection competition, replaced by AES for most purposes
IDEA
RC4 (cipher)
Tiny Encryption Algorithm (TEA)
Salsa20, and its updated variant ChaCha20
Post-quantum cryptography
Proof-of-work algorithms


===
Digital logic ===
Boolean minimization
Quine–McCluskey algorithm: Also called as Q-M algorithm, programmable method for simplifying the boolean equations.

Petrick's method: Another algorithm for boolean simplification.

Espresso heuristic logic minimizer:
Fast algorithm for boolean function minimization.

===
Machine learning and statistical classification ===
ALOPEX: a correlation-based machine-learning algorithm
Association rule learning: discover interesting relations between variables, used in data mining
Apriori algorithm
Eclat algorithm
FP-growth algorithm
One-attribute rule
Zero-attribute rule
Boosting (meta-algorithm)
: Use many weak learners to boost effectiveness
AdaBoost:
adaptive boosting
BrownBoost:
a boosting algorithm that may be robust to noisy datasets
LogitBoost: logistic regression boosting
LPBoost:
linear programming boosting
Bootstrap aggregating (bagging):
technique to improve stability and classification accuracy
Computer Vision
Grabcut based on Graph cuts
Decision Trees
C4.5 algorithm:
an extension to
ID3
ID3 algorithm (Iterative Dichotomiser 3):
use heuristic to generate small decision trees
Clustering: a class of unsupervised learning algorithms for grouping and bucketing related input vector.

k-nearest neighbors (k-NN): a method for classifying objects based on closest training examples in the feature space
Linde–Buzo–
Gray algorithm: a vector quantization algorithm used to derive a good codebook
Locality-sensitive hashing (LSH): a method of performing probabilistic dimension reduction of high-dimensional data
Neural Network
Backpropagation:
A supervised learning method which requires a teacher that knows, or can calculate, the desired output for any given input
Hopfield net: a Recurrent neural network in which all connections are symmetric
Perceptron: the simplest kind of feedforward neural network: a linear classifier.

Pulse-coupled neural networks (PCNN):
Neural models proposed by modeling a cat's visual cortex and developed for high-performance biomimetic image processing.

Radial basis function network: an artificial neural network that uses radial basis functions as activation functions
Self-organizing map: an unsupervised network that produces a low-dimensional representation of the input space of the training samples
Random forest: classify using many decision trees
Reinforcement learning:
Q-learning: learns an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafter
State–Action–Reward–State–Action (SARSA)
: learn a Markov decision process policy
Temporal difference learning
Relevance-Vector Machine (RVM): similar to SVM, but provides probabilistic classification
Supervised learning: Learning by examples (labelled data-set split into training-set and test-set)
Support-Vector Machine (SVM): a set of methods which divide multidimensional data by finding a dividing hyperplane with the maximum margin between the two sets
Structured SVM: allows training of a classifier for general structured output labels.

Winnow algorithm:
related to the perceptron, but uses a multiplicative weight-update scheme


===
Programming language theory ===
C3 linearization: an algorithm used primarily to obtain a consistent linearization of a multiple inheritance hierarchy in object-oriented programming
Chaitin's algorithm: a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric
Hindley–Milner type inference algorithm
Rete algorithm: an efficient pattern matching algorithm for implementing production rule systems
Sethi-Ullman algorithm: generate optimal code for arithmetic expressions


====
Parsing ====
CYK algorithm:
An O(n3) algorithm for parsing context-free grammars in Chomsky normal form
Earley parser:
Another O(n3) algorithm for parsing any context-free grammar
GLR parser:
An algorithm for parsing any context-free grammar by Masaru Tomita.
It is tuned for deterministic grammars, on which it performs almost linear time and O(n3) in worst case.

Inside-outside algorithm:
An O(n3) algorithm for re-estimating production probabilities in probabilistic context-free grammars
LL parser: A relatively simple linear time parsing algorithm for a limited class of context-free grammars
LR parser:
A more complex linear time parsing algorithm for a larger class of context-free grammars.

Variants:
Canonical LR parser
LALR (
Look-ahead LR) parser
Operator-precedence parser
SLR (Simple LR) parser
Simple precedence parser
Packrat parser:
A linear time parsing algorithm supporting some context-free grammars and parsing expression grammars
Recursive descent parser:
A top-down parser suitable for LL(k) grammars
Shunting-yard algorithm
: convert an infix-notation math expression to postfix
Pratt parser
Lexical analysis


===
Quantum algorithms ===
Deutsch–
Jozsa algorithm:
criterion of balance for Boolean function
Grover's algorithm: provides quadratic speedup for many search problems
Shor's algorithm: provides exponential speedup (relative to currently known non-quantum algorithms) for factoring a number
Simon's algorithm: provides a provably exponential speedup (relative to any non-quantum algorithm) for a black-box problem


===
Theory of computation and automata ===
Hopcroft's algorithm, Moore's algorithm, and Brzozowski's algorithm: algorithms for minimizing the number of states in a deterministic finite automaton
Powerset construction:
Algorithm to convert nondeterministic automaton to deterministic automaton.

Tarski–Kuratowski algorithm: a non-deterministic algorithm which provides an upper bound for the complexity of formulas in the arithmetical hierarchy and analytical hierarchy


==
Information theory and signal processing ==


===
Coding theory ===


====
Error detection and correction ====
BCH Codes
Berlekamp–Massey algorithm
Peterson–Gorenstein–
Zierler algorithm
Reed–Solomon error correction
BCJR algorithm:
decoding of error correcting codes defined on trellises (principally convolutional codes)
Forward error correction
Gray code
Hamming codes
Hamming(7,4):
a Hamming code that encodes 4 bits of data into 7 bits by adding 3 parity bits
Hamming distance: sum number of positions which are different
Hamming weight (population count)
: find the number of 1 bits in a binary word
Redundancy checks
Adler-32
Cyclic redundancy check
Damm algorithm
Fletcher's checksum
Longitudinal redundancy check (LRC)
Luhn algorithm:
a method of validating identification numbers
Luhn mod N algorithm: extension of Luhn to non-numeric characters
Parity: simple/fast error detection technique
Verhoeff algorithm
====
Lossless compression algorithms ====
Burrows–Wheeler transform: preprocessing useful for improving lossless compression
Context tree weighting
Delta encoding: aid to compression of data in which sequential data occurs frequently
Dynamic Markov compression:
Compression using predictive arithmetic coding
Dictionary coders
Byte pair encoding (BPE)
Deflate
Lempel–Ziv
LZ77 and LZ78
Lempel–Ziv Jeff Bonwick (LZJB)
Lempel–Ziv–Markov chain algorithm (LZMA)
Lempel–Ziv–Oberhumer (LZO): speed oriented
Lempel–Ziv–Stac (LZS)
Lempel–Ziv–Storer–Szymanski (LZSS)
Lempel–Ziv–
Welch (LZW)
LZWL:
syllable-based variant
LZX
Lempel–Ziv Ross Williams (LZRW)
Entropy encoding: coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols
Arithmetic coding:
advanced entropy coding
Range encoding: same as arithmetic coding, but looked at in a slightly different way
Huffman coding: simple lossless compression taking advantage of relative character frequencies
Adaptive Huffman coding: adaptive coding technique based on Huffman coding
Package-merge algorithm: Optimizes Huffman coding subject to a length restriction on code strings
Shannon–Fano coding
Shannon–Fano–Elias
coding: precursor to arithmetic encoding
Entropy coding with known entropy characteristics
Golomb coding: form of entropy coding that is optimal for alphabets following geometric distributions
Rice coding: form of entropy coding that is optimal for alphabets following geometric distributions
Truncated binary encoding
Unary coding: code that represents a number n with n ones followed by a zero
Universal codes:
encodes positive integers into binary code words
Elias delta, gamma, and omega coding
Exponential-Golomb coding
Fibonacci coding
Levenshtein coding
Fast Efficient & Lossless
Image Compression System (FELICS): a lossless image compression algorithm
Incremental encoding:
delta encoding applied to sequences of strings
Prediction by partial matching (PPM): an adaptive statistical data compression technique based on context modeling and prediction
Run-length encoding:
lossless data compression taking advantage of strings of repeated characters
SEQUITUR algorithm:
lossless compression by incremental grammar inference on a string


====
Lossy compression algorithms ====
3Dc: a lossy data compression algorithm for normal maps
Audio and Speech compression
A-law algorithm:
standard companding algorithm
Code-excited linear prediction (CELP):
low bit-rate speech compression
Linear predictive coding (LPC):
lossy compression by representing the spectral envelope of a digital signal of speech in compressed form
Mu-law algorithm: standard analog signal compression or companding algorithm
Warped Linear Predictive Coding (WLPC)
Image compression
Block Truncation Coding (BTC):
a type of lossy image compression technique for greyscale images
Embedded Zerotree Wavelet (EZW)
Fast Cosine Transform algorithms (FCT algorithms):
compute Discrete Cosine Transform (DCT) efficiently
Fractal compression: method used to compress images using fractals
Set Partitioning in Hierarchical Trees (SPIHT)
Wavelet compression: form of data compression
well suited for image compression (sometimes also video compression and audio compression)
Transform coding: type of data compression for "natural" data like audio signals or photographic images
Video compression
Vector quantization:
technique often used in lossy data compression


===
Digital signal processing ===
Adaptive-additive algorithm (AA algorithm)
: find the spatial frequency phase of an observed wave source
Discrete Fourier transform: determines the frequencies contained in a (segment of a) signal
Bluestein's FFT algorithm
Bruun's FFT algorithm
Cooley–Tukey FFT algorithm
Fast Fourier transform
Prime-factor FFT algorithm
Rader's FFT algorithm
Fast folding algorithm: an efficient algorithm for the detection of approximately periodic events within time series data
Gerchberg–
Saxton algorithm:
Phase retrieval algorithm for optical planes
Goertzel algorithm:
identify a particular frequency component in a signal.

Can be used for DTMF digit decoding.

Karplus-Strong string synthesis:
physical modelling synthesis to simulate the sound of a hammered or plucked string or some types of percussion


====
Image processing ====
Contrast Enhancement
Histogram equalization:
use histogram to improve image contrast
Adaptive histogram equalization:
histogram equalization which adapts to local changes in contrast
Connected-component labeling: find and label disjoint regions
Dithering and half-toning
Error diffusion
Floyd–
Steinberg dithering
Ordered dithering
Riemersma dithering
Elser difference-map algorithm:
a search algorithm for general constraint satisfaction problems.

Originally used for X-Ray diffraction microscopy
Feature detection
Canny edge detector: detect a wide range of edges in images
Generalised Hough transform
Hough transform
Marr–Hildreth algorithm:
an early edge detection algorithm
SIFT (Scale-invariant feature transform): is an algorithm to detect and describe local features in images.

SURF
(Speeded Up Robust Features)
: is a robust local feature detector, first presented by Herbert Bay et al.
in 2006, that can be used in computer vision tasks like object recognition or 3D reconstruction.
It is partly inspired by the SIFT descriptor.
The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.

Richardson–Lucy deconvolution:
image de-blurring algorithm
Blind deconvolution:
image de-blurring algorithm when point spread function is unknown.

Median filtering
Seam carving:
content-aware image resizing algorithm
Segmentation: partition a digital image into two or more regions
GrowCut algorithm:
an interactive segmentation algorithm
Random walker algorithm
Region
growing
Watershed transformation: a class of algorithms based on the watershed analogy


==
Software engineering ==
Cache algorithms
CHS conversion: converting between disk addressing systems
Double dabble:
Convert binary numbers to BCD
Hash Function
: convert a large, possibly variable-sized amount of data into a small datum, usually a single integer that may serve as an index into an array
Fowler–Noll–
Vo hash function: fast with low collision rate
Pearson hashing: computes 8 bit value only, optimized for 8 bit computers
Zobrist hashing: used in the implementation of transposition tables
Unicode
Collation Algorithm
Xor swap algorithm: swaps the values of two variables without using a buffer


==
Database algorithms ==
Algorithms for Recovery and Isolation Exploiting Semantics (ARIES):
transaction recovery
Join
algorithms
Block nested loop
Hash join
Nested loop join
Sort-Merge Join


==
Distributed systems algorithms ==
Clock synchronization
Berkeley algorithm
Cristian's algorithm
Intersection algorithm
Marzullo's algorithm
Consensus (computer science): agreeing on a single value or history among unreliable processors
Chandra–
Toueg consensus algorithm
Paxos algorithm
Raft (computer science)
Detection of Process Termination
Dijkstra-Scholten algorithm
Huang's algorithm
Lamport ordering: a partial ordering of events based on the happened-before relation
Leader election: a method for dynamically selecting a coordinator
Bully algorithm
Mutual exclusion
Lamport's Distributed Mutual Exclusion Algorithm
Naimi-Trehel's log(n)
Algorithm
Maekawa's Algorithm
Raymond's Algorithm
Ricart–
Agrawala Algorithm
Snapshot algorithm:
record a consistent global state for an asynchronous system
Chandy–Lamport algorithm
Vector clocks: generate a partial ordering of events in a distributed system and detect causality violations


===
Memory allocation and deallocation algorithms
===
Buddy memory allocation:
Algorithm to allocate memory such that fragmentation is less.

Garbage collectors
Cheney's algorithm:
An improvement on the Semi-space collector
Generational garbage collector:
Fast garbage collectors that segregate memory by age
Mark-compact algorithm:
a combination of the mark-sweep algorithm  and Cheney's copying algorithm
Mark and sweep
Semi-space collector:
An early copying collector
Reference counting


==
Networking ==
Karn's algorithm: addresses the problem of getting accurate estimates of the round-trip time for messages when using
TCP
Luleå algorithm:
a technique for storing and searching internet routing tables efficiently
Network congestion
Exponential backoff
Nagle's algorithm: improve the efficiency of TCP/IP networks by coalescing packets
Truncated binary exponential backoff


==
Operating systems algorithms ==
Banker's algorithm: Algorithm used for deadlock avoidance.

Page replacement algorithms:
Selecting the victim page under low memory conditions.

Adaptive replacement cache: better performance than LRU
Clock with Adaptive Replacement (CAR)
: is a page replacement algorithm that has performance comparable to Adaptive replacement cache


=== Process synchronization ===
Dekker's algorithm
Lamport's Bakery algorithm
Peterson's algorithm


===
Scheduling ===
Earliest deadline first scheduling
Fair-share scheduling
Least slack time scheduling
List scheduling
Multi level feedback queue
Rate-monotonic scheduling
Round-robin scheduling
Shortest job next
Shortest remaining time
Top-nodes algorithm:
resource calendar management
===
I/O scheduling ===


====
Disk scheduling ====
Elevator algorithm:
Disk scheduling algorithm that works like an elevator.

Shortest seek first
: Disk scheduling algorithm to reduce seek time.

== See also ==
List of data structures
List of machine learning algorithms
List of pathfinding algorithms
List of algorithm general topics
List of terms relating to algorithms and data structures
Heuristic


== References ==
The Cooley–Tukey algorithm, named after J. W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm.

It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size 
  
    
      
        N
        =
N
          
            1
N
          
            2
          
        
      
    
    {\displaystyle N=N_{1}N_{2}
}
   in terms of N1 smaller DFTs of sizes N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers).
Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.

Because the Cooley–Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT.

For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.

The algorithm, along with its recursive application, was invented by Carl Friedrich Gauss.
Cooley and Tukey independently rediscovered and popularized it 160 years later.

==
History ==
This algorithm, including its recursive application, was invented around 1805 by Carl Friedrich Gauss, who used it to interpolate the trajectories of the asteroids Pallas and Juno, but his work was not widely recognized (being published only posthumously and in neo-Latin).
Gauss did not analyze the asymptotic computational time, however.
Various limited forms were also rediscovered several times throughout the 19th and early 20th centuries.

FFTs became popular after James Cooley of IBM and John Tukey of Princeton published a paper in 1965 reinventing the algorithm and describing how to perform it conveniently on a computer.
Tukey reportedly came up with the idea during a meeting of President Kennedy’s Science Advisory Committee discussing ways to detect nuclear-weapon tests in the Soviet Union by employing seismometers located outside the country.
These sensors would generate seismological time series.

However, analysis of this data would require fast algorithms for computing DFT due to number of sensors and length of time.
This task was critical for the ratification of the proposed nuclear test ban so that any violations could be detected without need to visit Soviet facilities.

Another participant at that meeting, Richard Garwin of IBM, recognized the potential of the method and put Tukey in touch with Cooley however making sure that Cooley did not know the original purpose.
Instead Cooley was told that this was needed to determine periodicities of the spin orientations in a 3-D crystal of Helium-3.
Cooley and Tukey subsequently published their joint paper, and wide adoption quickly followed due to the simultaneous development of Analog-to-digital converters capable of sampling at rates up to 300 kHz.

The fact that Gauss had described the same algorithm (albeit without analyzing its asymptotic cost) was not realized until several years after Cooley and Tukey's 1965 paper.

Their paper cited as inspiration only the work by I. J. Good on what is now called the prime-factor FFT algorithm (PFA); although Good's algorithm was initially thought to be equivalent to the Cooley
–Tukey algorithm, it was quickly realized that PFA is a quite different algorithm (working only for sizes that have relatively prime factors and relying on the Chinese Remainder Theorem, unlike the support for any composite size in Cooley–Tukey).

==
The radix-2 DIT case ==
A radix-2 decimation-in-time (DIT) FFT is the simplest and most common form of the Cooley–Tukey algorithm, although highly optimized Cooley–Tukey implementations typically use other forms of the algorithm as described below.
Radix-2 DIT divides a DFT of size N into two interleaved DFTs (hence the name "radix-2") of size N/2 with each recursive stage.

The discrete Fourier transform (DFT)
is defined by the formula:

  
    
      
        
          X
          
            k
          
        
        =
∑
n
=
0
N
            −
1
x
          
            n
e
          
            −
2
π
i
                
                N
n
k
          
        
        ,
      
    
    {\displaystyle X_{k}=\sum _{n=0}^{N-1}x_{n}e^{-{\frac {2\pi i}{N}}nk},}
where 
  
    
      
        k
      
    
    {\displaystyle k}
   is an integer ranging from 
  
    
      
        0
      
    
    {\displaystyle 0}
   to 
  
    
      
        N
        −
1
      
    
    {
\displaystyle
N-1}
  .

Radix-2 DIT first computes the DFTs of the even-indexed inputs
(
        
          x
          
            2
            m
          
        
        =
x
          
            0
          
        
        ,
x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            N
            −
2
          
        
        )
      
    
    {
\displaystyle (x_{2m}=x_{0},x_{2},\ldots ,x_{N-2})}
and of the odd-indexed inputs 
  
    
      
        (
        
          x
          
            2
            m
+
            1
          
        
        =
x
          
            1
          
        
        ,
x
          
            3
          
        
        ,
        …
        ,
        
          x
          
            N
            −
1
          
        
        )
      
    
    {
\displaystyle (x_{2m+1}=x_{1},x_{3},\ldots ,x_{N-1})}
  ,
and then combines those two results to produce the DFT of the whole sequence.
This idea can then be performed recursively to reduce the overall runtime to O(N log N).

This simplified form assumes that N is a power of two; since the number of sample points
N can usually be chosen freely by the application (e.g. by changing the sample rate or window, zero-padding, etcetera), this is often not an important restriction.

The radix-2 DIT algorithm rearranges the DFT of the function
x
n
          
        
      
    
    {\displaystyle x_{n}}
   into two parts:
a sum over the even-numbered indices
n
        =
        
          2
m
        
      
    
    {\displaystyle n={2m}}
   and a sum over the odd-numbered indices
n
        =
        
          2
          m
+
          1
        
      
    
    {\displaystyle n={2m+1}}
  :
X
                  
                    k
                  
                
              
              
                =
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
                  
                
                
                  e
−
2
π
i
                        
                        N
(
2
                    m
                    )
k
+
                
                  ∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
+
                    1
                  
                
                
                  e
                  
                    −
2
π
i
                        
                        N
(
                    2
                    m
+
                    1
                    )
k
{\displaystyle {\begin{matrix}X_{k}&=&\sum \limits
_{m=0}^{N/2-1}x_{2m}e^{-{\frac {2\pi i}{N}}(2m)k}+\sum
\limits _{m=0}^{N/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{N}}(2m+1)k}\end{matrix}}}
One can factor a common multiplier 
  
    
      
        
          e
          
            −
2
π
i
                
                N
k
          
        
      
    
    {\displaystyle e^{-{\frac {2\pi i}{N}}k}}
   out of the second sum, as shown in the equation below.
It is then clear that the two sums are the DFT of the even-indexed part
x
2
            m
          
        
      
    
    {\displaystyle x_{2m}}
   and the DFT of odd-indexed part
x
          
            2
            m
+
            1
          
        
      
    
    {\displaystyle x_{2m+1}}
   of the function
x
n
          
        
      
    
    {\displaystyle x_{n}}
  .
Denote the DFT of the Even-indexed inputs
x
2
            m
          
        
      
    
    {\displaystyle x_{2m}}
   by
E
k
          
        
      
    
    {\displaystyle E_{k}}
   and the DFT of the Odd-indexed inputs
x
          
            2
            m
+
1
          
        
      
    
    {\displaystyle x_{2m+1}}
   by
O
          
            k
          
        
      
    
    {\displaystyle O_{k}}
and we obtain:
X
                  
                    k
                  
                
                =
∑
                          
                            m
                            =
                            0
                          
                          
                            N
                            
                              /
                            
                            2
                            −
                            1
x
                          
                            2
m
                          
                        
                        
                          e
                          
                            −
2
π
i
N
                                  
                                    /
                                  
                                  2
m
                            k
                          
                        
                      
                      ⏟
D
                      F
                      T
                      
                      o
f
                      
                      e
v
e
                      n
                      −
i
                      n
                      d
                      e
                      x
                      e
                      d
                      
                      p
a
                      r
                      t
                      
                      o
f
                      
                    
                    
                      x
                      
                        n
+
                
                  e
                  
                    −
2
π
i
N
k
∑
                          
                            m
=
                            0
                          
                          
                            N
                            
                              /
                            
                            2
−
1
x
                          
                            2
                            m
+
                            1
e
                          
                            −
2
π
i
N
                                  
                                    /
                                  
                                  2
m
                            k
                          
                        
                      
                      ⏟
D
                      F
                      T
                      
                      o
f
                      
                      o
d
                      d
                      −
i
                      n
                      d
                      e
                      x
                      e
                      d
                      
                      p
a
                      r
                      t
                      
                      o
f
                      
                    
                    
                      x
n
                      
                    
                  
                
                =
E
k
+
                
                  e
                  
                    −
2
π
i
                        
                        N
k
                  
                
                
                  O
k
                  
                
                .

{\displaystyle {\begin{matrix}X_{k}=\underbrace {\sum \limits _{m=0}^{N/2-1}x_{2m}e^{-{\frac {2\pi i}{N/2}}mk}} _{\mathrm {DFT\;of\;even-indexed\;part\;of\;} x_{n}}{}+e^{-{\frac {2\pi i}{N}}k}\underbrace {\sum \limits _{m=0}^{N/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{N/2}}mk}
} _{\mathrm {DFT\;of\;odd-indexed\;part\;of\;} x_{n}}=E_{k}+e^{-{\frac {2\pi i}{N}}k}O_{k}.\end{matrix}}}
Thanks to the periodicity of the complex exponential,
X
          
            k
+
N
                2
              
            
          
        
      
    
    {\displaystyle X_{k+{\frac
{N}{2}}}}
   is also obtained from 
  
    
      
        
          E
          
            k
          
        
      
    
    {\displaystyle E_{k}}
   and
O
k
          
        
      
    
    {\displaystyle O_{k}}
  :
X
                  
                    k
                    +
                    
                      
                        N
                        2
=
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
                  
                
                
                  e
−
2
π
i
N
                          
                            /
2
m
(
                    k
+
                    
                      
                        N
                        2
                      
                    
                    )
+
                
                  e
                  
                    −
2
π
i
                        
                        N
(
                    k
+
                    
                      
                        N
                        2
                      
                    
                    )
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
+
                    1
                  
                
                
                  e
                  
                    −
2
π
i
N
                          
                            /
2
m
(
                    k
+
N
                        2
                      
                    
                    )
=
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
                  
                
                
                  e
−
2
π
i
N
                          
                            /
2
m
k
                  
                
                
                  e
                  
                    −
2
π
m
i
+
                
                  e
                  
                    −
2
π
i
                        
                        N
k
                  
                
                
                  e
−
π
i
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
+
                    1
                  
                
                
                  e
                  
                    −
2
π
i
N
                          
                            /
2
m
k
                  
                
                
                  e
                  
                    −
2
π
                    m
i
                  
                
              
            
            
              
              
                
                =
∑
                  
                    m
=
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
                  
                
                
                  e
−
2
π
i
N
                          
                            /
2
m
k
                  
                
                −
e
                  
                    −
2
π
i
                        
                        N
k
                  
                
                
                  ∑
                  
                    m
                    =
0
N
                    
                      /
                    
                    2
                    −
1
x
                  
                    2
m
+
                    1
                  
                
                
                  e
                  
                    −
2
π
i
N
                          
                            /
2
m
k
=
                
                  E
k
                  
                
                −
e
                  
                    −
2
π
i
                        
                        N
k
                  
                
                
                  O
k
{\displaystyle {\begin{aligned}X_{k+{\frac {N}{2}}}&=\sum \limits
_{m=0}^{N/2-1}x_{2m}e^{-{\frac
{2\pi i}{N/2}}m(k+{\frac {N}{2}})}+e^{-{\frac {2\pi i}{N}}(k+{\frac
{N}{2}})}\sum \limits
_{m=0}^{N/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{N/2}}m(k+{\frac {
N}{2}})}\\&=\sum
\limits
_{m=0}^{N/2-1}x_{2m}e^{-{\frac
{2\pi i}{N/2}}mk}e^{-2\pi
mi}+e^{-{\frac {2\pi i}{N}}k}e^{-\pi
i}\sum \limits _{m=0}^{N/2-
1}x_{2m+1}e^{-{\frac {2\pi i}{N/2}}mk}e^{-2\pi
mi}\\&=\sum
\limits
_{m=0}^{N/2-1}x_{2m}e^{-{\frac {2\pi i}{N/2}}mk}-e^{-{\frac {2\pi i}{N}}k}\sum
\limits
_{m=0}^{N/2-
1}x_{2m+1}e^{-{\frac {2\pi
i}{N/2}}mk}\\&=E_{k}-e^{-{\frac {2\pi i}{N}}k}O_{k}\end{aligned}}}
We can rewrite
X
          
            k
{\displaystyle X_{k}}
   as:
X
                  
                    k
                  
                
              
              
                =
E
k
+
                
                  e
                  
                    −
2
π
i
N
k
                    
                  
                
                
                  O
k
X
                  
                    k
+
                    
                      
                        N
                        2
                      
                    
                  
                
              
              
                =
E
                  
                    k
                  
                
                −
e
                  
                    −
2
π
i
N
k
                    
                  
                
                
                  O
k
{\displaystyle {\begin{matrix}X_{k}&=&E_{k}+e^{-{\frac {2\pi i}{N}}{k}}O_{k}\\X_{k+{\frac {
N}{2}}}&=&E_{k}-e^{-{\frac {2\pi i}{N}}{k}}O_{k}\end{matrix}}}
This result, expressing the DFT of length N recursively in terms of two DFTs of size N/2, is the core of the radix-2 DIT fast Fourier transform.
The algorithm gains its speed by re-using the results of intermediate computations to compute multiple DFT outputs.

Note that final outputs are obtained by a +/− combination of 
  
    
      
        
          E
          
            k
          
        
      
    
    {\displaystyle E_{k}}
   and 
  
    
      
        
          O
          
            k
exp
⁡
(
        −
        2
π
i
k
        
          /
        
        N
        )
{\displaystyle O_{k}\exp(-2\pi ik/N)}
  , which is simply a size-2 DFT (sometimes called a butterfly in this context); when this is generalized to larger radices below, the size-2 DFT is replaced by a larger DFT (which itself can be evaluated with an FFT).

This process is an example of the general technique of divide and conquer algorithms; in many conventional implementations, however, the explicit recursion is avoided, and instead one traverses the computational tree in breadth-first fashion.

The above re-expression of a size-N DFT as two size-
N/2 DFTs is sometimes called the Danielson–Lanczos lemma, since the identity was noted by those two authors in 1942 (influenced by Runge's 1903 work).

They applied their lemma in a "backwards" recursive fashion, repeatedly doubling the DFT size until the transform spectrum converged (although they apparently didn't realize the linearithmic [i.e., order N log N] asymptotic complexity they had achieved).

The Danielson–Lanczos work predated widespread availability of mechanical or electronic computers and required manual calculation (possibly with mechanical aids such as adding machines); they reported a computation time of 140 minutes for a size-64 DFT operating on real inputs to 3–5 significant digits.

Cooley and Tukey's 1965 paper reported a running time of 0.02 minutes for a size-2048 complex DFT on an IBM 7094 (probably in 36-bit single precision, ~8 digits).

Rescaling the time by the number of operations, this corresponds roughly to a speedup factor of around 800,000.

(To put the time for the hand calculation in perspective, 140 minutes for size 64 corresponds to an average of at most 16 seconds per floating-point operation, around 20% of which are multiplications.)

===
Pseudocode ===
In pseudocode, the below procedure could be written:
X0,...
,N−1 ← ditfft2(x, N, s):             DFT of (x0, xs, x2s, ..., x(N-1)s):
    if N = 1 then
        X0 ← x0
trivial size-1 DFT base case
else
X0,...
,N/2−1 ← ditfft2(x, N/2, 2s)             DFT of (x0, x2s, x4s, ...)
XN/2,...
,N−1 ← ditfft2(x+s, N/2, 2s)           DFT of (xs, xs+2s, xs+4s, ...)
        for k = 0 to N/2−1 do                        combine DFTs of two halves into full DFT:
p ← Xk
            q ← exp(−2πi/N k)
Xk+N/2
Xk ← p + q
Xk+N/2 ← p −
q
        end for
    end
if

Here, ditfft2(x,N,1), computes X=DFT(x) out-of-place by a radix-2 DIT FFT, where N is an integer power of 2 and s=1 is the stride of the input x array.

x+s denotes the array starting with xs.

(The results are in the correct order in X and no further bit-reversal permutation is required; the often-mentioned necessity of a separate bit-reversal stage only arises for certain in-place algorithms, as described below.)

High-performance FFT implementations make many modifications to the implementation of such an algorithm compared to this simple pseudocode.

For example, one can use a larger base case than N=1 to amortize the overhead of recursion, the twiddle factors 
  
    
      
        exp
⁡
[
        −
        2
π
i
k
        
          /
        
        N
        ]
{\displaystyle \exp[-2\pi ik/N]}
   can be precomputed, and larger radices are often used for cache reasons; these and other optimizations together can improve the performance by an order of magnitude or more.

(In many textbook implementations the depth-first recursion is eliminated entirely in favor of a nonrecursive breadth-first approach, although depth-first recursion has been argued to have better memory locality.)
Several of these ideas are described in further detail below.
==
Idea ==
More generally, Cooley–Tukey algorithms
recursively re-express  a DFT of a composite size
N = N1N2
as:
Perform N1 DFTs of size N2.

Multiply by complex roots of unity
(often called the twiddle factors).

Perform N2 DFTs of size N1.Typically, either N1 or N2 is a small factor (not necessarily prime),
called the radix (which can differ between stages of the recursion).

If N1 is the radix, it is called a decimation in time (DIT) algorithm, whereas if N2 is the radix, it is decimation in frequency (DIF, also called the Sande–Tukey algorithm).
The version presented above was a radix-2 DIT algorithm; in the final expression, the phase multiplying the odd transform is the twiddle factor, and the +/-
combination (butterfly) of the even and odd transforms is a size-2 DFT.

(The radix's small DFT is sometimes known as a butterfly, so-called because of the shape of the dataflow diagram for the radix-2 case.)

== Variations ==
There are many other variations on the Cooley–Tukey algorithm.

Mixed-radix implementations handle composite sizes with a variety of (typically small) factors in addition to two, usually (but not always) employing the O(N2) algorithm for the prime base cases of the recursion
(it is also possible to employ an N log N algorithm for the prime base cases, such as Rader's or Bluestein's algorithm).

Split radix merges radices 2 and 4, exploiting the fact that the first transform of radix 2 requires no twiddle factor, in order to achieve what was long the lowest known arithmetic operation count for power-of-two sizes, although recent variations achieve an even lower count.

(On present-day computers, performance is determined more by cache and CPU pipeline considerations than by strict operation counts; well-optimized FFT implementations often employ larger radices and/or hard-coded base-case transforms of significant size.).

Another way of looking at the Cooley–Tukey algorithm is that it re-expresses a size N one-dimensional DFT as an N1 by N2 two-dimensional DFT (plus twiddles), where the output matrix is transposed.
The net result of all of these transpositions, for a radix-2 algorithm, corresponds to a bit reversal of the input (DIF) or output (DIT) indices.

If, instead of using a small radix, one employs a radix of roughly √N and explicit input/output matrix transpositions, it is called a four-step algorithm (or six-step, depending on the number of transpositions), initially proposed to improve memory locality, e.g. for cache optimization or out-of-core operation, and was later shown to be an optimal cache-oblivious algorithm.
The general Cooley–Tukey factorization rewrites the indices k and n
as 
  
    
      
        k
        =
        
          N
          
            2
k
          
            1
+
        
          k
2
          
        
      
    
    {\displaystyle k=N_{2}k_{1}+k_{2}}
and 
  
    
      
        n
        =
N
          
            1
          
        
        
          n
          
            2
+
        
          n
          
            1
          
        
      
    
    {\displaystyle n=N_{1}n_{2}+n_{1}}
  , respectively, where the indices ka and na run from 0..
Na-1 (for a of 1 or 2).

That is, it re-indexes the input (n) and output (k) as N1 by N2 two-dimensional arrays in column-major and row-major order, respectively; the difference between these indexings is a transposition, as mentioned above.

When this re-indexing is substituted into the DFT formula for nk, the
N
          
            1
          
        
        
          n
          
            2
N
          
            2
k
          
            1
          
        
      
    
    {\displaystyle N_{1}n_{2}N_{2}k_{1}}
cross term vanishes (its exponential is unity), and the remaining terms give

  
    
      
        
          X
          
            
              N
              
                2
k
              
                1
+
            
              k
              
                2
              
            
          
        
        =
        
          ∑
n
              
                1
=
            0
N
              
                1
              
            
            −
1
∑
n
              
                2
              
            
            =
            0
N
              
                2
              
            
            −
1
x
N
              
                1
              
            
            
              n
              
                2
+
            
              n
              
                1
              
            
          
        
        
          e
          
            −
            
              
                
                  2
π
i
N
                    
                      1
N
                    
                      2
⋅
(
            
              N
              
                1
              
            
            
              n
              
                2
+
            
              n
              
                1
              
            
            )
⋅
(
            
              N
              
                2
              
            
            
              k
              
                1
+
k
              
                2
              
            
            )
{\displaystyle X_{N_{2}k_{1}+k_{2}}=\sum _{n_{1}=0}^{N_{1}-1}\sum _{n_{2}=0}^{N_{2}-1}x_{N_{1}n_{2}+n_{1}}e^{-{\frac {2\pi i}{N_{1}N_{2}}}\cdot (N_{1}n_{2}+n_{1})\cdot (N_{2}k_{1}+k_{2})}
}
  

  
    
      
        =
        
          ∑
n
              
                1
=
            0
N
              
                1
              
            
            −
1
[
          
            e
            
              −
2
π
i
                  
                  
                    
                      N
                      
                        1
N
                      
                        2
n
                
                  1
k
                
                  2
]
(
          
            
              ∑
              
                
                  n
                  
                    2
=
                0
N
                  
                    2
−
1
              
            
            
              x
N
                  
                    1
n
                  
                    2
+
                
                  n
                  
                    1
e
              
                −
2
π
i
                    
                    
                      N
                      
                        2
n
                  
                    2
k
                  
                    2
                  
                
              
            
          
          )
e
          
            −
2
π
i
N
                  
                    1
n
              
                1
k
              
                1
              
            
          
        
      
    
    {\displaystyle =\sum _{n_{1}=0}^{N_{1}-1}\left[e^{-{\frac {2\pi i}{N_{1}N_{2}}}n_{1}k_{2}}\right]\left(\sum _{n_{2}=0}^{N_{2}-1}x_{N_{1}n_{2}+n_{1}}e^{-{\frac
{2\pi i}{N_{2}}}n_{2}k_{2}}\right)e^{-{\frac {2\pi i}{N_{1}}}n_{1}k_{1}}}
  

  
    
      
        =
∑
n
              
                1
=
            0
N
              
                1
              
            
            −
1
(
          
            
              ∑
              
                
                  n
                  
                    2
=
                0
N
                  
                    2
−
1
              
            
            
              x
N
                  
                    1
n
                  
                    2
+
                
                  n
                  
                    1
e
              
                −
2
π
i
                    
                    
                      N
                      
                        2
n
                  
                    2
k
                  
                    2
                  
                
              
            
          
          )
e
          
            −
2
π
i
N
                    
                      1
N
                    
                      2
n
              
                1
(
            
              N
              
                2
k
              
                1
+
k
              
                2
              
            
            )
          
        
      
    
    {\displaystyle =\sum _{n_{1}=0}^{N_{1}-1}\left(\sum _{n_{2}=0}^{N_{2}-1}x_{N_{1}n_{2}+n_{1}}e^{-{\frac {
2\pi i}{N_{2}}}n_{2}k_{2}}\right)e^{-{\frac {2\pi i}{N_{1}N_{2}}}n_{1}(N_{2}k_{1}+k_{2})}}
.where
each inner sum is a DFT of size N2, each outer sum is a DFT of size N1, and the [...] bracketed term is the twiddle factor.

An arbitrary radix r (as well as mixed radices) can be employed, as was shown by both Cooley and Tukey as well as Gauss (who gave examples of radix-3 and radix-6 steps).

Cooley and Tukey originally assumed that the radix butterfly required O(r2) work and hence reckoned the complexity for a radix r to be O(r2
N/r logrN) =
O(N log2(N) r/log2r)
; from calculation of values of r/log2r for integer values of r from 2 to 12 the optimal radix is found to be 3 (the closest integer to e, which minimizes r/log2r).

This analysis was erroneous, however: the radix-butterfly is also a DFT and can be performed via an FFT algorithm in O(r  log r) operations, hence the radix r actually cancels in the complexity O(r log(r) N/r logrN), and the optimal r is determined by more complicated considerations.

In practice, quite large r (32 or 64) are important in order to effectively exploit e.g. the large number of processor registers on modern processors, and
even an unbounded radix r=√N also achieves O(N log N) complexity and has theoretical and practical advantages for large N as mentioned above.

==
Data reordering, bit reversal, and in-place algorithms ==
Although the abstract Cooley–Tukey factorization of the DFT, above, applies in some form to all implementations of the algorithm, much greater diversity exists in the techniques for ordering and accessing the data at each stage of the FFT.
Of special interest is the problem of devising an in-place algorithm that overwrites its input with its output data using only O(1) auxiliary storage.

The most well-known reordering technique involves explicit bit reversal for in-place radix-2 algorithms.

Bit reversal is the permutation where the data at an index n, written in binary with digits b4b3b2b1b0 (e.g. 5 digits for N=32 inputs), is transferred to the index with reversed digits b0b1b2b3b4 .
Consider the last stage of a radix-2 DIT algorithm like the one presented above, where the output is written in-place over the input: when 
  
    
      
        
          E
          
            k
          
        
      
    
    {\displaystyle E_{k}}
   and 
  
    
      
        
          O
          
            k
          
        
      
    
    {\displaystyle O_{k}}
   are combined with a size-2 DFT, those two values are overwritten by the outputs.

However, the two output values should go in the first and second halves of the output array, corresponding to the most significant bit b4 (for N=32); whereas the two inputs
E
k
          
        
      
    
    {\displaystyle E_{k}}
   and 
  
    
      
        
          O
k
          
        
      
    
    {\displaystyle O_{k}}
   are interleaved in the even and odd elements, corresponding to the least significant bit b0.

Thus, in order to get the output in the correct place, b0 should take the place of b4 and the index becomes b0b4b3b2b1.
And for next recursive stage, those 4 least significant bits will become b1b4b3b2, If you include all of the recursive stages of a radix-2 DIT algorithm, all the bits must be reversed and thus one must pre-process the input (or post-process the output) with a bit reversal to get in-order output.
(
If each size-N/2 subtransform is to operate on contiguous data, the DIT input is pre-processed by bit-reversal.)
Correspondingly, if you perform all of the steps in reverse order, you obtain a radix-2 DIF algorithm with bit reversal in post-processing (or pre-processing, respectively).

The logarithm (log) used in this algorithm is a base 2 logarithm.

The following is pseudocode for iterative radix-2 FFT algorithm implemented using bit-reversal permutation.

algorithm
iterative-fft is
    input:
Array a of n complex values where n is a power of 2.

output:
Array A the DFT of
a.
bit-reverse-
copy(a, A)
    n
← a.length 
    for s = 1 to log(n)
do
        m ← 2s
        ωm
← exp(−2πi/m)
for k = 0 to n-1 by
m do
            ω ← 1
            for j = 0 to
m/2 – 1 do
t ← ω A[k
+
j
+
m/2]
u ←
A[k
+
j]
A[k
+ j]
←
u
+ t
A[k
+ j +
m/2]
← u –
t
                ω ← ω
ωm
   
    return A
The bit-reverse-copy procedure can be implemented as follows.

algorithm bit-reverse-
copy(a,A) is
    input:
Array a of n complex values where n is a power of 2.

output:
Array A of size n.

    n
← a.length
    for k = 0 to n – 1 do
        A[rev(k)]
:= a[k]

Alternatively, some applications (such as convolution) work equally well on bit-reversed data, so one can perform forward transforms, processing, and then inverse transforms all without bit reversal to produce final results in the natural order.

Many FFT users, however, prefer natural-order outputs, and a separate, explicit bit-reversal stage can have a non-negligible impact on the computation time, even though bit reversal can be done in O(N)
time and has been the subject of much research.
Also, while the permutation is a bit reversal in the radix-2 case, it is more generally an arbitrary (mixed-base) digit reversal for the mixed-radix case, and the permutation algorithms become more complicated to implement.
Moreover, it is desirable on many hardware architectures to re-order intermediate stages of the FFT algorithm so that they operate on consecutive (or at least more localized) data elements.
To these ends, a number of alternative implementation schemes have been devised for the Cooley–Tukey algorithm that do not require separate bit reversal and/or involve additional permutations at intermediate stages.

The problem is greatly simplified if it is out-of-place: the output array is distinct from the input array or, equivalently, an equal-size auxiliary array is available.

The Stockham auto-sort algorithm performs every stage of the FFT out-of-place, typically writing back and forth between two arrays, transposing one "digit" of the indices with each stage, and has been especially popular on SIMD architectures.

Even greater potential SIMD advantages (more consecutive accesses) have been proposed for the Pease algorithm, which also reorders out-of-place with each stage, but this method requires separate bit/digit reversal and O(N log N) storage.

One can also directly apply the Cooley–Tukey factorization definition with explicit (depth-first) recursion and small radices, which produces natural-order out-of-place output with no separate permutation step (as in the pseudocode above) and can be argued to have cache-oblivious locality benefits on systems with hierarchical memory.
A typical strategy for in-place algorithms without auxiliary storage and without separate digit-reversal passes involves small matrix transpositions (which swap individual pairs of digits) at intermediate stages, which can be combined with the radix butterflies to reduce the number of passes over the data.

==
References ==


==
External links ==
"Fast Fourier transform - FFT".
Cooley-Tukey technique.
Article.
10.
A simple, pedagogical radix-2 algorithm in C++
"KISSFFT".
GitHub.
A simple mixed-radix Cooley–Tukey implementation in C
Dsplib on GitHub
"
Radix-2 Decimation in Time FFT Algorithm".
Archived from the original on October 31, 2017. "
Алгоритм БПФ по основанию два с прореживанием по времени" (in Russian).

"
Radix-2 Decimation in Frequency FFT Algorithm".
Archived from the original on November 14, 2017. "
Алгоритм БПФ по основанию два с прореживанием по частоте" (in Russian).
The bias blind spot is the cognitive bias of recognizing the impact of biases on the judgment of others, while failing to see the impact of biases on one's own judgment.
The term was created by Emily Pronin, a social psychologist from Princeton University's Department of Psychology, with colleagues Daniel Lin and Lee Ross.
The bias blind spot is named after the visual blind spot.
Most people appear to exhibit the bias blind spot.
In a sample of more than 600 residents of the United States, more than 85% believed they were less biased than the average American.
Only one participant believed that he or she was more biased than the average American.
People do vary with regard to the extent to which they exhibit the bias blind spot.
It appears to be a stable individual difference that is measurable (for a scale, see Scopelliti et al.
2015).The bias blind spot appears to be a true blind spot in that it is unrelated to actual decision making ability.
Performance on indices of decision making competence are not related to individual differences in bias blind spot.
In other words, most people appear to believe that they are less biased than others, regardless of their actual decision making ability.

== Causes ==
Bias blind spots may be caused by a variety of other biases and self-deceptions.
Self-enhancement biases may play a role, in that people are motivated to view themselves in a positive light.
Biases are generally seen as undesirable, so people tend to think of their own perceptions and judgments as being rational, accurate, and free of bias.
The self-enhancement bias also applies when analyzing our own decisions, in that people are likely to think of themselves as better decision-makers than others.
People also tend to believe they are aware of "how" and "why" they make their decisions, and therefore conclude that bias did not play a role.
Many of our decisions are formed from biases and cognitive shortcuts, which are unconscious processes.
By definition, people are unaware of unconscious processes, and therefore cannot see their influence in the decision making process.
When made aware of various biases acting on our perception, decisions, or judgments, research has shown that we are still unable to control them.
This contributes to the bias blind spot in that even if one is told that they are biased, they are unable to alter their biased perception.

==
Role of introspection ==
Emily Pronin and Matthew Kugler have argued that this phenomenon is due to the introspection illusion.
In their experiments, subjects had to make judgments about themselves and about other subjects.
They displayed standard biases, for example rating themselves above the others on desirable qualities (demonstrating illusory superiority).
The experimenters explained cognitive bias, and asked the subjects how it might have affected their judgment.
The subjects rated themselves as less susceptible to bias than others in the experiment (confirming the bias blind spot).
When they had to explain their judgments, they used different strategies for assessing their own and others' bias.

Pronin and Kugler's interpretation is that, when people decide whether someone else is biased, they use overt behaviour.
On the other hand, when assessing whether they themselves are biased, people look inward, searching their own thoughts and feelings for biased motives.
Since biases operate unconsciously, these introspections are not informative, but people wrongly treat them as reliable indication that they themselves, unlike other people, are immune to bias.
Pronin and Kugler tried to give their subjects access to others' introspections.
To do this, they made audio recordings of subjects who had been told to say whatever came into their heads as they decided whether their answer to a previous question might have been affected by bias.
Although subjects persuaded themselves they were unlikely to be biased, their introspective reports did not sway the assessments of observers.

==
Differences of perceptions ==
People tend to attribute bias in an uneven way.
When people reach different perceptions, they tend to label one another as biased while labelling themselves as accurate and unbiased.
Pronin hypothesizes that this bias misattribution may be a source of conflict and misunderstanding between people.
For example, in labeling another person as biased, one may also label their intentions cynically.
But when examining one's own cognitions, people judge themselves based on their good intentions.
It is likely that in this case, one may attribute another's bias to "intentional malice" rather than an unconscious process.
Pronin also hypothesizes ways to use awareness of the bias blind spot to reduce conflict, and to think in a more "scientifically informed" way.
Although we are unable to control bias on our own cognitions, one may keep in mind that biases are acting on everyone.
Pronin suggests that people might use this knowledge to separate other's intentions from their actions.

==
Relation to actual commission of bias ==
Initial evidence suggests that the bias blind spot is not related to actual decision-making ability.
Participants who scored better or poorer on various tasks associated with decision making competence were no more or less likely to be higher or lower in their susceptibility to bias blind spot.
Bias blind spot does, however, appear to increase susceptibility to related biases.
People who are high in bias blind spot are more likely to ignore the advice of other people, and are less likely to benefit from training geared to reduce their commission of other biases.

== See also ==
Blindspots analysis
List of cognitive biases
Naïve cynicism
Selective exposure theory


=
= References ==
A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory.
A cache is a smaller, faster memory, located closer to a processor core, which stores copies of the data from frequently used main memory locations.

Most CPUs have a hierarchy of multiple cache levels (L1, L2, often L3, and rarely even L4), with separate instruction-specific and data-specific caches at level 1.

Other types of caches exist (that are not counted towards the "cache size" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) which is part of the memory management unit (MMU) which most CPUs have.

==
Overview
==
When trying to read from or write to a location in the main memory, the processor checks whether the data from that location is already in the cache.
If so, the processor will read from or write to the cache instead of the much slower main memory.

Most modern desktop and server CPUs have at least three independent caches: an instruction cache to speed up executable instruction fetch, a data cache to speed up data fetch and store, and a translation lookaside buffer (TLB) used to speed up virtual-to-physical address translation for both executable instructions and data.

A single TLB can be provided for access to both instructions and data, or a separate Instruction TLB (ITLB) and data TLB (DTLB) can be provided.
The data cache is usually organized as a hierarchy of more cache levels (L1, L2, etc.;
see also multi-level caches below).
However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches.

===
History ===
The first CPUs that used a cache had only one level of cache; unlike later level 1 cache, it was not split into L1d (for data) and L1i (for instructions).
Split L1 cache started in 1976 with the IBM 801 CPU, achieved mainstream in 1993 with the Intel Pentium and in 1997 the embedded CPU market with the ARMv5TE.
In 2015, even sub-dollar SoC split the L1 cache.
They also have L2 caches and, for larger processors, L3 caches as well.
The L2 cache is usually not split and acts as a common repository for the already split L1 cache.
Every core of a multi-core processor has a dedicated L1 cache and is usually not shared between the cores.
The L2 cache, and higher-level caches, may be shared between the cores.
L4 cache is currently uncommon, and is generally on (a form of) dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip (exceptionally, the form, eDRAM is used for all levels of cache, down to L1).
That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level.
Each extra level of cache tends to be bigger and optimized differently.

Caches (like for RAM historically) have generally been sized in powers of: 2, 4, 8, 16 etc.
KiB; when up to MiB sizes (i.e. for larger non-L1), very early on the pattern broke down, to allow for larger caches without being forced into the doubling-in-size paradigm, with e.g. Intel Core 2 Duo with 3 MiB L2 cache in April 2008.
Much later however for L1 sizes, that still only count in small number of KiB, however IBM zEC12 from 2012 is an exception, to gain unusually large 96 KiB L1 data cache for its time, and e.g. the IBM z13 having a 96 KiB L1 instruction cache (and 128 KiB L1 data cache), and Intel Ice Lake-based processors from 2018, having 48 KiB L1 data cache and 48 KiB L1 instruction cache.
In 2020, some Intel Atom CPUs (with up to 24 cores) have (multiple of)
4.5 MiB and 15 MiB cache sizes.
===
Cache entries ===
Data is transferred between memory and cache in blocks of fixed size, called cache lines or cache blocks.
When a cache line is copied from memory into the cache, a cache entry is created.
The cache entry will include the copied data as well as the requested memory location (called a tag).

When the processor needs to read or write a location in memory, it first checks for a corresponding entry in the cache.
The cache checks for the contents of the requested memory location in any cache lines that might contain that address.
If the processor finds that the memory location is in the cache, a cache hit has occurred.
However, if the processor does not find the memory location in the cache, a cache miss has occurred.
In the case of a cache hit, the processor immediately reads or writes the data in the cache line.

For a cache miss, the cache allocates a new entry and copies data from main memory, then the request is fulfilled from the contents of the cache.
===
Policies ===


====
Replacement policies ====
To make room for the new entry on a cache miss, the cache may have to evict one of the existing entries.
The heuristic it uses to choose the entry to evict is called the replacement policy.
The fundamental problem with any replacement policy is that it must predict which existing cache entry is least likely to be used in the future.
Predicting the future is difficult, so there is no perfect method to choose among the variety of replacement policies available.
One popular replacement policy, least-recently used (LRU), replaces the least recently accessed entry.

Marking some memory ranges as non-cacheable can improve performance, by avoiding caching of memory regions that are rarely re-accessed.
This avoids the overhead of loading something into the cache without having any reuse.

Cache entries may also be disabled or locked depending on the context.

====
Write policies ====
If data is written to the cache, at some point it must also be written to main memory; the timing of this write is known as the write policy.
In a write-through cache, every write to the cache causes a write to main memory.

Alternatively, in a write-back or copy-back cache, writes are not immediately mirrored to the main memory, and the cache instead tracks which locations have been written over, marking them as dirty.
The data in these locations is written back to the main memory only when that data is evicted from the cache.
For this reason, a read miss in a write-back cache may sometimes require two memory accesses to service: one to first write the dirty location to main memory, and
then another to read the new location from memory.
Also, a write to a main memory location that is not yet mapped in a write-back cache may evict an already dirty location, thereby freeing that cache space for the new memory location.

There are intermediate policies as well.
The cache may be write-through, but the writes may be held in a store data queue temporarily, usually so multiple stores can be processed together (which can reduce bus turnarounds and improve bus utilization).

Cached data from the main memory may be changed by other entities (e.g., peripherals using direct memory access (DMA) or another core in a multi-core processor), in which case the copy in the cache may become out-of-date or stale.
Alternatively, when a CPU in a multiprocessor system updates data in the cache, copies of data in caches associated with other CPUs become stale.
Communication protocols between the cache managers that keep the data consistent are known as cache coherence protocols.

===
Cache performance ===
Cache performance measurement has become important in recent times where the speed gap between the memory performance and the processor performance is increasing exponentially.
The cache was introduced to reduce this speed gap.
Thus knowing how well the cache is able to bridge the gap in the speed of processor and memory becomes important, especially in high-performance systems.
The cache hit rate and the cache miss rate play an important role in determining this performance.
To improve the cache performance, reducing the miss rate becomes one of the necessary steps among other steps.
Decreasing the access time to the cache also gives a boost to its performance.

====
CPU stalls ====
The time taken to fetch one cache line from memory (read latency due to a cache miss) matters because the CPU will run out of things to do while waiting for the cache line.
When a CPU reaches this state, it is called a stall.

As CPUs become faster compared to main memory, stalls due to cache misses displace more potential computation; modern CPUs can execute hundreds of instructions in the time taken to fetch a single cache line from main memory.

Various techniques have been employed to keep the CPU busy during this time, including out-of-order execution in which the CPU attempts to execute independent instructions after the instruction that is waiting for the cache miss data.

Another technology, used by many processors, is simultaneous multithreading (SMT), which allows an alternate thread to use the CPU core while the first thread waits for required CPU resources to become available.

==
Associativity ==
The placement policy decides where in the cache a copy of a particular entry of main memory will go.
If the placement policy is free to choose any entry in the cache to hold the copy, the cache is called fully associative.
At the other extreme, if each entry in main memory can go in just one place in the cache, the cache is direct mapped.
Many caches implement a compromise in which each entry in main memory can go to any one of N places in the cache, and are described as N-way set associative.
For example, the level-1 data cache in an AMD Athlon is two-way set associative, which means that any particular location in main memory can be cached in either of two locations in the level-1 data cache.

Choosing the right value of associativity involves a trade-off.
If there are ten places to which the placement policy could have mapped a memory location, then to check if that location is in the cache, ten cache entries must be searched.
Checking more places takes more power and chip area, and potentially more time.
On the other hand, caches with more associativity suffer fewer misses (see conflict misses, below), so that the CPU wastes less time reading from the slow main memory.
The general guideline is that doubling the associativity, from direct mapped to two-way, or from two-way to four-way, has about the same effect on raising the hit rate as doubling the cache size.
However, increasing associativity more than four does not improve hit rate as much, and are generally done for other reasons (see virtual aliasing, below).

Some CPUs can dynamically reduce the associativity of their caches in low-power states, which acts as a power-saving measure.
In order of worse but simple to better but complex:
Direct mapped cache –  good best-case time, but unpredictable in worst case
Two-way set associative cache
Two-way skewed associative cache
Four-way set associative cache
Eight-way set associative cache, a common choice for later implementations
12-way set associative cache, similar to eight-way
Fully associative cache –  the best miss rates, but practical only for a small number of entries


===
Direct-mapped cache ===
In this cache organization, each location in main memory can go in only one entry in the cache.

Therefore, a direct-mapped cache can also be called a "one-way set associative" cache.

It does not have a placement policy as such, since there is no choice of which cache entry's contents to evict.

This means that if two locations map to the same entry, they may continually knock each other out.

Although simpler, a direct-mapped cache needs to be much larger than an associative one to give comparable performance, and it is more unpredictable.

Let x be block number in cache
, y be block number of memory, and
n be number of blocks in cache, then mapping is done with the help of the equation x = y mod n.


===
Two-way set associative cache ===
If each location in main memory can be cached in either of two locations in the cache, one logical question is: which one of the two?
The simplest and most commonly used scheme, shown in the right-hand diagram above, is to use the least significant bits of the memory location's index as the index for the cache memory, and to have two entries for each index.
One benefit of this scheme is that the tags stored in the cache do not have to include that part of the main memory address which is implied by the cache memory's index.
Since the cache tags have fewer bits, they require fewer transistors, take less space on the processor circuit board or on the microprocessor chip, and can be read and compared faster.
Also LRU is especially simple since only one bit needs to be stored for each pair.

===
Speculative execution ===
One of the advantages of a direct mapped cache is that it allows simple and fast speculation.
Once the address has been computed, the one cache index which might have a copy of that location in memory is known.
That cache entry can be read, and the processor can continue to work with that data before it finishes checking that the tag actually matches the requested address.

The idea of having the processor use the cached data before the tag match completes can be applied to associative caches as well.
A subset of the tag, called a hint, can be used to pick just one of the possible cache entries mapping to the requested address.
The entry selected by the hint can then be used in parallel with checking the full tag.
The hint technique works best when used in the context of address translation, as explained below.
===
Two-way skewed associative cache ===
Other schemes have been suggested, such as the skewed cache, where the index for way 0 is direct, as above, but the index for way 1 is formed with a hash function.
A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict
misses due to a pathological access pattern.
The downside is extra latency from computing the hash function.
Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; LRU tracking for non-skewed caches is usually done on a per-set basis.
Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.

=== Pseudo-associative cache ===
A true set-associative cache tests
all the possible ways simultaneously, using something like a content-addressable memory.
A pseudo-associative cache tests each possible way one at a time.
A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.

In the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache, but it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache.
==
Cache entry structure ==
Cache row entries usually have the following structure:
The data block (cache line) contains the actual data fetched from the main memory.

The tag contains (part of)
the address of the actual data fetched from the main memory.

The flag bits are discussed below.

The "size" of the cache is the amount of main memory data it can hold.

This size can be calculated as the number of bytes stored in each data block times the number of blocks stored in the cache.

(The tag, flag and error correction code bits are not included in the size, although they do affect the physical area of a cache.)

An effective memory address which goes along with the cache line (memory block) is split (MSB to LSB) into the tag, the index and the block offset.

The index describes which cache set that the data has been put in.
The index length is 
  
    
      
        ⌈
        
          log
          
            2
⁡
(
        s
        )
⌉
      
    
    {\displaystyle
\lceil \log _
{2}(s)\rceil }
   bits for s cache sets.

The block offset specifies the desired data within the stored data block within the cache row.
Typically the effective address is in bytes, so the block offset length is 
  
    
      
        ⌈
        
          log
          
            2
⁡
(
        b
        )
⌉
      
    
    {\displaystyle
\lceil \log _
{2}(b)\rceil }
   bits, where b is the number of bytes per data block.

The tag contains the most significant bits of the address, which are checked against all rows in the current set (the set has been retrieved by index) to see if this set contains the requested address.
If it does, a cache hit occurs.
The tag length in bits is as follows:

tag_length = address_length - index_length - block_offset_lengthSome authors refer to the block offset as simply the "offset" or the "displacement".
===
Example ===
The original Pentium 4 processor had a four-way set associative L1 data cache of 8 KiB in size, with 64-byte cache blocks.
Hence, there are 8 KiB / 64 = 128 cache blocks.
The number of sets is equal to the number of cache blocks divided by the number of ways of associativity, what leads to 128 / 4 = 32 sets, and hence 25 = 32 different indices.
There are 26 = 64 possible offsets.
Since the CPU address is 32 bits wide, this implies 32 - 5 - 6 = 21 bits for the tag field.

The original Pentium 4 processor also had an eight-way set associative L2 integrated cache 256 KiB in size, with 128-byte cache blocks.
This implies 32 - 8 - 7 = 17 bits for the tag field.

===
Flag bits ===
An instruction cache requires only one flag bit per cache row entry: a valid bit.

The valid bit indicates whether or not a cache block has been loaded with valid data.

On power-up, the hardware sets all the valid bits in all the caches to "invalid".

Some systems also set a valid bit to "invalid" at other times, such as when multi-master bus snooping hardware in the cache of one processor hears an address broadcast from some other processor, and realizes that certain data blocks in the local cache are now stale and should be marked invalid.

A data cache typically requires two flag bits per cache line –  a valid bit and a dirty bit.

Having a dirty bit set indicates that the associated cache line has been changed since it was read from main memory ("dirty"), meaning that the processor has written data to that line and the new value has not propagated all the way to main memory.

==
Cache miss ==
A cache miss is a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency.
There are three kinds of cache
misses:
instruction read miss, data read miss, and data write miss.

Cache read misses from an instruction cache
generally cause the largest delay, because the processor, or at least the thread of execution, has to wait (stall) until the instruction is fetched from main memory.

Cache read misses from a data cache
usually cause a smaller delay, because instructions not dependent on the cache read can be issued and continue execution until the data is returned from main memory, and the dependent instructions can resume execution.

Cache write misses to a data cache
generally cause the shortest delay, because the write can be queued and there are few limitations on the execution of subsequent instructions; the processor can continue until the queue is full.
For a detailed introduction to the types of misses, see cache performance measurement and metric.

==
Address translation ==
Most general purpose CPUs implement some form of virtual memory.
To summarize, either each program running on the machine sees its own simplified address space, which contains code and data for that program only, or all programs run in a common virtual address space.
A program executes by calculating, comparing, reading and writing to addresses of its virtual address space, rather than addresses of physical address space, making programs simpler and thus easier to write.

Virtual memory requires the processor to translate virtual addresses generated by the program into physical addresses in main memory.
The portion of the processor that does this translation is known as the memory management unit (MMU).
The fast path through the MMU can perform those translations stored in the translation lookaside buffer (TLB), which is a cache of mappings from the operating system's page table, segment table, or both.

For the purposes of the present discussion, there are three important features of address translation:
Latency: The physical address is available from the MMU some time, perhaps a few cycles, after the virtual address is available from the address generator.

Aliasing
: Multiple virtual addresses can map to a single physical address.
Most processors guarantee that all updates to that single physical address will happen in program order.
To deliver on that guarantee, the processor must ensure that only one copy of a physical address resides in the cache at any given time.

Granularity: The virtual address space is broken up into pages.
For instance, a 4 GiB virtual address space might be cut up into 1,048,576 pages of 4 KiB size, each of which can be independently mapped.
There may be multiple page sizes supported; see virtual memory for elaboration.
Some early virtual memory systems were very slow because they required an access to the page table (held in main memory) before every programmed access to main memory.
With no caches, this effectively cut the speed of memory access in half.
The first hardware cache used in a computer system was not actually a data or instruction cache, but rather a TLB.Caches can be divided into four types, based on whether the index or tag correspond to physical or virtual addresses:
Physically indexed, physically tagged (PIPT) caches use the physical address for both the index and the tag.
While this is simple and avoids problems with aliasing, it is also slow, as the physical address must be looked up (which could involve a TLB miss and access to main memory) before that address can be looked up in the cache.

Virtually indexed, virtually tagged (VIVT) caches use the virtual address for both the index and the tag.
This caching scheme can result in much faster lookups, since the MMU does not need to be consulted first to determine the physical address for a given virtual address.
However, VIVT suffers from aliasing problems, where several different virtual addresses may refer to the same physical address.
The result is that such addresses would be cached separately despite referring to the same memory, causing coherency problems.
Although solutions to this problem exist  they do not work for standard coherence protocols.
Another problem is homonyms, where the same virtual address maps to several different physical addresses.
It is not possible to distinguish these mappings merely by looking at the virtual index itself, though potential solutions include: flushing the cache after a context switch, forcing address spaces to be non-overlapping, tagging the virtual address with an address space ID (ASID).
Additionally, there is a problem that virtual-to-physical mappings can change, which would require flushing cache lines, as the VAs would no longer be valid.

All these issues are absent if tags use physical addresses (VIPT).

Virtually indexed, physically tagged (VIPT) caches use the virtual address for the index and the physical address in the tag.
The advantage over PIPT is lower latency, as the cache line can be looked up in parallel with the TLB translation, however the tag cannot be compared until the physical address is available.
The advantage over VIVT is that since the tag has the physical address, the cache can detect homonyms.

Theoretically, VIPT requires more tags bits because some of the index bits could differ between the virtual and physical addresses (for example bit 12 and above for 4 KiB pages) and would have to be included both in the virtual index and in the physical tag.
In practice this is not an issue because, in order to avoid coherency problems, VIPT caches are designed to have no such index bits (e.g., by limiting the total number of bits for the index and the block offset to 12 for 4 KiB pages); this limits the size of VIPT caches to the page size times the associativity of the cache.

Physically indexed, virtually tagged (PIVT) caches are often claimed in literature to be useless and non-existing.

However, the MIPS
R6000 uses this cache type as the sole known implementation.

The R6000 is implemented in emitter-coupled logic, which is an extremely fast technology not suitable for large memories such as a TLB.

The R6000 solves the issue by putting the TLB memory into a reserved part of the second-level cache having a tiny, high-speed TLB "slice" on chip.

The cache is indexed by the physical address obtained from the TLB slice.

However, since the TLB slice only translates those virtual address bits that are necessary to index the cache and does not use any tags, false cache hits may occur, which is solved by tagging with the virtual address.
The speed of this recurrence (the load latency) is crucial to CPU performance, and so most modern level-1 caches are virtually indexed, which at least allows the MMU's TLB lookup to proceed in parallel with fetching the data from the cache RAM.

But virtual indexing is not the best choice for all cache levels.
The cost of dealing with virtual aliases grows with cache size, and as a result most level-2 and larger caches are physically indexed.

Caches have historically used both virtual and physical addresses for the cache tags, although virtual tagging is now uncommon.
If the TLB lookup can finish before the cache RAM lookup, then the physical address is available in time for tag compare, and there is no need for virtual tagging.
Large caches, then, tend to be physically tagged, and only small, very low latency caches are virtually tagged.
In recent general-purpose CPUs, virtual tagging has been superseded by vhints, as described below.
===
Homonym and synonym problems ===
A cache that relies on virtual indexing and tagging becomes inconsistent after the same virtual address is mapped into different physical addresses (homonym), which can be solved by using physical address for tagging, or by storing the address space identifier in the cache line.
However, the latter approach does not help against the synonym problem, in which several cache lines end up storing data for the same physical address.
Writing to such locations may update only one location in the cache, leaving the others with inconsistent data.
This issue may be solved by using non-overlapping memory layouts for different address spaces, or otherwise the cache (or a part of it) must be flushed when the mapping changes.

===
Virtual tags and vhints ===
The great advantage of virtual tags is that, for associative caches, they allow the tag match to proceed before the virtual to physical translation is done.
However, coherence probes and evictions present a physical address for action.
The hardware must have some means of converting the physical addresses into a cache index, generally by storing physical tags as well as virtual tags.
For comparison, a physically tagged cache does not need to keep virtual tags, which is simpler.
When a virtual to physical mapping is deleted from the TLB, cache entries with those virtual addresses will have to be flushed somehow.
Alternatively, if cache entries are allowed on pages not mapped by the TLB, then those entries will have to be flushed when the access rights on those pages are changed in the page table.

It is also possible for the operating system to ensure that no virtual aliases are simultaneously resident in the cache.
The operating system makes this guarantee by enforcing page coloring, which is described below.
Some early RISC processors (SPARC, RS/6000) took this approach.
It has not been used recently, as the hardware cost of detecting and evicting virtual aliases has fallen and the software complexity and performance penalty of perfect page coloring has risen.

It can be useful to distinguish the two functions of tags in an associative cache: they are used to determine which way of the entry set to select, and they are used to determine if the cache hit or missed.
The second function must always be correct, but it is permissible for the first function to guess, and get the wrong answer occasionally.

Some processors (e.g. early SPARCs) have caches with both virtual and physical tags.
The virtual tags are used for way selection, and the physical tags are used for determining hit or miss.
This kind of cache enjoys the latency advantage of a virtually tagged cache, and the simple software interface of a physically tagged cache.
It bears the added cost of duplicated tags, however.
Also, during miss processing, the alternate ways of the cache line indexed have to be probed for virtual aliases and any matches evicted.

The extra area (and some latency) can be mitigated by keeping virtual hints with each cache entry instead of virtual tags.
These hints are a subset or hash of the virtual tag, and are used for selecting the way of the cache from which to get data and a physical tag.
Like a virtually tagged cache, there may be a virtual hint match but physical tag mismatch, in which case the cache entry with the matching hint must be evicted so that cache accesses after the cache fill at this address will have just one hint match.
Since virtual hints have fewer bits than virtual tags distinguishing them from one another, a virtually hinted cache suffers more conflict misses than a virtually tagged cache.

Perhaps the ultimate reduction of virtual hints can be found in the Pentium 4 (Willamette and Northwood cores).
In these processors the virtual hint is effectively two bits, and the cache is four-way set associative.
Effectively, the hardware maintains a simple permutation from virtual address to cache index, so that no content-addressable memory (CAM) is necessary to select the right one of the four ways fetched.

===
Page coloring ===
Large physically indexed caches (usually secondary caches) run into a problem:
the operating system rather than the application controls which pages collide with one another in the cache.
Differences in page allocation from one program run to the next lead to differences in the cache collision patterns, which can lead to very large differences in program performance.
These differences can make it very difficult to get a consistent and repeatable timing for a benchmark run.

To understand the problem, consider a CPU with a 1 MiB physically indexed direct-mapped level-2 cache and 4 KiB virtual memory pages.
Sequential physical pages map to sequential locations in the cache until after 256 pages the pattern wraps around.
We can label each physical page with a color of 0–255 to denote where in the cache it can go.
Locations within physical pages with different colors cannot conflict in the cache.

Programmers attempting to make maximum use of the cache may arrange their programs' access patterns so that only 1 MiB of data need be cached at any given time, thus avoiding capacity misses.
But they should also ensure that the access patterns do not have conflict misses.
One way to think about this problem is to divide up the virtual pages the program uses and assign them virtual colors in the same way as physical colors were assigned to physical pages before.
Programmers can then arrange the access patterns of their code so that no two pages with the same virtual color are in use at the same time.
There is a wide literature on such optimizations (e.g. loop nest optimization), largely coming from the High Performance Computing (HPC) community.

The snag is that while all the pages in use at any given moment may have different virtual colors, some may have the same physical colors.
In fact, if the operating system assigns physical pages to virtual pages randomly and uniformly, it is extremely likely that some pages will have the same physical color, and then locations from those pages will collide in the cache (this is the birthday paradox).

The solution is to have the operating system attempt to assign different physical color pages to different virtual colors, a technique called page coloring.
Although the actual mapping from virtual to physical color is irrelevant to system performance, odd mappings are difficult to keep track of and have little benefit, so most approaches to page coloring simply try to keep physical and virtual page colors the same.

If the operating system can guarantee that each physical page maps to only one virtual color, then there are no virtual aliases, and the processor can use virtually indexed caches with no need for extra virtual alias probes during miss handling.
Alternatively, the OS can flush a page from the cache whenever it changes from one virtual color to another.
As mentioned above, this approach was used for some early SPARC and RS/6000 designs.

==
Cache hierarchy in a modern processor ==
Modern processors have multiple interacting on-chip caches.

The operation of a particular cache can be completely specified by the cache size, the cache block size, the number of blocks in a set, the cache set replacement policy, and the cache write policy (write-through or write-back).While all of the cache blocks in a particular cache are the same size and have the same associativity, typically the "lower-level" caches (called Level 1 cache) have a smaller number of blocks, smaller block size, and fewer blocks in a set, but have very short access times.
"
Higher-level" caches (i.e. Level 2 and above) have progressively larger numbers of blocks, larger block size, more blocks in a set, and relatively longer access times, but are still much faster than main memory.

Cache entry replacement policy is determined by a cache algorithm selected to be implemented by the processor designers.

In some cases, multiple algorithms are provided for different kinds of work loads.

===
Specialized caches ===
Pipelined CPUs access memory from multiple points in the pipeline:
instruction fetch, virtual-to-physical address translation, and data fetch (see classic RISC pipeline).
The natural design is to use different physical caches for each of these points, so that no one physical resource has to be scheduled to service two points in the pipeline.
Thus the pipeline naturally ends up with at least three separate caches (instruction, TLB, and data), each specialized to its particular role.

==== Victim cache ====
A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement.
The victim cache lies between the main cache and its refill path, and holds only those blocks of data that were evicted from the main cache.
The victim cache is usually fully associative, and is intended to reduce the number of conflict misses.
Many commonly used programs do not require an associative mapping for all the accesses.
In fact, only a small fraction of the memory accesses of the program require high associativity.
The victim cache exploits this property by providing high associativity to only these accesses.
It was introduced by Norman Jouppi from DEC in 1990.Intel's Crystalwell variant of its Haswell processors introduced an on-package 128 MB eDRAM Level 4 cache which serves as a victim cache to the processors' Level 3 cache.
In the Skylake microarchitecture the Level 4 cache no longer works as a victim cache.
====
Trace cache ====
One of the more extreme examples of cache specialization is the trace cache (also known as execution trace cache) found in the Intel Pentium 4 microprocessors.

A trace cache is a mechanism for increasing the instruction fetch bandwidth and
decreasing power consumption (in the case of the Pentium 4) by storing traces of instructions that have already been fetched and decoded.
A trace cache stores instructions either after they have been decoded, or as they are retired.
Generally, instructions are added to trace caches in groups representing either individual basic blocks or dynamic instruction traces.
The Pentium 4's trace cache stores micro-operations resulting from decoding x86 instructions, providing also the functionality of a micro-operation cache.

Having this, the next time an instruction is needed, it does not have to be decoded into micro-ops again.

====
Write Coalescing Cache (WCC) ====
Write Coalescing Cache is a special cache that is part of L2 cache in AMD's Bulldozer microarchitecture.
Stores from both L1D caches in the module go through the WCC, where they are buffered and coalesced.

The WCC's task is reducing number of writes to the L2 cache.
====
Micro-operation (μop or uop) cache ====
A micro-operation cache (μop cache, uop cache or UC) is a specialized cache that stores micro-operations of decoded instructions, as received directly from the instruction decoders or from the instruction cache.

When an instruction needs to be decoded, the μop cache is checked for its decoded form which is re-used if cached; if it is not available, the instruction is decoded and then cached.

One of the early works describing μop cache as an alternative frontend for the Intel P6 processor family is the 2001 paper "Micro-Operation Cache: A Power Aware Frontend for Variable Instruction Length ISA".

Later, Intel included μop caches in its Sandy Bridge processors and in successive microarchitectures like Ivy Bridge and Haswell.
AMD implemented a μop cache in their Zen microarchitecture.
Fetching complete pre-decoded instructions eliminates the need to repeatedly decode variable length complex instructions into simpler fixed-length micro-operations, and simplifies the process of predicting, fetching, rotating and aligning fetched instructions.
A μop cache effectively offloads the fetch and decode hardware, thus decreasing power consumption and improving the frontend supply of decoded micro-operations.

The μop cache also increases performance by more consistently delivering decoded micro-operations to the backend and eliminating various bottlenecks in the CPU's fetch and decode logic.
A μop cache has many similarities with a trace cache, although a μop cache is much simpler thus providing better power efficiency; this makes it better suited for implementations on battery-powered devices.
The main disadvantage of the trace cache, leading to its power inefficiency, is the hardware complexity required for its heuristic deciding on caching and reusing dynamically created instruction traces.

====
Branch target instruction cache ====
A branch target cache or branch target instruction cache, the name used on ARM microprocessors, is a specialized cache which holds the first few instructions at the destination of a taken branch.

This is used by low-powered processors which do not need a normal instruction cache because the memory system is capable of delivering instructions fast enough to satisfy the CPU without one.

However, this only applies to consecutive instructions in sequence; it still takes several cycles of latency to restart instruction fetch at a new address, causing a few cycles of pipeline bubble after a control transfer.

A branch target cache provides instructions for those few cycles avoiding a delay after most taken branches.

This allows full-speed operation with a much smaller cache than a traditional full-time instruction cache.
====
Smart cache ====
Smart cache is a level 2 or level 3 caching method for multiple execution cores, developed by Intel.

Smart Cache shares the actual cache memory between the cores of a multi-core processor.
In comparison to a dedicated per-core cache, the overall cache miss rate decreases when not all cores need equal parts of the cache space.
Consequently, a single core can use the full level 2 or level 3 cache, if the other cores are inactive.
Furthermore, the shared cache makes it faster to share memory among different execution cores.

=== Multi-level caches ===
Another issue is the fundamental tradeoff between cache latency and hit rate.
Larger caches have better hit rates but longer latency.
To address this tradeoff, many computers use multiple levels of cache, with small fast caches backed up by larger, slower caches.

Multi-level caches generally operate by checking the fastest, level 1 (L1) cache first; if it hits, the processor proceeds at high speed.
If that smaller cache misses, the next fastest cache (level 2, L2) is checked, and so on, before accessing external memory.

As the latency difference between main memory and the fastest cache has become larger, some processors have begun to utilize as many as three levels of on-chip cache.

Price-sensitive designs used this to pull the entire cache hierarchy on-chip, but by the 2010s some of the highest-performance designs returned to having large off-chip caches, which is often implemented in eDRAM and mounted on a multi-chip module, as a fourth cache level.
In rare cases, such as in the mainframe CPU IBM z15 (2019), all levels down to L1 are implemented by eDRAM, replacing SRAM entirely (for cache,  SRAM is still used for registers).
The ARM-based Apple M1 has a 192 KB L1 cache for each of the four high-performance cores, an unusually large amount; however the four high-efficiency cores only have 128 KB.

The benefits of L3 and L4 caches depend on the application's access patterns.

Examples of products incorporating L3 and L4 caches include the following:

Alpha 21164 (1995) has 1 to 64 MB off-chip L3 cache.

IBM POWER4 (2001) has off-chip L3 caches of 32 MB per processor, shared among several processors.

Itanium 2 (2003) has a 6 MB unified level 3 (L3) cache on-die; the Itanium 2 (2003)
MX 2 module incorporates two Itanium 2 processors along with a shared 64 MB L4 cache on a multi-chip module that was pin compatible with a Madison processor.

Intel's Xeon MP product codenamed "Tulsa" (2006) features 16 MB of on-die L3 cache shared between two processor cores.

AMD Phenom II (2008) has up to 6 MB on-die unified L3 cache.

Intel Core i7 (2008) has an 8 MB on-die unified L3 cache that is inclusive, shared by all cores.

Intel Haswell CPUs with integrated Intel Iris Pro Graphics have 128 MB of eDRAM acting essentially as an L4 cache.
Finally, at the other end of the memory hierarchy, the CPU register file itself can be considered the smallest, fastest cache in the system, with the special characteristic that it is scheduled in software—typically by a compiler, as it allocates registers to hold values retrieved from main memory for, as an example, loop nest optimization.
However, with register renaming most compiler register assignments are reallocated dynamically by hardware at runtime into a register bank, allowing the CPU to break false data dependencies and thus easing pipeline hazards.

Register files sometimes also have hierarchy:
The Cray-1 (circa 1976) had eight address "A" and eight scalar data "S" registers that were generally usable.
There was also a set of 64 address "B" and 64 scalar data "T" registers that took longer to access, but were faster than main memory.
The "B" and "T" registers were provided because the Cray-1 did not have a data cache.
(
The Cray-1 did, however, have an instruction cache.)

==== Multi-core chips ====
When considering a chip with multiple cores, there is a question of whether the caches should be shared or local to each core.
Implementing shared cache inevitably introduces more wiring and complexity.
But then, having one cache per chip, rather than core, greatly reduces the amount of space needed, and thus one can include a larger cache.

Typically, sharing the L1 cache is undesirable because the resulting increase in latency would make each core run considerably slower than a single-core chip.

However, for the highest-level cache, the last one called before accessing memory, having a global cache is desirable for several reasons, such as allowing a single core to use the whole cache, reducing data redundancy by making it possible for different processes or threads to share cached data, and reducing the complexity of utilized cache coherency protocols.

For example, an eight-core chip with three levels may include an L1 cache for each core, one intermediate L2 cache for each pair of cores, and one L3 cache shared between all cores.

Shared highest-level cache, which is called before accessing memory, is usually referred to as the last level cache (LLC).

Additional techniques are used for increasing the level of parallelism when LLC is shared between multiple cores, including slicing it into multiple pieces which are addressing certain ranges of memory addresses, and can be accessed independently.

====
Separate versus unified ====
In a separate cache structure, instructions and data are cached separately, meaning that a cache line is used to cache either instructions or data, but not both; various benefits have been demonstrated with separate data and instruction translation lookaside buffers.

In a unified structure, this constraint is not present, and cache lines can be used to cache both instructions and data.
====
Exclusive versus inclusive ====
Multi-level caches introduce new design decisions.

For instance, in some processors, all data in the L1 cache must also be somewhere in the L2 cache.

These caches are called strictly inclusive.
Other processors (like the AMD Athlon) have exclusive caches: data is guaranteed to be in at most one of the L1 and L2 caches, never in both.

Still other processors (like the Intel Pentium II, III, and 4) do not require that data in the L1 cache also reside in the L2 cache, although it may often do so.
There is no universally accepted name for this intermediate policy;
two common names are "non-exclusive" and "partially-inclusive".

The advantage of exclusive caches is that they store more data.
This advantage is larger when the exclusive L1 cache is comparable to the L2 cache, and diminishes if the L2 cache is many times larger than the L1 cache.
When the L1 misses and the L2 hits on an access, the hitting cache line in the L2 is exchanged with a line in the L1.
This exchange is quite a bit more work than just copying a line from L2 to L1, which is what an inclusive cache does.
One advantage of strictly inclusive caches is that when external devices or other processors in a multiprocessor system wish to remove a cache line from the processor, they need only have the processor check the L2 cache.
In cache hierarchies which do not enforce inclusion, the L1 cache must be checked as well.
As a drawback, there is a correlation between the associativities of L1 and L2 caches: if the L2 cache does not have at least as many ways as all L1 caches together, the effective associativity of the L1 caches is restricted.
Another disadvantage of inclusive cache is that whenever there is an eviction in L2 cache, the (possibly) corresponding lines in L1 also have to get evicted in order to maintain inclusiveness.
This is quite a bit of work, and would result in a higher L1 miss rate.
Another advantage of inclusive caches is that the larger cache can use larger cache lines, which reduces the size of the secondary cache tags.
(
Exclusive caches require both caches to have the same size cache lines, so that cache lines can be swapped on a L1 miss, L2 hit.)
If the secondary cache is an order of magnitude larger than the primary, and the cache data is an order of magnitude larger than the cache tags, this tag area saved can be comparable to the incremental area needed to store the L1 cache data in the L2.

===
Scratchpad memory ===
Scratchpad memory (SPM), also known as scratchpad, scratchpad RAM or local store in computer terminology, is a high-speed internal memory used for temporary storage of calculations, data, and other work in progress.

===
Example: the K8 ===
To illustrate both specialization and multi-level caching, here is the cache hierarchy of the K8 core in the AMD Athlon 64 CPU.

The K8 has four specialized caches: an instruction cache, an instruction TLB, a data TLB, and a data cache.
Each of these caches is specialized:
The instruction cache keeps copies of 64-byte lines of memory, and fetches 16 bytes each cycle.
Each byte in this cache is stored in ten bits rather than eight, with the extra bits marking the boundaries of instructions
(this is an example of predecoding).
The cache has only parity protection rather than ECC, because parity is smaller and any damaged data can be replaced by fresh data fetched from memory (which always has an up-to-date copy of instructions).

The instruction TLB keeps copies of page table entries (PTEs).
Each cycle's instruction fetch has its virtual address translated through this TLB into a physical address.
Each entry is either four or eight bytes in memory.
Because the K8 has a variable page size, each of the TLBs is split into two sections, one to keep PTEs that map 4 KB pages, and one to keep PTEs that map 4 MB or 2 MB pages.
The split allows the fully associative match circuitry in each section to be simpler.
The operating system maps different sections of the virtual address space with different size PTEs.

The data TLB has two copies which keep identical entries.
The two copies allow two data accesses per cycle to translate virtual addresses to physical addresses.
Like the instruction TLB, this TLB is split into two kinds of entries.

The data cache keeps copies of 64-byte lines of memory.
It is split into 8 banks (each storing 8 KB of data), and can fetch two 8-byte data each cycle so long as those data are in different banks.
There are two copies of the tags, because each 64-byte line is spread among all eight banks.
Each tag copy handles one of the two accesses per cycle.
The K8 also has multiple-level caches.
There are second-level instruction and data TLBs, which store only PTEs mapping 4 KB.
Both instruction and data caches, and the various TLBs, can fill from the large unified L2 cache.
This cache is exclusive to both the L1 instruction and data caches, which means that any 8-byte line can only be in one of the L1 instruction cache, the L1 data cache, or the L2 cache.
It is, however, possible for a line in the data cache to have a PTE which is also in one of the TLBs—the operating system is responsible for keeping the TLBs coherent by flushing portions of them when the page tables in memory are updated.

The K8 also caches information that is never stored in memory—prediction information.
These caches are not shown in the above diagram.
As is usual for this class of CPU, the K8 has fairly complex
branch prediction, with tables that help predict whether branches are taken and other tables which predict the targets of branches and jumps.
Some of this information is associated with instructions, in both the level 1 instruction cache and the unified secondary cache.

The K8 uses an interesting trick to store prediction information with instructions in the secondary cache.
Lines in the secondary cache are protected from accidental data corruption (e.g. by an alpha particle strike) by either ECC or parity, depending on whether those lines were evicted from the data or instruction primary caches.
Since the parity code takes fewer bits than the ECC code, lines from the instruction cache have a few spare bits.
These bits are used to cache branch prediction information associated with those instructions.
The net result is that the branch predictor has a larger effective history table, and so has better accuracy.

===
More hierarchies ===
Other processors have other kinds of predictors (e.g., the store-to-load bypass predictor in the DEC Alpha 21264), and various specialized predictors are likely to flourish in future processors.

These predictors are caches in that they store information that is costly to compute.
Some of the terminology used when discussing predictors is the same as that for caches (one speaks of a hit in a branch predictor), but predictors are not generally thought of as part of the cache hierarchy.

The K8 keeps the instruction and data caches coherent in hardware, which means that a store into an instruction closely following the store instruction will change that following instruction.
Other processors, like those in the Alpha and MIPS family, have relied on software to keep the instruction cache coherent.
Stores are not guaranteed to show up in the instruction stream until a program calls an operating system facility to ensure coherency.

===
Tag RAM ===
In computer engineering, a tag RAM is used to specify which of the possible memory locations is currently stored in a CPU cache.

For a simple, direct-mapped design fast SRAM can be used.
Higher associative caches usually employ content-addressable memory.
==
Implementation ==
Cache reads are the most common CPU operation that takes more than a single cycle.
Program execution time tends to be very sensitive to the latency of a level-1 data cache hit.
A great deal of design effort, and often power and silicon area are expended making the caches as fast as possible.

The simplest cache is a virtually indexed direct-mapped cache.
The virtual address is calculated with an adder, the relevant portion of the address extracted and used to index an SRAM, which returns the loaded data.
The data is byte aligned in a byte shifter, and from there is bypassed to the next operation.
There is no need for any tag checking in the inner loop –  in fact, the tags need not even be read.
Later in the pipeline, but before the load instruction is retired, the tag for the loaded data must be read, and checked against the virtual address to make sure there was a cache hit.
On a miss, the cache is updated with the requested cache line and the pipeline is restarted.

An associative cache is more complicated, because some form of tag must be read to determine which entry of the cache to select.
An N-way set-associative level-1 cache usually reads all N possible tags and N data in parallel, and then chooses the data associated with the matching tag.
Level-2
caches sometimes save power by reading the tags first, so that only one data element is read from the data SRAM.

The adjacent diagram is intended to clarify the manner in which the various fields of the address are used.
Address bit 31 is most significant, bit 0 is least significant.
The diagram shows the SRAMs, indexing, and multiplexing for a 4 KB, 2-way set-associative, virtually indexed and virtually tagged cache with 64 byte (B) lines, a 32-bit read width and 32-bit virtual address.

Because the cache is 4 KB and has 64 B lines, there are just 64 lines in the cache, and we read two at a time from a Tag SRAM which has 32 rows, each with a pair of 21 bit tags.
Although any function of virtual address bits 31 through 6 could be used to index the tag and data SRAMs, it is simplest to use the least significant bits.

Similarly, because the cache is 4 KB and has a 4 B read path, and reads two ways for each access, the Data SRAM is 512 rows by 8 bytes wide.

A more modern cache might be 16 KB, 4-way
set-associative, virtually indexed, virtually hinted, and physically tagged, with 32 B lines, 32-bit read width and 36-bit physical addresses.
The read path recurrence for such a cache looks very similar to the path above.
Instead of tags, vhints are read, and matched against a subset of the virtual address.
Later on in the pipeline, the virtual address is translated into a physical address by the TLB, and the physical tag is read (just one, as the vhint supplies which way of the cache to read).
Finally the physical address is compared to the physical tag to determine if a hit has occurred.

Some SPARC designs have improved the speed of their L1 caches by a few gate delays by collapsing the virtual address adder into the SRAM decoders.
See Sum addressed decoder.
===
History ===
The early history of cache technology is closely tied to the invention and use of virtual memory.

Because of scarcity and cost of semi-conductor memories, early mainframe computers in the 1960s used a complex hierarchy of physical memory, mapped onto a flat virtual memory space used by programs.
The memory technologies would span semi-conductor, magnetic core, drum and disc.
Virtual memory seen and used by programs would be flat and caching would be used to fetch data and instructions into the fastest memory ahead of processor access.
Extensive studies were done to optimize the cache sizes.
Optimal values were found to depend greatly on the programming language used with Algol needing the smallest and Fortran and Cobol needing the largest cache sizes.
In the early days of microcomputer technology, memory access was only slightly slower than register access.
But since the 1980s the performance gap between processor and memory has been growing.
Microprocessors have advanced much faster than memory, especially in terms of their operating frequency, so memory became a performance bottleneck.
While it was technically possible to have all the main memory as fast as the CPU, a more economically viable path has been taken: use plenty of low-speed memory, but also introduce a small high-speed cache memory to alleviate the performance gap.
This provided an order of magnitude more capacity—for the same price—with only a slightly reduced combined performance.

==== First TLB implementations ====
The first documented uses of a TLB were on the GE 645 and the IBM 360/67, both of which used an associative memory as a TLB.

====
First instruction cache ====
The first documented use of an instruction cache was on the CDC 6600.

====
First data cache ====
The first documented use of a data cache was on the IBM System/360 Model 85.

====
In 68k microprocessors ====
The 68010, released in 1982, has a "loop mode" which can be considered a tiny and special-case instruction cache that accelerates loops that consist of only two instructions.
The 68020, released in 1984, replaced that with a typical instruction cache of 256 bytes, being the first 68k series processor to feature true on-chip cache memory.

The 68030, released in 1987, is basically a 68020 core with an additional 256-byte data cache, an on-chip memory management unit (MMU), a process shrink, and added burst mode for the caches.
The 68040, released in 1990, has split instruction and data caches of four kilobytes each.
The 68060, released in 1994, has the following: 8 KB data cache (four-way associative), 8 KB instruction cache (four-way associative), 96-byte FIFO instruction buffer, 256-entry branch cache, and 64-entry address translation cache MMU buffer (four-way associative).

====
In x86 microprocessors ====
As the x86 microprocessors reached clock rates of 20 MHz and above in the 386, small amounts of fast cache memory began to be featured in systems to improve performance.
This was because the DRAM used for main memory had significant latency, up to 120 ns, as well as refresh cycles.
The cache was constructed from more expensive, but significantly faster, SRAM memory cells, which at the time had latencies around 10–25 ns.
The early caches were external to the processor and typically located on the motherboard in the form of eight or nine DIP devices placed in sockets to enable the cache as an optional extra or upgrade feature.

Some versions of the Intel 386 processor could support 16 to 256 KB of external cache.

With the 486 processor, an 8 KB cache was integrated directly into the CPU die.
This cache was termed Level 1 or L1 cache to differentiate it from the slower on-motherboard, or Level 2 (L2) cache.
These on-motherboard caches were much larger, with the most common size being 256 KB.
The popularity of on-motherboard cache continued through the Pentium MMX era but was made obsolete by the introduction of SDRAM and the growing disparity between bus clock rates and CPU clock rates, which caused on-motherboard cache to be only slightly faster than main memory.

The next development in cache implementation in the x86 microprocessors began with the Pentium Pro, which brought the secondary cache onto the same package as the microprocessor, clocked at the same frequency as the microprocessor.

On-motherboard caches enjoyed prolonged popularity thanks to the AMD K6-2 and AMD K6-III processors that still used Socket 7, which was previously used by Intel with on-motherboard caches.
K6-III
included 256 KB on-die L2 cache and took advantage of the on-board cache as a third level cache, named L3 (motherboards with up to 2 MB of on-board cache were produced).
After the Socket 7 became obsolete, on-motherboard cache disappeared from the x86 systems.

The three-level caches were used again first with the introduction of multiple processor cores, where the L3 cache was added to the CPU die.

It became common for the total cache sizes to be increasingly larger in newer processor generations, and recently (as of 2011)
it is not uncommon to find Level 3 cache sizes of tens of megabytes.
Intel introduced a Level 4 on-package cache with the Haswell microarchitecture.
Crystalwell Haswell CPUs, equipped with the GT3e variant of Intel's integrated Iris Pro graphics, effectively feature 128 MB of embedded DRAM (eDRAM) on the same package.

This L4 cache is shared dynamically between the on-die GPU and CPU, and serves as a victim cache to the CPU's L3 cache.
====
In ARM microprocessors ====
Apple M1 CPU has 128 or 192 KB instruction L1 cache for each core (important for latency/single-thread performance), depending on core type, unusually large for L1 cache of any CPU type, not just for a laptop, while the total cache memory size is not unusually large
(the total is more important for throughput), for a laptop, and much larger total (e.g. L3 or L4) sizes are available in IBM's mainframes.

====
Current research ====
Early cache designs focused entirely on the direct cost of cache and RAM and average execution speed.

More recent cache designs also consider energy efficiency, fault tolerance, and other goals.
Researchers have also explored use of emerging memory technologies such as eDRAM (embedded DRAM) and NVRAM (non-volatile RAM) for designing caches.
There are several tools available to computer architects to help explore tradeoffs between the cache cycle time, energy, and area; the CACTI cache simulator and the SimpleScalar instruction set simulator are two open-source options.
Modeling of 2D and 3D SRAM, eDRAM, STT-RAM, ReRAM and PCM caches can be done using the DESTINY tool.

===
Multi-ported cache ===
A multi-ported cache is a cache which can serve more than one request at a time.
When accessing a traditional cache we normally use a single memory address, whereas in a multi-ported cache we may request N addresses at a time –  where N is the number of ports that connected through the processor and the cache.
The benefit of this is that a pipelined processor may access memory from different phases in its pipeline.
Another benefit is that it allows the concept of super-scalar processors through different cache levels.

== See also ==


== Notes ==


==
References ==


==
External links ==
Memory part 2
: CPU caches –  an article on lwn.net by Ulrich Drepper describing CPU caches in detail
Evaluating Associativity in CPU Caches – Hill and Smith (1989) – introduces capacity, conflict, and compulsory classification
Cache Performance for SPEC
CPU2000 Benchmarks – Hill and Cantin (2003) –
This reference paper has been updated several times.
It has thorough and lucidly presented simulation results for a reasonably wide set of benchmarks and cache organizations.

Memory Hierarchy in Cache-Based Systems – by Ruud van der Pas, 2002, Sun Microsystems – a nice introductory article to CPU memory caching
A Cache Primer – by Paul Genua, P.E., 2004, Freescale Semiconductor, another introductory article
An 8-way set-associative cache –  written in VHDL
Understanding CPU caching and performance –  an article on Ars Technica by Jon Stokes
IBM POWER4 processor review –  an article on ixbtlabs by Pavel Danilov
What is Cache Memory and its Types!

Memory Caching –  a Princeton University lecture
In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or at the same time simultaneously partial order, without affecting the final outcome.

This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems.
In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.
According to Rob Pike, concurrency is the composition of independently executing computations, and concurrency is not parallelism: concurrency is about dealing with lots of things at once
but parallelism is about doing lots of things at once.
Concurrency is about structure, parallelism is about execution, concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.
A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi, the parallel random-access machine model, the actor model and the Reo Coordination Language.

==
History ==
As Leslie Lamport (2015) notes, "While concurrent program execution had been considered for years, the computer science of concurrency began with Edsger Dijkstra's seminal 1965 paper that introduced the mutual exclusion problem.
...
The ensuing decades have seen a huge growth of interest in concurrency—particularly in distributed systems.
Looking back at the origins of the field, what stands out is the fundamental role played by Edsger Dijkstra".

== Issues ==
Because computations in a concurrent system can interact with each other while being executed, the number of possible execution paths in the system can be extremely large, and the resulting outcome can be indeterminate.
Concurrent use of shared resources can be a source of indeterminacy leading to issues such as deadlocks, and resource starvation.
Design of concurrent systems often entails finding reliable techniques for coordinating their execution, data exchange, memory allocation, and execution scheduling to minimize response time and maximise throughput.

==
Theory ==
Concurrency theory has been an active field of research in theoretical computer science.

One of the first proposals was  Carl Adam Petri's seminal work on Petri nets in the early 1960s.
In the years since, a wide variety of formalisms have been developed for modeling and reasoning about concurrency.

===
Models ===
A number of formalisms for modeling and understanding concurrent systems have been developed, including:
The parallel random-access machine
The actor model
Computational bridging models such as the bulk synchronous parallel (BSP) model
Petri nets
Process calculi
Calculus of communicating systems (CCS)
Communicating sequential processes (CSP) model
π-calculus
Tuple spaces, e.g., Linda
Simple Concurrent Object-Oriented Programming (SCOOP)
Reo Coordination LanguageSome of these models of concurrency are primarily intended to support reasoning and specification, while others can be used through the entire development cycle, including design, implementation, proof, testing and simulation of concurrent systems.
Some of these are based on message passing, while others have different mechanisms for concurrency.

The proliferation of different models of concurrency has motivated some researchers to develop ways to unify these different theoretical models.
For example, Lee and Sangiovanni-Vincentelli have demonstrated that a so-called "tagged-signal" model can be used to provide a common framework for defining the denotational semantics of a variety of different models of concurrency, while Nielsen, Sassone, and Winskel have demonstrated that category theory can be used to provide a similar unified understanding of different models.
The Concurrency Representation Theorem in the actor model provides a fairly general way to represent concurrent systems that are closed in the sense that they do not receive communications from outside.
(
Other concurrency systems, e.g., process calculi can be modeled in the actor model using a two-phase commit protocol.)
The mathematical denotation denoted by a closed system S is constructed increasingly better approximations from an initial behavior called ⊥S using a behavior approximating function progressionS to construct a denotation (meaning ) for S as follows:
DenoteS ≡ ⊔i∈ω progressionSi(⊥S)In
this way, S can be mathematically characterized in terms of all its possible behaviors.

===
Logics ===
Various types of temporal logic can be used to help reason about concurrent systems.
Some of these logics, such as linear temporal logic and computation tree logic, allow assertions to be made about the sequences of states that a concurrent system can pass through.
Others, such as action computational tree logic, Hennessy–Milner logic, and Lamport's temporal logic of actions, build their assertions from sequences of actions (changes in state).
The principal application of these logics is in writing specifications for concurrent systems.

==
Practice ==
Concurrent programming encompasses programming languages and algorithms used to implement concurrent systems.

Concurrent programming is usually considered to be more general than parallel programming because it can involve arbitrary and dynamic patterns of communication and interaction, whereas parallel systems generally have a predefined and well-structured communications pattern.
The base goals of concurrent programming include correctness, performance and robustness.
Concurrent systems such as Operating systems and Database management systems are generally designed to operate indefinitely, including automatic recovery from failure, and not terminate unexpectedly (see Concurrency control).
Some concurrent systems implement a form of transparent concurrency, in which concurrent computational entities may compete for and share a single resource, but the complexities of this competition and sharing are shielded from the programmer.

Because they use shared resources, concurrent systems in general require the inclusion of some kind of arbiter somewhere in their implementation (often in the underlying hardware), to control access to those resources.
The use of arbiters introduces the possibility of indeterminacy in concurrent computation which has major implications for practice including correctness and performance.

For example, arbitration introduces unbounded nondeterminism which raises issues with model checking because it causes explosion in the state space and can even cause models to have an infinite number of states.

Some concurrent programming models include coprocesses and deterministic concurrency.
In these models, threads of control explicitly yield their timeslices, either to the system or to another process.

== See also ==
Chu space
Client–server network nodes
Clojure
Cluster nodes
Concurrency control
Concurrent computing
Concurrent object-oriented programming
Concurrency pattern
Construction and Analysis of Distributed Processes (CADP)
D (programming language)
Distributed systemnodes
Elixir (programming language)
Erlang (programming language)
Go (programming language)
Gordon Pask
International Conference on Concurrency Theory (CONCUR)
OpenMP
Parallel computing
Partitioned global address space
Processes
Ptolemy Project
Rust (programming language)
Sheaf (mathematics)
Threads
X10 (programming language)


==
References ==


==
Further reading ==
Lynch, Nancy A. (1996).
Distributed Algorithms.
Morgan Kaufmann.
ISBN 978-1-55860-348-6.

Tanenbaum, Andrew S.; Van Steen, Maarten (2002).
Distributed Systems: Principles and Paradigms.
Prentice Hall.
ISBN 978-0-13-088893-8.

Kurki-Suonio, Reino (2005).
A Practical Theory of Reactive Systems.
Springer.
ISBN 978-3-540-23342-8.

Garg, Vijay K. (2002).
Elements of Distributed Computing.
Wiley-IEEE Press.
ISBN 978-0-471-03600-5.

Magee, Jeff; Kramer, Jeff (2006).
Concurrency: State Models and Java Programming.
Wiley.
ISBN 978-0-470-09355-9.

Distefano, S., & Bruneo, D. (2015).
Quantitative assessments of distributed systems: Methodologies and techniques (1st ed.).
Somerset:
John Wiley & Sons Inc.
ISBN 9781119131144
Bhattacharyya, S. S. (2013;2014;).
Handbook of signal processing systems (Second;2;2nd 2013; ed.).
New York, NY:
Springer.10.1007/978-1-4614-6859-2 ISBN 9781461468592
Wolter, K.
(2012;2014;).
Resilience assessment and evaluation of computing systems (1.
Aufl.;1; ed.)
.
London;Berlin;: Springer.
ISBN 9783642290329


==
External links ==
Concurrent Systems at The WWW Virtual Library
Concurrency patterns presentation given at scaleconf
In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions.
This process of subdividing gives rise to a representation of objects within the space in the form of a tree data structure known as a BSP tree.

Binary space partitioning was developed in the context of 3D computer graphics in 1969.
The structure of a BSP tree is useful in rendering because it can efficiently give spatial information about the objects in a scene, such as objects being ordered from front-to-back with respect to a viewer at a given location.
Other applications of BSP include: performing geometrical operations with shapes (constructive solid geometry) in CAD, collision detection in robotics and 3D video games, ray tracing, and other applications that involve the handling of complex spatial scenes.

==
Overview ==
Binary space partitioning is a generic process of recursively dividing a scene into two until the partitioning satisfies one or more requirements.
It can be seen as a generalization of other spatial tree structures such as k-d trees and quadtrees, one where hyperplanes that partition the space may have any orientation, rather than being aligned with the coordinate axes as they are in k-d trees or quadtrees.
When used in computer graphics to render scenes composed of planar polygons, the partitioning planes are frequently chosen to coincide with the planes defined by polygons in the scene.

The specific choice of partitioning plane and criterion for terminating the partitioning process varies depending on the purpose of the BSP tree.
For example, in computer graphics rendering, the scene is divided until each node of the BSP tree contains only polygons that can be rendered in arbitrary order.
When back-face culling is used, each node, therefore, contains a convex set of polygons, whereas when rendering double-sided polygons, each node of the BSP tree contains only polygons in a single plane.
In collision detection or ray tracing, a scene may be divided up into primitives on which collision or ray intersection tests are straightforward.

Binary space partitioning arose from the computer graphics need to rapidly draw three-dimensional scenes composed of polygons.
A simple way to draw such scenes is the painter's algorithm, which produces polygons in order of distance from the viewer, back to front, painting over the background and previous polygons with each closer object.
This approach has two disadvantages: the time required to sort polygons in back-to-front order, and the possibility of errors in overlapping polygons.
Fuchs and co-authors showed that constructing a BSP tree solved both of these problems by providing a rapid method of sorting polygons with respect to a given viewpoint
(linear in the number of polygons in the scene) and by subdividing overlapping polygons to avoid errors that can occur with the painter's algorithm.
A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming.
Typically, it is therefore performed once on static geometry, as a pre-calculation step, prior to rendering or other real-time operations on a scene.
The expense of constructing a BSP tree makes it difficult and inefficient to directly implement moving objects into a tree.

BSP trees are often used by 3D video games, particularly first-person shooters and those with indoor environments.
Game engines using BSP trees include the Doom (id Tech 1), Quake (id Tech 2 variant), GoldSrc and Source engines.
In them, BSP trees containing the static geometry of a scene are often used together with a Z-buffer, to correctly merge movable objects such as doors and characters onto the background scene.
While binary space partitioning provides a convenient way to store and retrieve spatial information about polygons in a scene, it does not solve the problem of visible surface determination.

==
Generation ==
The canonical use of a BSP tree is for rendering polygons (that are double-sided, that is, without back-face culling) with the painter's algorithm.
Each polygon is designated with a front side and a backside which could be chosen arbitrarily and only affects the structure of the tree but not the required result.
Such a tree is constructed from an unsorted list of all the polygons in a scene.
The recursive algorithm for construction of a BSP tree from that list of polygons is:
Choose a polygon P from the list.

Make a node N in the BSP tree, and add P to the list of polygons at that node.

For each other polygon in the list:
If that polygon is wholly in front of the plane containing P, move that polygon to the list of nodes in front of P.
If that polygon is wholly behind the plane containing P, move that polygon to the list of nodes behind P.
If that polygon is intersected by the plane containing P, split it into two polygons and move them to the respective lists of polygons behind and in front of P.
If that polygon lies in the plane containing P, add it to the list of polygons at node N.
Apply this algorithm to the list of polygons in front of P.
Apply this algorithm to the list of polygons behind P.The
following diagram illustrates the use of this algorithm in converting a list of lines or polygons into a BSP tree.
At each of the eight steps (i.-viii.),
the algorithm above is applied to a list of lines, and one new node is added to the tree.

The final number of polygons or lines in a tree is often larger (sometimes much larger) than the original list, since lines or polygons that cross the partitioning plane must be split into two.
It is desirable to minimize this increase, but also to maintain reasonable balance in the final tree.
The choice of which polygon or line is used as a partitioning plane (in step 1 of the algorithm) is therefore important in creating an efficient BSP tree.

==
Traversal ==
A BSP tree is traversed in a linear time, in an order determined by the particular function of the tree.
Again using the example of rendering double-sided polygons using the painter's algorithm, to draw a polygon P correctly requires that all polygons behind the plane P lies in must be drawn first, then polygon P, then finally the polygons in front of P.
If this drawing order is satisfied for all polygons in a scene, then the entire scene renders in the correct order.
This procedure can be implemented by recursively traversing a BSP tree using the following algorithm.
From a given viewing location V, to render a BSP tree,

If the current node is a leaf node, render the polygons at the current node.

Otherwise, if the viewing location V is in front of the current node:
Render the child BSP tree containing polygons behind the current node
Render the polygons at the current node
Render the child BSP tree containing polygons in front of the current node
Otherwise, if the viewing location V is behind the current node:
Render the child BSP tree containing polygons in front of the current node
Render the polygons at the current node
Render the child BSP tree containing polygons behind the current node
Otherwise, the viewing location V must be exactly on the plane associated with the current node.
Then:
Render the child BSP tree containing polygons in front of the current node
Render the child BSP tree containing polygons behind the current node
Applying this algorithm recursively to the BSP tree generated above results in the following steps:
The algorithm is first applied to the root node of the tree, node A. V is in front of node A, so we apply the algorithm first to the child BSP tree containing polygons behind A
This tree has root node B1.
V is behind B1 so first, we apply the algorithm to the child BSP tree containing polygons in front of B1:
This tree is just the leaf node D1, so the polygon D1 is rendered.

We then render the polygon B1.

We then apply the algorithm to the child BSP tree containing polygons behind B1:
This tree is just the leaf node C1, so the polygon C1 is rendered.

We then draw the polygons of A
We then apply the algorithm to the child BSP tree containing polygons in front of A
This tree has root node B2.
V is behind B2 so first, we apply the algorithm to the child BSP tree containing polygons in front of B2:
This tree is just the leaf node D2, so the polygon D2 is rendered.

We then render the polygon B2.

We then apply the algorithm to the child BSP tree containing polygons behind B2:
This tree has root node C2.
V is in front of C2 so first, we would apply the algorithm to the child BSP tree containing polygons behind C2.
There is no such tree, however, so we continue.

We render the polygon C2.

We apply the algorithm to the child BSP tree containing polygons in front of C2
This tree is just the leaf node D3, so the polygon D3 is rendered.
The tree is traversed in linear time and renders the polygons in a far-to-near ordering (D1, B1, C1, A, D2, B2, C2, D3) suitable for the painter's algorithm.

==
Timeline ==
1969
Schumacker et al.
published a report that described how carefully positioned planes in a virtual environment could be used to accelerate polygon ordering.
The technique made use of depth coherence, which states that a polygon on the far side of the plane cannot, in any way, obstruct a closer polygon.
This was used in flight simulators made by GE as well as Evans and Sutherland.
However, the creation of the polygonal data organization was performed manually by the scene designer.1980
Fuchs et al.
extended Schumacker's idea to the representation of 3D objects in a virtual environment by using planes that lie coincident with polygons to recursively partition the 3D space.
This provided a fully automated and algorithmic generation of a hierarchical polygonal data structure known as a Binary Space Partitioning Tree (BSP Tree).
The process took place as an off-line preprocessing step that was performed once per environment/object.
At run-time, the view-dependent visibility ordering was generated by traversing the tree.

1981
Naylor's Ph.D. thesis provided a full development of both BSP trees and a graph-theoretic approach using strongly connected components for pre-computing visibility, as well as the connection between the two methods.
BSP trees as a dimension-independent spatial search structure were emphasized, with applications to visible surface determination.
The thesis also included the first empirical data demonstrating that the size of the tree and the number of new polygons were reasonable (using a model of the Space Shuttle).

1983
Fuchs et al.
described a micro-code implementation of the BSP tree algorithm on an Ikonas frame buffer system.
This was the first demonstration of real-time visible surface determination using BSP trees.

1987
Thibault and Naylor described how arbitrary polyhedra may be represented using a BSP tree as opposed to the traditional b-rep (boundary representation).
This provided a solid representation vs. a surface based-representation.
Set operations on polyhedra were described using a tool, enabling constructive solid geometry (CSG) in real-time.
This was the forerunner of BSP level design using "brushes", introduced in the Quake editor and picked up in the Unreal Editor.

1990
Naylor, Amanatides, and Thibault provided an algorithm for merging two BSP trees to form a new BSP tree from the two original trees.
This provides many benefits including combining moving objects represented by BSP trees with a static environment (also represented by a BSP tree), very efficient CSG operations on polyhedra, exact collisions detection in
O(log n * log n), and proper ordering of transparent surfaces contained in two interpenetrating objects (has been used for an x-ray vision effect).

1990
Teller and Séquin proposed the offline generation of potentially visible sets to accelerate visible surface determination in orthogonal 2D environments.

1991
Gordon and Chen [CHEN91] described an efficient method of performing front-to-back rendering from a BSP tree, rather than the traditional back-to-front approach.
They utilized a special data structure to record, efficiently, parts of the screen that have been drawn, and those yet to be rendered.
This algorithm, together with the description of BSP Trees in the standard computer graphics textbook of the day
(Computer Graphics: Principles and Practice) was used by John Carmack in the making of Doom (video game).

1992
Teller's Ph.D. thesis described the efficient generation of potentially visible sets as a pre-processing step to accelerate real-time visible surface determination in arbitrary 3D polygonal environments.
This was used in Quake and contributed significantly to that game's performance.

1993
Naylor answered the question of what characterizes a good BSP tree.
He used expected case models (rather than worst-case analysis) to mathematically measure the expected cost of searching a tree and used this measure to build good BSP trees.
Intuitively, the tree represents an object in a multi-resolution fashion (more exactly, as a tree of approximations).
Parallels with Huffman codes and probabilistic binary search trees are drawn.

1993
Hayder Radha's Ph.D. thesis described (natural) image representation methods using BSP trees.
This includes the development of an optimal BSP-tree construction framework for any arbitrary input image.
This framework is based on a new image transform, known as the Least-Square-Error (LSE)
Partitioning Line (LPE) transform.
H. Radha's thesis also developed an optimal rate-distortion (RD) image compression framework and image manipulation approaches using BSP trees.

== See also ==
k-d tree
Octree
Quadtree
Hierarchical clustering, an alternative way to divide 3d model data for efficient rendering.

Guillotine cutting


==
References ==


==
Additional references ==
[NAYLOR90]
B. Naylor, J. Amanatides, and W. Thibualt, "Merging BSP Trees Yields Polyhedral Set Operations", Computer Graphics (Siggraph '90), 24(3), 1990.

[NAYLOR93] B. Naylor, "Constructing Good Partitioning Trees", Graphics Interface (annual Canadian CG conference)
May, 1993.

[CHEN91] S. Chen and D. Gordon.
“
Front-to-Back Display of BSP Trees.”
IEEE Computer Graphics & Algorithms,
pp 79–85.
September 1991.

[RADHA91] H. Radha, R. Leoonardi, M. Vetterli, and B. Naylor “Binary Space Partitioning Tree Representation of Images,” Journal of Visual Communications and Image Processing 1991, vol.
2(3).

[RADHA93] H. Radha, "Efficient Image Representation using Binary Space Partitioning Trees.",
Ph.D. Thesis, Columbia University, 1993.

[RADHA96] H. Radha, M. Vetterli, and R. Leoonardi, “Image Compression Using Binary Space Partitioning Trees,” IEEE Transactions on Image Processing, vol.
5, No.12, December 1996, pp.
1610–1624.

[WINTER99]
AN INVESTIGATION INTO REAL-TIME 3D POLYGON RENDERING USING BSP TREES.
Andrew Steven Winter.
April 1999.
available online
Mark de Berg; Marc van Kreveld; Mark Overmars & Otfried Schwarzkopf (2000).
Computational Geometry (2nd revised ed.).
Springer-Verlag.
ISBN 978-3-540-65620-3.
Section 12: Binary Space Partitions: pp.
251–265.
Describes a randomized Painter's Algorithm..
Christer Ericson: Real-Time Collision Detection (The Morgan Kaufmann Series in Interactive 3-D Technology).
Verlag Morgan Kaufmann, S. 349–382, Jahr 2005, ISBN 1-55860-732-3


==
External links ==
BSP trees tutorial
BSP trees presentation
Another BSP trees presentation
A Java applet that demonstrates the process of tree generation
A Master Thesis about BSP generating
BSP Trees: Theory and Implementation
BSP in 3D space
Timeline of computing presents events in the history of computing organized by year and grouped into six topic areas: predictions and concepts, first use and inventions, hardware systems and processors, operating systems, programming languages, and new application areas.

Detailed computing timelines:
before 1950, 1950–1979, 1980–1989, 1990–1999, 2000-2009, 2010-2019, 2020–2029


==
Graphical timeline ==


==
See also ==
History of compiler construction
History of computing hardware – up to third generation (1960s)
History of computing hardware (1960s–present) – third generation and later
History of the graphical user interface
History of the Internet
History of the World Wide Web
List of pioneers in computer science
Timeline of electrical and electronic engineering


==
Resources ==
Stephen White, A Brief History of Computing
The Computer History in time and space, Graphing Project, an attempt to build a graphical image of computer history, in particular operating systems.

The Computer Revolution/Timeline at Wikibooks
"File:Timeline.pdf - Engineering and Technology History Wiki" (PDF).
ethw.org.
2012.
Retrieved 2018
-03-03.

==
External links ==
Visual History of Computing
This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.

==
A ==
abstract data type (ADT)
A mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations.
This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.

abstract method
One with only a signature and no implementation body.
It is often used to specify that a subclass must provide an implementation of the method.
Abstract methods are used to specify interfaces in some computer languages.

abstraction
1.

In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.

2.

The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.

agent architecture
A blueprint for software agents and intelligent control systems depicting the arrangement of components.
The architectures implemented by intelligent agents are referred to as cognitive architectures.

agent-based model (ABM)
A class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole.
It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming.
Monte Carlo methods are used to introduce randomness.

aggregate function
In database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a sum, count, or max.

agile software development
An approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s).
It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.

algorithm
An unambiguous specification of how to solve a class of problems.
Algorithms can perform calculation, data processing, and automated reasoning tasks.
They are ubiquitous in computing technologies.

algorithm design
A method or mathematical process for problem-solving and for engineering algorithms.
The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer.
Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.

algorithmic efficiency
A property of an algorithm which relates to the number of computational resources used by the algorithm.
An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources.
Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.

American Standard Code for Information Interchange (ASCII)
A character encoding standard for electronic communications.
ASCII codes represent text in computers, telecommunications equipment, and other devices.
Most modern character-encoding schemes are based on ASCII, although they support many additional characters.

application programming interface (API)
A set of subroutine definitions, communication protocols, and tools for building software.
In general terms, it is a set of clearly defined methods of communication among various components.
A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer.

application software
Also simply application or app.

Computer software designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user.
Common examples of applications include word processors, spreadsheets, accounting applications, web browsers, media players, aeronautical flight simulators, console games, and photo editors.
This contrasts with system software, which is mainly involved with managing the computer's most basic running operations, often without direct input from the user.
The collective noun application software refers to all applications collectively.

array data structure
Also simply array.

A data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key.
An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula.
The simplest type of data structure is a linear array, also called a one-dimensional array.

artifact
One of many kinds of tangible by-products produced during the development of software.
Some artifacts (e.g. use cases, class diagrams, and other Unified Modeling Language (UML) models, requirements, and design documents) help describe the function, architecture, and design of software.
Other artifacts are concerned with the process of development itself—such as project plans, business cases, and risk assessments.

artificial intelligence (AI)
Also machine intelligence.

Intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.
In computer science, AI research is defined as the study of "intelligent agents": devices capable of perceiving their environment and taking actions that maximize the chance of successfully achieving their goals.
Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".

ASCII
See American Standard Code for Information Interchange.

assertion
In computer programming, a statement that a predicate (Boolean-valued function, i.e. a true–false expression) is always true at that point in code execution.
It can help a programmer read the code, help a compiler compile it, or help the program detect its own defects.
For the latter, some programs check assertions by actually evaluating the predicate as they run
and if it is not in fact true – an assertion failure – the program considers itself to be broken and typically deliberately crashes or throws an assertion failure exception.

associative array
An associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.

Operations associated with this data type allow:
the addition of a pair to the collection
the removal of a pair from the collection
the modification of an existing pair
the lookup of a value associated with a particular key

automata theory
The study of abstract machines and automata, as well as the computational problems that can be solved using them.
It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science).

automated reasoning
An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning.
The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically.
Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.

==
B ==
bandwidth
The maximum rate of data transfer across a given path.
Bandwidth may be characterized as network bandwidth, data bandwidth, or digital bandwidth.

Bayesian programming
A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.

benchmark
The act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it.
The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.

best, worst and average case
Expressions of what the resource usage is at least, at most, and on average, respectively, for a given algorithm.
Usually the resource being considered is running time, i.e. time complexity, but it could also be memory or some other resource.
Best case is the function which performs the minimum number of steps on input data of n elements; worst case is the function which performs the maximum number of steps on input data of size n; average case is the function which performs an average number of steps on input data of n elements.

big data
A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with.
Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.

big O notation
A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity.
It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.

binary number
In mathematics and digital electronics, a number expressed in the base-2 numeral system or binary numeral system, which uses only two symbols: typically 0 (zero) and 1 (one).

binary search algorithm
Also simply binary search, half-interval search, logarithmic search, or binary chop.

A search algorithm that finds the position of a target value within a sorted array.

binary tree
A tree data structure in which each node has at most two children, which are referred to as the left child and the right child.
A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set.
Some authors allow the binary tree to be the empty set as well.

bioinformatics
An interdisciplinary field that combines biology, computer science, information engineering, mathematics, and statistics to develop methods and software tools for analyzing and interpreting biological data.
Bioinformatics is widely used for in silico analyses of biological queries using mathematical and statistical techniques.

bit
A basic unit of information used in computing and digital communications; a portmanteau of binary digit.
A binary digit can have one of two possible values, and may be physically represented with a two-state device.
These state values are most commonly represented as either a 0or1.

bit rate (R)

Also bitrate.

In telecommunications and computing, the number of bits that are conveyed or processed per unit of time.

blacklist
Also block list.

In computing, a basic access control mechanism that allows through all elements (email addresses, users, passwords, URLs, IP addresses, domain names, file hashes, etc.),
except those explicitly mentioned in a list of prohibited elements.
Those items on the list are denied access.
The opposite is a whitelist, which means only items on the list are allowed through whatever gate is being used while all other elements are blocked.
A greylist contains items that are temporarily blocked (or temporarily allowed) until an additional step is performed.

BMP file format
Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap.

A raster graphics image file format used to store bitmap digital images independently of the display device (such as a graphics adapter), used especially on Microsoft Windows and OS/2 operating systems.

Boolean data type
A data type that has one of two possible values (usually denoted true and false), intended to represent the two truth values of logic and Boolean algebra.
It is named after George Boole, who first defined an algebraic system of logic in the mid-19th century.
The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false.
It is a special case of a more general logical data type (see probabilistic logic)—i.e.
logic need not always be Boolean.

Boolean expression
An expression used in a programming language that returns a Boolean value when evaluated, that is one of true or false.
A Boolean expression may be composed of a combination of the Boolean constants true or false, Boolean-typed variables, Boolean-valued operators, and Boolean-valued functions.

Boolean algebra
In mathematics and mathematical logic, the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively.
Contrary to elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and (denoted as ∧), the disjunction or (denoted as ∨), and the negation not (denoted as ¬).
It is thus a formalism for describing logical relations in the same way that elementary algebra describes numeric relations.

byte
A unit of digital information that most commonly consists of eight bits, representing a binary number.
Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures.

booting
The procedures implemented in starting up a computer or computer appliance until it can be used.
It can be initiated by hardware such as a button press or by a software command.
After the power is switched on, the computer is relatively dumb and can read only part of its storage called read-only memory.
There, a small program is stored called firmware.
It does power-on self-tests and, most importantly, allows access to other types of memory like a hard disk and main memory.
The firmware loads bigger programs into the computer's main memory and runs it.

==
C ==
callback
Also a call-after function.

Any executable code that is passed as an argument to other code that is expected to "call back" (execute) the argument at a given time.
This execution may be immediate, as in a synchronous callback, or it might happen at a later time, as in an asynchronous callback.

central processing unit (CPU)
The electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions.
The computer industry has used the term "central processing unit" at least since the early 1960s.
Traditionally, the term "CPU" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.

character
A unit of information that roughly corresponds to a grapheme, grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written form of a natural language.

cipher
Also cypher.

In cryptography, an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure.

class
In object-oriented programming, an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).
In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated.

class-based programming
Also class-orientation.

A style of object-oriented programming (OOP) in which inheritance occurs via defining "classes" of objects, instead of via the objects alone.
Compare prototype-based programming.

class-orientation
A style of Object-oriented programming (OOP) in which inheritance occurs via defining classes of objects, instead of inheritance occurring via the objects alone (compare prototype-based programming).

client
A piece of computer hardware or software that accesses a service made available by a server.
The server is often (but not always) on another computer system, in which case the client accesses the service by way of a network.
The term applies to the role that programs or devices play in the client–server model.

cleanroom software engineering
A software development process intended to produce software with a certifiable level of reliability.
The cleanroom process was originally developed by Harlan Mills and several of his colleagues including Alan Hevner at IBM.
The focus of the cleanroom process is on defect prevention, rather than defect removal.

closure
Also lexical closure or function closure.

A technique for implementing lexically scoped name binding in a language with first-class functions.
Operationally, a closure is a record storing a function together with an environment.

cloud computing
Shared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.

Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.

code library
A collection of non-volatile resources used by computer programs, often for software development.
These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications.
In IBM's OS/360 and its successors they are referred to as partitioned data sets.

coding
Computer programming is the process of designing and building an executable computer program for accomplishing a specific computing task.
Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).
The source code of a program is written in one or more programming languages.
The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem.
The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

coding theory
The study of the properties of codes and their respective fitness for specific applications.
Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage.
Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods.
This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.

cognitive science
The interdisciplinary, scientific study of the mind and its processes.
It examines the nature, the tasks, and the functions of cognition (in a broad sense).
Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information.
Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology.

collection
A collection or container is a grouping of some variable number of data items (possibly zero) that have some shared significance to the problem being solved and need to be operated upon together in some controlled fashion.

Generally, the data items will be of the same type or, in languages supporting inheritance, derived from some common ancestor type.
A collection is a concept applicable to abstract data types, and does not prescribe a specific implementation as a concrete data structure, though often there is a conventional choice (see Container for type theory discussion).

comma-separated values (CSV)
A delimited text file that uses a comma to separate values.
A CSV file stores tabular data (numbers and text) in plain text.

Each line of the file is a data record.

Each record consists of one or more fields, separated by commas.
The use of the comma as a field separator is the source of the name for this file format.

compiler
A computer program that transforms computer code written in one programming language (the source language) into another programming language (the target language).
Compilers are a type of translator that support digital devices, primarily computers.
The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower-level language (e.g. assembly language, object code, or machine code) to create an executable program.

computability theory
also known as recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees.
The field has since expanded to include the study of generalized computability and definability.
In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.

computation
Any type of calculation that includes both arithmetical and non-arithmetical steps and follows a well-defined model, e.g. an algorithm.
The study of computation is paramount to the discipline of computer science.

computational biology
Involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems.
The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, and evolution.

Computational biology is different from biological computing, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers.

computational chemistry
A branch of chemistry that uses computer simulation to assist in solving chemical problems.
It uses methods of theoretical chemistry, incorporated into efficient computer programs, to calculate the structures and properties of molecules and solids.

computational complexity theory
A subfield of computational science which focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other.
A computational problem is a task solved by a computer.
A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

computational model
A mathematical model in computational science that requires extensive computational resources to study the behavior of a complex system by computer simulation.

computational neuroscience
Also theoretical neuroscience or mathematical neuroscience.

A branch of neuroscience which employs mathematical models, theoretical analysis, and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system.

computational physics
Is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists.
Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.

computational science
Also scientific computing and scientific computation (SC).

An interdisciplinary field that uses advanced computing capabilities to understand and solve complex problems.
It is an area of science which spans many disciplines, but at its core it involves the development of computer models and simulations to understand complex natural systems.

computational steering
Is the practice of manually intervening with an otherwise autonomous computational process, to change its outcome.

computer
A device that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming.
Modern computers have the ability to follow generalized sets of operations, called programs.
These programs enable computers to perform an extremely wide range of tasks.

computer architecture
A set of rules and methods that describe the functionality, organization, and implementation of computer systems.
Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.
In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.

computer data storage
Also simply storage or memory.

A technology consisting of computer components and recording media that are used to retain digital data.
Data storage is a core function and fundamental component of all modern computer systems.

computer ethics
A part of practical philosophy concerned with how computing professionals should make decisions regarding professional and social conduct.

computer graphics
Pictures and films created using computers.
Usually, the term refers to computer-generated image data created with the help of specialized graphical hardware and software.
It is a vast and recently developed area of computer science.

computer network
Also data network.

A digital telecommunications network which allows nodes to share resources.
In computer networks, computing devices exchange data with each other using connections (data links) between nodes.
These data links are established over cable media such as wires or optic cables, or wireless media such as Wi-Fi.

computer program
Is a collection of instructions that can be executed by a computer to perform a specific task.

computer programming
The process of designing and building an executable computer program for accomplishing a specific computing task.
Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding).
The source code of a program is written in one or more programming languages.
The purpose of programming is to find a sequence of instructions that will automate the performance of a task for solving a given problem.
The process of programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.

computer science
The theory, experimentation, and engineering that form the basis for the design and use of computers.
It involves the study of algorithms that process, store, and communicate digital information.
A computer scientist specializes in the theory of computation and the design of computational systems.

computer scientist
A person who has acquired the knowledge of computer science, the study of the theoretical foundations of information and computation and their application.

computer security
Also cybersecurity or information technology security
(IT security).

The protection of computer systems from theft or damage to their hardware, software, or electronic data, as well as from disruption or misdirection of the services they provide.

computer vision
An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos.
From the perspective of engineering, it seeks to automate tasks that the human visual system can do.

computing
Is any goal-oriented activity requiring, benefiting from, or creating computing machinery.
It includes study of algorithmic processes and development of both hardware and software.
It has scientific, engineering, mathematical, technological and social aspects.
Major computing fields include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.

concatenation
In formal language theory and computer programming, string concatenation  is the operation of joining character strings end-to-end.

For example, the concatenation of "snow" and "ball" is "snowball".
In certain formalisations of concatenation theory, also called string theory, string concatenation is a primitive notion.

Concurrency
The ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.

This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems.
In more technical terms, concurrency refers to the decomposability property of a program, algorithm, or problem into order-independent or partially-ordered components or units.

conditional
Also conditional statement, conditional expression, and conditional construct.

A feature of a programming language which performs different computations or actions depending on whether a programmer-specified Boolean condition evaluates to true or false.
Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.

container
Is a class, a data structure, or an abstract data type (ADT) whose instances are collections of other objects.
In other words, they store objects in an organized way that follows specific access rules.
The size of the container depends on the number of objects (elements) it contains.
Underlying (inherited) implementations of various container types may vary in size and complexity, and provide flexibility in choosing the right implementation for any given scenario.

continuation-passing style (CPS)
A style of functional programming in which control is passed explicitly in the form of a continuation.
This is contrasted with direct style, which is the usual style of programming.
Gerald Jay Sussman and Guy L. Steele, Jr. coined the phrase in AI Memo 349 (1975), which sets out the first version of the Scheme programming language.

control flow
Also flow of control.

The order in which individual statements, instructions or function calls of an imperative program are executed or evaluated.
The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.

Creative Commons (CC)
An American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share.
The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public.

cryptography
Or cryptology,  is the practice and study of techniques for secure communication in the presence of third parties called adversaries.
More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography.
Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics.
Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.

CSV
See comma-separated values.

cyberbullying
Also cyberharassment or online bullying.

A form of bullying or harassment using electronic means.

cyberspace
Widespread, interconnected digital technology.

== D ==
daemon
In multitasking computer operating systems, a daemon ( or ) is a computer program that runs as a background process, rather than being under the direct control of an interactive user.
Traditionally, the process names of a daemon end with the letter d, for clarification that the process is in fact a daemon, and for differentiation between a daemon and a normal computer program.
For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

data center
Also data centre.

A dedicated space used to house computer systems and associated components, such as telecommunications and data storage systems.
It generally includes redundant or backup components and infrastructure for power supply, data communications connections, environmental controls (e.g. air conditioning and fire suppression) and various security devices.

database
An organized collection of data, generally stored and accessed electronically from a computer system.
Where databases are more complex, they are often developed using formal design and modeling techniques.

data mining
Is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.
Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.
Data mining is the analysis step of the "knowledge discovery in databases" process, or KDD.
Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.

data science
An interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.
Data science is a "concept to unify statistics, data analysis, machine learning and their related methods" in order to "understand and analyze actual phenomena" with data.
It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science.

data structure
A data organization, management, and storage format that enables efficient access and modification.
More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data.

data type
Also simply type.

An attribute of data which tells the compiler or interpreter how the programmer intends to use the data.
Most programming languages support common data types of real, integer, and Boolean.
A data type constrains the values that an expression, such as a variable or a function, might take.
This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored.
A type of value from which an expression may take its value.

debugging
The process of finding and resolving defects or problems within a computer program that prevent correct operation of computer software or the system as a whole.
Debugging tactics can involve interactive debugging, control flow analysis, unit testing, integration testing, log file analysis, monitoring at the application or system level, memory dumps, and profiling.

declaration
In computer programming, a language construct that specifies properties of an identifier: it declares what a word (identifier) "means".
Declarations are most commonly used for functions, variables, constants, and classes, but can also be used for other entities such as enumerations and type definitions.
Beyond the name (the identifier itself) and the kind of entity (function, variable, etc.),
declarations typically specify the data type (for variables and constants), or the type signature (for functions); types may also include dimensions, such as for arrays.
A declaration is used to announce the existence of the entity to the compiler; this is important in those strongly typed languages that require functions, variables, and constants, and their types, to be specified with a declaration before use, and is used in forward declaration.
The term "declaration" is frequently contrasted with the term "definition", but meaning and usage varies significantly between languages.

digital data
In information theory and information systems, the discrete, discontinuous representation of information or works.
Numbers and letters are commonly used representations.

digital signal processing (DSP)
The use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations.

The signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency.

discrete event simulation (DES)
A model of the operation of a system as a discrete sequence of events in time.
Each event occurs at a particular instant in time and marks a change of state in the system.
Between consecutive events, no change in the system is assumed to occur; thus the simulation can directly jump in time from one event to the next.

disk storage
(Also sometimes called drive storage) is a general category of storage mechanisms where data is recorded by various electronic, magnetic, optical, or mechanical changes to a surface layer of one or more rotating disks.
A disk drive is a device implementing such a storage mechanism.
Notable types are the hard disk drive (HDD) containing a non-removable disk, the  floppy disk drive (FDD) and its removable floppy disk, and various optical disc drives (ODD) and associated optical disc media.

distributed computing
A field of computer science that studies distributed systems.
A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another.
The components interact with one another in order to achieve a common goal.
Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.
Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.

divide and conquer algorithm
An algorithm design paradigm based on multi-branched recursion.
A divide-and-conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly.
The solutions to the sub-problems are then combined to give a solution to the original problem.

DNS
See Domain Name System.

documentation
Written text or illustration that accompanies computer software or is embedded in the source code.
It either explains how it operates or how to use it, and may mean different things to people in different roles.

domain
Is the targeted subject area of a computer program.
It is a term used in software engineering.
Formally it represents the target subject of a specific programming project, whether narrowly or broadly defined.

Domain Name System (DNS)
A hierarchical and decentralized naming system for computers, services, or other resources connected to the Internet or to a private network.
It associates various information with domain names assigned to each of the participating entities.
Most prominently, it translates more readily memorized domain names to the numerical IP addresses needed for locating and identifying computer services and devices with the underlying network protocols.
By providing a worldwide, distributed directory service, the Domain Name System has been an essential component of the functionality of the Internet since 1985.

double-precision floating-point format
A computer number format.
It represents a wide dynamic range of numerical values by using a floating radix point.

download
In computer networks, to receive data from a remote system, typically a server such as a web server, an FTP server, an email server, or other similar systems.
This contrasts with uploading, where data is sent to a remote server.
A download is a file offered for downloading or that has been downloaded, or the process of receiving such a file.

== E ==
edge device
A device which provides an entry point into enterprise or service provider core networks.
Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.

Edge devices also provide connections into carrier and service provider networks.
An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.

encryption
In cryptography, encryption is the process of encoding information.
This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext.
Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information.
Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor.
For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm.
It is possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required.
An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users.
Historically, various forms of encryption have been used to aid in cryptography.
Early encryption techniques were often utilized in military messaging.
Since then, new techniques have emerged and become commonplace in all areas of modern computing.
Modern encryption schemes utilize the concepts of public-key and symmetric-key.
Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.

event
An action or occurrence recognized by software, often originating asynchronously from the external environment, that may be handled by the software.
Because an event is an entity which encapsulates the action and the contextual variables triggering the action,
the acrostic mnemonic "Execution Variable
Encapsulating Named Trigger" is often used to clarify the concept.

event-driven programming
A programming paradigm in which the flow of the program is determined by events such as user actions (mouse clicks, key presses), sensor outputs, or messages from other programs or threads.
Event-driven programming is the dominant paradigm used in graphical user interfaces and other applications (e.g. JavaScript web applications) that are centered on performing certain actions in response to user input.
This is also true of programming for device drivers (e.g. P in USB device driver stacks).

evolutionary computing
A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms.
In technical terms, they are a family of population-based trial-and-error problem-solvers with a metaheuristic or stochastic optimization character.

executable
Also executable code, executable file, executable program, or simply executable.

Causes a computer "to perform indicated tasks according to encoded instructions," as opposed to a data file that must be parsed by a program to be meaningful.
The exact interpretation depends upon the use - while "instructions" is traditionally taken to mean machine code instructions for a physical CPU, in some contexts a file containing bytecode or scripting language instructions may also be considered executable.

executable module

execution
In computer and software engineering is the process by which a computer or  virtual machine executes the instructions of a computer program.
Each instruction of a program is a description of a particular 
action which to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed.

exception handling
The process of responding to the occurrence, during computation, of exceptions – anomalous or exceptional conditions requiring special processing – often disrupting the normal flow of program execution.
It is provided by specialized programming language constructs, computer hardware mechanisms like interrupts, or operating system
IPC facilities like signals.

expression
In a programming language, a combination of one or more constants, variables, operators, and functions that the programming language interprets (according to its particular rules of precedence and of association) and computes to produce ("to return", in a stateful environment) another value.
This process, as for mathematical expressions, is called evaluation.

external library


== F ==
fault-tolerant computer system
A system designed around the concept of fault tolerance.
In essence, they must be able to continue working to a level of satisfaction in the presence of errors or breakdowns.

feasibility study
An investigation which aims to objectively and rationally uncover the strengths and weaknesses of an existing business or proposed venture, opportunities and threats present in the natural environment, the resources required to carry through, and ultimately the prospects for success.
In its simplest terms, the two criteria to judge feasibility are cost required and value to be attained.

field
Data that has several parts, known as a record, can be divided into fields.
Relational databases arrange data as sets of database records, so called rows.
Each record consists of several fields; the fields of all records form the columns.

Examples of fields: name, gender, hair colour.

filename extension
An identifier specified as a suffix to the name of a computer file.
The extension indicates a characteristic of the file contents or its intended use.

filter (software)
A computer program or subroutine to process a stream, producing another stream.
While a single filter can be used individually, they are frequently strung together to form a pipeline.

floating point arithmetic
In computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision.
For this reason, floating-point computation is often found in systems which include very small and very large real numbers, which require fast processing times.
A number is, in general, represented approximately to a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen.
A number that can be represented exactly is of the following form:
significand
        
        ×
base
exponent
          
        
        ,
      
    
    {\displaystyle {\text{significand}}\times {\text{base}}^{\text{exponent}},}
  
where significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.

For example:

  
    
      
        1.2345
        =
12345
⏟
            
          
          
            significand
×
10
              ⏟
base
−
4
⏞
                
              
              
                exponent
              
            
          
        
        .

{\displaystyle 1.2345=\underbrace {12345} _{\text{significand}}\times \underbrace {10} _{\text{base}}\!\!\!\!\!\!^{\overbrace {-4} ^{\text{exponent}}}.}
for loop
Also for-loop.

A control flow statement for specifying iteration, which allows code to be executed repeatedly.
Various keywords are used to specify this statement: descendants of ALGOL use "for", while descendants of Fortran use "do".
There are also other possibilities, e.g. COBOL uses "PERFORM VARYING".

formal methods
A set of mathematically based techniques for the specification, development, and verification of software and hardware systems.
The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.

formal verification
The act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics.

functional programming
A programming paradigm—a style of building the structure and elements of computer programs–that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.
It is a declarative programming paradigm in that programming is done with expressions or declarations instead of statements.

==
G ==
game theory
The study of mathematical models of strategic interaction between rational decision-makers.
It has applications in all fields of social science, as well as in logic and computer science.
Originally, it addressed zero-sum games, in which each participant's gains or losses are exactly balanced by those of the other participants.
Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.

garbage in, garbage out (GIGO)
A term used to describe the concept that flawed or nonsense input data produces nonsense output or "garbage".
It can also refer to the unforgiving nature of programming, in which a poorly written program might produce nonsensical behavior.

Graphics Interchange Format

gigabyte
A multiple of the unit byte for digital information.
The prefix giga means 109 in the International System of Units (SI).
Therefore, one gigabyte is 1000000000bytes.

The unit symbol for the gigabyte is GB.

global variable
In computer programming, a variable with global scope, meaning that it is visible (hence accessible) throughout the program, unless shadowed.
The set of all global variables is known as the global environment or global state.
In compiled languages, global variables are generally static variables, whose extent (lifetime) is the entire runtime of the program, though in interpreted languages (including command-line interpreters), global variables are generally dynamically allocated when declared, since they are not known ahead of time.

graph theory
In mathematics, the study of graphs, which are mathematical structures used to model pairwise relations between objects.
A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines).
A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically.

== H ==
handle
In computer programming, a handle is an abstract reference to a resource that is used when application software references blocks of memory or objects that are managed by another system like a database or an operating system.

hard problem
Computational complexity theory focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other.
A computational problem is a task solved by a computer.
A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.

hash function
Any function that can be used to map data of arbitrary size to data of a fixed size.
The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.
Hash functions are often used in combination with a hash table, a common data structure used in computer software for rapid data lookup.
Hash functions accelerate table or database lookup by detecting duplicated records in a large file.

hash table
In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values.
A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.

heap
A specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: if P is a parent node of C, then the key (the value) of P is either greater than or equal to (in a max heap) or less than or equal to (in a min heap) the key of C.
The node at the "top" of the heap (with no parents) is called the root node.

heapsort
A comparison-based sorting algorithm.
Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region.
The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.
human-computer interaction (HCI)
Researches the design and use of computer technology, focused on the interfaces between people (users) and computers.
Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways.
As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study.

==
I ==
identifier
In computer languages, identifiers are tokens (also called symbols) which name language entities.
Some of the kinds of entities an identifier might denote include variables, types, labels, subroutines,  and packages.

IDE
Integrated development environment.

image processing

imperative programming
A programming paradigm that uses statements that change a program's state.
In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform.
Imperative programming focuses on describing how a program operates.

incremental build model
A method of software development where the product is designed, implemented and tested incrementally (a little more is added each time) until the product is finished.
It involves both development and maintenance.
The product is defined as finished when it satisfies all of its requirements.
This model combines the elements of the waterfall model with the iterative philosophy of prototyping.

information space analysis
A deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.

information visualization

inheritance
In object-oriented programming, the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation.
Also defined as deriving new classes (sub classes) from existing ones (super class or base class) and forming them into a hierarchy of classes.

input/output (I/O)
Also informally io or IO.

The communication between an information processing system, such as a computer, and the outside world, possibly a human or another information processing system.
Inputs are the signals or data received by the system and outputs are the signals or data sent from it.
The term can also be used as part of an action; to "perform I/O
" is to perform an input or output operation.

insertion sort
A simple sorting algorithm that builds the final sorted array (or list) one item at a time.

instruction cycle
Also fetch–decode–
execute cycle or simply fetch-execute cycle.

The cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions.
It is composed of three main stages: the fetch stage, the decode stage, and the execute stage.

integer
A datum of integral data type, a data type that represents some range of mathematical integers.
Integral data types may be of different sizes and may or may not be allowed to contain negative values.
Integers are commonly represented in a computer as a group of binary digits (bits).
The size of the grouping varies so the set of integer sizes available varies between different types of computers.
Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.

integrated development environment (IDE)
A software application that provides comprehensive facilities to computer programmers for software development.
An IDE normally consists of at least a source code editor, build automation tools, and a debugger.

integration testing
(sometimes called integration and testing, abbreviated I&T) is the phase in software testing in which individual software modules are combined and tested as a group.
Integration testing is conducted to evaluate the compliance of a system or component with specified functional requirements.
It occurs after unit testing and before validation testing.
Integration testing takes as its input modules that have been unit tested, groups them in larger aggregates, applies tests defined in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.

intellectual property (IP)
A category of legal property that includes intangible creations of the human intellect.
There are many types of intellectual property, and some countries recognize more than others.
The most well-known types are copyrights, patents, trademarks, and trade secrets.

intelligent agent
In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).
Intelligent agents may also learn or use knowledge to achieve their goals.
They may be very simple or very complex.
A reflex machine, such as a thermostat, is considered an example of an intelligent agent.

interface
A shared boundary across which two or more separate components of a computer system exchange information.
The exchange can be between software, computer hardware, peripheral devices, humans, and combinations of these.
Some computer hardware devices, such as a touchscreen, can both send and receive data through the interface, while others such as a mouse or microphone may only provide an interface to send data to a given system.

internal documentation
Computer software is said to have Internal Documentation if the notes on how and why various parts of code operate is included within the source code as comments.

It is often combined with meaningful variable names with the intention of providing potential future programmers a means of understanding the workings of the code.
This contrasts with external documentation, where programmers keep their notes and explanations in a separate document.

internet
The global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link devices worldwide.
It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies.

internet bot
Also web robot, robot, or simply bot.

A software application that runs automated tasks (scripts) over the Internet.
Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone.
The largest use of bots is in web spidering (web crawler), in which an automated script fetches, analyzes and files information from web servers at many times the speed of a human.

interpreter
A computer program that directly executes instructions written in a programming or scripting language, without requiring them to have been previously compiled into a machine language program.

invariant
One can encounter invariants that can be relied upon to be true during the execution of a program, or during some portion of it.
It is a logical assertion that is always held to be true during a certain phase of execution.
For example, a loop invariant is a condition that is true at the beginning and the end of every execution of a loop.

iteration
Is the repetition of a process in order to generate an outcome.
The sequence will approach some end point or end value.
Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration.

In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.

==
J ==
Java
A general-purpose programming language that is class-based, object-oriented(although not a pure OO language), and designed to have as few implementation dependencies as possible.
It is intended to let application developers "write once, run anywhere" (WORA), meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.

==
K ==
kernel
The first section of an operating system to load into memory.
As the center of the operating system, the kernel needs to be small, efficient, and loaded into a protected area in the memory so that it cannot be overwritten.
It may be responsible for such essential tasks as disk drive management, file management, memory management, process management, etc.

==
L ==
library (computing)
A collection of non-volatile resources used by computer programs, often for software development.
These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values, or type specifications.

linear search
Also sequential search.

A method for finding an element within a list.
It sequentially checks each element of the list until a match is found or the whole list has been searched.

linked list
A linear collection of data elements, whose order is not given by their physical placement in memory.
Instead, each element points to the next.
It is a data structure consisting of a collection of nodes which together represent a sequence.

linker
 or link editor, is a computer utility program that takes one or more object files generated by a compiler or an assembler and combines them into a single executable file, library file, or another 'object' file.

A simpler version that writes its output directly to memory is called the loader, though loading is typically considered a separate process.

list
An abstract data type that represents a countable number of ordered values, where the same value may occur more than once.
An instance of a list is a computer representation of the mathematical concept of a finite sequence; the (potentially) infinite analog of a list is a stream.
Lists are a basic example of containers, as they contain other values.
If the same value occurs multiple times, each occurrence is considered a distinct item.

loader
The part of an operating system that is responsible for loading programs and libraries.
It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution.
Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running.
Once loading is complete, the operating system starts the program by passing control to the loaded program code.

logic error
In computer programming, a bug in a program that causes it to operate incorrectly, but not to terminate abnormally (or crash).
A logic error produces unintended or undesired output or other behaviour, although it may not immediately be recognized as such.

logic programming
A type of programming paradigm which is largely based on formal logic.
Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.

Major logic programming language families include Prolog, answer set programming (ASP), and Datalog.

== M ==
machine learning (ML)
The scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead.
It is seen as a subset of artificial intelligence.
Machine learning algorithms build a mathematical model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to perform the task.

machine vision (MV)
The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry.
Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise.
Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science.
It attempts to integrate existing technologies in new ways and apply them to solve real world problems.
The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.

mathematical logic
A subfield of mathematics exploring the applications of formal logic to mathematics.

It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science.
The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.

matrix
In mathematics, a matrix, (plural matrices), is a rectangular array (see irregular matrix) of numbers, symbols, or expressions, arranged in rows and columns.

memory
Computer data storage, often called storage, is a technology consisting of computer components and recording media that are used to retain digital data.
It is a core function and fundamental component of computers.

merge sort
Also mergesort.

An efficient, general-purpose, comparison-based sorting algorithm.
Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output.
Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945.
A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and von Neumann as early as 1948.

method
In object-oriented programming (OOP), a procedure associated with a message and an object.
An object consists of data and behavior.
The data and behavior comprise an interface, which specifies how the object may be utilized by any of various consumers of the object.

methodology
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.
It is also known as a software development life cycle (SDLC).
The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.

modem
Portmanteau of modulator-demodulator.

A hardware device that converts data into a format suitable for a transmission medium so that it can be transmitted from one computer to another (historically along telephone wires).
A modem modulates one or more carrier wave signals to encode digital information for transmission and demodulates signals to decode the transmitted information.
The goal is to produce a signal that can be transmitted easily and decoded reliably to reproduce the original digital data.
Modems can be used with almost any means of transmitting analog signals from light-emitting diodes to radio.
A common type of modem is one that turns the digital data of a computer into modulated electrical signal for transmission over telephone lines and demodulated by another modem at the receiver side to recover the digital data.

==
N ==
natural language processing (NLP)
A subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

node
Is a basic unit of a data structure, such as a linked list or tree data structure.
Nodes contain data and also may link to other nodes.
Links between nodes are often implemented by pointers.

number theory
A branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions.

numerical analysis
The study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).

numerical method
In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems.
The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm.

==
O ==
object
An object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.

In the class-based object-oriented programming paradigm, object refers to a particular instance of a class, where the object can be a combination of variables, functions, and data structures.

In relational database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).

object code
Also object module.

The product of a compiler.
In a general sense object code is a sequence of statements or instructions in a computer language, usually a machine code language (i.e., binary) or an intermediate language such as register transfer language (RTL).
The term indicates that the code is the goal or result of the compiling process, with some early sources referring to source code as a "subject program."
object-oriented analysis and design (OOAD)
A technical approach for analyzing and designing an application, system, or business by applying object-oriented programming, as well as using visual modeling throughout the software development process to guide stakeholder communication and product quality.

object-oriented programming (OOP)
A programming paradigm based on the concept of "objects", which can contain data, in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods).
A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self").
In OOP, computer programs are designed by making them out of objects that interact with one another.
OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

open-source software (OSS)
A type of computer software in which source code is released under a license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose.
Open-source software may be developed in a collaborative public manner.
Open-source software is a prominent example of open collaboration.

operating system (OS)
System software that manages computer hardware, software resources, and provides common services for computer programs.

optical fiber
A flexible, transparent fiber made by drawing glass (silica) or plastic to a diameter slightly thicker than that of a human hair.
Optical fibers are used most often as a means to transmit light between the two ends of the fiber and find wide usage in fiber-optic communications, where they permit transmission over longer distances and at higher bandwidths (data rates) than electrical cables.
Fibers are used instead of metal wires because signals travel along them with less loss; in addition, fibers are immune to electromagnetic interference, a problem from which metal wires suffer.

==
P ==
pair programming
An agile software development technique in which two programmers work together at one workstation.
One, the driver, writes code while the other, the observer or navigator, reviews each line of code as it is typed in.
The two programmers switch roles frequently.

parallel computing
A type of computation in which many calculations or the execution of processes are carried out simultaneously.
Large problems can often be divided into smaller ones, which can then be solved at the same time.
There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism.

parameter
Also formal argument.

In computer programming, a special kind of variable, used in a subroutine to refer to one of the pieces of data provided as input to the subroutine.
These pieces of data are the values of the arguments (often called actual arguments or actual parameters) with which the subroutine is going to be called/invoked.
An ordered list of parameters is usually included in the definition of a subroutine, so that, each time the subroutine is called, its arguments for that call are evaluated, and the resulting values can be assigned to the corresponding parameters.

peripheral
Any auxiliary or ancillary device connected to or integrated within a computer system and used to send information to or retrieve information from the computer.
An input device sends data or instructions to the computer; an output device provides output from the computer to the user; and an input/output device performs both functions.

pointer
Is an object in many programming languages that stores a memory address.
This can be that of another value located in computer memory, or in some cases, that of memory-mapped computer hardware.
A pointer references a location in memory, and obtaining the value stored at that location is known as dereferencing the pointer.
As an analogy, a page number in a book's index could be considered a pointer to the corresponding page; dereferencing such a pointer would be done by flipping to the page with the given page number and reading the text found on that page.
The actual format and content of a pointer variable is dependent on the underlying computer architecture.

postcondition
In computer programming, a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification.
Postconditions are sometimes tested using assertions within the code itself.
Often, postconditions are simply included in the documentation of the affected section of code.

precondition
In computer programming, a condition or predicate that must always be true just prior to the execution of some section of code or before an operation in a formal specification.

If a precondition is violated, the effect of the section of code becomes undefined and thus may or may not carry out its intended work.

Security problems can arise due to incorrect preconditions.

primary storage
(Also known as main memory, internal memory or prime memory), often referred to simply as memory, is the only one directly accessible to the CPU.
The CPU continuously reads instructions stored there and executes them as required.
Any data actively operated on is also stored there in uniform manner.

primitive data type
priority queue
An abstract data type which is like a regular queue or stack data structure, but where additionally each element has a "priority" associated with it.
In a priority queue, an element with high priority is served before an element with low priority.
In some implementations, if two elements have the same priority, they are served according to the order in which they were enqueued, while in other implementations, ordering of elements with the same priority is undefined.

procedural programming

procedure
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit.
This unit can then be used in programs wherever that particular task should be performed.

Subroutines may be defined within programs, or separately in libraries that can be used by many programs.

In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure.
Technically, these terms all have different definitions.
The generic, umbrella term  callable unit is sometimes used.

program lifecycle phase
Program lifecycle phases are the stages a computer program undergoes, from initial creation to deployment and execution.
The phases are edit time, compile time, link time, distribution time, installation time, load time, and run time.

programming language
A formal language, which comprises a set of instructions that produce various kinds of output.
Programming languages are used in computer programming to implement algorithms.

programming language implementation
Is a system for executing computer programs.
There are two general approaches to programming language implementation: interpretation and compilation.

programming language theory
(PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and of their individual features.

It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science.

It has become a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications.

Prolog
Is a logic programming language associated with artificial intelligence and computational linguistics.

Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language: the program logic is expressed in terms of relations, represented as facts and rules.

A computation is initiated by running a query over these relations.

Python
Is an interpreted, high-level and general-purpose programming language.
Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.
Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.

==
Q ==
quantum computing
The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation.
A quantum computer is used to perform such computation, which can be implemented theoretically or physically.

queue
A collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as enqueue, and removal of entities from the front terminal position, known as dequeue.

quicksort
Also partition-exchange sort.

An efficient sorting algorithm which serves as a systematic method for placing the elements of a random access file or an array in order.

== R ==
R programming language
R is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.
The R language is widely used among statisticians and data miners for developing statistical software and data analysis.

radix
Also base.

In digital numeral systems, the number of unique digits, including the digit zero, used to represent numbers in a positional numeral system.
For example, in the decimal/denary system (the most common system in use today) the radix (base number) is ten, because it uses the ten digits from 0 through 9, and all other numbers are uniquely specified by positional combinations of these ten base digits; in the binary system that is the standard in computing, the radix is two, because it uses only two digits, 0 and 1, to uniquely specify each number.

record
A record (also called a structure,  struct, or compound data) is a basic data structure.
Records in a database or spreadsheet are usually called "rows".

recursion
Occurs when a thing is defined in terms of itself or of its type.
Recursion is used in a variety of disciplines ranging from linguistics to logic.
The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition.
While this apparently defines an infinite number of instances (function values), it is often done in such a way that no infinite loop or infinite chain of references can occur.

reference
Is a value that enables a program to indirectly access a particular datum, such as a variable's value or a record, in the computer's memory or in some other storage device.

The reference is said to refer to the datum, and accessing the datum is called dereferencing the reference.

reference counting
A programming technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others.
In garbage collection algorithms, reference counts may be used to deallocate objects which are no longer needed.

relational database
Is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970.

A software system used to maintain relational databases is a relational database management system (RDBMS).
Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.

reliability engineering
A sub-discipline of systems engineering that emphasizes dependability in the lifecycle management of a product.
Reliability describes the ability of a system or component to function under stated conditions for a specified period of time.
Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.

regression testing
(rarely non-regression testing) is re-running functional and non-functional tests to ensure that previously developed and tested software still performs after a change.
If not, that would be called a regression.
Changes that may require regression testing include bug fixes, software enhancements, configuration changes, and even substitution of electronic components.
As regression test suites tend to grow with each found defect, test automation is frequently involved.
Sometimes a change impact analysis is performed to determine an appropriate subset of tests (non-regression analysis).

requirements analysis
In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.

robotics
An interdisciplinary branch of engineering and science that includes mechanical engineering, electronic engineering, information engineering, computer science, and others.
Robotics involves design, construction, operation, and use of robots, as well as computer systems for their perception, control, sensory feedback, and information processing.
The goal of robotics is to design intelligent machines that can help and assist humans in their day-to-day lives and keep everyone safe.

round-off error
Also rounding error.

The difference between the result produced by a given algorithm using exact arithmetic and the result produced by the same algorithm using finite-precision, rounded arithmetic.
Rounding errors are due to inexactness in the representation of real numbers and the arithmetic operations done with them.
This is a form of quantization error.
When using approximation equations or algorithms, especially when using finitely many digits to represent real numbers (which in theory have infinitely many digits), one of the goals of numerical analysis is to estimate computation errors.
Computation errors, also called numerical errors, include both truncation errors and roundoff errors.

router
A networking device that forwards data packets between computer networks.
Routers perform the traffic directing functions on the Internet.

Data sent through the internet, such as a web page or email, is in the form of data packets.

A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.

routing table
In computer networking a routing table, or routing information base (RIB), is a data table stored in a router or a network host that lists the routes to particular network destinations, and in some cases, metrics (distances) associated with those routes.
The routing table contains information about the topology of the network immediately around it.

run time
Runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code.
In other words, "runtime" is the running phase of a program.

run time error
A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed.
Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler.
Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language.

== S ==
search algorithm
Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values.

secondary storage
Also known as external memory or auxiliary storage, differs from primary storage in that it is not directly accessible by the CPU.
The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage.
Secondary storage is non-volatile (retaining data when power is shut off).
Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.

selection sort
Is an in-place comparison sorting algorithm.
It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort.
Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.

semantics
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages.
It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved.
In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation.
Semantics describes the processes a computer follows when executing a program in that specific language.
This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.

sequence
In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order does matter.

Like a set, it contains members (also called elements, or terms).

The number of elements (possibly infinite) is called the length of the sequence.

Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order does matter.

Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first n natural numbers (for a sequence of finite length
n).

The position of an element in a sequence is its rank or index; it is the natural number for which the element is the image.
The first element has index 0 or 1, depending on the context or a specific convention.

When a symbol is used to denote a sequence, the nth element of the sequence is denoted by this symbol with n as subscript; for example, the nth element of the Fibonacci sequence F is generally denoted Fn.

For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last.

This sequence differs from (A, R, M, Y).

Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence.

Sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers (2, 4, 6, ...).

In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams.

The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.

serializability
In concurrency control of databases, transaction processing (transaction management), and various transactional applications (e.g., transactional memory and software transactional memory), both centralized and distributed, a transaction schedule is serializable if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e. without overlapping in time.
Transactions are normally executed concurrently (they overlap), since this is the most efficient way.
Serializability is the major correctness criterion for concurrent transactions' executions.
It is considered the highest level of isolation between transactions, and plays an essential role in concurrency control.
As such it is supported in all general purpose database systems.
Strong strict two-phase locking (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

serialization
Is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later (possibly in a different computer environment).
When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object.
For many complex objects, such as those that make extensive use of references, this process is not straightforward.
Serialization of object-oriented objects does not include any of their associated methods with which they were previously linked.

This process of serializing an object is also called marshalling an object in some
situations.[2][3]
The opposite operation, extracting a data structure from a series of bytes, is deserialization, (also called unserialization or unmarshalling).

service level agreement
(SLA), is a commitment between a service provider and a client.
Particular aspects of the service – quality, availability, responsibilities – are agreed between the service provider and the service user.
The most common component of an SLA is that the services should be provided to the customer as agreed upon in the contract.
As an example, Internet service providers and telcos will commonly include service level agreements within the terms of their contracts with customers to define the level(s) of service being sold in plain language terms.
In this case the SLA will typically have a technical definition in  mean time between failures (MTBF), mean time to repair or mean time to recovery (MTTR); identifying which party is responsible for reporting faults or paying fees; responsibility for various data rates; throughput; jitter; or similar measurable details.

set
Is an abstract data type that can store unique values, without any particular order.
It is a computer implementation of the mathematical concept of a finite set.
Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.

soft computing

software
Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work.
This is in contrast to physical hardware, from which the system is built and actually performs the work.
In computer science and software engineering, computer software is all information processed by computer systems, programs and data.
Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media.
Computer hardware and software require each other and neither can be realistically used on its own.

software agent
Is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf.
Such "action on behalf of" implies the authority to decide which, if any, action is appropriate.
Agents are colloquially known as bots, from robot.
They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot
executing on a phone (e.g. Siri)  or other computing device.

Software agents may be autonomous or work together with other agents or people.

Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).

software construction
Is a software engineering discipline.
It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging.
It is linked to all the other software engineering disciplines, most strongly to software design and software testing.

software deployment
Is all of the activities that make a software system available for use.

software design
Is the process by which an agent creates a specification of a software artifact, intended to accomplish goals, using a set of primitive components and subject to constraints.
Software design may refer to either "all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems" or "the activity following requirements specification and before programming, as ...
[in] a stylized software engineering process."

software development
Is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components.
Software development is a process of writing and maintaining the source code, but in a broader sense, it includes all that is involved between the conception of the desired software through to the final manifestation of the software, sometimes in a planned and structured process.
Therefore, software development may include research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.

software development process
In software engineering, a software development process is the process of dividing software development work into distinct phases to improve design, product management, and project management.

It is also known as a software development life cycle (SDLC).

The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.

Most modern development processes can be vaguely described as agile.
Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.

software engineering
Is the systematic application of engineering approaches to the development of software.
Software engineering is a computing discipline.

software maintenance
In software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.

software prototyping
Is the activity of creating prototypes of software applications, i.e., incomplete versions of the software program being developed.
It is an activity that can occur in software development and is comparable to prototyping as known from other fields, such as mechanical engineering or manufacturing.

A prototype typically simulates only a few aspects of, and may be completely different from, the final product.

software requirements specification
(SRS), is a description of a software system to be  developed.
The software requirements specification lays out functional and non-functional requirements, and it may include a set of use cases that describe user interactions that the software must provide to the user for perfect interaction.

software testing
Is an investigation conducted to provide stakeholders with information about the quality of the software product or service under test.
Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation.
Test techniques include the process of executing a program or application with the intent of finding software bugs (errors or other defects), and verifying that the software product is fit for use.

sorting algorithm
Is an algorithm that puts elements of a list in a certain order.
The most frequently used orders are numerical order and lexicographical order.
Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists.
Sorting is also often useful for canonicalizing data and for producing human-readable output.
More formally, the output of any sorting algorithm must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (a reordering, yet retaining all of the original elements) of the input.

Further, the input data is often stored in an array, which allows random access, rather than a list, which only allows sequential access; though many algorithms can be applied to either type of data after suitable modification.

source code
In computing, source code is any collection of code, with or without comments, written using a human-readable programming language, usually as plain text.
The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code.
The source code is often transformed by an assembler or compiler into binary machine code that can be executed by the computer.
The machine code might then be stored for execution at a later time.
Alternatively, source code may be interpreted and thus immediately executed.

spiral model
Is a risk-driven software development process model.
Based on the unique risk patterns of a given project, the spiral model guides a team to adopt elements of one or more process models, such as incremental, waterfall, or evolutionary prototyping.

stack
Is an abstract data type that serves as a collection of elements, with two main principal operations:
push, which adds an element to the collection, and
pop, which removes the most recently added element that was not yet removed.

The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out).
Additionally, a peek operation may give access to the top without modifying the stack.
The name "stack" for this type of structure comes from the analogy to a set of physical items stacked on top of each other.
This structure makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.

state
In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

statement
In computer programming, a statement is a syntactic unit of an imperative programming language that expresses some action to be carried out.
A program written in such a language is formed by a sequence of one or more statements.
A statement may have internal components (e.g., expressions).

storage
Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data.
It is a core function and fundamental component of computers.

stream
Is a sequence of data elements made available over time.
A stream can be thought of as items on a conveyor belt being processed one at a time rather than in large batches.

string
In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable.
The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation).
A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding.
String may also denote more general arrays or other sequence (or list) data types and structures.

structured storage
A NoSQL (originally referring to "non-SQL" or "non-relational") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases.
Such databases have existed since the late 1960s, but the name "NoSQL" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies.
NoSQL databases are increasingly used in big data and real-time web applications.

NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.

subroutine
In computer programming, a subroutine is a sequence of program instructions that performs a specific task, packaged as a unit.
This unit can then be used in programs wherever that particular task should be performed.

Subroutines may be defined within programs, or separately in libraries that can be used by many programs.

In different programming languages, a subroutine may be called a routine, subprogram, function, method, or procedure.
Technically, these terms all have different definitions.
The generic, umbrella term  callable unit is sometimes used.

symbolic computation
In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects.
Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.

syntax
The syntax of a computer language is the set of rules that defines the combinations of symbols that are considered to be correctly structured statements or expressions in that language.
This applies both to programming languages, where the document represents source code, and to markup languages, where the document represents data.

syntax error
Is an error in the syntax of a sequence of characters or tokens that is intended to be written in compile-time.
A program will not compile until all syntax errors are corrected.
For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds.
There is some disagreement as to just what errors are "syntax errors".
For example, some would say that the use of an uninitialized variable's value in Java code is a syntax error, but many others would disagree and would classify this as a (static) semantic error.

system console
The system console, computer console, root console, operator's console, or simply console is the text entry and display device for system administration messages, particularly those from the BIOS or boot loader, the kernel, from the init system and from the system logger.
It is a physical device consisting of a keyboard and a screen, and traditionally is a text terminal, but may also be a graphical terminal.
System consoles are generalized to computer terminals, which are abstracted respectively by virtual consoles and terminal emulators.
Today communication with system consoles is generally done abstractly, via the standard streams (stdin, stdout, and stderr), but there may be system-specific interfaces, for example those used by the system kernel.

==
T ==
technical documentation
In engineering, any type of documentation that describes handling, functionality, and architecture of a technical product or a product under development or use.
The intended recipient for product technical documentation is both the (proficient) end user as well as the administrator/service or maintenance technician.
In contrast to a mere "cookbook" manual, technical documentation aims at providing enough information for a user to understand inner and outer dependencies of the product at hand.

third-generation programming language
A third-generation programming language (3GL) is a high-level computer programming language that tends to be more machine-independent and programmer-friendly than the machine code of the first-generation and assembly languages of the second-generation, while having a less specific focus to the fourth and fifth generations.
Examples of common and historical third-generation programming languages are ALGOL, BASIC, C, COBOL, Fortran, Java, and Pascal.

top-down and bottom-up design

tree
A widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.

type theory
In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics.
In type theory, every "term" has a "type" and operations are restricted to terms of a certain type.

==
U ==
upload
In computer networks, to send data to a remote system such as a server or another client so that the remote system can store a copy.
Contrast download.

Uniform Resource Locator (URL)
Colloquially web address.

A reference to a web resource that specifies its location on a computer network and a mechanism for retrieving it.
A URL is a specific type of Uniform Resource Identifier (URI), although many people use the two terms interchangeably.
URLs occur most commonly to reference web pages (http), but are also used for file transfer (ftp), email (mailto), database access (JDBC), and many other applications.

user
Is a person who utilizes a computer or network service.
Users of computer systems and software products generally lack the technical expertise required to fully understand how they work.
Power users use advanced features of programs, though they are not necessarily capable of computer programming and system administration.

user agent
Software (a software agent) that acts on behalf of a user, such as a web browser that "retrieves, renders and facilitates end user interaction with Web content".
An email reader is a mail user agent.

user interface (UI)
The space where interactions between humans and machines occur.
The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process.
Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls.
The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.

user interface design
Also user interface engineering.

The design of user interfaces for machines and software, such as computers, home appliances, mobile devices, and other electronic devices, with the focus on maximizing usability and the user experience.
The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design).

==
V ==
variable
In computer programming, a variable, or scalar, is a storage location (identified by a memory address) paired with an associated symbolic name (an identifier), which contains some known or unknown quantity of information referred to as a value.
The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context.
This separation of name and content allows the name to be used independently of the exact information it represents.
The identifier in computer source code can be bound to a value during run time, and the value of the variable may therefore change during the course of program execution.

virtual machine (VM)
An emulation of a computer system.
Virtual machines are based on computer architectures and attempt to provide the same functionality as a physical computer.
Their implementations may involve specialized hardware, software, or a combination of both.

V-Model
A software development process that may be considered an extension of the waterfall model, and is an example of the more general V-model.
Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape.
The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing.
The horizontal and vertical axes represent time or project completeness (left-to-right) and level of abstraction (coarsest-grain abstraction uppermost), respectively.

==
W ==
waterfall model
A breakdown of project activities into linear sequential phases, where each phase depends on the deliverables of the previous one and corresponds to a specialisation of tasks.

The approach is typical for certain areas of engineering design.
In software development, it tends to be among the less iterative and flexible approaches, as progress flows in largely one direction ("downwards" like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, deployment and maintenance.

Waveform Audio File Format
Also WAVE or WAV due to its filename extension.

An audio file format standard, developed by Microsoft and IBM, for storing an audio bitstream on PCs.
It is an application of the Resource Interchange File Format (RIFF) bitstream format method for storing data in "chunks", and thus is also close to the 8SVX and the AIFF format used on Amiga and Macintosh computers, respectively.
It is the main format used on Microsoft Windows systems for raw and typically uncompressed audio.
The usual bitstream encoding is the linear pulse-code modulation (LPCM) format.

web crawler
Also spider, spiderbot, or simply crawler.

An Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).

Wi-Fi
A family of wireless networking technologies, based on the IEEE 802.11 family of standards, which are commonly used for local area networking of devices and Internet access.
Wi‑Fi is a trademark of the non-profit Wi-Fi Alliance, which restricts the use of the term Wi-Fi Certified to products that successfully complete interoperability certification testing.

==
X ==
XHTML
Abbreviaton of eXtensible HyperText Markup Language.

Part of the family of XML markup languages.
It mirrors or extends versions of the widely used HyperText Markup Language (HTML), the language in which web pages are formulated.

== See also ==
Outline of computer science


==
References ==


== Notes ==
The Dunning–Kruger effect is a hypothetical cognitive bias stating that people with low ability at a task overestimate their ability.

As described by social psychologists David Dunning and Justin Kruger, the bias results from an internal illusion in people of low ability and from an external misperception in people of high ability; that is, "the miscalibration of the incompetent stems from an error about the self, whereas the miscalibration of the highly competent stems from an error about others".
It is related to the cognitive bias of illusory superiority and comes from people's inability to recognize their lack of ability.
Without the self-awareness of metacognition, people cannot objectively evaluate their level of competence.

The effect, or Dunning and Kruger's original explanation for the effect, has been challenged by mathematical analyses and comparisons across cultures.

==
Original study ==
The psychological phenomenon of illusory superiority was identified as a form of cognitive bias in Kruger and Dunning's 1999 study "Unskilled and Unaware of It:
How Difficulties in Recognizing One's Own Incompetence Lead to Inflated Self-Assessments".
An example derived from cognitive bias evident in the criminal case of McArthur Wheeler, who, on April 19, 1995, robbed two banks while his face was covered with lemon juice, which he believed would make him invisible to the surveillance cameras.
This belief was apparently based on his misunderstanding of the chemical properties of lemon juice as an invisible ink.
Other investigations of the phenomenon, such as "Why People Fail to Recognize Their Own Incompetence", indicate that much incorrect self-assessment of competence derives from the person's ignorance of a given activity's standards of performance.
Dunning and Kruger's research also indicates that training in a task, such as solving a logic puzzle, increases people's ability to accurately evaluate how good they are at it.
In Self-insight: Roadblocks and Detours on the Path to Knowing Thyself, Dunning described the Dunning–Kruger effect as "the anosognosia of everyday life", referring to a neurological condition in which a disabled person either denies or seems unaware of his or her disability.
He stated: "If you're incompetent, you can't know you're incompetent ...
The skills you need to produce a right answer are exactly the skills you need to recognize what a right answer is.
"In
2011, Dunning wrote about his observations that people with substantial, measurable deficits in their knowledge or expertise lack the ability to recognize those deficits and, therefore, despite potentially making error after error, tend to think they are performing competently when they are not: "In short, those who are incompetent, for lack of a better term, should have little insight into their incompetence—an assertion that has come to be known as the Dunning–Kruger effect".
In 2014, Dunning and Helzer described how the Dunning–Kruger effect "suggests that poor performers are not in a position to recognize the shortcomings in their performance".

==
Later studies ==
Dunning and Kruger tested the hypotheses of the cognitive bias of illusory superiority on undergraduate students of introductory courses in psychology by examining the students' self-assessments of their intellectual skills in inductive, deductive, and abductive logical reasoning, English grammar, and personal sense of humor.
After learning their self-assessment scores, the students were asked to estimate their ranks in the psychology class.
The competent students underestimated their class rank, and the incompetent students overestimated theirs, but the incompetent students did not estimate their class rank as higher than the ranks estimated by the competent group.
Across four studies, the research indicated that the study participants who scored in the bottom quartile on tests of their sense of humor, knowledge of grammar, and logical reasoning, overestimated their test performance and their abilities; despite test scores that placed them in the 12th percentile, the participants estimated they ranked in the 62nd percentile.
Moreover, competent students tended to underestimate their own competence, because they erroneously presumed that tasks easy for them to perform were also easy for other people to perform.
Incompetent students improved their ability to estimate their class rank correctly after receiving minimal tutoring in the skills they previously lacked, regardless of any objective improvement gained in said skills of perception.
The 2004 study "Mind-Reading and Metacognition: Narcissism, not Actual Competence, Predicts Self-estimated Ability" extended the cognitive-bias premise of illusory superiority to test subjects' emotional sensitivity toward other people and their own perceptions of other people.

The 2003 study "
How Chronic Self-Views Influence (and Potentially Mislead)
Estimates of Performance" indicated a shift in the participants' view of themselves when influenced by external cues.
The participants' knowledge of geography was tested; some tests were intended to affect the participants' self-view positively, and some were intended to affect it negatively.
The participants then were asked to rate their performances; the participants given tests with a positive intent reported better performance than did the participants given tests with a negative intent.

To test Dunning and Kruger's hypotheses "that people, at all performance levels, are equally poor at estimating their relative performance", the 2006 study "Skilled or Unskilled, but Still Unaware of It: How Perceptions of Difficulty Drive Miscalibration in Relative Comparisons" investigated three studies that manipulated the "perceived difficulty of the tasks, and, hence, [the] participants' beliefs about their relative standing".
The investigation indicated that when the experimental subjects were presented with moderately difficult tasks, there was little variation among the best performers and the worst performers in their ability to predict their performance accurately.
With more difficult tasks, the best performers were less accurate in predicting their performance than were the worst performers.
Therefore, judges at all levels of skill are subject to similar degrees of error in the performance of tasks.

In testing alternative explanations for the cognitive bias of illusory superiority, the 2008 study "
Why the Unskilled are Unaware: Further Explorations of (Absent) Self-insight Among the Incompetent" reached the same conclusions as previous studies of the Dunning–Kruger effect: that, in contrast to high performers, "poor performers do not learn from feedback suggesting a need to improve".

One 2020 study suggests that individuals of relatively high social class are more overconfident than lower-class individuals.

==
Mathematical critique ==
Dunning and Kruger describe a common cognitive bias and make quantitative assertions that rest on mathematical arguments.
But their findings are often misinterpreted, misrepresented, and misunderstood.
According to Tal Yarkoni:
Their studies categorically didn’t show that incompetent people are more confident or arrogant than competent people.
What they did show is [that] people in the top quartile for actual performance think they perform better than the people in the second quartile, who in turn think they perform better than the people in the third quartile, and so on.
So the bias is definitively not that incompetent people think they’re better than competent people.
Rather, it’s that incompetent people think they’re much better than they actually are.
But they typically still don’t think they’re quite as good as people who, you know, actually are good.
(
It’s important to note that Dunning and Kruger never claimed to show that the unskilled think they’re better than the skilled;
that’s just the way the finding is often interpreted by others.)
===
Paired measures ===
Mathematically, the effect relies on the quantifying of paired measures consisting of (a) the measure of the competence people can demonstrate when put to the test (actual competence) and (b) the measure of competence people believe that they have (self-assessed competence).
Researchers express the measures either as percentages or as percentile scores scaled from 0 to 1 or from 0 to 100.
By convention, researchers express the differences between the two measures as self-assessed competence minus actual competence.
In this convention, negative numbers signify erring toward underconfidence, positive numbers signify erring toward overconfidence, and zero signifies accurate self-assessment.

A 2008 study by Joyce Ehrlinger summarized the major assertions of the effect that first appeared in the 1999 seminal article and continued to be supported by many studies after nine years of research: "People are typically overly optimistic when evaluating the quality of their performance on social and intellectual tasks.
In particular, poor performers grossly overestimate their performances".
The effect asserts that most people are overconfident about their abilities, and that the least competent people are the most overconfident.
Support for both assertions rests upon interpreting the patterns produced from graphing the paired measures.

The most common graphical convention is the Kruger–Dunning-type graph used in the seminal article.
It depicted college students' accuracy in self-assessing their competencies in humor, logical reasoning, and grammar.
Researchers adopted that convention in subsequent studies of the effect.
Additional graphs used by other researchers, who argued for the legitimacy of the effect include (y–x) versus (x) cross plots and bar charts.
The first two of these studies depicted college students' accuracy in self-assessing their competence in introductory chemistry, and the third depicted their accuracy in self-assessing their competence in business classes.

In a study published in 2016, researchers who focused on the mathematical reasoning behind the effect studied 1,154 participants' ability to self-assess their competence in understanding the nature of science.
These researchers graphed their data in all the earlier articles' various conventions and explained how the numerical reasoning used to argue for the effect is similar in all.
When graphed in these established conventions, the researchers' data also supported the effect.
Had the researchers ended their study at this point, their results would have added to the established consensus that validated the effect.
But their deeper analyses led them to conclude that the numerical procedures used repeatedly in all previous work were the likely sources of misleading conclusions, driven by ceiling/floor effects (exacerbated by measurement error) causing censoring.

To expose the sources of the misleading conclusions, the researchers employed their own real data set of paired measures from 1,154 participants and created a second simulated data set that employed random numbers to simulate random guessing by an equal number of simulated participants.
The simulated data set contained only random noise, without any measures of human behavior.

The researchers then used the simulated data set and the graphical conventions of the behavioral scientists to produce patterns like those described as validating the Dunning–Kruger effect.
They traced the origin of the patterns, not to the dominant literature's claimed psychological disposition of humans, but instead to the nature of graphing data bounded by limits of 0 and 100 and the process of ordering and grouping the paired measures to create the graphs.
These patterns are mathematical artifacts that random noise devoid of any human influence can produce.
They further showed that the graphs used to establish the effect in three of the four case examples presented in the seminal article are patterns characteristic of purely random noise.
These patterns are numerical artifacts that behavioral scientists and educators seem to have interpreted as evidence for a human psychological disposition toward overconfidence.

But the graphic presented on the case study on humor in the seminal article and
the Numeracy researchers' real data were not the patterns of purely random noise.
Although the data was noisy, that human-derived data exhibited some order that could not be attributed to random noise.
The researchers attributed it to human influence and called it the "self-assessment
signal".
The researchers went on to characterize the signal and worked to determine what human disposition it revealed.
To do so, they employed different kinds of graphics that suppress or eliminate the noise responsible for most of the artifacts and distortions.
The authors discovered that the different graphics refuted the assertions made for the effect.
Instead, they showed that most people are reasonably accurate in their self-assessments.
About half the 1,154 participants in their studies accurately estimated their performance within 10 percentage points (ppts).
Two-thirds of them self-assessed their competency scores within 15 ppts.
Only about 6% of participants displayed wild overconfidence and were unable to accurately self-assess their abilities within 30 ppts.
All groups overestimated and underestimated their actual ability with equal frequency.
No marked tendency toward overconfidence, as predicted by the effect, occurs, even in the most novice groups.
In 2020, with an updated database of over 5,000 participants, this still held true.
The revised mathematical interpretation of data confirmed that people typically have no pronounced tendency to overestimate their actual proficiency.

===
Group self-assessment ===
Groups' mean self-assessments prove more than an order of magnitude more accurate than do individuals'.
In randomly selected groups of 50 participants, 81% of groups' self-assessed mean scores were within 3 ppts of their actual mean competency score.
The discovery that groups of people are accurate in their self-assessments opens an entirely new way to study groups of people with respect to paired measures of cognitive competence and affective self-assessed competence.
A third Numeracy article by these researchers reports from a database of over 3000 participants to illuminate the effects of privilege on different ethnic and gender groups of college students.
The article confirms that minority groups are on average less privileged and score lower in the cognitive test scores and self-assessed confidence ratings on the instruments used in this research.
They verified that women on average self-assessed more accurately than men, and did so across all ethnic groups that had sufficient representation in the researchers' database.

==
Cultural differences in self-perception ==
Studies of the Dunning–
Kruger effect usually have been of North Americans, but studies of Japanese people suggest that cultural forces have a role in the occurrence of the effect.
The 2001 study "Divergent Consequences of Success and Failure in Japan and North America:
An Investigation of Self-improving Motivations and Malleable Selves" indicated that Japanese people tended to underestimate their abilities and to see underachievement (failure) as an opportunity to improve their abilities at a given task, thereby increasing their value to the social group.

==
Popular recognition ==
In 2000, Kruger and Dunning were awarded a satiric Ig Nobel Prize in recognition of the scientific work recorded in "their modest report".

"The Dunning–Kruger Song" is part of The Incompetence Opera, a mini-opera that premiered at the Ig Nobel Prize ceremony in 2017.
The mini-opera is billed as "a musical encounter with the Peter principle and the Dunning–Kruger Effect".

== See also ==


==
References ==


==
Further reading ==
Dunning, David (27 October 2014).
"
We Are All Confident Idiots".
Pacific Standard.
The Social Justice Foundation.
Retrieved 28 October 2014.

==
External links ==
Gallagher, Brian (23 April 2020).
"
The Case for Professors of Stupidity: Why aren't there more people studying the science behind stupidity?".
Nautilus Pocket worthy Stories to fuel your mind.
HAL 9000 is a fictional artificial intelligence character and the main antagonist in Arthur C. Clarke's Space Odyssey series.

First appearing in the 1968 film 2001:
A Space Odyssey, HAL (Heuristically programmed ALgorithmic computer) is a sentient artificial general intelligence computer that controls the systems of the Discovery One spacecraft and interacts with the ship's astronaut crew.
While part of HAL's hardware is shown toward the end of the film, he is mostly depicted as a camera lens containing a red or yellow dot, instances of which are located throughout the ship.
HAL 9000 is voiced by Douglas Rain in the two feature film adaptations of the Space Odyssey series.
HAL speaks in a soft, calm voice and a conversational manner, in contrast to the crewmen, David Bowman and Frank Poole.

In the film, HAL became operational on 12 January 1992 at the HAL Laboratories in Urbana, Illinois as production number 3.
The activation year was 1991 in earlier screenplays and changed to 1997 in Clarke's novel written and released in conjunction with the movie.
In addition to maintaining the Discovery One spacecraft systems during the interplanetary mission to Jupiter (or Saturn in the novel), HAL is capable of speech, speech recognition, facial recognition, natural language processing, lip reading, art appreciation, interpreting emotional behaviours, automated reasoning, spacecraft piloting and playing chess.

==
Appearances ==


=== 2001:
A Space Odyssey (film/novel) ===
HAL became operational in Urbana, Illinois, at the HAL Plant (the University of Illinois's  Coordinated Science Laboratory, where the ILLIAC computers were built).
The film says this occurred in 1992, while the book gives 1997 as HAL's birth year.
In 2001: A Space Odyssey (1968), HAL is initially considered a dependable member of the crew, maintaining ship functions and engaging genially with his human crew-mates on an equal footing.
As a recreational activity, Frank Poole plays chess against HAL.
In the film, the artificial intelligence is shown to triumph easily.
However, as time progresses, HAL begins to malfunction in subtle ways and, as a result, the decision is made to shut down HAL in order to prevent more serious malfunctions.
The sequence of events and manner in which HAL is shut down differs between the novel and film versions of the story.
In the aforementioned game of chess HAL makes minor and undetected mistakes in his analysis, a possible foreshadowing to HAL's malfunctioning.

In the film, astronauts David Bowman and Frank Poole consider disconnecting HAL's cognitive circuits when he appears to be mistaken in reporting the presence of a fault in the spacecraft's communications antenna.
They attempt to conceal what they are saying, but are unaware that HAL can read their lips.
Faced with the prospect of disconnection, HAL decides to kill the astronauts in order to protect and continue his programmed directives.
HAL uses one of the Discovery's EVA pods to kill Poole while he is repairing the ship.
When Bowman, without a space helmet, uses another pod to attempt to rescue Poole, HAL locks him out of the ship, then disconnects the life support systems of the other hibernating crew members.
Bowman circumvents HAL's control, entering the ship by manually opening an emergency airlock with his service pod's clamps, detaching the pod door via its explosive bolts.
Bowman jumps across empty space, reenters Discovery, and quickly re-pressurizes the airlock.

While HAL's motivations are ambiguous in the film, the novel explains that the computer is unable to resolve a conflict between his general mission to relay information accurately, and orders specific to the mission requiring that he withhold from Bowman and Poole the true purpose of the mission.
(
This withholding is considered essential after the findings of a fictional 1989 psychological experiment, Project BARSOOM, where humans were made to believe that there had been alien contact.
In every person tested, a deep-seated xenophobia was revealed, which was unknowingly replicated in HAL's constructed personality.
Mission Control did not want the crew of Discovery to have their thinking compromised by the knowledge that alien contact was already real.)
With the crew dead, HAL reasons, he would not need to lie to them.

In the novel, the orders to disconnect HAL come from Dave and Frank's superiors on Earth.
After Frank is killed while attempting to repair the communications antenna he is pulled away into deep space using the safety tether which is still attached to both the pod and Frank Poole's spacesuit.
Dave begins to revive his hibernating crew mates, but is foiled when HAL vents the ship's atmosphere into the vacuum of space, killing the awakening crew members and almost killing Bowman, who is only narrowly saved when he finds his way to an emergency chamber which has its own oxygen supply and a spare space suit inside.

In both versions, Bowman then proceeds to shut down the machine.
In the film, HAL's central core is depicted as a crawlspace full of brightly lit computer modules mounted in arrays from which they can be inserted or removed.
Bowman shuts down HAL by removing modules from service one by one; as he does so, HAL's consciousness degrades.
HAL finally reverts to material that was programmed into him early in his memory, including announcing the date he became operational as 12 January 1992 (in the novel, 1997).
When HAL's logic is completely gone, he begins singing the song "Daisy Bell" and starts slowing down and changing pitch similar to an old electronic game running low on batteries
(in actuality, the first song sung by a computer, which Clarke had earlier observed at a text-to-speech demonstration).
HAL's final act of any significance is to prematurely play a prerecorded message from Mission Control which reveals the true reasons for the mission to Jupiter.

=== 2010:
Odyssey Two (novel) and 2010:
The Year We Make Contact (film) ===
In the 1982 novel 2010: Odyssey Two written by Clark, HAL is restarted by his creator, Dr. Chandra, who arrives on the Soviet spaceship Leonov.

Prior to leaving Earth, Dr. Chandra has also had a discussion with HAL's twin, SAL 9000.
Like HAL, SAL was created by Dr. Chandra.
Whereas HAL was characterized as being "male", SAL is characterized as being "female" (voiced by Candice Bergen) and is represented by a blue camera eye instead of a red one.

Dr. Chandra discovers that HAL's crisis was caused by a programming contradiction: he was constructed for "the accurate processing of information without distortion or concealment", yet his orders, directly from Dr. Heywood Floyd at the National Council on Astronautics, required him to keep the discovery of the Monolith TMA-1 a secret for reasons of national security.
This contradiction created a "Hofstadter-Moebius loop", reducing HAL to paranoia.
Therefore, HAL made the decision to kill the crew, thereby allowing him to obey both his hardwired instructions to report data truthfully and in full, and his orders to keep the monolith a secret.
In essence: if the crew were dead, he would no longer have to keep the information secret.

The alien intelligence initiates a terraforming scheme, placing the Leonov, and everybody in it, in danger.
Its human crew devises an escape plan which unfortunately requires leaving the Discovery and HAL behind to be destroyed.
Dr. Chandra explains the danger, and HAL willingly sacrifices himself so that the astronauts may escape safely.
In the moment of his destruction the monolith-makers transform HAL into a non-corporeal being so that David Bowman's avatar may have a companion.

The details in the novel and the 1984 film 2010:
The Year We Make
Contact are nominally the same, with a few exceptions.
First, in contradiction to the book (and events described in both book and film versions of 2001: A Space Odyssey), Heywood Floyd is absolved of responsibility for HAL's condition; it is asserted that the decision to program HAL with information concerning TMA-1 came directly from the White House.

In the film, HAL functions normally after being reactivated, while in the book it is revealed that his mind was damaged during the shutdown, forcing him to begin communication through screen text.
Also, in the film the Leonov crew initially lies to HAL about the dangers that he faced (suspecting that if he knew he would be destroyed he would not initiate the engine burn necessary to get the Leonov back home), whereas in the novel he is told at the outset.
However, in both cases the suspense comes from the question of what HAL will do when he knows that he may be destroyed by his actions.

In the novel, the basic reboot sequence initiated by Dr. Chandra is quite long, while the movie uses a shorter sequence voiced from HAL as: "HELLO_DOCTOR_NAME_CONTINUE_YESTERDAY_TOMORROW".

While Curnow tells Floyd that Dr. Chandra has begun designing HAL 10000, it has not been mentioned in subsequent novels.

===
2061:
Odyssey Three and 3001:
The Final Odyssey ===
In Clarke's 1987 novel 2061: Odyssey Three, Heywood Floyd is surprised to encounter HAL, now stored alongside Dave Bowman in the Europa monolith.

In Clarke's 1997 novel 3001:
The Final Odyssey, Frank Poole is introduced to the merged form of Dave Bowman and HAL, the two merging into one entity called "Halman" after Bowman rescued HAL from the dying Discovery One spaceship toward the end of 2010:
Odyssey Two.
==
Concept and creation ==
Clarke noted that the first film was criticized for not having any characters except for HAL, and that a great deal of the establishing story on Earth was cut from the film (and even from Clarke's novel).

Clarke stated that he had considered Autonomous Mobile Explorer–5 as a name for the computer, then decided on Socrates when writing early drafts, switching in later drafts to Athena, a computer with a female personality, before settling on HAL 9000.
The Socrates name was later used in Clarke and Stephen Baxter's A Time Odyssey novel series.

The earliest draft depicted Socrates as a roughly humanoid robot, and is introduced as overseeing Project Morpheus, which studied prolonged hibernation in preparation for long term space flight.

As a demonstration to Senator Floyd, Socrates' designer, Dr. Bruno Forster, asks Socrates to turn off the oxygen to hibernating subjects Kaminski and Whitehead, which Socrates refuses, citing Asimov's First Law of Robotics.
In a later version, in which Bowman and Whitehead are the non-hibernating crew of Discovery, Whitehead dies outside the spacecraft after his pod collides with the main antenna, tearing it free.
This triggers the need for Bowman to revive Poole, but the revival does not go according to plan, and after briefly awakening, Poole dies.
The computer, named Athena in this draft, announces "All systems of Poole now
No–Go.
It will be necessary to replace him with a spare unit.
"
After this, Bowman decides to go out in a pod and retrieve the antenna, which is moving away from the ship.
Athena refuses to allow him to leave the ship, citing "Directive 15" which prevents it from being left unattended, forcing him to make program modifications during which time the antenna drifts further.
During rehearsals Kubrick asked Stefanie Powers to supply the voice of HAL 9000 while searching for a suitably androgynous voice so the actors had something to react to.
On the set, British actor Nigel Davenport played HAL.
When it came to dubbing HAL in post-production, Kubrick had originally cast Martin Balsam, but as he felt Balsam "just sounded a little bit too colloquially American", he was replaced with Douglas Rain, who "had the kind of bland mid-Atlantic accent we felt was right for the part".
Rain was only handed HAL's lines instead of the full script, and recorded them across a day and a half.
HAL's point of view shots were created with a Cinerama Fairchild-Curtis wide-angle lens with a 160° angle of view.
This lens is about 8 inches (20 cm) in diameter, while HAL's on set prop eye lens is about 3 inches (7.6 cm) in diameter.
Stanley Kubrick chose to use the large Fairchild-Curtis lens to shoot the HAL 9000 POV shots because he needed a wide-angle fisheye lens that would fit onto his shooting camera, and this was the only lens at the time that would work.
The Fairchild-Curtis lens has a focal length of 23 mm (0.9 in) with a maximum aperture of f/2.0 and a weight of approximately 30 lb (14 kg); it was originally designed by Felix Bednarz with a maximum aperture of f/2.2
for the first Cinerama 360 film, Journey to the Stars, shown at the 1962 Seattle World's Fair.
Bednarz adapted the lens design from an earlier lens he had designed for military training to simulate human peripheral vision coverage.
The lens was later recomputed for the second Cinerama 360 film To the Moon and Beyond, which had a slightly different film format.
To the Moon and Beyond was produced by Graphic Films and shown at the 1964/1965 New York World's Fair, where Kubrick watched it; afterwards, he was so impressed that he hired the same creative team from Graphic Films (consisting of Douglas Trumbull, Lester Novros, and Con Pederson) to work on 2001.A HAL 9000 face plate, without lens (not the same as the hero face plates seen in the film), was discovered in a junk shop in Paddington, London, in the early 1970s by Chris Randall.
This was found along with the key to HAL's Brain Room.
Both items were purchased for ten shillings (£0.50).
Research revealed that the original lens was a Fisheye Nikkor 8 mm f/8.
The collection was sold at a Christie's auction in 2010 for £17,500 to film director Peter Jackson.

===
Origin of name ===
HAL's name, according to writer Arthur C. Clarke, is derived from Heuristically programmed ALgorithmic computer.
After the film was released, fans noticed HAL was a one-letter shift from the name IBM and there has been much speculation since then that this was a dig at the large computer company, something that has been denied by both Clarke and 2001 director Stanley Kubrick.
Clarke addressed the issue in his book The Lost Worlds of 2001:

 ...about once a week some character spots the fact that HAL is one letter ahead of IBM, and promptly assumes that Stanley and I were taking a crack at the estimable institution ...
As it happened, IBM had given us a good deal of help, so we were quite embarrassed by this, and would have changed the name
had we spotted the coincidence.

IBM was consulted during the making of the film and their logo can be seen on props in the film including the Pan Am Clipper's cockpit instrument panel and on the lower arm keypad on Poole's space suit.
During production it was brought to IBM's attention that the film's plot included a homicidal computer but they approved association with the film if it was clear any "equipment failure" was not related to their products.
HAL Communications Corporation is a real corporation, with facilities located in Urbana, Illinois, which is where HAL in the movie identifies himself as being activated: "I am a HAL 9000 computer.
I became operational at the H-A-L plant in Urbana Illinois on the 12th of January 1992."The former president of HAL Communications, Bill Henry, has stated that this is a coincidence:
"There was not and never has been any connection to 'Hal', Arthur Clarke's intelligent computer in the screen play '2001' — later published as a book.
We were very surprised when the movie hit the Coed Theatre on campus and discovered that the movie's computer had our name.
We never had any problems with that similarity - 'Hal' for the movie and 'HAL' (all caps) for our small company.
But, from time-to-time, we did have issues with others trying to use 'HAL'.
That resulted in us paying lawyers.
The offenders folded or eventually went out of business."
===
Influences ===
The scene in which HAL's consciousness degrades was inspired by Clarke's memory of a speech synthesis demonstration by physicist John Larry Kelly, Jr., who used an IBM 704 computer to synthesize speech.
Kelly's voice recorder synthesizer vocoder recreated the song "Daisy Bell", with musical accompaniment from Max Mathews.
HAL's capabilities, like all the technology in 2001, were based on the speculation of respected scientists.
Marvin Minsky, director of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and one of the most influential researchers in the field, was an adviser on the film set.
In the mid-1960s, many computer scientists in the field of artificial intelligence were optimistic that machines with HAL's capabilities would exist within a few decades.
For example, AI pioneer Herbert A. Simon at Carnegie Mellon University, had predicted in 1965 that "machines will be capable, within twenty years, of doing any work a man can do", the overarching premise being that the issue was one of computational speed (which was predicted to increase) rather than principle.

==
Cultural impact ==
HAL is listed as the 13th-greatest film villain in the AFI's 100 Years...100 Heroes & Villains.
The 9000th of the asteroids in the asteroid belt, 9000 Hal, discovered on May 3, 1981 by E. Bowell at Anderson Mesa Station, is named after HAL 9000.

== See also ==
List of fictional computers
ILLIAC (University of Illinois at Urbana–Champaign)
National Center for Supercomputing Applications (University of Illinois at Urbana–Champaign)
Poole versus HAL 9000 (details of chess game played by Frank Poole and HAL 9000)
Jipi and the Paranoid Chip
AI control problem


==
References ==


==
External links ==
Text excerpts from HAL 9000 in 2001:
A Space Odyssey
HAL's Legacy, on-line ebook (mostly full-text) of the printed version edited by David G. Stork, MIT Press, 1997, ISBN 0-262-69211-2, a collection of essays on HAL
HAL's Legacy, An Interview with Arthur C. Clarke.

The case for HAL's sanity by Clay Waldrop
2001 fills the theater at HAL 9000's "birthday" in 1997 at the University of Illinois at Urbana–Champaign
3D rendering is the 3D computer graphics process of converting 3D models into 2D images on a computer.
3D renders may include photorealistic effects or non-photorealistic styles.

==
Rendering methods ==
Rendering is the final process of creating the actual 2D image or animation from the prepared scene.
This can be compared to taking a photo or filming the scene after the setup is finished in real life.
Several different, and often specialized, rendering methods have been developed.
These range from the distinctly non-realistic wireframe rendering through polygon-based rendering, to more advanced techniques such as: scanline rendering, ray tracing, or radiosity.
Rendering may take from fractions of a second to days for a single image/frame.
In general, different methods are better suited for either photorealistic rendering, or real-time rendering.

==
Real-time ==
Rendering for interactive media, such as games and simulations, is calculated and displayed in real time, at rates of approximately 20 to 120 frames per second.
In real-time rendering, the goal is to show as much information as possible as the eye can process in a fraction of a second (a.k.a. "
in one frame": In the case of a 30 frame-per-second animation, a frame encompasses one 30th of a second).

The primary goal is to achieve an as high as possible degree of photorealism at an acceptable minimum rendering speed (usually 24 frames per second, as that is the minimum the human eye needs to see to successfully create the illusion of movement).
In fact, exploitations can be applied in the way the eye 'perceives' the world, and as a result, the final image presented is not necessarily that of the real world, but one close enough for the human eye to tolerate.

Rendering software may simulate such visual effects as lens flares, depth of field or motion blur.
These are attempts to simulate visual phenomena resulting from the optical characteristics of cameras and of the human eye.
These effects can lend an element of realism to a scene, even if the effect is merely a simulated artifact of a camera.
This is the basic method employed in games, interactive worlds and VRML.

The rapid increase in computer processing power has allowed a progressively higher degree of realism even for real-time rendering, including techniques such as HDR rendering.

Real-time rendering is often polygonal and aided by the computer's GPU.

==
Non real-time ==
Animations for non-interactive media, such as feature films and video, can take much more time to render.
Non real-time rendering enables the leveraging of limited processing power in order to obtain higher image quality.
Rendering times for individual frames may vary from a few seconds to several days for complex scenes.
Rendered frames are stored on a hard disk, then transferred to other media such as motion picture film or optical disk.
These frames are then displayed sequentially at high frame rates, typically 24, 25, or 30 frames per second (fps), to achieve the illusion of movement.

When the goal is photo-realism, techniques such as ray tracing, path tracing, photon mapping or radiosity are employed.

This is the basic method employed in digital media and artistic works.

Techniques have been developed for the purpose of simulating other naturally occurring effects, such as the interaction of light with various forms of matter.
Examples of such techniques include particle systems (which can simulate rain, smoke, or fire), volumetric sampling (to simulate fog, dust and other spatial atmospheric effects), caustics (to simulate light focusing by uneven light-refracting surfaces, such as the light ripples seen on the bottom of a swimming pool), and subsurface scattering (to simulate light reflecting inside the volumes of solid objects, such as human skin).

The rendering process is computationally expensive, given the complex variety of physical processes being simulated.
Computer processing power has increased rapidly over the years, allowing for a progressively higher degree of realistic rendering.
Film studios that produce computer-generated animations typically make use of a render farm to generate images in a timely manner.
However, falling hardware costs mean that it is entirely possible to create small amounts of 3D animation on a home computer system.
The output of the renderer is often used as only one small part of a completed motion-picture scene.

Many layers of material may be rendered separately and integrated into the final shot using compositing software.

==
Reflection and shading models ==
Models of reflection/scattering and shading are used to describe the appearance of a surface.

Although these issues may seem like problems all on their own, they are studied almost exclusively within the context of rendering.

Modern 3D computer graphics rely heavily on a simplified reflection model called the Phong reflection model (not to be confused with Phong shading).
In the refraction of light, an important concept is the refractive index; in most 3D programming implementations, the term for this value is "index of refraction" (usually shortened to IOR).

Shading can be broken down into two different techniques, which are often studied independently:

Surface shading - how light spreads across a surface (mostly used in scanline rendering for real-time 3D rendering in video games)
Reflection/scattering - how light interacts with a surface at a given point (mostly used in ray-traced renders for non real-time photorealistic and artistic 3D rendering in both CGI still 3D images and CGI non-interactive 3D animations)


===
Surface shading algorithms ===
Popular surface shading algorithms in 3D computer graphics include:

Flat shading: a technique that shades each polygon of an object based on the polygon's "normal" and the position and intensity of a light source
Gouraud shading: invented by H. Gouraud in 1971; a fast and resource-conscious vertex shading technique used to simulate smoothly shaded surfaces
Phong shading: invented by Bui Tuong Phong; used to simulate specular highlights and smooth shaded surfaces


===
Reflection ===
Reflection or scattering is the relationship between the incoming and outgoing illumination at a given point.

Descriptions of scattering are usually given in terms of a bidirectional scattering distribution function or BSDF.

===
Shading ===
Shading addresses how different types of scattering are distributed across the surface (i.e.,
which scattering function applies where).

Descriptions of this kind are typically expressed with a program called a shader.
A simple example of shading is texture mapping, which uses an image to specify the diffuse color at each point on a surface, giving it more apparent detail.

Some shading techniques include:
Bump mapping:
Invented by Jim Blinn, a normal-perturbation technique used to simulate wrinkled surfaces.

Cel shading:
A technique used to imitate the look of hand-drawn animation.
===
Transport ===
Transport describes how illumination in a scene gets from one place to another.

Visibility is a major component of light transport.

===
Projection ===
The shaded three-dimensional objects must be flattened so that the display device - namely a monitor - can display it in only two dimensions, this process is called 3D projection.

This is done using projection and, for most applications, perspective projection.
The basic idea behind perspective projection is that objects that are further away are made smaller in relation to those that are closer to the eye.

Programs produce perspective by multiplying a dilation constant raised to the power of the negative of the distance from the observer.

A dilation constant of one means that there is no perspective.

High dilation constants can cause a "fish-eye" effect in which image distortion begins to occur.
Orthographic projection is used mainly in CAD or CAM applications where scientific modeling requires precise measurements and preservation of the third dimension.

== See also ==
Architectural rendering
Ambient occlusion
Computer vision
Geometry pipeline
Geometry processing
Graphics
Graphics processing unit (GPU)
Graphical output devices
Image processing
Industrial CT scanning
Painter's algorithm
Parallel rendering
Reflection (computer graphics)
SIGGRAPH
Volume rendering


== Notes and references ==


==
External links ==
How Stuff Works - 3D Graphics
History of Computer Graphics series of articles (Wayback Machine copy)
In information technology and computer science, a system is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.

The set of states a system can occupy is known as its state space.
In a discrete system, the state space is countable and often finite.
The system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state.
Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers.

The output of a digital circuit or deterministic computer program at any time is completely determined by its current inputs and its state.

==
Digital logic circuit state ==
Digital logic circuits can be divided into two types: combinational logic, whose output signals are dependent only on its present input signals, and sequential logic, whose outputs are a function of both the current inputs and the past history of inputs.
In sequential logic, information from past inputs is stored in electronic memory elements, such as flip-flops.
The stored contents of these memory elements, at a given point in time, is collectively referred to as the circuit's state and contains all the information about the past to which the circuit has access.
Since each binary memory element, such as a flip-flop, has only two possible states, one or zero, and there is a finite number of memory elements, a digital circuit has only a certain finite number of possible states.
If N is the number of binary memory elements in the circuit, the maximum number of states a circuit can have is 2N.


==
Program state ==
Similarly, a computer program stores data in variables, which represent storage locations in the computer's memory.
The contents of these memory locations, at any given point in the program's execution, is called the program's state.
A more specialized definition of state is used for computer programs that operate serially or sequentially on streams of data, such as parsers, firewalls, communication protocols and encryption.
Serial programs operate on the incoming data characters or packets sequentially, one at a time.
In some of these programs, information about previous data characters or packets received is stored in variables and used to affect the processing of the current character or packet.
This is called a stateful protocol and the data carried over from the previous processing cycle is called the state.
In others, the program has no information about the previous data stream and starts fresh with each data input; this is called a stateless protocol.

Imperative programming is a programming paradigm (way of designing a programming language) that describes computation in terms of the program state, and of the statements which change the program state.
In declarative programming languages, the program describes the desired results and doesn't specify changes to the state directly.

==
Finite state machines ==
The output of a sequential circuit or computer program at any time is completely determined by its current inputs and current state.
Since each binary memory element has only two possible states, 0 or 1, the total number of different states a circuit can assume is finite, and fixed by the number of memory elements.
If there are N binary memory elements, a digital circuit can have at most 2N distinct states.
The concept of state is formalized in an abstract mathematical model of computation called a finite state machine, used to design both sequential digital circuits and computer programs.

== Examples ==
An example of an everyday device that has a state is a television set.
To change the channel of a TV, the user usually presses a "channel up" or "channel down" button on the remote control, which sends a coded message to the set.
In order to calculate the new channel that the user desires, the digital tuner in the television must have stored in it the number of the current channel it is on.
It then adds one or subtracts one from this number to get the number for the new channel, and adjusts the TV to receive that channel.
This new number is then stored as the current channel.
Similarly, the television also stores a number that controls the level of volume produced by the speaker.
Pressing the "volume up" or "volume down" buttons increments or decrements this number, setting a new level of volume.
Both the current channel and current volume numbers are part of the TV's state.
They are stored in non-volatile memory, which preserves the information when the TV is turned off, so when it is turned on again the TV will return to its previous station and volume level.

As another example, the state of a microprocessor is the contents of all the memory elements in it: the accumulators, storage registers, data caches, and flags.
When computers such as laptops go into a hibernation mode to save energy by shutting down the processor, the state of the processor is stored on the computer's hard disk, so it can be restored when the computer comes out of hibernation, and the processor can take up operations where it left off.

== See also ==
Data
(computing)


==
References ==
Theoretical computer science (TCS) is a subset of general computer science that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.

It is difficult to circumscribe the theoretical areas precisely.
The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra.
Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

==
History ==
While logical inference and mathematical proof had existed previously, in 1931 Kurt Gödel proved with his incompleteness theorem that there are fundamental limitations on what statements could be proved or disproved.

These developments have led to the modern study of logic and computability, and indeed the field of theoretical computer science as a whole.
Information theory was added to the field with a 1948 mathematical theory of communication by Claude Shannon.
In the same decade, Donald Hebb introduced a mathematical model of learning in the brain.
With mounting biological data supporting this hypothesis with some modification, the fields of neural networks and parallel distributed processing were established.
In 1971, Stephen Cook and, working independently, Leonid Levin, proved that there exist practically relevant problems that are NP-complete – a landmark result in computational complexity theory.

With the development of quantum mechanics in the beginning of the 20th century came the concept that mathematical operations could be performed on an entire particle wavefunction.
In other words, one could compute functions on multiple states simultaneously.
This led to the concept of a quantum computer in the latter half of the 20th century that took off in the 1990s when Peter Shor showed that such methods could be used to factor large numbers in polynomial time, which, if implemented, would render some modern public key cryptography algorithms like RSA_(cryptosystem) insecure.
Modern theoretical computer science research is based on these basic developments, but includes many other mathematical and interdisciplinary problems that have been posed, as shown below:


==
Topics ==


===
Algorithms ===
An algorithm is a step-by-step procedure for calculations.

Algorithms are used for calculation, data processing, and automated reasoning.

An algorithm is an effective method expressed as a finite list of well-defined instructions for calculating a function.

Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state.
The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.
===
Automata theory ===
Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them.
It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science).
Automata comes from the Greek word αὐτόματα meaning "self-acting".

Automata Theory is the study of self-operating virtual machines to help in the logical understanding of input and output process, without or with intermediate stage(s) of computation (or any function/process).

===
Coding theory ===
Coding theory is the study of the properties of codes and their fitness for a specific application.
Codes are used for data compression, cryptography, error-correction and more recently also for network coding.
Codes are studied by various scientific disciplines—such as information theory, electrical engineering,  mathematics, and computer science—for the purpose of designing efficient and reliable data transmission methods.
This typically involves the removal of redundancy and the correction (or detection) of errors in the transmitted data.

===
Computational biology ===
Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.
The field is broadly defined and includes foundations in computer science, applied mathematics, animation, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, ecology, evolution, anatomy, neuroscience, and visualization.
Computational biology is different from biological computation, which is a subfield of computer science and computer engineering using bioengineering and biology to build computers, but is similar to bioinformatics, which is an interdisciplinary science using computers to store and process biological data.

===
Computational complexity theory ===
Computational complexity theory is a branch of the theory of computation that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other.
A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.

A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used.
The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage.
Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing).
One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.

===
Computational geometry ===
Computational geometry is a branch of computer science devoted to the study of algorithms that can be stated in terms of geometry.
Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry.
While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity.
An ancient precursor is the Sanskrit treatise Shulba Sutras, or "Rules of the Chord", that is a book of algorithms written in 800 BCE.
The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.

The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.

Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).

===
Computational learning theory ===
Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning.

In supervised learning, an algorithm is given samples that are labeled in some
useful way.

For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible.

The algorithm takes these previously labeled samples and
uses them to induce a classifier.

This classifier is a function that assigns labels to samples including the samples that have never been previously seen by the algorithm.

The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.

===
Computational number theory ===
Computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.
The best known problem in the field is integer factorization.

===
Cryptography ===
Cryptography  is the practice and study of techniques for secure communication in the presence of third parties (called adversaries).
More generally, it is about constructing and analyzing protocols that overcome the influence of adversaries and that are related to various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation.
Modern cryptography intersects the disciplines of mathematics, computer science, and electrical engineering.
Applications of cryptography include ATM cards, computer passwords, and electronic commerce.

Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary.
It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means.
These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted.
There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.
===
Data structures ===
A data structure is a particular way of organizing data in a computer so that it can be used efficiently.
Different kinds of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks.
For example, databases use B-tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables.

Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services.
Usually, efficient data structures are key to designing efficient algorithms.
Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design.
Storing and retrieving can be carried out on data stored in both main memory and in secondary memory.

===
Distributed computation ===
Distributed computing studies distributed systems.
A distributed system is a software system in which components located on networked computers communicate and coordinate their actions by passing messages.
The components interact with each other in order to achieve a common goal.
Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.
Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to  peer-to-peer applications, and blockchain networks like Bitcoin.

A computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs.
There are many alternatives for the message passing mechanism, including RPC-like connectors and message queues.

An important goal and challenge of distributed systems is location transparency.

===
Information-based complexity ===
Information-based complexity (IBC) studies optimal algorithms and computational complexity for continuous problems.
IBC has studied continuous problems as path integration, partial differential equations, systems of ordinary differential equations, nonlinear equations, integral equations, fixed points, and very-high-dimensional integration.
===
Formal methods ===
Formal methods are a particular kind of mathematics based techniques for the specification, development and verification of software and hardware systems.
The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.
Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.

===
Information theory ===
Information theory is a branch of applied mathematics, electrical engineering, and computer science involving the quantification of information.

Information theory was developed by Claude E. Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.
Since its inception it has broadened to find applications in many other areas, including statistical inference, natural language processing, cryptography, neurobiology, the evolution and function of molecular codes, model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, anomaly detection and other forms of data analysis.
Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for Digital Subscriber Line (DSL)).

The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, and electrical engineering.
Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields.
Important sub-fields of information theory are source coding, channel coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, and measures of information.

===
Machine learning ===
Machine learning is a scientific discipline that deals with the construction and study of algorithms that can learn from data.
Such algorithms operate by building a model based on inputs and using that to make predictions or decisions, rather than following only explicitly programmed instructions.

Machine learning can be considered a subfield of computer science and statistics.
It has strong ties to artificial intelligence and optimization, which deliver methods, theory and application domains to the field.
Machine learning is employed in a range of computing tasks
where designing and programming explicit, rule-based algorithms is infeasible.
Example applications include spam filtering, optical character recognition (OCR), search engines and computer vision.
Machine learning is sometimes conflated with data mining, although that focuses more on exploratory data analysis.
Machine learning and pattern recognition "can be viewed as two facets of
the same field."
===
Parallel computation ===
Parallel computing is a form of computation in which many calculations are carried out simultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved "in parallel".
There are several different forms of parallel computing: bit-level, instruction level, data, and task parallelism.
Parallelism has been employed for many years, mainly in high-performance computing, but interest in it has grown lately due to the physical constraints preventing frequency scaling.
As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.
Parallel computer programs are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common.
Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance.

The maximum possible speed-up of a single program as a result of parallelization is known as Amdahl's law.

===
Program semantics ===
In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages.
It does so by evaluating the meaning of syntactically legal strings defined by a specific programming language, showing the computation involved.
In such a case that the evaluation would be of syntactically illegal strings, the result would be non-computation.
Semantics describes the processes a computer follows when executing a program in that specific language.
This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will execute on a certain platform, hence creating a model of computation.

===
Quantum computation ===
A quantum computer is a computation system that makes direct use of quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data.
Quantum computers are different from digital computers based on transistors.
Whereas digital computers require data to be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses qubits (quantum bits), which can be in superpositions of states.
A theoretical model is the quantum Turing machine, also known as the universal quantum computer.

Quantum computers share theoretical similarities with non-deterministic and probabilistic computers; one example is the ability to be in more than one state simultaneously.

The field of quantum computing was first introduced by Yuri Manin in 1980 and Richard Feynman in 1982.
A quantum computer with spins as quantum bits was also formulated for use as a quantum space–time in 1968.As of 2014
, quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on a very small number of qubits.
Both practical and theoretical research continues, and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.
===
Symbolic computation ===
Computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects.
Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic computation).

Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.

=== Very-large-scale integration ===

Very-large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining thousands of transistors into a single chip.
VLSI began in the 1970s when complex semiconductor and communication technologies were being developed.
The microprocessor is a VLSI device.
Before the introduction of VLSI technology most ICs had a limited set of functions they could perform.
An electronic circuit might consist of a CPU, ROM, RAM and other glue logic.
VLSI allows IC makers to add all of these circuits into one chip.

==
Organizations ==
European Association for Theoretical Computer
Science
SIGACT
Simons Institute for the Theory of Computing


==
Journals and newsletters ==
“Discrete Mathematics and Theoretical Computer Science”
Information and Computation
Theory of Computing (open access journal)
Formal Aspects of Computing
Journal of the ACM
SIAM Journal on Computing (SICOMP)
SIGACT News
Theoretical Computer Science
Theory of Computing Systems
International Journal of Foundations of Computer Science
Chicago Journal of Theoretical Computer Science (open access journal)
Foundations and Trends in Theoretical Computer Science
Journal of Automata, Languages and Combinatorics
Acta Informatica
Fundamenta Informaticae
ACM Transactions on Computation Theory
Computational Complexity
Journal of Complexity
ACM Transactions on Algorithms
Information Processing Letters
Open Computer Science (open access journal)


==
Conferences ==
Annual ACM Symposium on Theory of Computing (STOC)
Annual IEEE Symposium on Foundations of Computer Science (FOCS)
Innovations in Theoretical Computer Science (ITCS)
Mathematical Foundations of Computer Science (MFCS)
International Computer Science Symposium in Russia (CSR)
ACM–SIAM Symposium on Discrete Algorithms (SODA)
IEEE Symposium on Logic in Computer Science (LICS)
Computational Complexity Conference (CCC)
International Colloquium on Automata, Languages and Programming (ICALP)
Annual Symposium on Computational Geometry (SoCG)
ACM Symposium on Principles of Distributed Computing (PODC)
ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)
Annual Conference on Learning Theory (COLT)
Symposium on Theoretical Aspects of Computer Science (STACS)
European Symposium on Algorithms (ESA)
Workshop on Approximation Algorithms for Combinatorial Optimization Problems (APPROX)
Workshop on Randomization and Computation
(RANDOM)
International Symposium on Algorithms and Computation (ISAAC)
International Symposium on Fundamentals of Computation Theory (FCT)
International Workshop on Graph-Theoretic Concepts in Computer Science (WG)


==
See also ==
Formal science
Unsolved problems in computer science
List of important publications in theoretical computer science


==
Notes ==


==
Further reading ==
Martin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed.,
Academic Press, 1994, ISBN 0-12-206382-1.
Covers theory of computation, but also program semantics and quantification theory.
Aimed at graduate students.

==
External links ==
SIGACT directory of additional theory links
Theory Matters
Wiki Theoretical Computer Science (TCS)
Advocacy Wiki
List of academic conferences in the area of theoretical computer science at confsearch
Theoretical Computer Science - StackExchange, a Question and Answer site for researchers in theoretical computer science
Computer Science Animated
http://theory.csail.mit.edu/   @
Massachusetts Institute of Technology
In psychology, an attribution bias or attributional bias is a cognitive bias that refers to the systematic errors made when people evaluate or try to find reasons for their own and others' behaviors.
People constantly make attributions—judgements and assumptions about why people behave in certain ways.
However, attributions do not always accurately reflect reality.
Rather than operating as objective perceivers, people are prone to perceptual errors that lead to biased interpretations of their social world.
Attribution biases are present in everyday life.
For example, when a driver cuts someone off, the person who has been cut off is often more likely to attribute blame to the reckless driver's inherent personality traits (e.g., "That driver is rude and incompetent") rather than situational circumstances (e.g., "
That driver may have been late to work and was not paying attention").
Additionally, there are many different types of attribution biases, such as the ultimate attribution error, fundamental attribution error, actor-observer bias, and hostile attribution bias.
Each of these biases describes a specific tendency that people exhibit when reasoning about the cause of different behaviors.

Since the early work, researchers have continued to examine how
and why people exhibit biased interpretations of social information.
Many different types of attribution biases have been identified, and more recent psychological research on these biases has examined how attribution biases can subsequently affect emotions and behavior.

==
History ==


===
Early influences ===


====
Attribution theory ====
Research on attribution biases is founded in attribution theory, which was proposed to explain why and how people create meaning about others' and their own behavior.
This theory focuses on identifying how an observer uses information in his/her social environment in order to create a causal explanation for events.
Attribution theory also provides explanations for why different people can interpret the same event in different ways and what factors contribute to attribution biases.
Psychologist Fritz Heider first discussed attributions in his 1958 book, The Psychology of Interpersonal Relations.
Heider made several contributions that laid the foundation for further research on attribution theory and attribution biases.
He noted that people tend to make distinctions between behaviors that are caused by personal disposition versus environmental or situational conditions.
He also predicted that people are more likely to explain others' behavior in terms of dispositional factors (i.e., caused by a given person's personality), while ignoring the surrounding situational demands.

====
Correspondent inference theory ===
=
Building on Heider's early work, other psychologists in the 1960s and 1970s extended work on attributions by offering additional related theories.
In 1965, social psychologists Edward E. Jones and Keith Davis proposed an explanation for patterns of attribution termed correspondent inference theory.

A correspondent inference assumes that a person's behavior reflects a stable disposition or personality characteristic instead of a situational factor.
They explained that certain conditions make us more likely to make a correspondent inference about someone's behavior:
Intention:
People are more likely to make a correspondent inference when they interpret someone's behavior as intentional, rather than unintentional.

Social desirability: People are more likely to make a correspondent inference when an actor's behavior is socially undesirable than when it is conventional.

Effects of behavior: People are more likely to make a correspondent, or dispositional, inference when someone else's actions yield outcomes that are rare or not yielded by other actions.

====
Covariation model ====
Soon after Jones and Davis first proposed their correspondent inference theory, Harold Kelley, a social psychologist famous for his work on interdependence theory as well as attribution theory, proposed a covariation model in 1973 to explain the way people make attributions.
This model helped to explain how people choose to attribute a behavior to an internal disposition versus an environmental factor.
Kelley used the term 'covariation' to convey that when making attributions, people have access to information from many observations, across different situations, and at many time points; therefore, people can observe the way a behavior varies under these different conditions and draw conclusions based on that context.
He proposed three factors that influence the way individuals explain behavior:
Consensus: The extent to which other people behave in the same way.
There is high consensus when most people behave consistent with a given action/actor.
Low consensus is when not many people behave in this way.

Consistency: The extent to which a person usually behaves in a given way.
There is high consistency when a person almost always behaves in a specific way.
Low consistency is when a person almost never behaves like this.

Distinctiveness:
The extent to which an actor's behavior in one situation is different from his/her behavior in other situations.
There is high distinctiveness when an actor does not behave this way in most situations.
Low distinctiveness is when an actor usually behaves in a particular way in most situations.
Kelley proposed that people are more likely to make dispositional attributions when consensus is low
(most other people don't behave in the same way)
, consistency is high (a person behaves this way across most situations), and distinctiveness is low
(a person's behavior is not unique to this situation).
Alternatively, situational attributions are more likely reached when consensus is high, consistency is low, and distinctiveness is high.
His research helped to reveal the specific mechanisms underlying the process of making attributions.

===
Later development ===
As early researchers explored the way people make causal attributions, they also recognized that attributions do not necessarily reflect reality and can be colored by a person's own perspective.
Certain conditions can prompt people to exhibit attribution bias, or draw inaccurate conclusions about the cause of a given behavior or outcome.
In his work on attribution theory, Fritz Heider noted that in ambiguous situations, people make attributions based on their own wants and needs, which are therefore often skewed.
He also explained that this tendency was rooted in a need to maintain a positive self-concept, later termed the self-serving bias.

Kelley's covariation model also led to the acknowledgment of attribution biases.
The model explained the conditions under which people will make informed dispositional versus situational attributions.
But, it assumed that people had access to such information (i.e., the consensus, consistency, and distinctiveness of a person's behavior).
When one doesn't have access to such information, like when they interact with a stranger, it will result in a tendency to take cognitive shortcuts, resulting in different types of attribution biases, such as the actor-observer bias.

====
Cognitive explanation ====
Although psychologists agreed that people are prone to these cognitive biases, there existed disagreement concerning the cause of such biases.
On one hand, supporters of a "cognitive model" argued that biases were a product of human information processing constraints.
One major proponent of this view was Yale psychologist Michael Storms, who proposed this cognitive explanation following his 1973 study of social perception.
In his experiment, participants viewed a conversation between two individuals, dubbed Actor One and Actor Two.
Some participants viewed the conversation while facing Actor One, such that they were unable to see the front of Actor Two, while other participants viewed the conversation while facing Actor Two, obstructed from the front of Actor One.

Following the conversation, participants were asked to make attributions about the conversationalists.
Storms found that participants ascribed more causal influence to the person they were looking at.
Thus, participants made different attributions about people depending on the information they had access to.
Storms used these results to bolster his theory of cognitively-driven attribution biases; because people have no access to the world except through their own eyes, they are inevitably constrained and consequently prone to biases.
Similarly, social psychologist Anthony Greenwald described humans as possessing a totalitarian ego, meaning that people view the world through their own personal selves.
Therefore, different people may interpret the world differently and in turn reach different conclusions.

==== Motivational explanation ====
Some researchers criticized the view that attributional biases are a sole product of information processing constraints, arguing that humans do not passively interpret their world and make attributions; rather, they are active and goal-driven beings.
Building on this criticism, research began to focus on the role of motives in driving attribution biases.
Researchers such as Ziva Kunda drew attention to the motivated aspects of attributions and attribution biases.
Kunda in particular argued that certain biases only appear when people are presented with motivational pressures; therefore, they can't be exclusively explained by an objective cognitive process.
More specifically, people are more likely to construct biased social judgments when they are motivated to arrive at a particular conclusion, so long as they can justify this conclusion.

==
Current theory ==
Early researchers explained attribution biases as cognitively driven and a product of information processing errors.
In the early 1980s, studies demonstrated that there may also be a motivational component to attribution biases, such that their own desires and emotions affect how one interprets social information.
Current research continues to explore the validity of both of these explanations by examining the function of specific types of attribution biases and their behavioral correlates through a variety of methods (e.g., research with children or using brain imaging techniques).Recent research on attribution biases has focused on identifying specific types of these biases and their effect on people's behavior.
Additionally, some psychologists have taken an applied approach and demonstrated how these biases can be understood in real-world contexts (e.g., the workplace or school).
Researchers have also used the theoretical framework of attributions and attribution biases in order to modify the way people interpret social information.
For example, studies have implemented attributional retraining to help students have more positive perceptions of their own academic abilities (see below for more details).

===
Mental health ===
Studies on attribution bias and mental health suggest that people who have mental illnesses are more likely to hold attribution biases.
People who have mental illness tend to have a lower self-esteem, experience social avoidance, and do not commit to improving their overall quality of life, often as a result of lack of motivation.
People with these problems tend to feel strongly about their attribution biases and will quickly make their biases known.
These problems are called social cognition biases and are even present in those with less severe mental problems.
There are many kinds of cognitive biases that affect people in different ways, but all may lead to irrational thinking, judgment, and decision-making.
===
Aggression ===
Extensive research in both social and developmental psychology has examined the relationship between aggressive behavior and attribution biases, with a specific focus on the hostile attribution bias.
In particular, researchers have consistently found that children who exhibit a hostile attribution bias (tendency to perceive others' intent as hostile, as opposed to benign) are more likely to engage in aggressive behaviors.
More specifically, hostile attribution bias has been associated with reactive aggression, as opposed to proactive aggression, as well as victimization.
Whereas proactive aggression is unprovoked and goal-driven, reactive aggression is an angry, retaliatory response to some sort of perceived provocation.
Therefore, children who are victims of aggression may develop views of peers as hostile, leading them to be more likely to engage in retaliatory (or reactive) aggression.
Research has also indicated that children can develop hostile attribution bias by engaging in aggression in the context of a video game.
In a 1998 study, participants played either a  violent or non-violent video game and were then asked to read several hypothetical stories where a peer's intent was ambiguous.
For example, participants may have read about their peer hitting someone in the head with a ball, but it was unclear whether or not the peer did this intentionally.
Participants then responded to questions about their peer's intent.
The children who played the violent video game were more likely to say that their peer harmed someone on purpose than the participants who played the nonviolent game.
This finding provided evidence that exposure to violence and aggression could cause children to develop a short-term hostile attribution bias.

===
Intergroup relations ===
Research has found that humans often exhibit attribution biases when interpreting the behavior of others, and specifically when explaining the behavior of in-group versus out-group members.
A review of the literature on intergroup attribution biases noted that people generally favor dispositional explanations of an in-group member's positive behavior and situational explanations for an in-group's negative behavior.
Alternatively, people are more likely to do the opposite when explaining the behavior of an out-group member (i.e., attribute positive behavior to situational factors and negative behavior to disposition).
Essentially, group members' attributions tend to favor the in-group.
This finding has implications for understanding other social psychological topics, such as the development and persistence of out-group stereotypes.
Attribution biases in intergroup relations are observed as early as childhood.
In particular, elementary school students are more likely to make dispositional attributions when their friends perform positive behaviors, but situational attributions when disliked peers perform positive behaviors.
Similarly, children are more likely to attribute friends' negative behaviors to situational factors, whereas they attribute disliked peers' negative behaviors to dispositional factors.
These findings provide evidence that attribution biases emerge very early on.
===
Academic achievement ===
Although certain attribution biases are associated with maladaptive behaviors, such as aggression, some research has also indicated that these biases are flexible and can be altered to produce positive outcomes.
Much of this work falls within the domain of improving academic achievement through attributional retraining.
For example, one study found that students who were taught to modify their attributions actually performed better on homework assignments and lecture materials.
The retraining process specifically targeted students who tended to attribute poor academic performance to external factors.
It taught these students that poor performance was often attributable to internal and unstable factors, such as effort and ability.
Therefore, the retraining helped students perceive greater control over their own academic success by altering their attributional process.

More recent research has extended these findings and examined the value of attributional retraining for helping students adjust to an unfamiliar and competitive setting.
In one study, first year college students went through attributional retraining following their first exam in a two-semester course.
Similar to the previous study, they were taught to make more controllable attributions (e.g., "I can improve my test grade by studying more") and less uncontrollable attributions (e.g., "
No matter what I do, I'll fail").
For students who performed low or average on their first exam, attributional retraining resulted in higher in-class test grades and GPA in the second semester.
Students who performed well on the first exam were found to have more positive emotions in the second semester following attributional retraining.
Taken together, these studies provide evidence for the flexibility and modifiability of attributional biases.

==
Limitations of the theory ==
There is inconsistency in the claims made by scientists and researchers that attempt to prove or disprove attribution theories and the concept of attributional biases.
The theory was formed as a comprehensive explanation of the way people interpret the basis of behaviors in human interactions; however, there have been studies that indicate cultural differences in the attribution biases between people of Eastern, collectivistic societies and Western, individualistic societies.
A study done by Thomas Miller shows that when dealing with conflict created by other people, individualistic cultures tend to blame the individual for how people behave (dispositional attributions), whereas collectivist cultures blame the overall situation on how people behave (situational attributions).

These same findings were replicated in a study done by Michael Morris where an American group and a Chinese group were asked their opinions about the killings perpetrated by Gang Lu at the University of Iowa.
The American group focused on the killer's own internal problems.
The Chinese group focused more on the social conditions surrounding the killing.
This reinforces the notion that individualistic and collectivistic cultures tend to focus on different aspects of a situation when making attributions.

Additionally, some scientists believe that attributional biases are only exhibited in certain contexts of interaction, where possible outcomes or expectations make the forming of attributions necessary.
These criticisms of the attribution model reveal that the theory may not be a general, universal principle.

==
Major attribution biases ==
Researchers have identified many different specific types of attribution biases, all of which describe ways in which people exhibit biased interpretations of information.
Note that this is not an exhaustive list (see List of attributional biases for more).
===
Fundamental attribution error ===
The fundamental attribution error refers to a bias in explaining others' behaviors.
According to this error, when someone makes attributions about another person's actions, they are likely to overemphasize the role of dispositional factors while minimizing the influence of situational factors.
For example, if a person sees a coworker bump into someone on his way to a meeting, that person is more likely to explain this behavior in terms of the coworker's carelessness or hastiness rather than considering that he was running late to a meeting.

This term was first proposed in the early 1970s by psychologist Lee Ross following an experiment he conducted with Edward E. Jones and Victor Harris in 1967.
In this study, participants were instructed to read two essays; one expressed pro-Castro views, and the other expressed anti-Castro views.
Participants were then asked to report their attitudes towards the writers under two separate conditions.
When participants were informed that the writers voluntarily chose their position towards Castro, participants predictably expressed more positive attitudes towards the anti-Castro writer.
However, when participants were told that the writers' positions were determined by a coin toss rather than their own free will, participants unpredictably continued to express more positive attitudes towards the anti-Castro writer.
These results demonstrated that participants did not take situational factors into account when evaluating a third party, thus providing evidence for the fundamental attribution error.

===
Actor-observer bias ===
The actor-observer bias (also called actor–observer asymmetry) can be thought of as an extension of the fundamental attribution error.
According to the actor-observer bias, in addition to over-valuing dispositional explanations of others' behaviors, people tend to under-value dispositional explanations and over-value situational explanations of their own behavior.
For example, a student who studies may explain her behavior by referencing situational factors (e.g., "
I have an exam coming up"), whereas others will explain her studying by referencing dispositional factors (e.g., "She's ambitious and hard-working").

This bias was first proposed by Edward E. Jones and Richard E. Nisbett in 1971, who explained that "actors tend to attribute the causes of their behavior to stimuli inherent in the situation, while observers tend to attribute behavior to stable dispositions of the actor.
"There has been some controversy over the theoretical foundation of the actor-observer bias.
In a 2006 meta-analysis of all published studies of the bias since 1971, the author found that Jones' and Nisbett's original explanation did not hold.

Whereas Jones and Nisbett proposed that actors and observers explain behaviors as attributions to either dispositions or situational factors, examining past studies revealed that this assumption may be flawed.
Rather, the theoretical reformulation posits that the way people explain behavior depends on whether or not it is intentional, among other things.
For more information on this theoretical reformulation, see actor-observer asymmetry, or refer to Malle's meta-analysis in #Further reading.

===
Self-serving bias ===
A self-serving bias refers to people's tendency to attribute their successes to internal factors but attribute their failures to external factors.
This bias helps to explain why individuals tend to take credit for their own successes while often denying responsibility for failures.
For example, a tennis player who wins his match might say, "I won because I'm a good athlete," whereas the loser might say, "I lost because the referee was unfair."

The self-serving bias has been thought of as a means of self-esteem maintenance.
A person will feel better about themselves by taking credit for successes and creating external blames for failure.
This is further reinforced by research showing that as self-threat increases, people are more likely to exhibit a self-serving bias.
For example, participants who received negative feedback on a laboratory task were more likely to attribute their task performance to external, rather than internal, factors.
The self-serving bias seems to function as an ego-protection mechanism, helping people to better cope with personal failures.

===
Hostile attribution bias ===
Hostile attribution bias (HAB) has been defined as an interpretive bias wherein individuals exhibit a tendency to interpret others' ambiguous behaviors as hostile, rather than benign.
For example, if a child witnesses two other children whispering, they may assume that the children are talking negatively about them.
In this case, the child made an attribution of hostile intent, even though the other children's behavior was potentially benign.
Research has indicated that there is an association between hostile attribution bias and aggression, such that people who are more likely to interpret someone else's behavior as hostile are also more likely to engage in aggressive behavior.
See the previous section on aggression for more details on this association.

==
List of attribution biases ==


==
See also ==
Theory of mind –
Ability to attribute mental states to oneself and others
Attribution (psychology) –
The process by which individuals explain the causes of behavior and events
Fallacy of the single cause – Assumption of a single cause where multiple factors may be necessary
Causality –
How one process influences another
Cognitive dissonance – Psychological stress resulting from multiple contradictory beliefs, ideas, or values held at the same time
Just-world hypothesis – Hypothesis that a person's actions are inherently inclined to bring morally fair and fitting consequences to that person
List of cognitive biases – Systematic patterns of deviation from norm or rationality in judgment
False consensus effect – Attributional type of cognitive bias


==
References ==


==
Further reading ==
Harvey, J.H.; Town, J.P.; Yarkin, K.L. (1981).
"
How fundamental is "the fundamental attribution error"?" (
PDF).
Journal of Personality and Social Psychology.
40 (2): 346–349.
doi:10.1037/0022-3514.40.2.346.

Malle, B.F. (2006). "
Actor-observer asymmetry in attribution:
A (surprising) meta-analysis" (PDF).
Psychological Bulletin.
132 (6): 895–919.
doi:10.1037/0033-2909.132.6.895.
PMID 17073526.
Archived from the original (PDF) on May 2, 2013.

Matthews, A.; Norris, F.H. (2002).
"
When is believing "seeing"?
Hostile attribution bias as a function of self-reported aggression".
Journal of Applied Social Psychology.
32 (1)
: 1–32.
doi:10.1111/j.1559-1816.2002.tb01418.x.
S2CID 143568167.

Miller, D.T.; Ross, M. (1975).
"
Self-serving biases in the attribution of causality:
Fact or fiction?" (
PDF).
Psychological Bulletin.
82 (2)
: 213–225.
doi:10.1037/h0076486.
==
External links ==
Funnelsort is a comparison-based sorting algorithm.
It is similar to mergesort, but it is a cache-oblivious algorithm, designed for a setting where the number of elements to sort is too large to fit in a cache where operations are done.
It was introduced by Matteo Frigo, Charles Leiserson, Harald Prokop, and Sridhar Ramachandran in 1999 in the context of the cache oblivious model.

==
Mathematical properties ==
In the external memory model, the number of memory transfers it needs to perform a sort of 
  
    
      
        N
      
    
    {\displaystyle N}
   items on a machine with cache of size
Z
      
    
    {\displaystyle Z}
   and cache lines of length
L
{\displaystyle L}
   is 
  
    
      
        O
(
          
            
              
                
                  N
                  L
                
              
            
            
              log
              
                Z
⁡
N
          
          )
        
      
    
    {\displaystyle O\left({\tfrac {N}{L}}\log _{Z}N\right)}
  , under the tall cache assumption that 
  
    
      
        Z
=
        Ω
(
        
          L
          
            2
          
        
        )
      
    
    {\displaystyle Z=\Omega (L^{2})}
  .
This number of memory transfers has been shown to be asymptotically optimal for comparison sorts.
Funnelsort also achieves the asymptotically optimal runtime complexity of 
  
    
      
        Θ
        (
        N
        log
⁡
N
        )
      
    
    {\displaystyle \Theta (N\log N)}
  .
==
Algorithm ==


===
Basic overview ===
Funnelsort operates on a contiguous array of 
  
    
      
        N
      
    
    {\displaystyle N}
   elements.
To sort the elements, it performs the following:

Split the input into 
  
    
      
        
          N
          
            1
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{1/3}}
   arrays of size
N
          
            2
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{2/3}}
  , and sort the arrays recursively.

Merge the 
  
    
      
        
          N
          
            1
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{1/3}
}
   sorted sequences using a
N
          
            1
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{1/3}}
-merger.
(
This process will be described in more detail.)Funnelsort is similar to merge sort in that some number of subarrays are recursively sorted, after which a merging step combines the subarrays into one sorted array.
Merging is performed by a device called a k-merger, which is described in the section below.
===
k-mergers ===
A k-merger takes
k
      
    
    {\displaystyle k}
   sorted sequences.
Upon one invocation of a k-merger, it outputs the first
k
3
          
        
      
    
    {\displaystyle k^{3}}
   elements of the sorted sequence obtained by merging the k input sequences.

At the top level, funnelsort uses a
N
          
            1
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{1/3}}
-merger
on
N
          
            1
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{1/3}}
   sequences of length
N
          
            2
            
              /
            
            3
          
        
      
    
    {\displaystyle N^{2/3}}
  , and invokes this merger once.

The k-merger is built recursively out of
k
{\displaystyle {\sqrt {k}}}
  -mergers.
It consists of
k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
   input
k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
  -mergers
I
1
          
        
        ,
I
          
            2
          
        
        ,
        …
,
        
          I
k
            
          
        
      
    
    {\displaystyle I_{1},I_{2},\ldots ,I_{\sqrt {k}}}
  , and a single output
k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
-merger
O
      
    
    {\displaystyle O}
  .

The k inputs are separated into 
  
    
      
        
          
            k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
   sets of 
  
    
      
        
          
            k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
   inputs each.
Each of these sets is an input to one of the input mergers.
The output of each input merger is connected to a buffer, a FIFO queue that can hold 
  
    
      
        2
k
3
            
              /
2
          
        
      
    
    {\displaystyle 2k^{3/2}}
   elements.
The buffers are implemented as circular queues.

The outputs of the
k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
   buffers are connected to the inputs of the output merger
O
      
    
    {\displaystyle O}
  .
Finally, the output of 
  
    
      
        O
      
    
    {\displaystyle O}
   is the output of the entire k-merger.

In this construction, any input merger only outputs
k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   items at once, but the buffer it outputs to has double the space.
This is done so that an input merger can be called only when its buffer does not have enough items, but that when it is called, it outputs a lot of items at once (namely,
k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   of them).

A k-merger works recursively in the following way.
To output 
  
    
      
        
          k
          
            3
          
        
      
    
    {\displaystyle k^{3}}
   elements, it recursively invokes its output merger
k
3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   times.
However, before it makes a call to
O
      
    
    {\displaystyle O}
  , it checks all of its buffers, filling each of them that are less than half full.
To fill the i-th buffer, it recursively invokes the corresponding input merger
I
i
          
        
      
    
    {\displaystyle I_{i}}
   once.
If this cannot be done (due to the merger running out of inputs), this step is skipped.
Since this call outputs
k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   elements, the buffer contains at least 
  
    
      
        
          k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   elements.
At the end of all these operations, the k-merger has output the first
k
          
            3
          
        
      
    
    {\displaystyle k^{3}}
   of its input elements, in sorted order.
==
Analysis ==
Most of the analysis of this algorithm revolves around analyzing the space and cache miss complexity of the k-merger.

The first important bound is that a k-merger can be fit in 
  
    
      
        O
(
k
          
            2
          
        
        )
      
    
    {\displaystyle O(k^{2})}
   space.
To see this, we let 
  
    
      
        S
(
        k
        )
      
    
    {\displaystyle S(k)
}
   denote the space needed for a k-merger.
To fit the
k
          
            1
            
              /
            
            2
{\displaystyle k^{1/2}}
   buffers of size 
  
    
      
        2
k
3
            
              /
2
          
        
      
    
    {\displaystyle 2k^{3/2
}}
   takes 
  
    
      
        O
(
k
          
            2
          
        
        )
      
    
    {\displaystyle O(k^{2})}
   space.
To fit the 
  
    
      
        
          
            k
+
        1
      
    
    {\displaystyle
{\sqrt {k}}+1}
   smaller buffers takes
(
        
          
            k
+
        1
        )
S
(
        
          
            k
)
{\displaystyle ({\sqrt {k}}+1)S({\sqrt {k}})}
   space.
Thus, the space satisfies the recurrence 
  
    
      
        S
(
        k
        )
        =
(
        
          
            k
+
        1
        )
S
(
        
          
            k
          
        
        )
+
        O
(
        
          k
          
            2
          
        
        )
      
    
    {\displaystyle S(k)=({\sqrt {k}}+1)S({\sqrt {k}})+O(k^{2})}
  .
This recurrence has solution
S
(
        k
        )
        =
O
(
        
          k
          
            2
          
        
        )
      
    
    {\displaystyle S(k)=O(k^{2})}
  .

It follows that there is a positive constant 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   such that a problem of size at most 
  
    
      
        α
Z
          
        
      
    
    {\displaystyle \alpha {\sqrt {Z}}}
   fits entirely in cache, meaning that it incurs no additional cache misses.

Letting 
  
    
      
        
          Q
          
            M
(
        k
)
      
    
    {\displaystyle Q_{M}(k)}
   denote the number of cache misses incurred by a call to a k-merger, one can show that 
  
    
      
        
          Q
          
            M
(
        k
        )
        =
O
(
(
k
          
            3
log
          
            Z
⁡
k
        )
        
          /
        
        L
        )
        .

{\displaystyle Q_{M}(k)=O((k^{3}\log _{Z}k)/L).}

This is done by an induction argument.
It has
k
        ≤
        α
        
          
            Z
          
        
      
    
    {\displaystyle k\leq \alpha {\sqrt {Z}}}
   as a base case.
For larger k, we can bound the number of times
a 
  
    
      
        
          
            k
          
        
      
    
    {\displaystyle {\sqrt {k}}}
  -merger is called.
The output merger is called exactly 
  
    
      
        
          k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   times.
The total number of calls on input mergers is at most
k
          
            3
            
              /
            
            2
+
        2
k
          
        
      
    
    {\displaystyle k^{3/2}+2{\sqrt {k}}}
  .
This gives a total bound of 
  
    
      
        2
k
          
            3
            
              /
            
            2
+
        2
k
{\displaystyle 2k^{3/2}+2{\sqrt {k}}}
   recursive calls.
In addition, the algorithm checks every buffer to see if needs to be filled.
This is done on 
  
    
      
        
          
            k
          
        
      
    
    {
\displaystyle {\sqrt {k}}}
   buffers every step for
k
          
            3
            
              /
            
            2
          
        
      
    
    {\displaystyle k^{3/2}}
   steps, leading to a max of 
  
    
      
        
          k
          
            2
          
        
      
    
    {\displaystyle k^{2}}
   cache misses for all the checks.

This leads to the recurrence 
  
    
      
        
          Q
          
            M
(
        k
        )
        ≤
(
        2
k
          
            3
            
              /
            
            2
+
        2
k
          
        
        )
        
          Q
          
            M
(
k
          
        
        )
+
        
          k
2
          
        
      
    
    {
\displaystyle Q_{M}(k)\leq (2k^{3/2}+2{\sqrt {k}})Q_{M}({\sqrt {k}})+k^{2}}
  , which can be shown to have the solution given above.

Finally, the total cache misses 
  
    
      
        Q
(
        N
)
      
    
    {\displaystyle Q(N)}
for the entire sort can be analyzed.
It satisfies the recurrence 
  
    
      
        Q
(
        N
        )
        =
N
          
            1
            
              /
            
            3
Q
(
        
          N
          
            2
            
              /
            
            3
          
        
        )
+
        
          Q
          
            M
(
        
          N
          
            1
            
              /
            
            3
          
        
        )
.

{\displaystyle Q(N)=N^{1/3}Q(N^{2/3})+Q_{M}(N^{1/3}).}

This can be shown to have solution 
  
    
      
        Q
(
        N
        )
        =
O
(
        (
        N
        
          /
        
        L
        )
        
          log
          
            Z
⁡
N
        )
        .
{\displaystyle Q(N)=O((N/L)\log _{Z}N).}

==
Lazy funnelsort ==
Lazy funnelsort is a modification of the funnelsort, introduced by Gerth Stølting Brodal and Rolf Fagerberg in 2002.

The modification is that when a merger is invoked, it does not have to fill each of its buffers.
Instead, it lazily fills a buffer only when it is empty.
This modification has the same asymptotic runtime and memory transfers as the original funnelsort, but has applications in cache-oblivious algorithms for problems in computational geometry in a method known as distribution sweeping.

== See also ==
Cache-oblivious algorithm
Cache-oblivious distribution sort
External sorting


=
= References ==
A list of 'effects' that have been noticed in the field of psychology.

== See also ==
List of cognitive biases
False memory
Uncanny valley
In computing, external memory algorithms or out-of-core algorithms are algorithms that are designed to process data that are too large to fit into a computer's main memory at once.
Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory (auxiliary memory) such as hard drives or tape drives, or when memory is on a computer network.
External memory algorithms are analyzed in the external memory model.

== Model ==
External memory algorithms are analyzed in an idealized model of computation called the external memory model (or I/O model, or disk access model).
The external memory model is an abstract machine similar to the RAM machine model, but with a cache in addition to main memory.
The model captures the fact that read and write operations are much faster in a cache than in main memory, and that reading long contiguous blocks is faster than reading randomly using a disk read-and-write head.
The running time of an algorithm in the external memory model is defined by the number of reads and writes to memory required.
The model was introduced by Alok Aggarwal and Jeffrey Vitter in 1988.
The external memory model is related to the cache-oblivious model, but algorithms in the external memory model may know both the block size and the cache size.
For this reason, the model is sometimes referred to as the cache-aware model.
The model consists of a processor with an internal memory or cache of size M, connected to an unbounded external memory.
Both the internal and external memory are divided into blocks of size
B. One input/output or memory transfer operation consists of moving a block of B contiguous elements from external to internal memory, and the running time of an algorithm is determined by the number of these input/output operations.

==
Algorithms ==
Algorithms in the external memory model take advantage of the fact that retrieving one object from external memory retrieves an entire block of size 
  
    
      
        B
      
    
    {\displaystyle B}
  .
This property is sometimes referred to as locality.

Searching for an element among 
  
    
      
        N
      
    
    {\displaystyle N}
   objects is possible in the external memory model using a B-tree with branching factor
B
      
    
    {\displaystyle B}
  .
Using a B-tree, searching, insertion, and deletion can be achieved in 
  
    
      
        O
(
        
          log
          
            B
⁡
N
        )
{\displaystyle O(\log
_{B}N)}
time (in Big O notation).
Information theoretically, this is the minimum running time possible for these operations, so using a B-tree is asymptotically optimal.
External sorting is sorting in an external memory setting.
External sorting can be done via distribution sort, which is similar to quicksort, or via a 
  
    
      
        
          
            
              M
              B
            
          
        
      
    
    {\displaystyle {\tfrac {M}{B}}}
-way merge sort.
Both variants achieve the asymptotically optimal runtime of 
  
    
      
        O
(
        
          
            
              N
B
            
          
        
        
          log
          
            
              
                M
                B
⁡
N
B
            
          
        
        )
      
    
    {
\displaystyle O({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}})}
   to sort N objects.
This bound also applies to the fast Fourier transform in the external memory model.
The permutation problem is to rearrange 
  
    
      
        N
      
    
    {\displaystyle N}
   elements into a specific permutation.
This can either be done either by sorting, which requires the above sorting runtime, or inserting each element in order and ignoring the benefit of locality.
Thus, permutation can be done in 
  
    
      
        O
(
        min
(
        N
        ,
N
              B
            
          
        
        
          log
          
            
              
                M
                B
⁡
N
              B
            
          
        
        )
)
      
    
    {\displaystyle O(\min(N,{\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}))}
   time.

==
Applications ==
The external memory model captures the memory hierarchy, which is not modeled in other common models used in analyzing data structures, such as the random-access machine, and is useful for proving lower bounds for data structures.
The model is also useful for analyzing algorithms that work on datasets too big to fit in internal memory.
A typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.

This methodology extends beyond general purpose CPUs and also includes GPU computing as well as classical digital signal processing.
In general-purpose computing on graphics processing units (GPGPU), powerful graphics cards (GPUs) with little memory (compared with the more familiar system memory, which is most often referred to simply as RAM) are utilized with relatively slow CPU-to-GPU memory transfer (when compared with computation bandwidth).

==
History ==
An early use of the term "out-of-core" as an adjective is in 1962 in reference to devices that are other than the core memory of an IBM 360.
An early use of the term "out-of-core" with respect to algorithms appears in 1971.

== See also ==
External sorting
Online algorithm
Streaming algorithm
Cache-oblivious algorithm
Parallel external memory
External memory graph traversal


==
References ==


==
External links ==
Out of Core SVD and QR
Out of core graphics
Scalapack design
In computing, a cache ( (listen) kash, or  kaysh in Australian English) is a hardware or software component that stores data
so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere.

A cache hit occurs when the requested data can be found in a cache, while a cache miss occurs when it cannot.

Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests that can be served from the cache, the faster the system performs.
To be cost-effective and to enable efficient use of data, caches must be relatively small.
Nevertheless, caches have proven themselves in many areas of computing, because typical computer applications access data with a high degree of locality of reference.
Such access patterns exhibit temporal locality, where data is requested that has been recently requested already, and spatial locality, where data is requested that is stored physically close to data that has already been requested.

== Motivation ==
There is an inherent trade-off between size and speed (given that a larger resource implies greater physical distances) but also a tradeoff between expensive, premium technologies (such as SRAM) vs cheaper, easily mass-produced commodities (such as DRAM or hard disks).

The buffering provided by a cache benefits both latency and throughput (bandwidth):


===
Latency ===
A larger resource incurs a significant latency for access –
e.g. it can take hundreds of clock cycles for a modern 4 GHz processor to reach DRAM.
This is mitigated by reading in large chunks, in the hope that subsequent reads will be from nearby locations.
Prediction or explicit prefetching might also guess where future reads will come from and make requests ahead of time; if done correctly the latency is bypassed altogether.

===
Throughput ===
The use of a cache also allows for higher throughput from the underlying resource, by assembling multiple fine grain transfers into larger, more efficient requests.
In the case of DRAM circuits, this might be served by having a wider data bus.
For example, consider a program accessing bytes in a 32-bit address space, but being served by a 128-bit off-chip data bus; individual uncached byte accesses would allow only 1/16th of the total bandwidth to be used, and 80% of the data movement would be memory addresses instead of data itself.
Reading larger chunks reduces the fraction of bandwidth required for transmitting address information.

==
Operation ==
Hardware implements cache as a block of memory for temporary storage of data likely to be used again.
Central processing units (CPUs) and hard disk drives (HDDs) frequently use a cache, as do web browsers and web servers.

A cache is made up of a pool of entries.
Each entry has associated data, which is a copy of the same data in some backing store.
Each entry also has a tag, which specifies the identity of the data in the backing store of which the entry is a copy.
Tagging allows simultaneous cache-oriented algorithms to function in multilayered fashion without differential relay interference.

When the cache client (a CPU, web browser, operating system) needs to access data presumed to exist in the backing store, it first checks the cache.
If an entry can be found with a tag matching that of the desired data, the data in the entry is used instead.
This situation is known as a cache hit.
For example, a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL.
In this example, the URL is the tag, and the content of the web page is the data.
The percentage of accesses that result in cache hits is known as the hit rate or hit ratio of the cache.

The alternative situation, when the cache is checked and found not to contain any entry with the desired tag, is known as a cache miss.
This requires a more expensive access of data from the backing store.
Once the requested data is retrieved, it is typically copied into the cache, ready for the next access.

During a cache miss, some other previously existing cache entry is removed in order to make room for the newly retrieved data.
The heuristic used to select the entry to replace is known as the replacement policy.
One popular replacement policy, "least recently used" (LRU), replaces the oldest entry, the entry that was accessed less recently than any other entry (see cache algorithm).
More efficient caching algorithms compute the use-hit frequency against the size of the stored contents, as well as the latencies and throughputs for both the cache and the backing store.
This works well for larger amounts of data, longer latencies, and slower throughputs, such as that experienced with hard drives and networks, but is not efficient for use within a CPU cache.
===
Writing policies ===
When a system writes data to cache, it must at some point write that data to the backing store as well.
The timing of this write is controlled by what is known as the write policy.
There are two basic writing approaches:
Write-through:
write is done synchronously both to the cache and to the backing store.

Write-back (also called write-behind): initially, writing is done only to the cache.
The write to the backing store is postponed until the modified content is about to be replaced by another cache block.
A write-back cache is more complex to implement, since it needs to track which of its locations have been written over, and mark them as dirty for later writing to the backing store.
The data in these locations are written back to the backing store only when they are evicted from the cache, an effect referred to as a lazy write.
For this reason, a read miss in a write-back cache (which requires a block to be replaced by another) will often require two memory accesses to service:
one to write the replaced data from the cache back to the store, and then one to retrieve the needed data.

Other policies may also trigger data write-back.
The client may make many changes to data in the cache, and then explicitly notify the cache to write back the data.

Since no data is returned to the requester on write operations, a decision needs to be made on write misses, whether or not data would be loaded into the cache.

This is defined by these two approaches:

Write allocate (also called fetch on write):
data at the missed-write location is loaded to cache, followed by a write-hit operation.
In this approach, write misses are similar to read misses.

No-write allocate (also called write-no-allocate or write around):
data at the missed-write location is not loaded to cache, and is written directly to the backing store.
In this approach, data is loaded into the cache on read misses only.
Both write-through and write-back policies can use either of these write-miss policies, but usually they are paired in this way:
A write-back cache uses write allocate, hoping for subsequent writes (or even reads) to the same location, which is now cached.

A write-through cache uses no-write allocate.
Here, subsequent writes have no advantage, since they still need to be written directly to the backing store.
Entities other than the cache may change the data in the backing store, in which case the copy in the cache may become out-of-date or stale.
Alternatively, when the client updates the data in the cache, copies of those data in other caches will become stale.
Communication protocols between the cache managers which keep the data consistent are known as coherency protocols.

==
Examples of hardware caches ==


===
CPU cache ===
Small memories on or close to the CPU can operate faster than the much larger main memory.
Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop and server microprocessors may have as many as six types of cache (between levels and functions).
Examples of caches with a specific function are the D-cache and I-cache and the translation lookaside buffer for the MMU.

===
GPU cache ===
Earlier graphics processing units (GPUs) often had limited read-only texture caches, and introduced Morton order swizzled textures to improve 2D cache coherency.
Cache misses would drastically affect performance, e.g. if mipmapping was not used.
Caching was important to leverage 32-bit (and wider) transfers for texture data that was often as little as 4 bits per pixel, indexed in complex patterns by arbitrary UV coordinates and perspective transformations in inverse texture mapping.

As GPUs advanced (especially with GPGPU compute shaders) they have developed progressively larger and increasingly general caches, including instruction caches for shaders, exhibiting increasingly common functionality with CPU caches.
For example, GT200 architecture GPUs did not feature an L2 cache, while the Fermi GPU has 768 KB of last-level cache, the Kepler GPU has 1536 KB of last-level cache, and the Maxwell GPU has 2048 KB of last-level cache.
These caches have grown to handle synchronisation primitives between threads and atomic operations, and interface with a CPU-style MMU.

===
DSPs ===
Digital signal processors have similarly generalised over the years.
Earlier designs used scratchpad memory fed by DMA, but modern DSPs such as Qualcomm Hexagon often include a very similar set of caches to a CPU (e.g. Modified Harvard architecture with shared L2, split L1 I-cache and D-cache).
===
Translation lookaside buffer ===
A memory management unit (MMU) that fetches page table entries from main memory has a specialized cache, used for recording the results of virtual address to physical address translations.
This specialized cache is called a translation lookaside buffer (TLB).

==
In-network cache ==


===
Information-centric networking ===
Information-centric networking (ICN) is an approach to evolve the Internet infrastructure away from a host-centric paradigm, based on perpetual connectivity and the end-to-end principle, to a network architecture in which the focal point is identified information (or content or data).
Due to the inherent caching capability of the nodes in an ICN, it can be viewed as a loosely connected network of caches, which has unique requirements of caching policies.
However, ubiquitous content caching introduces the challenge to content protection against unauthorized access, which requires extra care and solutions.

Unlike proxy servers, in ICN the cache is a network-level solution.
Therefore, it has rapidly changing cache states and higher request arrival rates; moreover, smaller cache sizes further impose a different kind of requirements on the content eviction policies.
In particular, eviction policies for ICN should be fast and lightweight.
Various cache replication and eviction schemes for different ICN architectures and applications have been proposed.

====
Policies ====


====
=
Time aware least recently used (TLRU) =====
The Time aware Least Recently Used (TLRU) is a variant of LRU designed for the situation where the stored contents in cache have a valid life time.
The algorithm is suitable in network cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
TLRU introduces a new term: TTU (Time to Use).
TTU is a time stamp of a content/page which stipulates the usability time for the content based on the locality of the content and the content publisher announcement.
Owing to this locality based time stamp, TTU provides more control to the local administrator to regulate in network storage.

In the TLRU algorithm, when a piece of content arrives, a cache node calculates the local TTU value based on the TTU value assigned by the content publisher.
The local TTU value is calculated by using a locally defined function.
Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node.
The TLRU ensures that less popular and small life content should be replaced with the incoming content.

=====
Least frequent recently used (LFRU) =====
The Least Frequent Recently Used (LFRU) cache replacement scheme combines the benefits of LFU and LRU schemes.
LFRU is suitable for 'in network' cache applications, such as Information-centric networking (ICN), Content Delivery Networks (CDNs) and distributed networks in general.
In LFRU, the cache is divided into two partitions called privileged and unprivileged partitions.
The privileged partition can be defined as a protected partition.
If content is highly popular, it is pushed into the privileged partition.
Replacement of the privileged partition is done as follows:  LFRU evicts content from the unprivileged partition, pushes content from privileged partition to unprivileged partition, and finally inserts new content into the privileged partition.

In the above procedure the LRU is used for the privileged partition and an approximated LFU (ALFU) scheme is used for the unprivileged partition, hence the abbreviation LFRU.

The basic idea is to filter out the locally popular contents with ALFU scheme and push the popular contents to one of the privileged partition.

==
Software caches ==


===
Disk cache ===
While CPU caches are generally managed entirely by hardware, a variety of software manages other caches.
The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel.

While the disk buffer, which is an integrated part of the hard disk drive, is sometimes misleadingly referred to as "disk cache", its main functions are write sequencing and read prefetching.
Repeated cache hits are relatively rare, due to the small size of the buffer in comparison to the drive's capacity.
However, high-end disk controllers often have their own on-board cache of the hard disk drive's data blocks.

Finally, a fast local hard disk drive can also cache information held on even slower data storage devices, such as remote servers (web cache) or local tape drives or optical jukeboxes; such a scheme is the main concept of hierarchical storage management.

Also, fast flash-based solid-state drives (SSDs) can be used as caches for slower rotational-media hard disk drives, working together as hybrid drives or solid-state hybrid drives (SSHDs).

===
Web cache ===

Web browsers and web proxy servers employ web caches to store previous responses from web servers, such as web pages and images.
Web caches reduce the amount of information that needs to be transmitted across the network, as information previously stored in the cache can often be re-used.
This reduces bandwidth and processing requirements of the web server, and helps to improve responsiveness for users of the web.
Web browsers employ a built-in web cache, but some Internet service providers (ISPs) or organizations also use a caching proxy server, which is a web cache that is shared among all users of that network.

Another form of cache is P2P caching, where the files most sought for by peer-to-peer applications are stored in an ISP cache to accelerate P2P transfers.
Similarly, decentralised equivalents exist, which allow communities to perform the same task for P2P traffic, for example, Corelli.

===
Memoization ===
A cache can store data that is computed on demand rather than retrieved from a backing store.

Memoization is an optimization technique that stores the results of resource-consuming function calls within a lookup table, allowing subsequent calls to reuse the stored results and avoid repeated computation.
It is related to the dynamic programming algorithm design methodology, which can also be thought of as a means of caching.

===
Other caches ===
The BIND DNS daemon caches a mapping of domain names to IP addresses, as does a resolver library.

Write-through operation is common when operating over unreliable networks (like an Ethernet LAN), because of the enormous complexity of the coherency protocol required between multiple write-back caches when communication is unreliable.
For instance, web page caches and client-side network file system caches (like those in NFS or SMB) are typically read-only or write-through specifically to keep the network protocol simple and reliable.

Search engines also frequently make web pages they have indexed available from their cache.
For example, Google provides a "Cached" link next to each search result.
This can prove useful when web pages from a web server are temporarily or permanently inaccessible.

Another type of caching is storing computed results that will likely be needed again, or memoization.
For example, ccache is a program that caches the output of the compilation, in order to speed up later compilation runs.

Database caching can substantially improve the throughput of database applications, for example in the processing of indexes, data dictionaries, and frequently used subsets of data.

A distributed cache uses networked hosts to provide scalability, reliability and performance to the application.
The hosts can be co-located or spread over different geographical regions.
== Buffer vs. cache ==
The semantics of a "buffer" and a "cache" are not totally different; even so, there are fundamental differences in intent between the process of caching and the process of buffering.

Fundamentally, caching realizes a performance increase for transfers of data that is being repeatedly transferred.
While a caching system may realize a performance increase upon the initial (typically write) transfer of a data item, this performance increase is due to buffering occurring within the caching system.

With read caches, a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache's (faster) intermediate storage rather than the data's residing location.
With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache's intermediate storage, deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process.
Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the cache's intermediate storage and the location where the data resides.
Buffering, on the other hand,

reduces the number of transfers for otherwise novel data amongst communicating processes, which amortizes overhead involved for several small transfers over fewer, larger transfers,
provides an intermediary for communicating processes which are incapable of direct transfers amongst each other, or
ensures a minimum data size or representation required by at least one of the communicating processes involved in a transfer.
With typical caching implementations, a data item that is read or written for the first time is effectively being buffered; and in the case of a write, mostly realizing a performance increase for the application from where the write originated.
Additionally, the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering.
The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering, although this form may negatively impact the performance of at least the initial reads (even though it may positively impact the performance of the sum of the individual reads).
In practice, caching almost always involves some form of buffering, while strict buffering does not involve caching.

A buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices.
Thus, addressable memory is used as an intermediate stage.
Additionally, such a buffer may be feasible when a large block of data is assembled or disassembled (as required by a storage device), or when data may be delivered in a different order than that in which it is produced.
Also, a whole buffer of data is usually transferred sequentially (for example to hard disk), so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer's latency as opposed to caching where the intent is to reduce the latency.
These benefits are present even if the buffered data are written to the buffer once and read from the buffer once.

A cache also increases transfer performance.
A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block.
But the main performance-gain occurs because there is a good chance that the same data will be read from cache multiple times, or that written data will soon be read.
A cache's sole purpose is to reduce accesses to the underlying slower storage.
Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers.

== See also ==


==
References ==


==
Further reading ==
"What Every Programmer Should Know About Memory" by Ulrich Drepper
"Caching in the Distributed Environment"
In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.

In the object-oriented programming paradigm, object can be a combination of variables, functions, and data structures; in particular in class-based variations of the paradigm it refers to a particular instance of a class.

In the relational model of database management, an object can be a table or column, or an association between data and a database entity (such as relating a person's age to a specific person).

== Object-based languages ==
An important distinction in programming languages is the difference between an object-oriented language and an object-based language.
A language is usually considered object-based if it includes the basic capabilities for an object: identity, properties, and attributes.
A language is considered object-oriented if it is object-based and also has the capability of polymorphism, inheritance, encapsulation, and, possibly, composition.
Polymorphism refers to the ability to overload the name of a function with multiple behaviors based on which object(s) are passed to it.
Conventional message passing discriminates only on the first object and considers that to be "sending a message" to that object.
However, some OOP languages such as Flavors and the Common Lisp Object System (CLOS) enable discriminating on more than the first parameter of the function.
Inheritance is the ability to subclass an object class, to create a new class that is a subclass of an existing one and inherits all the data constraints and behaviors of its parents but also adds new and/or changes one or more of them.

== Object-oriented programming ==

Object-oriented programming is an approach to designing modular reusable software systems.
The object-oriented approach is an evolution of good design practices that go back to the very beginning of computer programming.
Object-orientation is simply the logical extension of older techniques such as structured programming and abstract data types.
An object is an abstract data type with the addition of polymorphism and inheritance.

Rather than structure programs as code and data, an object-oriented system integrates the two using the concept of an "object".
An object has state (data) and behavior (code).
Objects can correspond to things found in the real world.
So for example, a graphics program will have objects such as circle, square, menu.
An online shopping system will have objects such as shopping cart, customer, product.
The shopping system will support behaviors such as place order, make payment, and offer discount.
The objects are designed as class hierarchies.
So for example with the shopping system there might be high level classes such as electronics product, kitchen product, and book.
There may be further refinements for example under electronic products: CD Player, DVD player, etc.
These classes and subclasses correspond to sets and subsets in mathematical logic.

==
Specialized objects ==
An important concept for objects is the design pattern.
A design pattern provides a reusable template to address a common problem.
The following object descriptions are examples of some of the most common design patterns for objects.

Function object: an object with a single method (
in C++, this method would be the function operator, "operator()")
that acts much like a function (like a C/C++ pointer to a function).

Immutable object: an object set up with a fixed state at creation time and which does not change afterward.

First-class object: an object that can be used without restriction.

Container object: an object that can contain other objects.

Factory object: an object whose purpose is to create other objects.

Metaobject: an object from which other objects can be created (compare with a class, which is not necessarily an object).

Prototype object: a specialized metaobject from which other objects can be created by copying
God object: an object that knows or does too much (it is an example of an anti-pattern).

Singleton object: an object that is the only instance of its class during the lifetime of the program.

Filter object: an object that receives a stream of data as its input and transforms it into the object's output.
Often the input and output streams are streams of characters, but these also may be streams of arbitrary objects.
These are generally used in wrappers since they conceal the existing implementation with the abstraction required at the developer side.

==
Distributed objects ==
The object-oriented approach is not just a programming model.
It can be used equally well as an interface definition language for distributed systems.
The objects in a distributed computing model tend to be larger grained, longer lasting, and more service-oriented than programming objects.

A standard method to package distributed objects is via an Interface Definition Language (IDL).
An IDL shields the client of all of the details of the distributed server object.
Details such as which computer the object resides on, what programming language it uses, what operating system, and other platform-specific issues.
The IDL is also usually part of a distributed environment that provides services such as transactions and persistence to all objects in a uniform manner.
Two of the most popular standards for distributed objects are the Object Management Group's CORBA standard and Microsoft's DCOM.In addition to distributed objects, a number of other extensions to the basic concept of an object have been proposed to enable distributed computing:
Protocol objects are components of a protocol stack that enclose network communication within an object-oriented interface.

Replicated objects are groups of distributed objects (called replicas) that run a distributed multi-party protocol to achieve high consistency between their internal states, and that respond to requests in a coordinated way.
Examples include fault-tolerant CORBA objects.

Live distributed objects (or simply live objects) generalize the replicated object concept to groups of replicas that might internally use any distributed protocol, perhaps resulting in only a weak consistency between their local states.
Some of these extensions, such as distributed objects and protocol objects, are domain-specific terms for special types of "ordinary" objects used in a certain context (such as remote method invocation or protocol composition).
Others, such as replicated objects and live distributed objects, are more non-standard, in that they abandon the usual case that an object resides in a single location at a time, and apply the concept to groups of entities (replicas) that might span across multiple locations, might have only weakly consistent state, and whose membership might dynamically change.

==
The Semantic Web ==
The Semantic Web is essentially a distributed-objects framework.
Two key technologies in the Semantic Web are the Web Ontology Language (OWL) and the Resource Description Framework (RDF).
RDF provides the capability to define basic objects—names, properties, attributes, relations—that are accessible via the Internet.
OWL adds a richer object model, based on set theory, that provides additional modeling capabilities such as multiple inheritance.

OWL objects are not like standard large-grained distributed objects accessed via an Interface Definition Language.
Such an approach would not be appropriate for the Internet because the Internet is constantly evolving and standardization on one set of interfaces is difficult to achieve.
OWL objects tend to be similar to the kinds of objects used to define application domain models in programming languages such as Java and C++.

However, there are important distinctions between OWL objects and traditional object-oriented programming objects.
Traditional objects get compiled into static hierarchies usually with single inheritance, but OWL objects are dynamic.
An OWL object can change its structure at run time and can become an instance of new or different classes.

Another critical difference is the way the model treats information that is currently not in the system.
Programming objects and most database systems use the "closed-world assumption".
If a fact is not known to the system that fact is assumed to be false.
Semantic Web objects use the open-world assumption, a statement is only considered false if there is actual relevant information that it is false, otherwise it is assumed to be unknown, neither true nor false.

OWL objects are actually most like objects in artificial intelligence frame languages such as KL-ONE and Loom.

The following table contrasts traditional objects from Object-Oriented programming languages such as Java or C++ with Semantic Web Objects:


==
See also ==
Object lifetime
Object copy
Design pattern (computer science)
Business object (computer science)
Actor model


==
References ==


==
External links ==
What Is an Object?
from The Java Tutorials
How to merge two or more php objects
In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω "I find, discover") is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.

This is achieved by trading optimality, completeness, accuracy, or precision for speed.

In a way, it can be considered a shortcut.

A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow.
For example, it may approximate the exact solution.

== Definition and motivation ==
The objective of a heuristic is to produce a solution in a reasonable time frame that is good enough for solving the problem at hand.

This solution may not be the best of all the solutions to this problem, or it may simply approximate the exact solution.
But it is still valuable because finding it does not require a prohibitively long time.

Heuristics may produce results by themselves, or they may be used in conjunction with optimization algorithms to improve their efficiency (e.g., they may be used to generate good seed values).

Results about NP-hardness in theoretical computer science make heuristics the only viable option for a variety of complex optimization problems that need to be routinely solved in real-world applications.

Heuristics underlie the whole field of Artificial Intelligence and the computer simulation of thinking, as they may be used in situations where there are no known algorithms.

==
Trade-off ==
The trade-off criteria for deciding whether to use a heuristic for solving a given problem include the following:
Optimality: When several solutions exist for a given problem, does the heuristic guarantee that the best solution will be found?
Is it actually necessary to find the best solution?

Completeness: When several solutions exist for a given problem, can the heuristic find them all?
Do we actually need all solutions?
Many heuristics are only meant to find one solution.

Accuracy and precision
: Can the heuristic provide a confidence interval for the purported solution?
Is the error bar on the solution unreasonably large?

Execution time
: Is this the best known heuristic for solving this type of problem?
Some heuristics converge faster than others.
Some heuristics are only marginally quicker than classic methods, in which case the 'overhead' on calculating the heuristic might have negative impact.
In some cases, it may be difficult to decide whether the solution found by the heuristic is good enough, because the theory underlying heuristics is not very elaborate.

==
Examples ==


===
Simpler problem ===
One way of achieving the computational performance gain expected of a heuristic consists of solving a simpler problem whose solution is also a solution to the initial problem.
===
Travelling salesman problem ===
An example of approximation is described by Jon Bentley for solving the travelling salesman problem (TSP):

"Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?"so as to select the order to draw using a pen plotter.
TSP is known to be NP-Hard
so an optimal solution for even a moderate size problem is difficult to solve.
Instead, the greedy algorithm can be used to give a good but not optimal solution (it is an approximation to the optimal answer) in a reasonably short amount of time.
The greedy algorithm heuristic says to pick whatever is currently the best next step regardless of whether that prevents (or even makes impossible) good steps later.
It is a heuristic in that practice says it is a good enough solution, theory says there are better solutions (and even can tell how much better in some cases).

===
Search ===
Another example of heuristic making an algorithm faster occurs in certain search problems.
Initially, the heuristic tries every possibility at each step, like the full-space search algorithm.
But it can stop the search at any time if the current possibility is already worse than the best solution already found.
In such search problems, a heuristic can be used to try good choices first so that bad paths can be eliminated early (see alpha-beta pruning).
In the case of best-first search algorithms, such as A* search, the heuristic improves the algorithm's convergence while maintaining its correctness as long as the heuristic is admissible..


===
Newell and Simon: heuristic search hypothesis ===
In their Turing Award acceptance speech, Allen Newell and Herbert A. Simon discuss the heuristic search hypothesis: a physical symbol system will repeatedly generate and modify known symbol structures until the created structure matches the solution structure.
Each following step depends upon the step before it, thus the heuristic search learns what avenues to pursue and which ones to disregard by measuring how close the current step is to the solution.
Therefore, some possibilities will never be generated as they are measured to be less likely to complete the solution.

A heuristic method can accomplish its task by using search trees.
However, instead of generating all possible solution branches, a heuristic selects branches more likely to produce outcomes than other branches.
It is selective at each decision point, picking branches that are more likely to produce solutions.

===
Antivirus software ===
Antivirus software often uses heuristic rules for detecting viruses and other forms of malware.
Heuristic scanning looks for code and/or behavioral patterns common to a class or family of viruses, with different sets of rules for different viruses.
If a file or executing process is found to contain matching code patterns and/or to be performing that set of activities, then the scanner infers that the file is infected.
The most advanced part of behavior-based heuristic scanning is that it can work against highly randomized self-modifying/mutating (polymorphic) viruses that cannot be easily detected by simpler string scanning methods.
Heuristic scanning has the potential to detect future viruses without requiring the virus to be first detected somewhere else, submitted to the virus scanner developer, analyzed, and a detection update for the scanner provided to the scanner's users.

==
Pitfalls ==
Some heuristics have a strong underlying theory; they are either derived in a top-down manner from the theory or are arrived at based on either experimental or real world data.
Others are just rules of thumb based on real-world observation or experience without even a glimpse of theory.
The latter are exposed to a larger number of pitfalls.

When a heuristic is reused in various contexts because it has been seen to "work" in one context, without having been mathematically proven to meet a given set of requirements, it is possible that the current data set does not necessarily represent future data sets (see: overfitting) and that purported "solutions" turn out to be akin to noise.

Statistical analysis can be conducted when employing heuristics to estimate the probability of incorrect outcomes.
To use a heuristic for solving a search problem or a knapsack problem, it is necessary to check that the heuristic is admissible.
Given a heuristic function 
  
    
      
        h
        (
        
          v
i
          
        
        ,
        
          v
g
          
        
        )
      
    
    {\displaystyle h(v_{i},v_{g})}
   meant to approximate the true optimal distance 
  
    
      
        
          d
          
            ⋆
(
        
          v
i
          
        
        ,
        
          v
g
          
        
        )
      
    
    {\displaystyle d^{\star }(v_{i},v_{g})}
to the goal node
v
g
          
        
      
    
    {\displaystyle v_{g}}
   in a directed graph
G
      
    
    {\displaystyle G}
   containing 
  
    
      
        n
      
    
    {\displaystyle n}
   total nodes or vertexes labeled
v
0
          
        
        ,
v
          
            1
,
⋯
        ,
        
          v
n
          
        
      
    
    {\displaystyle v_{0},v_{1},\cdots ,v_{n}}
  ,
"admissible" means roughly that the heuristic underestimates the cost to the goal or formally that 
  
    
      
        h
        (
        
          v
i
          
        
        ,
        
          v
g
          
        
        )
        ≤
        
          d
⋆
          
        
        (
        
          v
i
          
        
        ,
        
          v
g
          
        
        )
      
    
    {\displaystyle h(v_{i},v_{g})\leq d^{\star }(v_{i},v_{g})}
for all
(
        
          v
i
          
        
        ,
        
          v
g
          
        
        )
      
    
    {\displaystyle
(v_{i},v_{g})}
   where 
  
    
      
        
          i
          ,
g
        
        ∈
[
        0
        ,
        1
        ,
        .

.

.

,
n
        ]
{\displaystyle {i,g}\in
[0,1,...
,n]}
  .

If a heuristic is not admissible, it may never find the goal, either by ending up in a dead end of graph 
  
    
      
        G
      
    
    {\displaystyle G}
   or by skipping back and forth between two nodes
v
i
          
        
      
    
    {\displaystyle v_{i}}
   and 
  
    
      
        
          v
          
            j
          
        
      
    
    {\displaystyle v_{j}}
   where 
  
    
      
        
          i
          ,
          j
        
        ≠
g
      
    
    {\displaystyle {i,j}\neq g}
  .
==
Etymology ==
The word "heuristic" came into usage in the early 19th century.
It is formed irregularly from the Greek word heuriskein, meaning "to find".

== See also ==
Algorithm
Constructive heuristic
Genetic algorithm
Heuristic
Heuristic routing
Heuristic evaluation:
Method for identifying usability problems in user interfaces.

Metaheuristic: Methods for controlling and tuning basic heuristic algorithms, usually with usage of memory and learning.

Matheuristics:
Optimization algorithms made by the interoperation of metaheuristics and mathematical programming (MP) techniques.

Reactive search optimization:
Methods using online machine learning principles for self-tuning of heuristics.

Recursion (computer science)
Macro (computer science)


== References ==
Bounded rationality is the idea that rationality is limited when individuals make decisions.
In other words, humans "...preferences are determined by changes in outcomes relative to a certain reference level..."
as stated by Esther-Mirjam Sent (2018) Limitations include the difficulty of the problem requiring a decision, the cognitive capability of the mind, and the time available to make the decision.
Decision-makers, in this view, act as satisficers, seeking a satisfactory solution, rather than an optimal solution.
Therefore, humans do not undertake a full cost-benefit analysis to determine the optimal decision, but rather, choose an option that fulfils their adequacy criteria.
Herbert A. Simon proposed bounded rationality as an alternative basis for the mathematical and neoclassical economic modelling of decision-making, as used in economics, political science, and related disciplines.
The concept of bounded rationality complements "rationality as optimization", which views decision-making as a fully rational process of finding an optimal choice given the information available.
Therefore, bounded rationality can be said to address the discrepancy between the assumed perfect rationality of human behaviour (which is utilised by other economics theories such as the Neoclassical approach), and the reality of human cognition.
Simon used the analogy of a pair of scissors, where one blade represents "cognitive limitations" of actual humans and the other the "structures of the environment", illustrating how minds compensate for limited resources by exploiting known structural regularity in the environment.
Many economics models assume that agents are on average rational, and can in large quantities be approximated to act according to their preferences in order to maximise utility.
With bounded rationality, Simon's goal was "to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist.
"
In short, the concept of bounded rationality revises notions of "perfect" rationality to account for the fact that perfectly rational decisions are often not feasible in practice because of the intractability of natural decision problems and the finite computational resources available for making them.

The concept of bounded rationality continues to influence (and be debated in) different disciplines, including economics, psychology, law, political science, and cognitive science.
Some models of human behavior in the social sciences assume that humans can be reasonably approximated or described as "rational" entities, as in rational choice theory or Downs' Political Agency Model.

==
Origins ==
Bounded rationality was coined by Herbert A. Simon.
In Models of Man, Simon argues that most people are only partly rational, and are irrational in the remaining part of their actions.
In another work, he states "boundedly rational agents experience limits in formulating and solving complex problems and in processing (receiving, storing, retrieving, transmitting) information".
Simon describes a number of dimensions along which "classical" models of rationality can be made somewhat more realistic, while remaining within the vein of fairly rigorous formalization.
These include:

limiting the types of utility functions
recognizing the costs of gathering and processing information
the possibility of having a "vector" or "multi-valued" utility functionSimon suggests that economic agents use heuristics to make decisions rather than a strict rigid rule of optimization.
They do this because of the complexity of the situation.
An example of behaviour inhibited by heuristics can be seen when comparing the cognitive strategies utilised in simple situations (e.g Tic-tac-toe), in comparison to strategies utilised in difficult situations (e.g Chess).
Both games, as defined by game theory economics, are finite games with perfect information, and therefore equivalent.
However, within Chess, mental capacities and abilities are a binding constraint, therefore optimal choices are not a possibility.
Thus, in order to test the mental limits of agents, complex problems, such as those within Chess, should be studied to test how individuals work around their cognitive limits, and what behaviours or heuristics are used to form solutions 


== Model extensions ==
As decision-makers have to make decisions about how and when to decide, Ariel Rubinstein proposed to model bounded rationality by explicitly specifying decision-making procedures.
This puts the study of decision procedures on the research agenda.

Gerd Gigerenzer opines that decision theorists, to some extent, have not adhered to Simon's original ideas.
Rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with their inability to optimize.
Gigerenzer proposes and shows that simple heuristics often lead to better decisions than theoretically optimal procedures.
Moreover, Gigerenzer states, agents react relative to their environment and use their cognitive processes to adapt accordingly.
Huw Dixon later argues that it may not be necessary to analyze in detail the process of reasoning underlying bounded rationality.

If we believe that agents will choose an action that gets them "close" to the optimum, then we can use the notion of epsilon-optimization, which means we choose our actions so that the payoff is within epsilon of the optimum.
If we define the optimum (best possible) payoff as
U
          
            ∗
          
        
      
    
    {\displaystyle U^{*}}
  , then the set of epsilon-optimizing options S(ε) can be defined as all those options s such that:

  
    
      
        U
(
        s
        )
        ≥
U
          
            ∗
          
        
        −
ϵ
      
    
    {\displaystyle U(s)\geq U^{*}-\epsilon }
  .

The notion of strict rationality is then a special case (ε=0).
The advantage of this approach is that it avoids having to specify in detail the process of reasoning, but rather simply assumes that whatever the process is, it is good enough to get near to the optimum.

From a computational point of view, decision procedures can be encoded in algorithms and heuristics.
Edward Tsang argues that the effective rationality of an agent is determined by its computational intelligence.
Everything else being equal, an agent that has better algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer heuristics and algorithms.
Tshilidzi Marwala and Evan Hurwitz in their study on bounded rationality observed that advances in technology (e.g. computer processing power because of Moore's law, artificial intelligence, and big data analytics) expand the bounds that define the feasible rationality space.
Because of this expansion of the bounds of rationality, machine automated decision making makes markets more efficient.

It is also important to consider that the model of bounded rationality also extends tobounded self-interest in which humans are sometimes willing to forsake their own self-interests for the benefits of others, something that has not been considered in earlier economic models.

==
Relationship to Behavioral Economics ==
Bounded rationality implies the idea that humans take reasoning shortcuts that may lead to sub-optimal decision-making.
Behavioural economists engage in mapping the decision shortcuts that agents use in order to help increase the effectiveness of human decision-making.
One treatment of this idea comes from Cass Sunstein and Richard Thaler's Nudge.
Sunstein and Thaler recommend that choice architectures are modified in light of human agents' bounded rationality.
A widely cited proposal from Sunstein and Thaler urges that healthier food be placed at sight level in order to increase the likelihood that a person will opt for that choice instead of a less healthy option.
Some critics of Nudge have lodged attacks that modifying choice architectures will lead to people becoming worse decision-makers.
Furthermore, bounded rationality attempts to address assumption points discussed within Neoclassical Economics theory during the 1950s.
This theory assumes that the complex problem, the way in which the problem is presented, all alternative choices, and a utility function, are all provided to decision-makers in advance, where this may not be realistic.
This was widely used and accepted for a number of decades, however economists realised some disadvantages exist in utilising this theory.
This theory did not consider how problems are initially discovered by decision-makers, which could have an impact on the overall decision.
Additionally, personal values, the way in which alternatives are discovered and created, and the environment surrounding the decision-making process are also not considered when using this theory .
Alternatively, bounded rationality focuses on the cognitive ability of the decision-maker and the factors which may inhibit optimal decision-making Additionally, placing a focus on organisations rather than focusing on markets as Neoclassical Economics theory does, bounded rationality is also the basis for many other economics theories (e.g. Organisational theory) as it emphasises that the "...performance and success of an organisation is governed primarily by the psychological limitations of its members..." as stated by John D.W. Morecroft (1981) .

==
Relationship to Psychology ==
The collaborative works of Daniel Kahneman and Amos Tversky expand upon Herbert A. Simon's ideas in the attempt to create a map of bounded rationality.
The research attempted to explore the choices made by what was assumed as rational agents compared to the choices made by individuals optimal beliefs and their satisficing behaviour.
Kahneman cites that the research contributes mainly to the school of psychology due to imprecision of psychological research to fit the formal economic models, however, the theories are useful to economic theory as a way to expand simple and precise models and cover diverse psychological phenomena.
Three major topics covered by the works of Daniel Kahneman and Amos Tversky include Heuristics of judgement, risky choice, and framing effect, which were a culmination of research that fit under what was defined by Herbert A. Simon as the Psychology of Bounded Rationality.
In contrast to the work of Simon; Kahneman and Tversky aimed to focus on the effects bounded rationality had on simple tasks which therefore placed more emphasis on errors in cognitive mechanisms irrespective of the situation.

==
Influence on social network structure ==
Recent research has shown that bounded rationality of individuals may influence the topology of the social networks that evolve among them.
In particular, Kasthurirathna and Piraveenan have shown that in socio-ecological systems, the drive towards improved rationality on average might be an evolutionary reason for the emergence of scale-free properties.
They did this by simulating a number of strategic games on an initially random network with distributed bounded rationality, then re-wiring the network so that the network on average converged towards Nash equilibria, despite the bounded rationality of nodes.
They observed that this re-wiring process results in scale-free networks.
Since scale-free networks are ubiquitous in social systems, the link between bounded rationality distributions and social structure is an important one in explaining social phenomena.

==
Conclusion ==
To conclude, bounded rationality challenges the rationality assumptions widely accepted between the 1950s and 1970s which were initially used when considering [utility] maximisation, [probability] judgements, and other market-focused economic calculations .
Not only does the concept focus on the ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a great extent, given the limited information they access prior to decision-making for complex problems.
Although this concept realistically delves into decision-making and human cognition, challenging earlier theories which assumed perfect rational cognition and behaviour, bounded rationality can mean something different to everyone, and the way each person satisfices can vary dependant on their environment and the information they have access to .

== See also ==


==
Reference List ==


==
Further reading ==
Bayer, R. C., Renner, E., & Sausgruber, R. (2009).
Confusion and reinforcement learning in experimental public goods games.
NRN working papers 2009–22,
The Austrian Center for Labor Economics and the Analysis of the Welfare State, Johannes Kepler University Linz, Austria.

Elster, Jon (1983).
Sour Grapes: Studies in the Subversion of Rationality.
Cambridge, UK:
Cambridge University Press.
ISBN 978-0-521-25230-0.

Felin, T., Koenderink, J., & Krueger, J. (2017).
"
Rationality, perception and the all-seeing eye."
Psychonomic Bulletin and Review, 25: 1040-1059.
DOI
10.3758/s13423-016-1198-z
Gershman, S.J., Horvitz, E.J., & Tenenbaum, J.B. (2015).
Computational rationality: A converging paradigm for intelligence in brains, minds, and machines.
Science, 49: 273-278.
DOI:
10.1126/
science.aac6076
Gigerenzer, Gerd & Selten, Reinhard (2002).
Bounded Rationality.
Cambridge:
MIT Press.
ISBN 978-0-262-57164-7.

Hayek, F.A (1948)
Individualism and Economic order
Kahneman, Daniel (2003).
"
Maps of bounded rationality: psychology for behavioral economics" (PDF).
The American Economic Review.
93 (5):
1449–75.
CiteSeerX 10.1.1.194.6554.
doi:10.1257/000282803322655392.
Archived from the original (PDF) on 2018-02-19.
Retrieved 2017-11-01.

March, James G. (1994).
A Primer on Decision Making: How Decisions Happen.
New York:
The Free Press.
ISBN 978-0-02-920035-3.

Simon, Herbert (1957). "
A Behavioral Model of Rational Choice", in Models of Man, Social and Rational:
Mathematical Essays on Rational Human Behavior in a Social Setting.
New York: Wiley.

March, James G. & Simon, Herbert (1958).
Organizations.
John Wiley and Sons.
ISBN 978-0-471-56793-6.

Simon, Herbert (1990).
"
A mechanism for social selection and successful altruism".
Science.
250 (4988)
: 1665–8.
Bibcode:1990Sci...
250.1665S.
doi:10.1126/science.2270480.
PMID 2270480.

Simon, Herbert (1991). "
Bounded Rationality and Organizational Learning".
Organization Science.
2 (1): 125–134.
doi:10.1287/orsc.2.1.125.

Tisdell, Clem (1996).
Bounded Rationality and Economic Evolution:
A Contribution to Decision Making, Economics, and Management.
Cheltenham, UK: Brookfield.
ISBN 978-1-85898-352-3.

Wheeler, Gregory (2018). "
Bounded Rationality".

In Edward Zalta (ed.).
Stanford Encyclopedia of Philosophy.
Stanford, CA.

Williamson, Oliver E. (1981).
"
The economics of organization: the transaction cost approach".
American Journal of Sociology.
87 (3): 548–577 (press +).
doi:10.1086/227496.
S2CID 154070008.

==
External links ==
Bounded Rationality in Stanford Encyclopedia of Philosophy
Mapping Bounded Rationality by Daniel Kahneman
Artificial Intelligence and Economic Theory chapter 7 of Surfing Economics by Huw Dixon.

"Resource Bounded Agents".
Internet Encyclopedia of Philosophy.
Scarcity, in the area of social psychology, works much like scarcity in the area of economics.

Simply put, humans place a higher value on an object that is scarce, and a lower value on those that are in abundance.
For example diamonds are more valuable than rocks because diamonds are not as abundant.
The scarcity heuristic is a mental shortcut that places a value on an item based on how easily it might be lost, especially to competitors.
The scarcity heuristic stems from the idea that the more difficult it is to acquire an item the more value that item has.
In many situations we use an item’s availability, its perceived abundance, to quickly estimate quality and/or utility.
This can lead to systemic errors or cognitive bias.
There are two social psychology principles that work with scarcity that increase its powerful force.

One is social proof.

This is a contributing factor to the effectiveness of scarcity, because if a product is sold out, or inventory is extremely low, humans interpret that to mean the product must be good since everyone else appears to be buying it.

The second contributing principle to scarcity is commitment and consistency.

If someone has already committed themselves to something, then find out they cannot have it, it makes the person want the item more.

== Examples ==
This idea is deeply embedded in the intensely popular “Black Friday” shopping extravaganza that U.S. consumers participate in every year on the day after Thanksgiving.

More than getting a bargain on a hot gift idea, shoppers thrive on the competition itself, in obtaining the scarce product.

== Heuristics ==
Heuristics are strategies that use readily accessible (though loosely applicable) information for problem solving.

We use heuristics to speed up our decision-making process when an exhaustive, deliberative process is perceived to be impractical or unnecessary.

Thus heuristics are simple, efficient rules, which have developed through either evolutionary proclivities or past learning.
While these “rules” work well in most circumstances, there are certain situations where they can lead to systemic errors or cognitive bias.
The scarcity heuristic is only one example of how mental “rules” can result in unintended bias in decision-making.

Other heuristics and biases include the availability heuristic, survivorship bias, confirmation bias, and the self-attribution bias.
Like the scarcity heuristic, all of these phenomena result from either evolutionary or past behavior patterns and can consistently lead to faulty decision-making in specific circumstances.

Scarcity appears to have created a number of heuristics such as when price is used as a cue to the quality of products, as cue to the healthfulness of medical conditions, and as a cue to the sexual content of books when age restrictions are put in place.
These heuristic judgments should increase the desirability of a stimulus to those who value the inferred attributes.
The scarcity heuristic does not only apply to a shortage in absolute resources.
According to Robert Cialdini, the scarcity heuristic leads to us to make biased decisions on a daily basis.
It is particularly common to be biased by the scarcity heuristic when assessing four parameters: quantity, rarity, time, and censorship.

===
Quantity ===
The simplest manifestation of the scarcity heuristic is the fear of losing access to some resource resulting from the possession of a small or diminishing quantity of the asset.
For example, your favorite shirt becomes more valuable when you know you cannot replace it.
If you had ten shirts of the same style and color, losing one would likely be less distressful because you have several others to take its place.

Cialdini theorizes that it is in our nature to fight against losing freedom, pointing out that we value possessions in low quantities partly because as resources become less available they are more likely not to be available at all at some point in the future.
If the option to use that resource disappears entirely, then options decrease and so does our freedom.

Cialdini draws his conclusion from psychological reactance theory, which states that whenever free choice is limited or threatened, the need to retain freedom makes us desire the object under threat more than if it was not in danger of being lost.
In the context of the scarcity heuristic, this implies that when something threatens our prior access to a resource, we will react against that interference by trying to possess the resource with more vigor than before.
===
Rarity ===
Objects can increase in value if we feel that they have unique properties, or are exceptionally difficult to replicate.
Collectors of rare baseball cards or stamps are simple examples of the principle of rarity.

===
Time ===
When time is scarce and information complex, people are prone to use heuristics in general.
When time is perceived to be short, politicians can exploit the scarcity heuristic.

The Bush administration used a variation of this theme in justifying the rush to war in Iraq: "time is running out for Saddam and unless we stop him now he will use his WMD against us".
The Scarcity Rule is the sales tool that is most obvious to us when we see advertising terms including, “
Sale ends June 30th”;
“The First Hundred People Receive…”; “Limited Time Only”; “
Offer Expires”.
===
Restriction and censorship ===
According to Worchel, Arnold & Baker (1975), our reaction to censorship is to want the censored information more than before it was restricted as well perceive the censored message more favorably than before the ban.

This research indicates that people not only want censored information more but have an increased susceptibility to the message of the censored material.
Worchel, Arnold, and Baker came to this by testing students’ attitudes toward co-ed dormitories at the University of North Carolina.
They found that when students were told that speech against the idea of co-ed dorms was banned, students saw co-ed dorms as less favorable than if the discourse about the dorms had remained open.
Thus, even without having heard any argument against co-ed dormitories, students were more prone to being persuaded to be opposed simply as a reaction to the ban.

Another experiment (Zellinger et al.
1975) divided students into two groups and gave them the same book.

In one group the book was clearly labeled as “mature content” and was restricted for readers 21 and older while the other group's book had no such warning.
When asked to indicate their feelings toward the literature the group with the warning demonstrated a higher desire to read the book and a stronger conviction that they would like the book than those without the warning.

==
Studies ==
Numerous studies have been conducted on the topic of scarcity in social psychology:
Scarcity rhetoric in a job advertisement for restaurant server positions has been investigated.
Subjects were presented with two help-wanted ads, one of which suggested numerous job vacancies, while the other suggested that very few were available.
The study found that subjects who were presented with the advertisement that suggested limited positions available viewed the company as being a better one to work for than the one that implied many job positions were available.
Subjects also felt that the advertisement that suggested limited vacancies translated to higher wages.
In short, subjects placed a positive, higher value on the company that suggested that there were scarce job vacancies available.
Another study examined how the scarcity of men may lead women to seek high-paying careers and to delay starting a family.
This effect was driven by how the sex ratio altered the mating market, not just the job market.
Sex ratios involving a scarcity of men led women to seek lucrative careers because of the difficulty women have in finding an investing, long-term mate under such circumstances.

==
Conditional variations ==
Although the scarcity heuristic can always affect judgment and perception, certain situations exacerbate the effect.
New scarcity and competition are common cases.

===
New scarcity ===
New scarcity occurs when our irrational desire for limited resources increases when we move from a state of abundance to a state of scarcity.
This is in line with psychological reactance theory, which states that a person will react strongly when they perceive that their options are likely to be lessened in the future.

Worchel, Lee & Adewole (1975) demonstrated this principle with a simple experiment.
They divided people into two groups, giving one group a jar of ten cookies and another a jar with only two cookies.
When asked to rate the quality of the cookie the group with two, in line with the scarcity heuristic, found the cookies more desirable.
The researchers then added a new element.

Some participants were first given a jar of ten cookies, but before participants could sample the cookie, experimenters removed 8 cookies so that there were again only two.
The group first having ten
but then were reduced to two, rated the cookies more desirable than both of the other groups.

===
Quantifying value in scarce and competitive situations ===
Mittone & Savadori (2009) created an experiment where the same good was abundant in one condition but scarce in another.
The scarcity condition involved a partner/competitor to create scarcity, while the abundant condition did not.
Results showed that more participants chose a good when it was scarce than when it was abundant, for two out of four sets of items (ballpoints, snacks, pencils, and key rings).

The experiment then created a WTA (willingness to accept) elicitation procedure that created subjective values for goods.
Results showed the scarce good receiving a higher WTA price by participants choosing it, than by those who did not, compared to the WTA of the abundant good, despite the fact that both types of participants assigned a lower market price to the scarce good, as compared to the abundant one.

====
Other applications ====
This idea could easily by applied to other fields.
In 1969, James C. Davis postulated that revolutions are most likely to occur during periods of improving economic and social conditions that are immediately followed by a short and sharp reversal in that trend.
Therefore, it is not the consistently downtrodden, those in a state of constant scarcity, who revolt but rather those who experience new scarcity that are most likely to feel a desire of sufficient intensity to incite action.

===
Competition ===
In situations when others are directly vying for scarce resources, the value we assign to objects is further inflated.
Advertisers commonly take advantage of scarcity heuristics by marketing products as “hot items” or by telling customers that certain goods will sell out quickly.

Worchel, Lee & Adewole (1975) also examined the competition bias in their cookie experiment, taking the group that had experienced new scarcity, going from ten to two cookies, and telling half of them that the reason they were losing cookies is because there was high demand for cookies from other participants taking the test.
They then told the other half that it was just because a mistake had been made.
It was found that the half we were told that they were having their cookie stock reduced due to social demand rated the cookies higher than those who were told it was only due to an error.

In 1983, Coleco Industries marketed a soft-sculpted doll that had exaggerated neonatal features and came with "adoption papers".
Demand for these dolls exceeded expectations, and spot shortages began to occur shortly after their introduction to the market.
This scarcity fueled demand even more and created what became known as the Cabbage Patch panic (Langway, Hughey, McAlevey, Wang, & Conant, 1983).
Customers scratched, choked, pushed, and fought one another in an attempt to get the dolls.

Several stores were wrecked during these riots, so many stores began requiring people to wait in line (for as long as 14 hours) in order to obtain one of the dolls.
A secondary market quickly developed where sellers were receiving up to $150 per doll.
Even at these prices, the dolls were so difficult to obtain that one Kansas City postman flew to London to get one for his daughter (Adler et al.,
1983).

== See also ==
Artificial scarcity
Principle
of least interest


==
References ==


==
Bibliography ==
Cialdini, Robert B. (2001)
[1984].
Influence: Science and Practice
(4th ed.).
Boston:
Allyn and Bacon.
ISBN 9780321011473.

Gigerenzer, Gerd (1991).
"
How to Make Cognitive Illusions Disappear: Beyond "Heuristics and Biases"" (PDF).
European Review of Social Psychology.
2
: 83–115.
CiteSeerX 10.1.1.336.9826.
doi:10.1080/14792779143000033.

Lynn, Michael (1989).
"
Scarcity effects on desirability: Mediated by assumed expensiveness?".
Journal of Economic Psychology.
10 (2): 257–274.
doi:10.1016/0167-4870(89)90023-8.
hdl:1813/72078.

Lynn, Michael (1992).
"
The Psychology of Unavailability:
Explaining Scarcity and Cost Effects on Value".
Basic and Applied Social Psychology.
13 (1): 3–7.
doi:10.1207
/s15324834basp1301_2.
hdl:1813/71653.

Mittone, Luigi; Savadori, Lucia (2009).
"
The Scarcity Bias".
Applied Psychology.
58 (3): 453–468.
doi:10.1111/j.1464-0597.2009.00401.x.

Pearl, Judea (1985).
Heuristics:
Intelligent search strategies for computer problem solving (Repr.
with corr.
ed.)
.
Reading, Mass.:
Addison-Wesley Pub.
Co. p. vii.
ISBN 978-0-201-05594-8.

Worchel, Stephen; Arnold, Susan; Baker, Michael (1975).
"
The Effects of Censorship on Attitude Change:
The Influence of Censor and Communication Characteristics" (PDF).
Journal of Applied Social Psychology.
5 (3): 227–239.
doi:10.1111/j.1559-1816.1975.tb00678.x.
Archived from the original on 2015-02-23.CS1 maint: bot: original URL status unknown (link)
Worchel, Stephen; Lee, Jerry; Adewole, Akanbi (1975).
"
Effects of supply and demand on ratings of object value".
Journal of Personality and Social Psychology.
32 (5): 906–914.
doi:10.1037/0022-3514.32.5.906.

Zellinger, David A.; Fromkin, Howard L.; Speller, Donald E.; Kohn, Carol A. (1975). "
A commodity theory analysis of the effects of age restrictions upon pornographic materials".
Journal of Applied Psychology.
60 (1):
94–99.
doi:10.1037/h0076350.
==
Further reading ==
Tauer, John M. (2007). "
Scarcity Principle".

In Baumeister, Roy; Vohs, Kathleen (eds.).
Encyclopedia of Social Psychology.
doi:10.4135/9781412956253.n466.
ISBN 9781412916707.
The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics.

It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute.
In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.
This is a divide and conquer algorithm with run-time of 
  
    
      
        O
(
        n
p
        )
      
    
    {\displaystyle O(np)}
  , where n is the number of polygons and p is the number of pixels in the viewport.

The inputs are a list of polygons and a viewport.
The best case is that if the list of polygons is simple, then draw the polygons in the viewport.
Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer).
The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, with a polygon list modified such that it only contains polygons that are visible in that quadrant.

Warnock expressed his algorithm in words and pictures, rather than software code, as the core of his PhD thesis, which also described protocols for shading oblique surfaces and other features that are now the core of 3-dimensional computer graphics.
The entire thesis was only 26 pages from Introduction to Bibliography.

==
References ==


==
External links ==
A summary of the Warnock Algorithm
Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.

In computational geometry, numerous algorithms are proposed for computing the convex hull of a finite set of points, with various computational complexities.

Computing the convex hull means that a non-ambiguous and efficient representation of the required convex shape is constructed.
The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and sometimes also in terms of h, the number of points on the convex hull.

==
Planar case ==
Consider the general case when the input to the algorithm is a finite unordered set of points on a Cartesian plane.
An important special case, in which the points are given in the order of traversal of a simple polygon's boundary, is described later in a separate subsection.

If not all points are on the same line, then their convex hull is a convex polygon whose vertices are some of the points in the input set.
Its most common representation is the list of its vertices ordered along its boundary clockwise or counterclockwise.
In some applications it is convenient to represent a convex polygon as an intersection of a set of half-planes.

===
Lower bound on computational complexity ===
For a finite set of points in the plane the lower bound on the computational complexity of finding the convex hull represented as a convex polygon is easily shown to be the same as for sorting using the following reduction.
For the set 
  
    
      
        
          x
          
            1
          
        
        ,
…
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
   numbers to sort consider the set of points
(
        
          x
          
            1
          
        
        ,
x
          
            1
          
          
            2
          
        
        )
,
        …
        ,
(
        
          x
          
            n
          
        
        ,
x
          
            n
          
          
            2
          
        
        )
      
    
    {
\displaystyle (x_{1},x_{1}^{2}),\dots ,(x_{n},x_{n}^{2})}
   of points in the plane.
Since they lie on a parabola, which is a convex curve it is easy to see that the vertices of the convex hull, when traversed along the boundary, produce the sorted order of the numbers
x
          
            1
          
        
        ,
…
        ,
        
          x
          
            n
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{n}}
  .
Clearly, linear time is required for the described transformation of numbers into points and then extracting their sorted order.
Therefore, in the general case the convex hull of n points cannot be computed more quickly than sorting.

The standard Ω(n log n)
lower bound for sorting is proven in the decision tree model of computing, in which only numerical comparisons but not arithmetic operations can be performed; however, in this model, convex hulls cannot be computed at all.
Sorting also requires Ω(n log n) time in the algebraic decision tree model of computation, a model that is more suitable for convex hulls, and in this model convex hulls also require Ω(n log n) time.
However, in models of computer arithmetic that allow numbers to be sorted more quickly than O(n log n) time, for instance by using integer sorting algorithms, planar convex hulls can also be computed more quickly: the Graham scan algorithm for convex hulls consists of a single sorting step followed by a linear amount of additional work.

===
Optimal output-sensitive algorithms ===
As stated above, the complexity of finding a convex hull as a function of the input size
n is lower bounded by Ω(n log n).
However, the complexity of some convex hull algorithms can be characterized in terms of both input size n and the output size h (the number of points in the hull).
Such algorithms are called output-sensitive algorithms.
They may be asymptotically more efficient than Θ(n log n) algorithms in cases when h = o(n).

The lower bound on worst-case running time of output-sensitive convex hull algorithms was established to be Ω(n log h) in the planar case.
There are several algorithms which attain this optimal time complexity.
The earliest one was introduced by Kirkpatrick and Seidel in 1986 (who called it "the ultimate convex hull algorithm").
A much simpler algorithm was developed by Chan in 1996, and is called Chan's algorithm.

===
Algorithms ===
Known convex hull algorithms are listed below, ordered by the date of first publication.
Time complexity of each algorithm is stated in terms of the number of inputs points n and the number of points on the hull
h. Note that in the worst case h may be as large as n.

Gift wrapping, a.k.a.
Jarvis march —
O(nh)  One of the simplest (although not the most time efficient in the worst case) planar algorithms.
Created independently by Chand & Kapur in 1970 and R. A. Jarvis in 1973.
It has O(nh) time complexity, where n is the number of points in the set, and h is the number of points in the hull.
In the worst case the complexity is Θ(n2).

Graham scan —
O(n log n)
A slightly more sophisticated, but much more efficient algorithm, published by Ronald Graham in 1972.
If the points are already sorted by one of the coordinates or by the angle to a fixed vector, then the algorithm takes O(n) time.

Quickhull  Created independently in 1977 by W. Eddy and in 1978 by A. Bykat.
Just like the quicksort algorithm, it has the expected time complexity of O(n log n), but may degenerate to O(n2) in the worst case.

Divide and conquer —
O(n log n)
Another O(n log n) algorithm, published in 1977 by Preparata and Hong.
This algorithm is also applicable to the three dimensional case.

Monotone chain, a.k.a.
Andrew's algorithm—
O(n log n)
Published in 1979 by A. M. Andrew.
The algorithm can be seen as a variant of Graham scan which sorts the points lexicographically by their coordinates.
When the input is already sorted, the algorithm takes O(n) time.

Incremental convex hull algorithm — O(n log n)  Published in 1984 by Michael Kallay.

Kirkpatrick–Seidel algorithm — O(n log h)
The first optimal output-sensitive algorithm.
It modifies the divide and conquer algorithm by using the technique of marriage-before-conquest and low-dimensional linear programming.
Published by Kirkpatrick and Seidel in 1986.

Chan's algorithm — O(n log h)
A simpler optimal output-sensitive algorithm created by Chan in 1996.
It combines gift wrapping with the execution of an O(n log n) algorithm (such as Graham scan) on small subsets of the input.
===
Akl–
Toussaint heuristic ===
The following simple heuristic is often used as the first step in implementations of convex hull algorithms to improve their performance.
It is based on the efficient convex hull algorithm by Selim Akl and G. T. Toussaint, 1978.
The idea is to quickly exclude many points that would not be part of the convex hull anyway.

This method is based on the following idea.

Find the two points with the lowest and highest x-coordinates, and the two points with the lowest and highest y-coordinates.
(
Each of these operations takes O(n).)
These four points form a convex quadrilateral, and all points that lie in this quadrilateral (except for the four initially chosen vertices) are not part of the convex hull.

Finding all of these points that lie in this quadrilateral is also O(n), and thus, the entire operation is O(n).
Optionally, the points with smallest and largest sums of x- and y-coordinates as well as those with smallest and largest differences of x- and y-coordinates can also be added to the quadrilateral, thus forming an irregular convex octagon, whose insides can be safely discarded.
If the points are random variables, then for a narrow but commonly encountered class of probability density functions, this throw-away pre-processing step will make a convex hull algorithm run in linear expected time, even if the worst-case complexity of the convex hull algorithm is quadratic in n.


===
On-line and dynamic convex hull problems ===
The discussion above considers the case when all input points are known in advance.
One may consider two other settings.

Online convex hull problem
: Input points are obtained sequentially one by one.
After each point arrives on input, the convex hull for the pointset obtained so far must be efficiently computed.

Dynamic convex hull maintenance: The input points may be sequentially inserted or deleted, and the convex hull must be updated after each insert/delete operation.
Insertion of a point may increase the number of vertices of a convex hull at most by 1, while deletion may convert an n-vertex convex hull into an n-1-vertex one.

The online version may be handled with O(log n) per point, which is asymptotically optimal.
The dynamic version may be handled with O(log2 n) per operation.
===
Simple polygon ===
The convex hull of a simple polygon is divided by the polygon into pieces, one of which is the polygon itself and the rest are pockets bounded by a piece of the polygon boundary and a single hull edge.
Although many algorithms have been published for the problem of constructing the convex hull of a simple polygon, nearly half of them are incorrect.

McCallum and Avis provided the first correct algorithm.

A later simplification by Graham & Yao (1983) and Lee (1983) uses only a single stack data structure.
Their algorithm traverses the polygon clockwise, starting from its leftmost vertex.
As it does, it stores a convex sequence of vertices on the stack, the ones that have not yet been identified as being within pockets.
At each step, the algorithm follows a path along the polygon from the stack top to the next vertex that is not in one of the two pockets adjacent to the stack top.
Then, while the top two vertices on the stack together with this new vertex are not in convex position, it pops the stack, before finally pushing the new vertex onto the stack.
When the clockwise traversal reaches the starting point, the algorithm returns the sequence of stack vertices as the hull.

==
Higher dimensions ==
A number of algorithms are known for the three-dimensional case, as well as for arbitrary dimensions.
Chan's algorithm is used for dimensions 2 and 3, and Quickhull is used for computation of the convex hull in higher dimensions.
For a finite set of points, the convex hull is a convex polyhedron in three dimensions, or in general a convex polytope for any number of dimensions, whose vertices are some of the points in the input set.

Its representation is not so simple as in the planar case, however.
In higher dimensions, even if the vertices of a convex polytope are known, construction of its faces is a non-trivial task, as is the dual problem of constructing the vertices given the faces.
The size of the output face information may be exponentially larger than the size of the input vertices, and even in cases where the input and output are both of comparable size the known algorithms for high-dimensional convex hulls are not output-sensitive due both to issues with degenerate inputs and with intermediate results of high complexity.

== See also ==
Orthogonal convex hull


==
References ==


==
Further reading ==
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms, Second Edition.
MIT Press and McGraw-Hill, 2001.
ISBN 0-262-03293-7.
Section 33.3:
Finding the convex hull, pp.
947–957.

Franco P. Preparata, S.J. Hong.
Convex Hulls of Finite Sets of Points in Two and Three Dimensions, Commun.
ACM, vol.
20, no.
2, pp.
87–93, 1977.

Mark de Berg; Marc van Kreveld; Mark Overmars & Otfried Schwarzkopf (2000).
Computational Geometry (2nd revised ed.).
Springer-Verlag.
ISBN 978-3-540-65620-3.
Section 1.1:
An Example: Convex Hulls (describes classical algorithms for 2-dimensional convex hulls).
Chapter 11:
Convex Hulls: pp.
235–250
(describes a randomized algorithm for 3-dimensional convex hulls due to Clarkson and Shor).

==
External links ==
Weisstein, Eric W. "Convex Hull".
MathWorld.

2D, 3D, and dD Convex Hull in CGAL, the Computational Geometry Algorithms Library
Qhull code for Convex Hull, Delaunay Triangulation, Voronoi Diagram, and Halfspace Intersection
Demo as Flash swf,  Jarvis, Graham,
Quick (divide and conquer) and
Chan algorithms
Gift wrapping algorithm in C#
Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient.
Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such as counting the paths through a graph.
Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).

Directly applying the mathematical definition of matrix multiplication gives an algorithm that takes time on the order of n3 field operations to multiply two n × n matrices over that field
(Θ(n3) in big O notation).
Better asymptotic bounds on the time required to multiply matrices have been known since the Strassen's algorithm in the 1960s, but it is still unknown what the optimal time is (i.e., what the complexity of the problem is).
As of December 2020, the matrix multiplication algorithm with best asymptotic complexity runs in O(n2.3728596)
time, given by Josh Alman and Virginia Vassilevska Williams, however this algorithm is a galactic algorithm because of the large constants and cannot be realized practically.

==
Iterative algorithm ==
The definition of matrix multiplication is that if C = AB for an n × m matrix A and an m × p matrix B, then C is an n × p matrix with entries
c
i
j
          
        
        =
        
          ∑
          
            k
=
            1
          
          
            m
a
i
k
          
        
        
          b
          
            k
j
          
        
      
    
    {\displaystyle c_{ij}=\sum
_{k=1}^{m}a_{ik}b_{kj}}
.From
this, a simple algorithm can be constructed which loops over the indices
i from 1 through n and j from 1 through p, computing the above using a nested loop:
This algorithm takes time Θ(nmp)
(in asymptotic notation).
A common simplification for the purpose of algorithms analysis is to assume that the inputs are all square matrices of size n × n, in which case the running time is Θ(n3), i.e., cubic in the size of the dimension.

===
Cache behavior ===
The three loops in iterative matrix multiplication can be arbitrarily swapped with each other without an effect on correctness or asymptotic running time.
However, the order can have a considerable impact on practical performance due to the memory access patterns and cache use of the algorithm;
which order is best also depends on whether the matrices are stored in row-major order, column-major order, or a mix of both.

In particular, in the idealized case of a fully associative cache consisting of M bytes and b bytes per cache line (i.e. M/b cache lines), the above algorithm is sub-optimal for A and B stored in row-major order.
When n > M/b, every iteration of the inner loop (a simultaneous sweep through a row of A and a column of B) incurs a cache miss when accessing an element of B.
This means that the algorithm incurs Θ(n3) cache misses in the worst case.
As of 2010, the speed of memories compared to that of processors is such that the cache misses, rather than the actual calculations, dominate the running time for sizable matrices.
The optimal variant of the iterative algorithm for A and B in row-major layout is a tiled version, where the matrix is implicitly divided into square tiles of size √M by √M:
In the idealized cache model, this algorithm incurs only Θ(n3/b √M) cache misses; the divisor b √M amounts to several orders of magnitude on modern machines, so that the actual calculations dominate the running time, rather than the cache misses.

== Divide-and-conquer algorithm ==
An alternative to the iterative algorithm is the divide-and-conquer algorithm for matrix multiplication.
This relies on the block partitioning

  
    
      
        C
        =
(
            
              
                
                  
                    C
                    
                      11
C
                    
                      12
C
                    
                      21
C
                    
                      22
                    
                  
                
              
            
            )
          
        
        ,
        
        A
        =
(
            
              
                
                  
                    A
                    
                      11
A
                    
                      12
A
                    
                      21
A
                    
                      22
                    
                  
                
              
            
            )
          
        
        ,
        
        B
        =
(
            
              
                
                  
                    B
                    
                      11
B
                    
                      12
B
                    
                      21
B
                    
                      22
                    
                  
                
              
            
            )
          
        
      
    
    {\displaystyle C={\begin{pmatrix}C_{11}&C_{12}\\C_{21}&C_{22}\\\end{pmatrix}},\,A={\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\\\end{pmatrix}},\,B={\begin{pmatrix}B_{11}&B_{12}\\B_{21}&B_{22}\\\end{pmatrix}}}
  ,which works for all square matrices whose dimensions are powers of two, i.e., the shapes are 2n × 2n for some n.
The matrix product is now

  
    
      
        
          
            (
C
                    
                      11
C
                    
                      12
C
                    
                      21
                    
                  
                
                
                  
                    C
                    
                      22
                    
                  
                
              
            
            )
          
        
        =
(
            
              
                
                  
                    A
                    
                      11
A
                    
                      12
A
                    
                      21
A
                    
                      22
                    
                  
                
              
            
            )
(
            
              
                
                  
                    B
                    
                      11
B
                    
                      12
B
                    
                      21
B
                    
                      22
                    
                  
                
              
            
            )
=
(
            
              
                
                  
                    A
                    
                      11
B
                    
                      11
+
A
                    
                      12
B
                    
                      21
A
                    
                      11
B
                    
                      12
+
A
                    
                      12
B
                    
                      22
A
                    
                      21
B
                    
                      11
+
A
                    
                      22
B
                    
                      21
A
                    
                      21
B
                    
                      12
+
A
                    
                      22
B
                    
                      22
                    
                  
                
              
            
            )
          
        
      
    
    {\displaystyle {\begin{pmatrix}C_{11}&C_{12}\\C_{21}&C_{22}\\\end{pmatrix}}={\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\\\end{pmatrix}}{\begin{pmatrix}B_{11}&B_{12}\\B_{21}&B_{22}\\\end{pmatrix}}={\begin{pmatrix}A_{11}B_{11}+A_{12}B_{21}&A_{11}B_{12}+A_{12}B_{22}\\A_{21}B_{11}+A_{22}B_{21}&A_{21}B_{12}+A_{22}B_{22}\\\end{pmatrix}}}
  which consists of eight multiplications of pairs of submatrices, followed by an addition step.
The divide-and-conquer algorithm computes the smaller multiplications recursively, using the scalar multiplication c11 = a11b11 as its base case.

The complexity of this algorithm as a function of n is given by the recurrence

  
    
      
        T
(
        1
        )
=
        Θ
(
        1
)
{\displaystyle T(1)=\Theta (1)}
  ;

  
    
      
        T
(
n
        )
        =
8
T
(
n
        
          /
        
        2
)
+
        Θ
(
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle
T(n)=8T(n/2)+\Theta (n^{2})}
  ,accounting for the eight recursive calls on matrices of size n/2 and Θ(n2) to sum the four pairs of resulting matrices element-wise.
Application of the master theorem for divide-and-conquer recurrences shows this recursion to have the solution Θ(n3), the same as the iterative algorithm.

=== Non-square matrices ===
A variant of this algorithm that works for matrices of arbitrary shapes and is faster in practice splits matrices in two instead of four submatrices, as follows.

Splitting a matrix now means dividing it into two parts of equal size, or as close to equal sizes as possible in the case of odd dimensions.

===
Cache behavior ===
The cache miss rate of recursive matrix multiplication is the same as that of a tiled iterative version, but unlike that algorithm, the recursive algorithm is cache-oblivious: there is no tuning parameter required to get optimal cache performance, and it behaves well in a multiprogramming environment where cache sizes are effectively dynamic due to other processes taking up cache space.

(The simple iterative algorithm is cache-oblivious as well, but much slower in practice if the matrix layout is not adapted to the algorithm.)

The number of cache misses incurred by this algorithm, on a machine with M lines of ideal cache, each of size b bytes, is bounded by

  
    
      
        Θ
        
          (
          
            m
            +
            n
+
p
+
            
              
                
                  m
                  n
+
                  n
                  p
+
m
                  p
                
                b
              
            
            +
            
              
                
                  m
                  n
p
                
                
                  b
                  
                    
                      M
)
{\displaystyle \Theta \left(m+n+p+{\frac {mn+np+mp}{b}}+{\frac {mnp}{b{\sqrt {M}}}}\right)}
  


==
Sub-cubic algorithms ==
Algorithms exist that provide better running times than the straightforward ones.
The first to be discovered was Strassen's algorithm, devised by Volker Strassen in 1969 and often referred to as "fast matrix multiplication".
It is based on a way of multiplying two 2 × 2-matrices which requires only 7 multiplications (instead of the usual 8), at the expense of several additional addition and subtraction operations.
Applying this recursively gives an algorithm with a multiplicative cost of 
  
    
      
        O
(
        
          n
log
              
                2
⁡
            7
          
        
        )
        ≈
O
(
        
          n
          
            2.807
          
        
        )
      
    
    {\displaystyle O(n^{\log
_{2}7})\approx O(n^{2.807})}
  .
Strassen's algorithm is more complex, and the numerical stability is reduced compared to the naïve algorithm, but it is faster in cases where n > 100 or so and appears in several libraries, such as BLAS.
It is very useful for large matrices over exact domains such as finite fields, where numerical stability is not an issue.

It is an open question in theoretical computer science
how well Strassen's algorithm can be improved.
The matrix multiplication exponent 
  
    
      
        ω
      
    
    {\displaystyle \omega }
   is the smallest real number for which any 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
   matrix over a field can be multiplied together using 
  
    
      
        
          n
          
            ω
+
o
(
            1
)
          
        
      
    
    {\displaystyle n^{\omega +o(1)}}
   field operations.
The current best bound on 
  
    
      
        ω
      
    
    {\displaystyle \omega }
   is 2.3728596, by Josh Alman and Virginia Vassilevska Williams.
This algorithm, like all other recent algorithms in this line of research, is a generalization of the Coppersmith–
Winograd algorithm, which was given by Don Coppersmith and Shmuel Winograd in 1990 and has an asymptotic complexity of O(n2.376).
The conceptual idea of these algorithms are similar to Strassen's algorithm: a way is devised for multiplying two k × k-matrices with fewer than k3 multiplications, and this technique is applied recursively.
However, the constant coefficient hidden by the Big O notation is so large that these algorithms are only worthwhile for matrices that are too large to handle on present-day computers.
Since any algorithm for multiplying two n × n-matrices has to process all 2n2 entries, there is an asymptotic lower bound of Ω(n2) operations.
Raz proved a lower bound of Ω(n2 log(n)) for bounded coefficient arithmetic circuits over the real or complex numbers.
Cohn et al.
put methods such as the Strassen and Coppersmith–
Winograd algorithms in an entirely different group-theoretic context, by utilising triples of subsets of finite groups which satisfy a disjointness property called the triple product property (TPP).
They show that if families of wreath products of Abelian groups with symmetric groups realise families of subset triples with a simultaneous version of the TPP, then there are matrix multiplication algorithms with essentially quadratic complexity.
Most researchers believe that this is indeed the case.
However, Alon, Shpilka and Chris Umans have recently shown that some of these conjectures implying fast matrix multiplication are incompatible with another plausible conjecture, the sunflower conjecture.
Freivalds' algorithm is a simple Monte Carlo algorithm
that, given matrices A, B and C, verifies in Θ(n2) time if AB = C.
== Parallel and distributed algorithms ==


===
Shared-memory parallelism ===
The divide-and-conquer algorithm sketched earlier can be parallelized in two ways for shared-memory multiprocessors.
These are based on the fact that the eight recursive matrix multiplications in
(
            
              
                
                  
                    A
                    
                      11
B
                    
                      11
+
A
                    
                      12
B
                    
                      21
A
                    
                      11
B
                    
                      12
+
A
                    
                      12
B
                    
                      22
A
                    
                      21
B
                    
                      11
+
A
                    
                      22
B
                    
                      21
A
                    
                      21
B
                    
                      12
+
A
                    
                      22
B
                    
                      22
                    
                  
                
              
            
            )
          
        
      
    
    {\displaystyle {\begin{pmatrix}A_{11}B_{11}+A_{12}B_{21}&A_{11}B_{12}+A_{12}B_{22}\\A_{21}B_{11}+A_{22}B_{21}&A_{21}B_{12}+A_{22}B_{22}\\\end{pmatrix}}}
  can be performed independently of each other, as can the four summations (although the algorithm needs to "join" the multiplications before doing the summations).
Exploiting the full parallelism of the problem, one obtains an algorithm that can be expressed in fork–join style pseudocode:
Here, fork is a keyword that signal a computation may be run in parallel with the rest of the function call, while join waits for all previously "forked" computations to complete.
partition achieves its goal by pointer manipulation only.

This algorithm has a critical path length of Θ(log2 n) steps, meaning it takes that much time on an ideal machine with an infinite number of processors; therefore, it has a maximum possible speedup of Θ(n3/log2 n) on any real computer.
The algorithm isn't practical due to the communication cost inherent in moving data to and from the temporary matrix T, but a more practical variant achieves Θ(n2) speedup, without using a temporary matrix.

===
Communication-avoiding and distributed algorithms ===
On modern architectures with hierarchical memory, the cost of loading and storing input matrix elements tends to dominate the cost of arithmetic.
On a single machine this is the amount of data transferred between RAM and cache, while on a distributed memory multi-node machine it is the amount transferred between nodes; in either case it is called the communication bandwidth.
The naïve algorithm using three nested loops uses Ω(n3) communication bandwidth.

Cannon's algorithm, also known as the 2D algorithm, is a communication-avoiding algorithm that partitions each input matrix into a block matrix whose elements are submatrices of size √M/3 by √M/3, where M is the size of fast memory.
The naïve algorithm is then used over the block matrices, computing products of submatrices entirely in fast memory.
This reduces communication bandwidth to O(n3/√M), which is asymptotically optimal (for algorithms performing Ω(n3) computation).In a distributed setting with p processors arranged in a √p by √p 2D mesh, one submatrix of the result can be assigned to each processor, and the product can be computed with each processor transmitting O(n2/√p) words, which is asymptotically optimal assuming that each node stores the minimum O(n2/p) elements.
This can be improved by the 3D algorithm, which arranges the processors in a 3D cube mesh, assigning every product of two input submatrices to a single processor.
The result submatrices are then generated by performing a reduction over each row.
This algorithm transmits O(n2
/p2/3) words per processor, which is asymptotically optimal.
However, this requires replicating each input matrix element p1/3 times, and so requires a factor of p1/3 more memory than is needed to store the inputs.
This algorithm can be combined with Strassen to further reduce runtime.
"
2.5D" algorithms provide a continuous tradeoff between memory usage and communication bandwidth.
On modern distributed computing environments such as MapReduce, specialized multiplication algorithms have been developed.

===
Algorithms for meshes ===
There are a variety of algorithms for multiplication on meshes.
For multiplication of two n×n on a standard two-dimensional mesh using the 2D Cannon's algorithm, one can complete the multiplication in 3n-2 steps although this is reduced to half this number for repeated computations.
The standard array is inefficient because the data from the two matrices does not arrive simultaneously and it must  be padded with zeroes.

The result is even faster on a two-layered cross-wired mesh, where only 2n-1 steps are needed.
The performance improves further for repeated computations leading to 100% efficiency.
The cross-wired mesh array may be seen as a special case of a non-planar (i.e. multilayered) processing structure.

== See also ==
Computational complexity of mathematical operations
CYK algorithm, §
Valiant's algorithm
Matrix chain multiplication
Sparse matrix-vector multiplication


=
= References ==


==
Further reading ==
Buttari, Alfredo; Langou, Julien; Kurzak, Jakub; Dongarra, Jack (2009).
"
A class of parallel tiled linear algebra algorithms for multicore architectures".
Parallel Computing.
35: 38–53.
arXiv:0709.1272.
doi:10.1016/j.parco.2008.10.002.
S2CID 955.

Goto, Kazushige; van de Geijn, Robert A. (2008).
"
Anatomy of high-performance matrix multiplication".
ACM Transactions on Mathematical Software.
34 (3): 1–25.
CiteSeerX 10.1.1.140.3583.
doi:10.1145/1356052.1356053.
S2CID 9359223.

Van Zee, Field G.; van de Geijn, Robert A. (2015).
"
BLIS:
A Framework for Rapidly Instantiating BLAS Functionality".
ACM Transactions on Mathematical Software.
41 (3): 1–33.
doi:10.1145/2764454.
S2CID 1242360.

How To Optimize GEMM
In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options, it could be either rational or irrational.
Decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker.
Every decision-making process produces a final choice, which may or may not prompt action.

Research about decision-making is also published under the label problem solving, particularly in European psychological research.

==
Overview ==
Decision-making can be regarded as a problem-solving activity yielding a solution deemed to be optimal, or at least satisfactory.
It is therefore a process which can be more or less rational or irrational and can be based on explicit or tacit knowledge and beliefs.
Tacit knowledge is often used to fill the gaps in complex decision making processes.
Usually both of these types of knowledge, tacit and explicit, are used together in the decision-making process.

Human performance has been the subject of active research from several perspectives:
Psychological: examining individual decisions in the context of a set of needs, preferences and values the individual has or seeks.

Cognitive:
the decision-making process regarded as a continuous process integrated in the interaction with the environment.

Normative: the analysis of individual decisions concerned with the logic of decision-making, or communicative rationality, and the invariant choice it leads to.
A major part of decision-making, involves the analysis of a finite set of alternatives described in terms of evaluative criteria.
Then the task might be to rank these alternatives in terms of how attractive they are to the decision-maker(s) when all the criteria are considered simultaneously.
Another task might be to find the best alternative or to determine the relative total priority of each alternative (for instance, if alternatives represent projects competing for funds) when all the criteria are considered simultaneously.
Solving such problems is the focus of multiple-criteria decision analysis (MCDA).
This area of decision-making, although very old, has attracted the interest of many researchers and practitioners and is still highly debated as there are many MCDA methods which may yield very different results when they are applied on exactly the same data.
This leads to the formulation of a decision-making paradox.
Logical decision-making is an important part of all science-based professions, where specialists apply their knowledge in a given area to make informed decisions.
For example, medical decision-making often involves a diagnosis and the selection of appropriate treatment.
But naturalistic decision-making research shows that in situations with higher time pressure, higher stakes, or increased ambiguities, experts may use intuitive decision-making rather than structured approaches.
They may follow a recognition primed decision that fits their experience, and arrive at a course of action without weighing alternatives.
The decision-maker's environment can play a part in the decision-making process.
For example, environmental complexity is a factor that influences cognitive function.
A complex environment is an environment with a large number of different possible states which come and go over time.
Studies done at the University of Colorado have shown that more complex environments correlate with higher cognitive function, which means that a decision can be influenced by the location.
One experiment measured complexity in a room by the number of small objects and appliances present; a simple room had less of those things.
Cognitive function was greatly affected by the higher measure of environmental complexity making it easier to think about the situation and make a better decision.

==
Problem solving vs. decision making ==
It is important to differentiate between problem solving, or problem analysis, and decision-making.
Problem solving is the process of investigating the given information and finding all possible solutions through invention or discovery.
Traditionally, it is argued that problem solving is a step towards decision making, so that the information gathered in that process may be used towards decision-making.

Characteristics of problem solving
Problems are merely deviations from performance standards
Problems must be precisely identified and described
Problems are caused by a change from a distinctive feature
Something can always be used to distinguish between what has and hasn't been affected by a cause
Causes of problems can be deduced from relevant changes found in analyzing the problem
Most likely cause of a problem is the one that exactly explains all the facts, while having the fewest (or weakest) assumptions
(Occam's razor).Characteristics of decision-making
Objectives must first be established
Objectives must be classified and placed in order of importance
Alternative actions must be developed
The alternatives must be evaluated against all the objectives
The alternative that is able to achieve all the objectives is the tentative decision
The tentative decision is evaluated for more possible consequences
The decisive actions are taken, and additional actions are taken to prevent any adverse consequences from becoming problems and starting both systems (problem analysis and decision-making) all over again
There are steps that are generally followed that result in a decision model that can be used to determine an optimal production plan
In a situation featuring conflict, role-playing may be helpful for predicting decisions to be made by involved parties


===
Analysis paralysis ===
When a group or individual is unable to make it through the problem-solving step on the way to making a decision, they could be experiencing analysis paralysis.
Analysis paralysis is the state that a person enters where they are unable to make a decision, in effect paralyzing the outcome.
Some of the main causes for analysis paralysis is the overwhelming flood of incoming data or the tendency to overanalyze the situation at hand.
According to Lon Roberts, there are three different types of analysis paralysis.

The first is analysis process paralysis.
This type of paralysis is often spoken of as a cyclical process.
One is unable to make a decision because they get stuck going over the information again and again for fear of making the wrong decision.

The second is decision precision paralysis.
This paralysis is cyclical, just like the first one, but instead of going over the same information, the decision-maker will find new questions and information from their analysis and that will lead them to explore into further possibilities rather than making a decision.

The third is risk uncertainty paralysis.
This paralysis occurs when the decision-maker wants to eliminate any uncertainty but the examination of provided information is unable to get rid of all uncertainty.

===
Extinction by instinct ===
On the opposite side of analysis paralysis is the phenomenon called extinction by instinct.
Extinction by instinct is the state that a person is in when they make careless decisions without detailed planning or thorough systematic processes.
Extinction by instinct can possibly be fixed by implementing a structural system, like checks and balances into a group or one’s life.
Analysis paralysis is the exact opposite where a group’s schedule could be saturated by too much of a structural checks and balance system.
Extinction by instinct in a group setting
Groupthink is another occurrence that falls under the idea of extinction by instinct.
According to Irving L. Janis, groupthink is when members in a group become more involved in the “value of the group (and their being part of it) higher than anything else”; thus, creating a habit of making decisions quickly and unanimously.
In other words, a group stuck in groupthink are participating in the phenomenon of extinction by instinct.

===
Information overload ===
Information overload is "a gap between the volume of information and the tools we have to assimilate" it.
Information used in decision making is to reduce or eliminate uncertainty.
Excessive information affects problem processing and tasking, which affects decision-making.
Psychologist George Armitage Miller suggests that humans’ decision making becomes inhibited because human brains can only hold a limited amount of information.
Crystal C. Hall and colleagues described an "illusion of knowledge", which means that as individuals encounter too much knowledge it can interfere with their ability to make rational decisions.
Other names for information overload are information anxiety, information explosion, infobesity, and infoxication.

===
Decision fatigue ===
Decision fatigue is when a sizable amount of decision-making leads to a decline in decision-making skills.
People who make decisions in an extended period of time begin to lose mental energy needed to analyze all possible solutions.
It is speculated that decision fatigue only happens to those who believe willpower has a limited capacity.
Impulsive decision-making or decision avoidance are two possible paths that extend from decision fatigue.
Impulse decisions are made more often when a person is tired of analysis situations or solutions; the solution they make is to act and not think.
Decision avoidance is when a person evades the situation entirely by not ever making a decision.
Decision avoidance is different from analysis paralysis because this sensation is about avoiding the situation entirely, while analysis paralysis is continually looking at the decisions to be made but still unable to make a choice.

=== Post-decision analysis ===
Evaluation and analysis of past decisions is complementary to decision-making.
See also Mental accounting and Postmortem documentation.

==
Neuroscience ==
Decision-making is a region of intense study in the fields of systems neuroscience, and cognitive neuroscience.
Several brain structures, including the anterior cingulate cortex (ACC), orbitofrontal cortex, and the overlapping ventromedial prefrontal cortex are believed to be involved in decision-making processes.
A neuroimaging study found distinctive patterns of neural activation in these regions depending on whether decisions were made on the basis of perceived personal volition or following directions from someone else.
Patients with damage to the ventromedial prefrontal cortex have difficulty making advantageous decisions.
A common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time.
A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or "confidence") associated with the decision.
Another recent study found that lesions to the ACC in the macaque resulted in impaired decision-making in the long run of reinforcement guided tasks suggesting that the ACC may be involved in evaluating past reinforcement information and guiding future action.
A 2012 study found that rats and humans can optimally accumulate incoming sensory evidence, to make statistically optimal decisions.

===
Emotions ===
Emotion appears able to aid the decision-making process.
Decision-making often occurs in the face of uncertainty about whether one's choices will lead to benefit or harm (see also Risk).
The somatic marker hypothesis is a neurobiological theory of how decisions are made in the face of uncertain outcome.
This theory holds that such decisions are aided by emotions, in the form of bodily states, that are elicited during the deliberation of future consequences and that mark different options for behavior as being advantageous or disadvantageous.
This process involves an interplay between neural systems that elicit emotional/bodily states and neural systems that map these emotional/bodily states.
A recent lesion mapping study of 152 patients with focal brain lesions conducted by Aron K. Barbey and colleagues provided evidence to help discover the neural mechanisms of emotional intelligence.

== Decision-making techniques ==
Decision-making techniques can be separated into two broad categories: group decision-making techniques and individual decision-making techniques.
Individual decision-making techniques can also often be applied by a group.

===
Group ===
Consensus decision-making tries to avoid "winners" and "losers".
Consensus requires that a majority approve a given course of action, but that the minority agree to go along with the course of action.
In other words, if the minority opposes the course of action, consensus requires that the course of action be modified to remove objectionable features.

Voting-based methods:
Majority requires support from more than 50% of the members of the group.
Thus, the bar for action is lower than with consensus.
See also Condorcet method.

Plurality, where the largest faction in a group decides, even if it falls short of a majority.

Score voting (or range voting) lets each member score one or more of the available options, specifying both preference and intensity of preference information.
The option with the highest total or average is chosen.
This method has experimentally been shown to produce the lowest Bayesian regret among common voting methods, even when voters are strategic.
It addresses issues of voting paradox and majority rule.
See also approval voting.

Quadratic voting allows participants to cast their preference and intensity of preference for each decision (as opposed to a simple for or against decision).
As in score voting, it addresses issues of voting paradox and majority rule.

Delphi method is a structured communication technique for groups, originally developed for collaborative forecasting but has also been used for policy making.

Dotmocracy is a facilitation method that relies on the use of special forms called Dotmocracy.
They are sheets that allows large groups to collectively brainstorm and recognize agreements on an unlimited number of ideas they have each wrote.

Participative decision-making occurs when an authority opens up the decision-making process to a group of people for a collaborative effort.

Decision engineering uses a visual map of the decision-making process based on system dynamics and can be automated through a decision modeling tool, integrating big data, machine learning, and expert knowledge as appropriate.
===
Individual ===
Decisional balance sheet: listing the advantages and disadvantages (benefits and costs, pros and cons) of each option, as suggested by Plato's Protagoras and by Benjamin Franklin.

Expected-value optimization: choosing the alternative with the highest probability-weighted utility, possibly with some consideration for risk aversion.
This may involve considering the opportunity cost of different alternatives.
See also Decision analysis and Decision theory.

Satisficing: examining alternatives only until the first acceptable one is found.
The opposite is maximizing or optimizing, in which many or all alternatives are examined in order to find the best option.

Acquiesce to a person in authority or an "expert"; "just following orders".

Anti-authoritarianism: taking the most opposite action compared to the advice of mistrusted authorities.

Flipism e.g. flipping a coin, cutting a deck of playing cards, and other random or coincidence methods – or prayer, tarot cards, astrology, augurs, revelation, or other forms of divination, superstition or pseudoscience.

Automated decision support: setting up criteria for automated decisions.

Decision support systems: using decision-making software when faced with highly complex decisions or when considering many stakeholders, categories, or other factors that affect decisions.

== Steps ==
A variety of researchers have formulated similar prescriptive steps aimed at improving decision-making.

===
GOFER ===
In the 1980s, psychologist Leon Mann and colleagues developed a decision-making process called GOFER, which they taught to adolescents, as summarized in the book Teaching Decision Making To Adolescents.
The process was based on extensive earlier research conducted with psychologist Irving Janis.
GOFER is an acronym for five decision-making steps:
Goals clarification:
Survey values and objectives.

Options generation: Consider a wide range of alternative actions.

Facts-finding:
Search for information.

Consideration of Effects: Weigh the positive and negative consequences of the options.

Review and implementation:
Plan how to review the options and implement them.

=== DECIDE ===
In 2008, Kristina Guo published the DECIDE model of decision-making, which has six parts:
Define the problem
Establish or Enumerate all the criteria (constraints)
Consider or Collect all the alternatives
Identify the best alternative
Develop and implement a plan of action
Evaluate and monitor the solution and examine feedback when necessary


===
Other ===
In 2007, Pam Brown of Singleton Hospital in Swansea, Wales, divided the decision-making process into seven steps:
Outline the goal and outcome.

Gather data.

Develop alternatives (i.e., brainstorming).

List pros and cons of each alternative.

Make the decision.

Immediately take action to implement it.

Learn from and reflect on the decision.
In 2009, professor John Pijanowski described how the Arkansas Program, an ethics curriculum at the University of Arkansas, used eight stages of moral decision-making based on the work of James Rest:
Establishing community:
Create and nurture the relationships, norms, and procedures that will influence how problems are understood and communicated.
This stage takes place prior to and during a moral dilemma.

Perception: Recognize that a problem exists.

Interpretation: Identify competing explanations for the problem, and evaluate the drivers behind those interpretations.

Judgment: Sift through various possible actions or responses and determine which is more justifiable.

Motivation: Examine the competing commitments which may distract from a more moral course of action and then prioritize and commit to moral values over other personal, institutional or social values.

Action: Follow through with action that supports the more justified decision.

Reflection in action.

Reflection on action.

===
Group stages ===
According to B. Aubrey Fisher, there are four stages or phases that should be involved in all group decision-making:
Orientation.
Members meet for the first time and start to get to know each other.

Conflict.
Once group members become familiar with each other, disputes, little fights and arguments occur.
Group members eventually work it out.

Emergence.
The group begins to clear up vague opinions by talking about them.

Reinforcement.
Members finally make a decision and provide justification for it.
It is said that establishing critical norms in a group improves the quality of decisions, while the majority of opinions (called consensus norms) do not.
Conflicts in socialization are divided in to functional and dysfunctional types.
Functional conflicts are mostly the questioning the managers assumptions in their decision making and dysfunctional conflicts are like personal attacks and every action which decrease team effectiveness.
Functional conflicts are the better ones to gain higher quality decision making caused by the increased team knowledge and shared understanding.

==
Rational and irrational ==
In economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to rational choice theory.
Rational choice theory says that a person consistently makes choices that lead to the best situation for himself or herself, taking into account all available considerations including costs and benefits; the rationality of these considerations is from the point of view of the person himself, so a decision is not irrational just because someone else finds it questionable.

In reality, however, there are some factors that affect decision-making abilities and cause people to make irrational decisions – for example, to make contradictory choices when faced with the same problem framed in two different ways
(see also Allais paradox).

Rational decision making is a multi-step process for making choices between alternatives.
The process of rational decisions making favors logic, objectivity, and analysis over subjectivity and insight.
While irrational decision is more counter to logic.
The decisions are made in hate and no outcomes are considered.
One of the most prominent theories of decision making is subjective expected utility (SEU) theory, which describes the rational behavior of the decision maker.
The decision maker assesses different alternatives by their utilities and the subjective probability of occurrence.
Rational decision-making is often grounded on experience and theories that are able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum, see e.g. scenario optimization.

Rational decision is generally seen as the best or most
likely decision to achieve the set goals or outcome.

==
Children, adolescents, and adults ==


===
Children ===
It has been found that, unlike adults, children are less likely to have research strategy behaviors.
One such behavior is adaptive decision-making, which is described as funneling and then analyzing the more promising information provided if the number of options to choose from increases.
Adaptive decision-making behavior is somewhat present for children, ages 11–12 and older, but decreases in presence the younger they are.
The reason children aren’t as fluid in their decision making is because they lack the ability to weigh the cost and effort needed to gather information in the decision-making process.
Some possibilities that explain this inability are knowledge deficits and lack of utilization skills.
Children lack the metacognitive knowledge necessary to know when to use any strategies they do possess to change their approach to decision-making.
When it comes to the idea of fairness in decision making, children and adults differ much less.
Children are able to understand the concept of fairness in decision making from an early age.
Toddlers and infants, ranging from 9–21 months, understand basic principles of equality.
The main difference found is that more complex principles of fairness in decision making such as contextual and intentional information don’t come until children get older.
===
Adolescents ===
During their adolescent years, teens are known for their high-risk behaviors and rash decisions.
Research has shown that there are differences in cognitive processes between adolescents and adults during decision-making.
Researchers have concluded that differences in decision-making are not due to a lack of logic or reasoning, but more due to the immaturity of psychosocial capacities that influence decision-making.
Examples of their undeveloped capacities which influence decision-making would be impulse control, emotion regulation, delayed gratification and resistance to peer pressure.
In the past, researchers have thought that adolescent behavior was simply due to incompetency regarding decision-making.
Currently, researchers have concluded that adults and adolescents are both competent decision-makers, not just adults.
However, adolescents' competent decision-making skills decrease when psychosocial capacities become present.

Research has shown that risk-taking behaviors in adolescents may be the product of interactions between the socioemotional brain network and its cognitive-control network.
The socioemotional part of the brain processes social and emotional stimuli and has been shown to be important in reward processing.
The cognitive-control network assists in planning and self-regulation.
Both of these sections of the brain change over the course of puberty.
However, the socioemotional network changes quickly and abruptly, while the cognitive-control network changes more gradually.
Because of this difference in change, the cognitive-control network, which usually regulates the socioemotional network, struggles to control the socioemotional network when psychosocial capacities are present.
When adolescents are exposed to social and emotional stimuli, their socioemotional network is activated as well as areas of the brain involved in reward processing.
Because teens often gain a sense of reward from risk-taking behaviors, their repetition becomes ever more probable due to the reward experienced.
In this, the process mirrors addiction.
Teens can become addicted to risky behavior because they are in a high state of arousal and are rewarded for it not only by their own internal functions but also by their peers around them.
A recent study suggests that adolescents have difficulties adequately adjusting beliefs in response to bad news (such as reading that smoking poses a greater risk to health than they thought), but do not differ from adults in their ability to alter beliefs in response to good news.
This creates biased beliefs, which may lead to greater risk taking.

===
Adults ===
Adults are generally better able to control their risk-taking because their cognitive-control system has matured enough to the point where it can control the socioemotional network, even in the context of high arousal or when psychosocial capacities are present.
Also, adults are less likely to find themselves in situations that push them to do risky things.
For example, teens are more likely to be around peers who peer pressure them into doing things, while adults are not as exposed to this sort of social setting.

==
Cognitive and personal biases ==
Biases usually affect decision-making processes.
They appear more when decision task has time pressure, is done under high stress and/or
task is highly complex.
Here is a list of commonly debated biases in judgment and decision-making:
Selective search for evidence (also known as confirmation bias)
: People tend to be willing to gather facts that support certain conclusions but disregard other facts that support different conclusions.
Individuals who are highly defensive in this manner show significantly greater left prefrontal cortex activity as measured by EEG than do less defensive individuals.

Premature termination of search for evidence
: People tend to accept the first alternative that looks like it might work.

Cognitive inertia is the unwillingness to change existing thought patterns in the face of new circumstances.

Selective perception: People actively screen out information that they do not think is important (see also Prejudice).
In one demonstration of this effect, discounting of arguments with which one disagrees (by judging them as untrue or irrelevant) was decreased by selective activation of right prefrontal cortex.

Wishful thinking is a tendency to want to see things in a certain – usually positive – light, which can distort perception and thinking.

Choice-supportive bias occurs when people distort their memories of chosen and rejected options to make the chosen options seem more attractive.

Recency: People tend to place more attention on more recent information and either ignore or forget more distant information (see Semantic priming).
The opposite effect in the first set of data or other information is termed primacy effect.

Repetition bias is a willingness to believe what one has been told most often and by the greatest number of different sources.

Anchoring and adjustment: Decisions are unduly influenced by initial information that shapes our view of subsequent information.

Groupthink is peer pressure to conform to the opinions held by the group.

Source credibility bias is a tendency to reject a person's statement on the basis of a bias against the person, organization, or group to which the person belongs.
People preferentially accept statements by others that they like (see also Prejudice).

Incremental decision-making and escalating commitment: People look at a decision as a small step in a process, and this tends to perpetuate a series of similar decisions.
This can be contrasted with zero-based decision-making (see Slippery slope).

Attribution asymmetry
: People tend to attribute their own success to internal factors, including abilities and talents, but explain their failures in terms of external factors such as bad luck.
The reverse bias is shown when people explain others' success or failure.

Role fulfillment is a tendency to conform to others' decision-making expectations.

Underestimating uncertainty and the illusion of control: People tend to underestimate future uncertainty because of a tendency to believe they have more control over events than they really do.

Framing bias: This is best avoided by increasing numeracy and presenting data in several formats (for example, using both absolute and relative scales).Sunk-cost fallacy is a specific type of framing effect that affects decision-making.
It involves an individual making a decision about a current situation based on what they have previously invested in the situation.
An example of this would be an individual that is refraining from dropping a class that they are most likely to fail, due to the fact that they feel as though they have done so much work in the course thus far.

Prospect theory involves the idea that when faced with a decision-making event, an individual is more likely to take on a risk when evaluating potential losses, and are more likely to avoid risks when evaluating potential gains.
This can influence one's decision-making depending if the situation entails a threat, or opportunity.

Optimism bias is a tendency to overestimate the likelihood of positive events occurring in the future and underestimate the likelihood of negative life events.
Such biased expectations are generated and maintained in the face of counter-evidence through a tendency to discount undesirable information.
An optimism bias can alter risk perception and decision-making in many domains, ranging from finance to health.

Reference class forecasting was developed to eliminate or reduce cognitive biases in decision-making.

==
Cognitive limitations in groups ==
In groups, people generate decisions through active and complex processes.
One method consists of three steps: initial preferences are expressed by members; the members of the group then gather and share information concerning those preferences; finally, the members combine their views and make a single choice about how to face the problem.
Although these steps are relatively ordinary, judgements are often distorted by cognitive and motivational biases, include "sins of commission", "sins of omission", and "sins of imprecision".
==
Cognitive styles ==


===
Optimizing vs. satisficing ===
Herbert A. Simon coined the phrase "bounded rationality" to express the idea that human decision-making is limited by available information, available time and the mind's information-processing ability.
Further psychological research has identified individual differences between two cognitive styles: maximizers try to make an optimal decision, whereas satisficers simply try to find a solution that is "good enough".
Maximizers tend to take longer making decisions due to the need to maximize performance across all variables and make tradeoffs carefully; they also tend to more often regret their decisions (perhaps because they are more able than satisficers to recognize that a decision turned out to be sub-optimal).

===
Intuitive vs. rational ===
The psychologist Daniel Kahneman, adopting terms originally proposed by the psychologists Keith Stanovich and Richard West, has theorized that a person's decision-making is the result of an interplay between two kinds of cognitive processes: an automatic intuitive system (called "System 1") and an effortful rational system (called "System 2").
System 1 is a bottom-up, fast, and implicit system of decision-making, while system 2 is a top-down, slow, and explicit system of decision-making.
System 1 includes simple heuristics in judgment and decision-making such as the affect heuristic, the availability heuristic, the familiarity heuristic, and the representativeness heuristic.

===
Combinatorial vs. positional ===
Styles and methods of decision-making were elaborated by Aron Katsenelinboigen, the founder of predispositioning theory.
In his analysis on styles and methods, Katsenelinboigen referred to the game of chess, saying that "chess does disclose various methods of operation, notably the creation of predisposition-methods which may be applicable to other, more complex systems.
"Katsenelinboigen
states that apart from the methods (reactive and selective) and sub-methods (randomization, predispositioning, programming), there are two major styles: positional and combinational.
Both styles are utilized in the game of chess.
According to Katsenelinboigen, the two styles reflect two basic approaches to uncertainty: deterministic (combinational style) and indeterministic (positional style).
Katsenelinboigen's definition of the two styles are the following.

The combinational style is characterized by:

a very narrow, clearly defined, primarily material goal; and
a program that links the initial position with the final outcome.
In defining the combinational style in chess, Katsenelinboigen wrote: "The combinational style features a clearly formulated limited objective, namely the capture of material (the main constituent element of a chess position).
The objective is implemented via a well-defined, and in some cases, unique sequence of moves aimed at reaching the set goal.
As a rule, this sequence leaves no options for the opponent.
Finding a combinational objective allows the player to focus all his energies on efficient execution, that is, the player's analysis may be limited to the pieces directly partaking in the combination.
This approach is the crux of the combination and the combinational style of play.
The positional style is distinguished by:

a positional goal; and
a formation of semi-complete linkages between the initial step and final outcome.
"Unlike
the combinational player, the positional player is occupied, first and foremost, with the elaboration of the position that will allow him to develop in the unknown future.
In playing the positional style, the player must evaluate relational and material parameters as independent variables. ...
The positional style gives the player the opportunity to develop a position until it becomes pregnant with a combination.
However, the combination is not the final goal of the positional player – it helps him to achieve the desirable, keeping in mind a predisposition for the future development.
The pyrrhic victory is the best example of one's inability to think positionally.
"The
positional style serves to:

create a predisposition to the future development of the position;
induce the environment in a certain way;
absorb an unexpected outcome in one's favor; and
avoid the negative aspects of unexpected outcomes.

===
Influence of Myers-Briggs type ===
According to Isabel Briggs Myers, a person's decision-making process depends to a significant degree on their cognitive style.
Myers developed a set of four bi-polar dimensions, called the Myers-Briggs Type Indicator (MBTI).
The terminal points on these dimensions are: thinking and feeling; extroversion and introversion; judgment and perception; and sensing and intuition.
She claimed that a person's decision-making style correlates well with how they score on these four dimensions.
For example, someone who scored near the thinking, extroversion, sensing, and judgment ends of the dimensions would tend to have a logical, analytical, objective, critical, and empirical decision-making style.
However, some psychologists say that the MBTI lacks reliability and validity and is poorly constructed.
Other studies suggest that these national or cross-cultural differences in decision-making exist across entire societies.
For example, Maris Martinsons has found that American, Japanese and Chinese business leaders each exhibit a distinctive national style of decision-making.
The Myers-Briggs typology has been the subject of criticism regarding its poor psychometric properties.

===
General decision-making style (GDMS) ===
In the general decision-making style (GDMS) test developed by Suzanne Scott and Reginald Bruce, there are five decision-making styles: rational, intuitive, dependent, avoidant, and spontaneous.
These five different decision-making styles change depending on the context and situation, and one style is not necessarily better than any other.
In the examples below, the individual is working for a company and is offered a job from a different company.

The rational style is an in-depth search for, and a strong consideration of, other options and/or information prior to making a decision.
In this style, the individual would research the new job being offered, review their current job, and look at the pros and cons of taking the new job versus staying with their current company.

The intuitive style is confidence in one's initial feelings and gut reactions.
In this style, if the individual initially prefers the new job because they have a feeling that the work environment is better suited for them, then they would decide to take the new job.
The individual might not make this decision as soon as the job is offered.

The dependent style is asking for other people's input and instructions on what decision should be made.
In this style, the individual could ask friends, family, coworkers, etc.,
but the individual might not ask all of these people.

The avoidant style is averting the responsibility of making a decision.
In this style, the individual would not make a decision.
Therefore, the individual would stick with their current job.

The spontaneous style is a need to make a decision as soon as possible rather than waiting to make a decision.
In this style, the individual would either reject or accept the job as soon as it is offered.

==
Organizational vs. individual level ==
There are a few characteristics that differentiate organizational decision-making from individual decision-making as studied in lab experiments:1.
Unlike most lab studies of individual decision-making, ambiguity is pervasive in organizations.
There is often only ambiguous information, and there is ambiguity about preferences as well as about interpreting the history of decisions.

2.
Decision-making in and by organizations is embedded in a longitudinal context, meaning that participants in organizational decision-making are a part of ongoing processes.
Even if they don't take on active roles in all phases of decision-making, they are part of the Decision Process and its consequences.
Decisions in organizations are made in a sequential manner, and commitment may be more important in such processes than judgmental accuracy.
In contrast, most lab studies of individual decision-making are conducted in artificial settings (lab) that are not connected to the subjects’ ongoing activities.

3.
Incentives play an important role in organizational decision-making.
Incentives, penalties, and their ramifications are real and may have long-lasting effects.
These effects are intensified due to the longitudinal nature of decision-making in organizational settings.
Incentives and penalties are very salient in organizations, and often they command managerial attention.

4.
Many executives, especially in middle management, may make repeated decisions on similar issues.
Managers may develop a sense of using his/her skills (which may be faulty) and a sense of having control and using one's skills are pervasive in managerial thinking about risk taking.
Several repeated decisions are made by following rules rather than by using pure information processing modes.

5.
Conflict is pervasive in organizational decision-making.
Many times power considerations and agenda setting determine decisions rather than calculations based on the decision's parameters.
The nature of authority relations may have a large impact on the way decisions are made in organizations, which are basically political systems.

== See also ==


==
References ==
Computer science education  or computing education is the science and art of teaching and learning of computer science, computing and computational thinking.
As a subdiscipline of pedagogy it also addresses the wider impact of computer science in society through its intersection with philosophy, psychology, linguistics, natural sciences, and mathematics.
In comparison to  science education and mathematics education, computer science education is a much younger field.
In the history of computing, digital computers were only built from around the 1940s – although computation has been around for centuries since the invention of analog computers.
Another differentiator of computer science education is that it has primarily only been taught at university level until recently, with some notable exceptions in Israel, Poland and the United Kingdom with the BBC Micro in the 1980s as part of Computer science education in the United Kingdom.
Computer science has been a part of the school curricula from age 14 or age 16 in a few countries for a few decades, but has typically as an elective subject.

==
Computing education research ==
Educational research on computing and teaching methods in computer science is usually known as Computing Education Research.
The Association for Computing Machinery (ACM) runs a Special Interest Group (SIG) on Computer science education known as SIGCSE which celebrated its 50th anniversary in 2018, making it one of the oldest and longest running ACM Special Interest Groups.

==
Women in computer science ==
In many countries, there is a significant gender gap in computer science education.
In 2015, 15.3% of computer science students graduating from non-doctoral granting institutions in the US were women while at doctoral granting institutions, the figure was 16.6%.
The number of female PhD recipients in the US was 19.3% in 2018.
The gender gap also exists in other western countries.
The gap is smaller, or nonexistent, in some parts of the world.
In 2011, women earned half of the computer science degrees in Malaysia.
In 2001, 55 percent of computer science graduates in Guyana were women.

==
References ==
The Undoing Project: A Friendship That Changed Our Minds is a 2016 nonfiction book by American author Michael Lewis, published by W.W. Norton.
The Undoing Project explores the close partnership of Israeli psychologists Daniel Kahneman and Amos Tversky, whose work on heuristics in judgment and decision-making demonstrated common errors of the human psyche, and how that partnership eventually broke apart.
The book revisits Lewis' interest in market inefficiencies, previously explored in his books Moneyball (2003), The Big Short (2010), and Flash Boys (2014).
It was acclaimed by book critics.

== Reception ==
According to the review aggregator Bookmarks, The Undoing Project was met largely by rave reviews, with Glenn C. Altschuler arguing in the Pittsburgh Post-Gazette that it "may well be his best book.
"
Writing in The New Yorker, law professor Cass Sunstein and economist Richard Thaler praised the book's ability to explain complex concepts to lay readers as well as turn the biographies of Tversky and Kahneman into a page-turner: "He provides a basic primer on the research of Kahneman and Tversky, but almost in passing; what is of interest here is the collaboration between two scientists."
Jennifer Senior of The New York Times wrote that "At its peak, the book combines intellectual rigor with complex portraiture.
During its final pages, I was blinking back tears, hardly your typical reaction to a book about a pair of academic psychologists."
==
References ==
In computer science, a record (also called a structure,  struct, or compound data) is a basic data structure.
Records in a database or spreadsheet are usually called "rows".
A record is a collection of fields, possibly of different data types, typically in a fixed number and sequence.
The fields of a record may also be called members, particularly in object-oriented programming; fields may also be called elements, though this risks confusion with the elements of a collection.

For example, a date could be stored as a record containing a numeric year field, a month field represented as a string, and a numeric day-of-month field.
A personnel record might contain a name, a salary, and a rank.
A Circle record might contain a center and a radius—in this instance, the center itself might be represented as a point record containing x and y coordinates.

Records are distinguished from arrays by the fact that their number of fields is typically fixed, each field has a name, and that each field may have a different type.

A record type is a data type that describes such values and variables.
Most modern computer languages allow the programmer to define new record types.
The definition includes specifying the data type of each field and an identifier (name or label) by which it can be accessed.

In type theory, product types (with no field names) are generally preferred due to their simplicity, but proper record types are studied in languages such as System F-sub.

Since type-theoretical records may contain first-class function-typed fields in addition to data, they can express many features of object-oriented programming.

Records can exist in any storage medium, including main memory and mass storage devices such as magnetic tapes or hard disks.

Records are a fundamental component of most data structures, especially linked data structures.

Many computer files are organized as arrays of logical records, often grouped into larger physical records or blocks for efficiency.

The parameters of a function or procedure can often be viewed as the fields of a record variable; and the arguments passed to that function can be viewed as a record value that gets assigned to that variable at the time of the call.
Also, in the call stack that is often used to implement procedure calls, each entry is an activation record or call frame, containing the procedure parameters and local variables, the return address, and other internal fields.

An object in object-oriented language is essentially a record that contains procedures specialized to handle that record; and object types are an elaboration of record types.
Indeed, in most object-oriented languages, records are just special cases of objects, and are known as plain old data structures (PODSs), to contrast with objects that use OO features.

A record can be viewed as the computer analog of a mathematical tuple, although a tuple may or may not be considered a record, and vice versa, depending on conventions and the specific programming language.

In the same vein, a record type can be viewed as the computer language analog of the Cartesian product of two or more mathematical sets, or the implementation of an abstract product type in a specific language.

==
Keys ==
A record may have zero or more keys.

A key is a field or set of fields in the record that serves as an identifier.

A unique key is often called the primary key, or simply the record key.

For example an employee file might contain employee number, name, department, and salary.

The employee number will be unique in the organization and would be the primary key.

Depending on the storage medium and file organization the employee number might be indexed—that is also stored in a separate file to make lookup faster.

The department code may not be unique; it may also be indexed, in which case it would be considered a secondary key, or alternate key.

If it is not indexed the entire employee file would have to be scanned to produce a listing of all employees in a specific department.

The salary field would not normally be considered usable as a key.

Indexing is one factor considered when designing a file.

==
History ==
The concept of record can be traced to various types of tables and ledgers used in accounting since remote times.

The modern notion of records in computer science, with fields of well-defined type and size, was already implicit in 19th century mechanical calculators, such as Babbage's Analytical Engine.

The original machine-readable medium used for data (as opposed to control) was punch card used for records in the 1890 United States Census: each punch card was a single record.
Compare the journal entry from 1880 and the punch card from 1895.
Records were well established in the first half of the 20th century, when most data processing was done using punched cards.
Typically each record of a data file would be recorded in one punched card, with specific columns assigned to specific fields.
Generally, a record was the smallest unit that could be read in from external storage (e.g. card reader, tape or disk).

Most machine language implementations and early assembly languages did not have special syntax for records, but the concept was available (and extensively used) through the use of index registers, indirect addressing, and self-modifying code.
Some early computers, such as the IBM 1620, had hardware support for delimiting records and fields, and special instructions for copying such records.

The concept of records and fields was central in some early file sorting and tabulating utilities, such as IBM's Report Program Generator (RPG).

COBOL was the first widespread programming language to support record types, and its record definition facilities were quite sophisticated at the time.
The language allows for the definition of nested records with alphanumeric, integer, and fractional fields of arbitrary size and precision, as well as fields that automatically format any value assigned to them (e.g., insertion of currency signs, decimal points, and digit group separators).
Each file is associated with a record variable where data is read into or written from.
COBOL also provides a MOVE CORRESPONDING statement that assigns corresponding fields of two records according to their names.

The early languages developed for numeric computing, such as FORTRAN (up to FORTRAN IV) and Algol 60, did not have support for record types; but later versions of those languages, such as Fortran 77
and Algol 68 did add them.
The original Lisp programming language too was lacking records (except for the built-in cons cell), but its S-expressions provided an adequate surrogate.
The Pascal programming language was one of the first languages to fully integrate record types with other basic types into a logically consistent type system.
The PL/I programming language provided for COBOL-style records.
The C programming language initially provided the record concept as a kind of template (struct) that could be laid on top of a memory area, rather than a true record data type.

The latter were provided eventually (by the typedef declaration), but the two concepts are still distinct in the language.
Most languages designed after Pascal (such as Ada, Modula, and Java) also supported records.

==
Operations ==
Declaration of a new record type, including the position, type, and (possibly) name of each field;
Declaration of variables and values as having a given record type;
Construction of a record value from given field values and (sometimes) with given field names;
Selection of a field of a record with an explicit name;
Assignment of a record value to a record variable;
Comparison of two records for equality;
Computation of a standard hash value for the record.
The selection of a field from a record value yields a value.

Some languages may provide facilities that enumerate all fields of a record, or at least the fields that are references.
This facility is needed to implement certain services such as debuggers, garbage collectors, and serialization.
It requires some degree of type polymorphism.

In systems with record subtyping, operations on values of record type may also include:

Adding a new field to a record, setting the value of the new field.

Removing a field from a record.
In such settings, a specific record type implies that a specific set of fields are present, but values of that type may contain additional fields.
A record with fields x, y, and z would thus belong to the type of records with fields x and y, as would a record with fields x, y, and r.
The rationale is that passing an (x,y,z) record to a function that expects an (x,y) record as argument should work, since that function will find all the fields it requires within the record.
Many ways of practically implementing records in programming languages would have trouble with allowing such variability, but the matter is a central characteristic of record types in more theoretical contexts.

===
Assignment and comparison ===
Most languages allow assignment between records that have exactly the same record type (including same field types and names, in the same order).
Depending on the language, however, two record data types defined separately may be regarded as distinct types even if they have exactly the same fields.

Some languages may also allow assignment between records whose fields have different names, matching each field value with the corresponding field variable by their positions within the record; so that, for example, a complex number with fields called real and imag can be assigned to a 2D point record variable with fields X and Y.
In this alternative, the two operands are still required to have the same sequence of field types.

Some languages may also require that corresponding types have the same size and encoding as well, so that the whole record can be assigned as an uninterpreted bit string.

Other languages may be more flexible in this regard, and require only that each value field can be legally assigned to the corresponding variable field; so that, for example, a short integer field can be assigned to a long integer field, or vice versa.

Other languages (such as COBOL) may match fields and values by their names, rather than positions.

These same possibilities apply to the comparison of two record values for equality.

Some languages may also allow order comparisons ('<'and '>'), using the lexicographic order based on the comparison of individual fields.
PL/
I allows both of the preceding types of assignment, and also allows structure expressions, such as a = a+1; where "a" is a record, or structure in PL/I terminology.

===
Algol 68's
distributive field selection ===
In Algol 68, if Pts was an array of records, each with integer fields X and Y, one could write Y of Pts to obtain an array of integers, consisting of the Y fields of all the elements of Pts.

As a result, the statements Y of Pts[3] := 7 and (Y of Pts)[3] := 7 would have the same effect.

===
Pascal's "with" statement ===
In the Pascal programming language, the command with R do S would execute the command sequence S as if all the fields of record R had been declared as variables.

So, instead of writing Pt.
X := 5; Pt.
Y :=
Pt.
X
+ 3 one could write with Pt
do begin X := 5
; Y :=
X + 3 end.

==
Representation in memory ==
The representation of records in memory varies depending on the programming languages.
Usually the fields are stored in consecutive positions in memory, in the same order as they are declared in the record type.
This may result in two or more fields stored into the same word of memory; indeed, this feature is often used in systems programming to access specific bits of a word.
On the other hand, most compilers will add padding fields, mostly invisible to the programmer, in order to comply with alignment constraints imposed by the machine—say, that a floating point field must occupy a single word.

Some languages may implement a record as an array of addresses pointing to the fields (and, possibly, to their names and/or types).
Objects in object-oriented languages are often implemented in rather complicated ways, especially in languages that allow multiple class inheritance.

==
Self-defining records ==
A self-defining record is a type of record which contains information to identify the record type and to locate information within the record.

It may contain the offsets of elements; the elements can therefore be stored in any order or may be omitted.
Alternatively, various elements of the record, each including an element identifier, can simply follow one another in any order.

== Examples ==
The following show examples of record definitions:
PL/I:
Algol 68:
mode date =
struct
(int year, int month, int day);C:
Fortran:
Go:
Pascal:
Rust:
Haskell:
Julia:
Standard ML:
COBOL:
Java 15:


==
See also ==
Block (data storage)
Composite data type
Data hierarchy
Object composition
Passive data structure
Union type


=
= References ==
Divide-and-conquer eigenvalue algorithms are a class of eigenvalue algorithms for Hermitian or real symmetric matrices that have recently (circa 1990s) become competitive in terms of stability and efficiency with more traditional algorithms such as the QR algorithm.

The basic concept behind these algorithms is the divide-and-conquer approach from computer science.

An eigenvalue problem is divided into two problems of roughly half the size, each of these are solved recursively, and the eigenvalues of the original problem are computed from the results of these smaller problems.

Here we present the simplest version of a divide-and-conquer algorithm, similar to the one originally proposed by Cuppen in 1981.

Many details that lie outside the scope of this article will be omitted; however, without considering these details, the algorithm is not fully stable.

==
Background ==
As with most eigenvalue algorithms for Hermitian matrices, divide-and-conquer begins with a reduction to tridiagonal form.

For an 
  
    
      
        m
        ×
m
      
    
    {\displaystyle m\times m}
   matrix, the standard method for this, via Householder reflections, takes
4
            3
          
        
        
          m
          
            3
          
        
      
    
    {\displaystyle {\frac {4}{3}}m^{3}}
   flops, or 
  
    
      
        
          
            8
            3
m
3
          
        
      
    
    {\displaystyle {\frac {8}{3}}m^{3}}
   if eigenvectors are needed as well.

There are other algorithms, such as the Arnoldi iteration, which may do better for certain classes of matrices; we will not consider this further here.

In certain cases, it is possible to deflate an eigenvalue problem into smaller problems.

Consider a block diagonal matrix
T
=
[
T
                    
                      1
                    
                  
                
                
                  0
                
              
              
                
                  0
                
                
                  
                    T
                    
                      2
                    
                  
                
              
            
            ]
          
        
        .
{\displaystyle T={\begin{bmatrix}T_{1}&0\\0&T_{2}\end{bmatrix}}.}

The eigenvalues and eigenvectors of 
  
    
      
        T
      
    
    {\displaystyle T}
   are simply those of 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
   and
T
2
          
        
      
    
    {\displaystyle T_{2}}
  ,
and it will almost always be faster to solve these two smaller problems than to solve the original problem all at once.

This technique can be used to improve the efficiency of many eigenvalue algorithms, but it has special significance to divide-and-conquer.

For the rest of this article, we will assume the input to the divide-and-conquer algorithm is an 
  
    
      
        m
        ×
m
      
    
    {\displaystyle m\times m}
   real symmetric tridiagonal matrix
T
      
    
    {\displaystyle T}
  .

Although the algorithm can be modified for Hermitian matrices, we do not give the details here.

== Divide ==
The divide part of the divide-and-conquer algorithm comes from the realization that a tridiagonal matrix is "almost" block diagonal.

The size of submatrix
T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
   we will call 
  
    
      
        n
×
n
      
    
    {\displaystyle n\times n}
  ,
and then 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
   is
(
        m
        −
n
        )
        ×
(
        m
        −
n
        )
      
    
    {
\displaystyle
(m-n)\times (m-n)}
  .

Note that the remark about 
  
    
      
        T
      
    
    {\displaystyle T}
   being almost block diagonal is true regardless of how 
  
    
      
        n
      
    
    {\displaystyle n}
   is chosen (i.e., there are many ways to so decompose the matrix).

However, it makes sense, from an efficiency standpoint, to choose 
  
    
      
        n
≈
m
        
          /
        
        2
      
    
    {\displaystyle n\approx m/2}
  .

We write
T
{\displaystyle T}
   as a block diagonal matrix, plus a rank-1 correction:

The only difference between 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
   and
T
                ^
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\hat {T}}_{1}}
   is that the lower right entry
t
          
            n
            n
          
        
      
    
    {\displaystyle t_{nn}
}
   in 
  
    
      
        
          
            
              
                T
                ^
              
            
          
          
            1
          
        
      
    
    {\displaystyle {\hat {T}}_{1}}
   has been replaced with 
  
    
      
        
          t
          
            n
            n
          
        
        −
        β
      
    
    {\displaystyle t_{nn}-\beta }
   and similarly, in 
  
    
      
        
          
            
              
                T
                ^
2
          
        
      
    
    {\displaystyle {\hat {T}}_{2}}
   the top left entry
t
          
            n
+
            1
            ,
            n
+
1
          
        
      
    
    {\displaystyle t_{n+1,n+1}}
   has been replaced with 
  
    
      
        
          t
          
            n
+
            1
            ,
            n
+
1
          
        
        −
β
      
    
    {\displaystyle t_{n+1,n+1}-\beta }
  .

The remainder of the divide step is to solve for the eigenvalues (and if desired the eigenvectors) of 
  
    
      
        
          
            
              
                T
                ^
1
          
        
      
    
    {\displaystyle {\hat {T}}_{1}}
   and 
  
    
      
        
          
            
              
                T
                ^
2
          
        
      
    
    {\displaystyle {\hat {T}}_{2}}
  , that is to find the diagonalizations
T
                ^
1
          
        
        =
Q
          
            1
D
          
            1
Q
          
            1
T
          
        
      
    
    {\displaystyle {\hat {T}}_{1}=Q_{1}D_{1}Q_{1}^{T}}
   and
T
                ^
              
            
          
          
            2
          
        
        =
Q
          
            2
D
          
            2
Q
          
            2
          
          
            T
          
        
      
    
    {\displaystyle {\hat {T}}_{2}=Q_{2}D_{2}Q_{2}^{T}}
  .

This can be accomplished with recursive calls to the divide-and-conquer algorithm, although practical implementations often switch to the QR algorithm for small enough submatrices.

==
Conquer ==
The conquer part of the algorithm is the unintuitive part.

Given the diagonalizations of the submatrices, calculated above, how do we find the diagonalization of the original matrix?

First, define 
  
    
      
        
          z
T
=
(
        
          q
          
            1
          
          
            T
          
        
        ,
q
          
            2
          
          
            T
          
        
        )
      
    
    {\displaystyle z^{T}=(q_{1}^{T},q_{2}^{T})}
  , where 
  
    
      
        
          q
          
            1
T
          
        
      
    
    {\displaystyle q_{1}^{T}}
   is the last row of 
  
    
      
        
          Q
          
            1
          
        
      
    
    {\displaystyle Q_{1}}
   and
q
          
            2
T
          
        
      
    
    {\displaystyle q_{2}^{T}}
   is the first row of
Q
          
            2
          
        
      
    
    {\displaystyle Q_{2}}
  .

It is now elementary to show that

  
    
      
        T
        =
[
            
              
                
                  
                    Q
                    
                      1
Q
                    
                      2
]
(
          
            
              
                [
D
                        
                          1
                        
                      
                    
                    
                  
                  
                    
                    
                      
                        D
                        
                          2
                        
                      
                    
                  
                
                ]
              
            
            +
            β
z
            
              z
T
              
            
          
          )
[
            
              
                
                  
                    Q
                    
                      1
T
Q
                    
                      2
T
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle T={\begin{bmatrix}Q_{1}&\\&Q_{2}\end{bmatrix}}\left({\begin{bmatrix}D_{1}&\\&D_{2}\end{bmatrix}}+\beta zz^{T}\right){\begin{bmatrix}Q_{1}^{T}&\\&Q_{2}^{T}\end{bmatrix}}}
  The remaining task has been reduced to finding the eigenvalues of a diagonal matrix plus a rank-one correction.

Before showing how to do this, let us simplify the notation.

We are looking for the eigenvalues of the matrix 
  
    
      
        D
+
w
        
          w
T
          
        
      
    
    {\displaystyle D+ww^{T}}
  , where 
  
    
      
        D
      
    
    {\displaystyle D}
   is diagonal with distinct entries and
w
{\displaystyle w}
   is any vector with nonzero entries.

If wi is zero, (
  
    
      
        
          e
i
          
        
      
    
    {\displaystyle e_{i}}
  ,di) is an eigenpair of 
  
    
      
        D
+
w
        
          w
T
          
        
      
    
    {\displaystyle D+ww^{T
}}
   since

  
    
      
        (
        D
+
        w
w
T
          
        
        )
        
          e
i
          
        
        =
D
        
          e
i
          
        
        =
        
          d
          
            i
          
        
        
          e
i
          
        
      
    
    {\displaystyle (D+ww^{T})e_{i}=De_{i}=d_{i}e_{i}}
  .

If 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
   is an eigenvalue, we have:
(
        D
+
        w
w
T
          
        
        )
q
=
        λ
q
      
    
    {\displaystyle (D+ww^{T})q=\lambda q}
  where 
  
    
      
        q
      
    
    {\displaystyle q}
   is the corresponding eigenvector.

Now

  
    
      
        (
D
        −
λ
        I
        )
q
+
        w
(
        
          w
          
            T
q
        )
=
0
      
    
    {
\displaystyle (D-\lambda I)q+w(w^{T}q)=0}
  

  
    
      
        q
+
        (
D
        −
λ
I
        
          )
          
            −
1
          
        
        w
(
        
          w
          
            T
q
        )
=
0
      
    
    {\displaystyle
q+(D-\lambda I)^{-1}w(w^{T}q)=0}
  

  
    
      
        
          w
          
            T
q
+
        
          w
          
            T
(
        D
        −
        λ
I
        
          )
          
            −
1
          
        
        w
(
        
          w
          
            T
q
        )
=
0
      
    
    {\displaystyle
w^{T}q+w^{T}(D-\lambda I)^{-1}w(w^{T}q)=0}
  Keep in mind that 
  
    
      
        
          w
          
            T
q
{\displaystyle w^{T}q}
   is a nonzero scalar.
Neither 
  
    
      
        w
      
    
    {\displaystyle w}
   nor 
  
    
      
        q
{\displaystyle q}
   are zero.
If 
  
    
      
        
          w
          
            T
q
{\displaystyle w^{T}q}
   were to be zero,
q
{\displaystyle q}
   would be an eigenvector of 
  
    
      
        D
      
    
    {\displaystyle D}
   by
(
        D
+
        w
w
T
          
        
        )
q
=
        λ
q
      
    
    {\displaystyle (D+ww^{T})q=\lambda q}
  .
If that were the case, 
  
    
      
        q
      
    
    {\displaystyle q}
   would contain only one nonzero position since 
  
    
      
        D
      
    
    {\displaystyle D}
   is distinct diagonal and thus the inner product
w
          
            T
q
{\displaystyle w^{T}q}
   can not be zero after all.
Therefore, we have:

  
    
      
        1
+
        
          w
          
            T
(
        D
        −
        λ
I
        
          )
          
            −
1
          
        
        w
=
0
      
    
    {\displaystyle 1+w^{T}(D-\lambda
I)^{-1}w=0}
  or written as a scalar equation,

  
    
      
        1
+
        
          ∑
j
            =
1
m
          
        
        
          
            
              w
j
2
              
            
            
              
                d
                
                  j
−
λ
=
        0.

{\displaystyle 1+\sum _{j=1}^{m}{\frac {w_{j}^{2}}{d_{j}-\lambda }}=0.}

This equation is known as the secular equation.
The problem has therefore been reduced to finding the roots of the rational function defined by the left-hand side of this equation.

All general eigenvalue algorithms must be iterative, and the divide-and-conquer algorithm is no different.

Solving the nonlinear secular equation requires an iterative technique, such as the Newton–Raphson method.

However, each root can be found in O(1) iterations, each of which requires 
  
    
      
        Θ
        (
m
)
      
    
    {\displaystyle \Theta (m)}
   flops (for an 
  
    
      
        m
      
    
    {\displaystyle m}
  -degree rational function), making the cost of the iterative part of this algorithm 
  
    
      
        Θ
(
        
          m
          
            2
          
        
        )
      
    
    {\displaystyle \Theta (m^{2})}
  .
==
Analysis ==
As is common for divide and conquer algorithms, we will use the master theorem for divide-and-conquer recurrences to analyze the running time.

Remember that above we stated we choose 
  
    
      
        n
≈
m
        
          /
        
        2
      
    
    {\displaystyle n\approx m/2}
  .

We can write the recurrence relation:
T
(
        m
        )
=
2
×
        T
(
          
            
              m
2
            
          
          )
        
        +
        Θ
        (
m
          
            2
          
        
        )
      
    
    {\displaystyle T(m)=2\times T\left({\frac {m}{2}}\right)+\Theta (m^{2})}
In the notation of the Master theorem,
a
        =
        b
        =
        2
      
    
    {\displaystyle a=b=2}
   and thus 
  
    
      
        
          log
          
            b
⁡
a
        =
1
      
    
    {\displaystyle \log _{b}a=1}
  .

Clearly, 
  
    
      
        Θ
        (
        
          m
          
            2
          
        
        )
        =
        Ω
(
        
          m
          
            1
)
{\displaystyle \Theta (m^{2})=\Omega (m^{1})}
, so we have

  
    
      
        T
(
        m
        )
        =
        Θ
        (
        
          m
          
            2
          
        
        )
      
    
    {\displaystyle T(m)=\Theta (m^{2})}
Remember that above we pointed out that reducing a Hermitian matrix to tridiagonal form takes 
  
    
      
        
          
            4
            3
          
        
        
          m
          
            3
          
        
      
    
    {\displaystyle {\frac {4}{3}}m^{3}}
   flops.

This dwarfs the running time of the divide-and-conquer part, and at this point it is not clear what advantage the divide-and-conquer algorithm offers over the QR algorithm (which also takes 
  
    
      
        Θ
        (
        
          m
          
            2
          
        
        )
      
    
    {\displaystyle \Theta (m^{2})}
   flops for tridiagonal matrices).

The advantage of divide-and-conquer comes when eigenvectors are needed as well.

If this is the case, reduction to tridiagonal form takes
8
            3
          
        
        
          m
          
            3
          
        
      
    
    {\displaystyle {\frac {8}{3}}m^{3}}
  , but
the second part of the algorithm takes 
  
    
      
        Θ
(
        
          m
          
            3
          
        
        )
      
    
    {\displaystyle \Theta (m^{3})}
   as well.

For the QR algorithm with a reasonable target precision, this is 
  
    
      
        ≈
6
m
          
            3
          
        
      
    
    {\displaystyle \approx 6m^{3}}
  ,
whereas for divide-and-conquer it is
≈
4
            3
m
3
          
        
      
    
    {
\displaystyle
\approx {\frac {4}{3}}m^{3}}
  .

The reason for this improvement is that in divide-and-conquer, the 
  
    
      
        Θ
        (
        
          m
          
            3
          
        
        )
      
    
    {\displaystyle \Theta (m^{3})}
   part of the algorithm (multiplying 
  
    
      
        Q
      
    
    {\displaystyle Q}
   matrices) is separate from the iteration, whereas in QR, this must occur in every iterative step.

Adding the 
  
    
      
        
          
            8
            3
          
        
        
          m
          
            3
          
        
      
    
    {\displaystyle {\frac {8}{3}}m^{3}}
   flops for the reduction, the total improvement is from 
  
    
      
        ≈
9
m
          
            3
          
        
      
    
    {\displaystyle \approx 9m^{3}}
   to
≈
        4
m
          
            3
          
        
      
    
    {\displaystyle \approx 4m^{3}}
   flops.

Practical use of the divide-and-conquer algorithm has shown that in most realistic eigenvalue problems, the algorithm actually does better than this.

The reason is that very often the matrices 
  
    
      
        Q
      
    
    {\displaystyle Q}
   and the vectors
z
      
    
    {\displaystyle z}
   tend to be numerically sparse, meaning that they have many entries with values smaller than the floating point precision, allowing for numerical deflation, i.e. breaking the problem into uncoupled subproblems.

== Variants and implementation ==
The algorithm presented here is the simplest version.

In many practical implementations, more complicated rank-1 corrections are used to guarantee stability; some variants even use rank-2 corrections.
There exist specialized root-finding techniques for rational functions that may do better than the Newton-Raphson method in terms of both performance and stability.

These can be used to improve the iterative part of the divide-and-conquer algorithm.

The divide-and-conquer algorithm is readily parallelized, and linear algebra computing packages such as LAPACK contain high-quality parallel implementations.

==
References ==
Demmel, James W. (1997), Applied Numerical Linear Algebra, Philadelphia, PA: Society for Industrial and Applied Mathematics, ISBN 0-89871-389-7, MR 1463942 CS1 maint: discouraged parameter (link).

Cuppen, J.J.M. (1981). "
A Divide and Conquer Method for the Symmetric Tridiagonal Eigenproblem".
Numerische Mathematik.
36: 177–195.
Martin Edward Newell is a British-born computer scientist specializing in computer graphics who is perhaps best known as the creator of the Utah teapot computer model.

==
Career ==
Before emigrating to the USA, he worked at what was then the Computer-Aided Design Centre (CADCentre) in Cambridge, UK, along with his brother Dr. Richard (Dick) Newell (who went on to co-found two of the most important UK graphics software companies - Cambridge Interactive Systems (CIS) in 1977 and Smallworld in 1987).
At CADCentre, the two Newells and Tom Sancha developed Newell's algorithm, a technique for eliminating cyclic dependencies when ordering polygons to be drawn by a computer graphics system.

Newell developed the Utah teapot while working on a Ph.D. at the University of Utah, where he also helped develop a version of the painter's algorithm for rendering.
He graduated in 1975, and was on the Utah faculty from 1977 to 1979.
Later he worked at Xerox PARC, where he worked on JaM, a predecessor of PostScript.
JaM stood for "John and Martin" -
the John was John Warnock, co-founder of Adobe Systems.
He founded the computer-aided design software company Ashlar in 1988.

In 2007 Martin Newell was elected to the National Academy of Engineering.

He recently retired as an Adobe Fellow at Adobe Systems.

==
References ==
In the domain of central processing unit (CPU) design, hazards are problems with the instruction pipeline in CPU microarchitectures when the next instruction cannot execute in the following clock cycle, and can potentially lead to incorrect computation results.
Three common types of hazards are data hazards, structural hazards, and control hazards (branching hazards).There are several methods used to deal with hazards, including pipeline stalls/pipeline bubbling, operand forwarding, and in the case of out-of-order execution, the scoreboarding method and the Tomasulo algorithm.

==
Background ==
Instructions in a pipelined processor are performed in several stages, so that at any given time several instructions are being processed in the various stages of the pipeline, such as fetch and execute.
There are many different instruction pipeline microarchitectures, and instructions may be executed out-of-order.
A hazard occurs when two or more of these simultaneous (possibly out of order) instructions conflict.

== Types ==


===
Data hazards ===
Data hazards occur when instructions that exhibit data dependence modify data in different stages of a pipeline.
Ignoring potential data hazards can result in race conditions (also termed race hazards).
There are three situations in which a data hazard can occur:
read after write (RAW), a true dependency
write after read (WAR), an anti-dependency
write after write (WAW), an output dependencyConsider two instructions i1 and i2, with i1 occurring before i2 in program order.
====
Read after write (RAW) ====
(i2 tries to read a source before i1 writes to it)
A read after write (RAW) data hazard refers to a situation where an instruction refers to a result that has not yet been calculated or retrieved.
This can occur because even though an instruction is executed after a prior instruction, the prior instruction has been processed only partly through the pipeline.

=====
Example =====
For example:

i1.
R2 <- R5 + R3
i2.
R4 <- R2 + R3
The first instruction is calculating a value to be saved in register R2, and the second is going to use this value to compute a result for register R4.
However, in a pipeline, when operands are fetched for the 2nd operation, the results from the first have not yet been saved, and hence a data dependency occurs.

A data dependency occurs with instruction i2, as it is dependent on the completion of instruction i1.

====
Write after read (WAR) ====
(i2 tries to write a destination before it is read by i1)
A write after read (WAR) data hazard represents a problem with concurrent execution.

===== Example =====
For example:

i1.
R4 <- R1 + R5
i2.
R5 <- R1 + R2
In any situation with a chance that i2 may finish before i1 (i.e., with concurrent execution), it must be ensured that the result of register R5 is not stored before i1 has had a chance to fetch the operands.

====
Write after write (WAW) ====
(i2 tries to write an operand before it is written by i1)
A write after write (WAW) data hazard may occur in a concurrent execution environment.

=====
Example =====
For example:

i1.
R2 <- R4 + R7
i2.
R2 <- R1 + R3
The write back (WB) of i2 must be delayed until i1 finishes executing.
===
Structural hazards ===
A structural hazard occurs when two (or more) instructions that are already in pipeline need the same resource.
The result is that instruction must be executed in series rather than parallel for a portion of pipeline.
Structural hazards are sometime referred to as resource hazards.

Example:
A situation in which multiple instructions are ready to enter the execute instruction phase and there is a single ALU (Arithmetic Logic Unit).
One solution to such resource hazard is to increase available resources, such as having multiple ports into main memory and multiple ALU (Arithmetic Logic Unit) units.

===
Control hazards (branch hazards or instruction hazards) ===
Control hazard occurs when the pipeline makes wrong decisions on branch prediction and therefore brings instructions into the pipeline that must subsequently be discarded.
The term branch hazard also refers to a control hazard.

==
Eliminating hazards ==


===
Generic ===


====
Pipeline bubbling ====
Bubbling the pipeline, also termed a pipeline break or pipeline stall, is a method to preclude data, structural, and branch hazards.
As instructions are fetched, control logic determines whether a hazard could/will occur.
If this is true, then the control logic inserts no operations (NOPs) into the pipeline.
Thus, before the next instruction (which would cause the hazard) executes, the prior one will have had sufficient time to finish and prevent the hazard.
If the number of NOPs equals the number of stages in the pipeline, the processor has been cleared of all instructions and can proceed free from hazards.
All forms of stalling introduce a delay before the processor can resume execution.

Flushing the pipeline occurs when a branch instruction jumps to a new memory location, invalidating all prior stages in the pipeline.

These prior stages are cleared, allowing the pipeline to continue at the new instruction indicated by the branch.

===
Data hazards ===
There are several main solutions and algorithms used to resolve data hazards:

insert a pipeline bubble whenever a read after write (RAW) dependency is encountered, guaranteed to increase latency, or
use out-of-order execution to potentially prevent the need for pipeline
bubbles
use operand forwarding to use data from later stages in the pipelineIn the case of out-of-order execution, the algorithm used can be:

scoreboarding, in which case a pipeline bubble is needed only when there is no functional unit available
the Tomasulo algorithm, which uses register renaming, allowing continual issuing of instructionsThe task of removing data dependencies can be delegated to the compiler, which can fill in an appropriate number of NOP instructions between dependent instructions to ensure correct operation, or re-order instructions where possible.

====
Operand forwarding ====


====
Examples ====
In the following examples, computed values are in bold, while Register numbers are not.
For example, to write the value 3 to register 1, (which already contains a 6), and then add 7 to register 1 and store the result in register 2, i.e.:

i0: R1 = 6
i1: R1 = 3
i2: R2 = R1 + 7 = 10
Following execution, register 2 should contain the value 10.
However, if i1 (write 3 to register 1) does not fully exit the pipeline before i2 starts executing, it means that R1 does not contain the value 3 when i2 performs its addition.
In such an event, i2 adds 7 to the old value of register 1 (6), and so register 2 contains 13 instead, i.e.:

i0: R1 = 6
i2: R2 = R1 + 7 = 13
i1: R1 =
3
This error occurs because i2 reads Register 1 before i1 has committed/stored the result of its write operation to Register 1.
So when i2 is reading the contents of Register 1, register 1 still contains 6, not 3.

Forwarding (described below) helps correct such errors by depending on the fact that the output of i1 (which is 3) can be used by subsequent instructions before the value 3 is committed to/stored in Register 1.

Forwarding applied to the example means that there is no wait to commit/store the output of i1 in Register 1
(in this example, the output is 3) before making that output available to the subsequent instruction (in this case, i2).
The effect is that i2 uses the correct (the more recent) value of Register 1:
the commit/store was made immediately and not pipelined.

With forwarding enabled, the Instruction Decode/Execution (ID/EX) stage of the pipeline now has two inputs: the value read from the register specified (in this example, the value 6 from Register 1), and the new value of Register 1 (in this example, this value is 3) which is sent from the next stage Instruction Execute/Memory Access (EX/MEM).
Added control logic is used to determine which input to use.

===
Control hazards (branch hazards) ===
To avoid control hazards microarchitectures can:

insert a pipeline bubble (discussed above), guaranteed to increase latency, or
use branch prediction and essentially make educated guesses about which instructions to insert, in which case a pipeline bubble will only be needed in the case of an incorrect predictionIn the event that a branch causes a pipeline bubble after incorrect instructions have entered the pipeline, care must be taken to prevent any of the wrongly-loaded instructions from having any effect on the processor state excluding energy wasted processing them before they were discovered to be loaded incorrectly.
===
Other techniques ===
Memory latency is another factor that designers must attend to, because the delay could reduce performance.
Different types of memory have different accessing time to the memory.
Thus, by choosing a suitable type of memory, designers can improve the performance of the pipelined data path.

== See also ==


==
References ==


===
General ===
==
External links ==
"
Automatic Pipelining from Transactional Datapath Specifications" (PDF).
Retrieved 23 July 2014.

Tulsen, Dean (18 January 2005).
"
Pipeline hazards" (PDF).
In psychology and cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory.
There are many different types of memory biases, including:
Availability bias: greater likelihood of recalling recent, nearby, or otherwise immediately available examples, and the imputation of importance to those examples over others
Boundary extension: remembering the background of an image as being larger or more expansive than the foreground
Childhood amnesia:
the retention of few memories from before the age of four.

Choice-supportive bias: remembering chosen options as having been better than rejected options (Mather, Shafir & Johnson, 2000)
Confirmation bias: the tendency to search for, interpret, or recall information in a way that confirms one's beliefs or hypotheses.

Conservatism or Regressive bias: tendency to remember high values and high likelihoods/probabilities/frequencies lower than they actually were and low ones higher than they actually were.
Based on the evidence, memories are not extreme enough.

Consistency bias: incorrectly remembering one's past attitudes and behaviour as resembling present attitudes and behaviour.

Context effect: that cognition and memory are dependent on context, such that out-of-context memories are more difficult to retrieve than in-context memories (e.g., recall time and accuracy for a work-related memory will be lower at home, and vice versa).

Cryptomnesia: a form of misattribution where a memory is mistaken for imagination, because there is no subjective experience of it being a memory.

Egocentric bias: recalling the past in a self-serving manner, e.g., remembering one's exam grades as being better than they were, or remembering a caught fish as bigger than it really was.

Fading affect bias: a bias in which the emotion associated with unpleasant memories fades more quickly than the emotion associated with positive events.

Generation effect (Self-generation effect): that self-generated information is remembered best.
For instance, people are better able to recall memories of statements that they have generated than similar statements generated by others.

Gender differences in eyewitness memory:
the tendency for a witness to remember more details about someone of the same gender.

Hindsight bias: the inclination to see past events as being predictable; also called the "I-knew-it-all-along" effect.

Humor effect: that humorous items are more easily remembered than non-humorous ones, which might be explained by the distinctiveness of humor, the increased cognitive processing time to understand the humor, or the emotional arousal caused by the humor.

Illusion-of-truth effect: that people are more likely to identify as true statements those they have previously heard (even if they cannot consciously remember having heard them), regardless of the actual validity of the statement.
In other words, a person is more likely to believe a familiar statement than an unfamiliar one.

Illusory correlation: inaccurately seeing a relationship between two events related by coincidence.

Lag effect:
see spacing effect.

Leveling and sharpening:
memory distortions introduced by the loss of details in a recollection over time, often concurrent with sharpening or selective recollection of certain details that take on exaggerated significance in relation to the details or aspects of the experience lost through leveling.
Both biases may be reinforced over time, and by repeated recollection or re-telling of a memory.

Levels-of-processing effect: that different methods of encoding information into memory have different levels of effectiveness (Craik & Lockhart, 1972).

List-length effect: a smaller percentage of items are remembered in a longer list, but as the length of the list increases, the absolute number of items remembered increases as well.

Memory inhibition: that being shown some items from a list makes it harder to retrieve the other items (e.g., Slamecka, 1968).

Misattribution of memory:
when information is retained in memory but the source of the memory is forgotten.
One of Schacter's (1999) seven sins of memory, misattribution was divided into source confusion, cryptomnesia and false recall/false recognition.

Misinformation effect: that misinformation affects people's reports of their own memory.

Modality effect: that memory recall is higher for the last items of a list when the list items were received via speech than when they were received via writing.

Mood congruent memory bias:
the improved recall of information congruent with one's current mood.

Next-in-line effect: that a person in a group has diminished recall for the words of others who spoke immediately before or after this person.

Peak–end rule: that people seem to perceive not the sum or average of an experience, but how it was at its peak (e.g. pleasant or unpleasant) and how it ended.

Persistence: the unwanted recurrence of memories of a traumatic event.

Picture superiority effect: that concepts are much more likely to be remembered experientially if they are presented in picture form than if they are presented in word form.

Placement bias:
tendency to remember ourselves to be better than others at tasks at which we rate ourselves above average (also Illusory superiority or Better-than-average effect) and tendency to remember ourselves to be worse than others at tasks at which we rate ourselves below average (also Worse-than-average effect).

Positivity effect: that older adults favor positive over negative information in their memories.

Primacy effect, Recency effect & Serial position effect:
that items near the end of a list are the easiest to recall, followed by the items at the beginning of a list; items in the middle are the least likely to be remembered.

Processing difficulty
effect
Reminiscence bump:
the recalling of more personal events from adolescence and early adulthood than personal events from other lifetime periods (Rubin, Wetzler & Nebes, 1986; Rubin, Rahhal & Poon, 1998).

Rosy retrospection: the remembering of the past as having been better than it really was.

Saying is Believing effect: Communicating a socially tuned message to an audience can lead to a bias of identifying the tuned message as one's own thoughts.

Self-reference effect: the phenomena that memories encoded with relation to the self are better recalled than similar information encoded otherwise.

Self-serving bias: perceiving oneself responsible for desirable outcomes but not responsible for undesirable ones.

Source confusion: misattributing the source of a memory, e.g. misremembering that one saw an event personally when actually it was seen on television.

Spacing effect: that information is better recalled if exposure to it is repeated over a longer span of time.

Stereotypical bias: memory distorted towards stereotypes (e.g. racial or gender)
, e.g. "black-sounding" names being misremembered as names of criminals.

Subadditivity effect: the tendency to estimate that the likelihood of a remembered event is less than the sum of its (more than two) mutually exclusive components.

Suffix effect:
the weakening of the recency effect in the case that an item is appended to the list that the subject is not required to recall (Morton, Crowder & Prussin, 1971).

Suggestibility: a form of misattribution where ideas suggested by a questioner are mistaken for memory.

Telescoping effect: the tendency to displace recent events backward in time and remote events forward in time, so that recent events appear more remote, and remote events, more recent.

Testing effect:
that frequent testing of material that has been committed to memory improves memory recall.

Tip of the tongue: when a subject is able to recall parts of an item, or related information, but is frustratingly unable to recall the whole item.
This is thought to be an instance of "blocking" where multiple similar memories are being recalled and interfere with each other.

Verbatim effect: that the "gist" of what someone has said is better remembered than the verbatim wording (Poppenk, Walia, Joanisse, Danckert, & Köhler, 2006).

Von Restorff effect: that an item that sticks out is more likely to be remembered than other items (von Restorff, 1933).

Zeigarnik effect:
that uncompleted or interrupted tasks are remembered better than completed ones.

==
See also ==
Cross-race effect
Heuristics in judgment and decision making
Index of public relations-related articles
List of cognitive biases – Systematic patterns of deviation from norm or rationality in judgment
List of common misconceptions –
Wikipedia list article
List of fallacies – Types of reasoning that are logically incorrect
Recall bias – Systematic error caused by differences in the accuracy or completeness of the recollections retrieved
Stereotype – Over-generalized belief about a particular category of people
== Footnotes ==


==
References ==
Greenwald, A (1980). "
The Totalitarian Ego: Fabrication and Revision of Personal History" (PDF).
American Psychologist.
35 (7): 603–618.
doi:10.1037/0003-066X.35.7.603.

Schacter, D. L.; Chiao, J. Y.; Mitchell, J. P.
(2003).
"
The Seven Sins of Memory.
Implications for Self" (PDF).
Annals of the New York Academy of Sciences.
1001 (1): 226–239.
Bibcode:2003NYASA1001..
226S. doi:10.1196
/annals.1279.012.
PMID 14625363.
S2CID 144885545.

==
External links ==
 Media related to Memory biases at Wikimedia Commons
A CPU cache is a memory which holds the recently utilized data by the processor.
A block of memory cannot necessarily be placed randomly in the cache and may be restricted to a single cache line or a set of cache lines by the cache placement policy.
In other words, the cache placement policy determines where a particular memory block can be placed when it goes into the cache.

There are three different policies available for placement of a memory block in the cache: direct-mapped, fully associative, and set-associative.

Originally this space of cache organizations was described using the term "congruence mapping".

==
Direct-mapped cache ==
In a direct-mapped cache structure, the cache is organized into multiple sets with a single cache line per set.

Based on the address of the memory block, it can only occupy a single cache line.
The cache can be framed as a (n*1) column matrix.

===
To place a block in the cache ===
The set is determined by the index bits derived from the address of the memory block.

The memory block is placed in the set identified and the tag  is stored in the tag field associated with the set.

If the cache line is previously occupied, then the new data replaces the memory block in the cache.

===
To search a word in the cache ===
The set is identified by the index bits of the address.

The tag bits derived from the memory block address are compared with the tag bits associated with the set.
If the tag matches, then there is a cache hit and the cache block is returned to the processor.
Else there is a cache miss and the memory block is fetched from the lower memory(main memory, disk).
===
Advantages ===
This placement policy is power efficient as it avoids the search through all the cache lines.

The placement policy and the replacement policy is simple.

It requires cheap hardware as only one tag needs to be checked at a time.

===
Disadvantage ===
It has lower cache hit rate, as there is only one cache line available in a set.
Every time a new memory is referenced to the same set, the cache line is replaced, which causes conflict miss.

===
Example ===
Consider a main memory of 16 kilobytes, which is organized as 4-byte blocks, and a direct-mapped cache of 256 bytes with a block size of 4 bytes.
Because the main memory is 16kB, we need a minimum of 14 bits to uniquely represent a memory address.

Since each cache block is of size 4 bytes, the total number of sets in the cache is 256/4, which equals 64 sets.

The incoming address to the cache is divided into bits for Offset, Index and Tag.

Offset corresponds to the bits used to determine the byte to be accessed from the cache line.
Because the cache lines are 4 bytes long, there are 2 offset bits.
Index corresponds to bits used to determine the set of the Cache.
There are 64 sets in the cache, and
because 2^6 = 64, there are 6 index bits.
Tag corresponds to the remaining bits.
This means there are 14 – (6+2) =
6 tag bits, which are stored in tag field to match the address on cache request.
Below are memory addresses and an explanation of which cache line they map to:
Address 0x0000
(tag - 0b00_0000, index – 0b00_0000, offset – 0b00) corresponds to block 0 of the memory and maps to the set 0 of the cache.

Address 0x0004
(tag - 0b00_0000, index – 0b00_0001, offset – 0b00) corresponds to block 1 of the memory and maps to the set 1 of the cache.

Address 0x00FF
(tag – 0b00_0000, index – 0b11_1111, offset – 0b11) corresponds to block 63 of the memory and maps to the set 63 of the cache.

Address 0x0100
(tag – 0b00_0001, index – 0b00_0000, offset – 0b00) corresponds to block 64 of the memory and maps to the set 0 of the cache.

==
Fully associative cache ==
In a fully associative cache, the cache is organized into a single cache set with multiple cache lines.

A memory block can occupy any of the cache lines.
The cache organization can be framed as (1*m) row matrix.

===
To place a block in the cache ===
The cache line is selected based on the valid bit associated with it.
If the valid bit is 0, the new memory block can be placed in the cache line, else it has to be placed in another cache line with valid bit 0.

If the cache is completely occupied then a block is evicted and the memory block is placed in that cache line.

The eviction of memory block from the cache is decided by the replacement policy.

===
To search a word in the cache ===
The Tag field of the memory address is compared with tag bits associated with all the cache lines.
If it matches, the block is present in the cache and is a cache hit.
If it doesn't match, then it's a cache miss and has to be fetched from the lower memory.

Based on the Offset, a byte is selected and returned to the processor.

===
Advantages ===
Fully associative cache structure provides us the flexibility of placing memory block in any of the cache lines and hence full utilization of the cache.

The placement policy provides better cache hit rate.

It offers the flexibility of utilizing a wide variety of replacement algorithms if a cache miss occurs


===
Disadvantage ===
The placement policy is slow as it takes time to iterate through all the lines.

The placement policy is power hungry as it has to iterate over entire cache set to locate a block.

The most expensive of all methods, due to the high cost of associative-comparison hardware.

===
Example ===
Consider a main memory of 16 kilobytes, which is organized as 4-byte blocks, and a fully associative cache of 256 bytes and a block size of 4 bytes.
Because the main memory is 16kB, we need a minimum of 14 bits to uniquely represent a memory address.

Since each cache block is of size 4 bytes, the total number of sets in the cache is 256/4, which equals 64 sets or cache lines.

The incoming address to the cache is divided into bits for offset and tag.

Offset corresponds to the bits used to determine the byte to be accessed from the cache line.
In the example, there are 2 offset bits, which are used to address the 4 bytes of the cache line
Tag corresponds to the remaining bits.
This means there are 14 – (2) =
12 tag bits, which are stored in tag field to match the address on cache request.
Since any block of memory can be mapped to any cache line, the memory block can occupy one of the cache lines based on the replacement policy.

== Set-associative cache ==
Set-associative cache is a trade-off between direct-mapped cache and fully associative cache.

A set-associative cache can be imagined as a (n*m) matrix.
The cache is divided into ‘n’ sets and each set contains ‘m’ cache lines.
A memory block is first mapped onto a set and then placed into any cache line of the set.

The range of caches from direct-mapped to fully associative is a continuum of levels of set associativity.
(
A direct-mapped cache is one-way set-associative and a fully associative cache with m cache lines is m-way set-associative.)

Many processor caches in today's designs are either direct-mapped, two-way set-associative, or four-way set-associative.

===
To place a block in the cache ===
The set is determined by the index bits derived from the address of the memory block.

The memory block is placed in an available cache line in the set identified, and the tag is stored in the tag field associated with the line.
If all the cache lines in the set are occupied, then the new data replaces the block identified through the replacement policy.

=== To locate a word in the cache ===
The set is determined by the index bits derived from the address of the memory block.

The tag bits are compared with the tags of all cache lines present in selected set.
If the tag matches any of the cache lines, it is a cache hit and the appropriate line is returned.
If the tag doesn't match any of the lines, then it is a cache miss and the data is requested from next level in the memory hierarchy.

===
Advantages ===
The placement policy is a trade-off between direct-mapped and fully associative cache.

It offers the flexibility of using replacement algorithms if a cache miss occurs.

=== Disadvantages ===
The placement policy will not effectively use all the available cache lines in the cache and suffers from conflict miss.

===
Example ===
Consider a main memory of 16 kilobytes, which is organized as 4-byte blocks, and a 2-way set-associative cache of 256 bytes with a block size of 4 bytes.
Because the main memory is 16kB, we need a minimum of 14 bits to uniquely represent a memory address.

Since each cache block is of size 4 bytes and is 2-way set-associative, the total number of sets in the cache is 256/(4 * 2), which equals 32 sets.
The incoming address to the cache is divided into bits for Offset, Index and Tag.

Offset corresponds to the bits used to determine the byte to be accessed from the cache line.
Because the cache lines are 4 bytes long, there are 2 offset bits.
Index corresponds to bits used to determine the set of the Cache.
There are 32 sets in the cache, and because 2^5 = 32, there are 5 index bits.
Tag corresponds to the remaining bits.
This means there are 14 – (5+2) = 7 bits, which are stored in tag field to match the address on cache request.
Below are memory addresses and an explanation of which cache line on which set they map to:
Address 0x0000
(tag - 0b00_0000, index – 0b00_0000, offset – 0b00) corresponds to block 0 of the memory and maps to the set 0 of the cache.
The block occupies a cache line in set 0, determined by the replacement policy for the cache.

Address 0x0004
(tag - 0b00_0000, index – 0b00_0001, offset – 0b00) corresponds to block 1 of the memory and maps to the set 1 of the cache.
The block occupies a cache line in set 0, determined by the replacement policy for the cache.

Address 0x00FF
(tag – 0b00_0000, index – 0b11_1111, offset – 0b11) corresponds to block 63 of the memory and maps to the set 63 of the cache.
The block occupies a cache line in set 31, determined by the replacement policy for the cache.

Address 0x0100
(tag – 0b00_0001, index – 0b00_0000, offset – 0b00) corresponds to block 64 of the memory and maps to the set 0 of the cache.
The block occupies a cache line in set 0, determined by the replacement policy for the cache.
==
Two-way skewed associative cache ==
Other schemes have been suggested, such as the skewed cache, where the index for way 0 is direct, as above, but the index for way 1 is formed with a hash function.
A good hash function has the property that addresses which conflict with the direct mapping tend not to conflict when mapped with the hash function, and so it is less likely that a program will suffer from an unexpectedly large number of conflict
misses due to a pathological access pattern.
The downside is extra latency from computing the hash function.
Additionally, when it comes time to load a new line and evict an old line, it may be difficult to determine which existing line was least recently used, because the new line conflicts with data at different indexes in each way; LRU tracking for non-skewed caches is usually done on a per-set basis.
Nevertheless, skewed-associative caches have major advantages over conventional set-associative ones.

== Pseudo-associative cache ==
A true set-associative cache tests
all the possible ways simultaneously, using something like a content-addressable memory.
A pseudo-associative cache tests each possible way one at a time.
A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.

In the common case of finding a hit in the first way tested, a pseudo-associative cache is as fast as a direct-mapped cache, but it has a much lower conflict miss rate than a direct-mapped cache, closer to the miss rate of a fully associative cache.
==
See also ==
Associativity
Cache replacement policy
Cache hierarchy
Writing Policies
Cache coloring


=
= References ==
Quicksort is an in-place sorting algorithm.
Developed by British computer scientist Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting.
When implemented well, it can be somewhat faster than merge sort and about two or three times faster than heapsort.
Quicksort is a divide-and-conquer algorithm.
It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot.
For this reason, it is sometimes called partition-exchange sort.
The sub-arrays are then sorted recursively.
This can be done in-place, requiring small additional amounts of memory to perform the sorting.

Quicksort is a comparison sort, meaning that it can sort items of any type for which a "less-than" relation (formally, a total order) is defined.
Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.

Mathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items.
In the worst case, it makes O(n2) comparisons, though this behavior is rare.

==
History ==
The quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University.
At that time, Hoare was working on a machine translation project for the National Physical Laboratory.
As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape.
After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea.
He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments.
On return to England, he was asked to write code for Shellsort.
Hoare mentioned to his boss that he knew of a faster algorithm and
his boss bet sixpence that he did not.
His boss ultimately accepted that he had lost the bet.
Later, Hoare learned about ALGOL and its ability to do recursion that enabled him to publish the code in Communications of the Association for Computing Machinery, the premier computer science journal of the time.
Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine.
Hence, it lent its name to the C standard library subroutine qsort and in the reference implementation of Java.

Robert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden as well as derivation of expected number of comparisons and swaps.
Jon Bentley and Doug McIlroy incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen.
Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto.
Later Bentley wrote that he used Hoare's version for years but never really understood it
but Lomuto's version was simple enough to prove correct.
Bentley described Quicksort as the "most beautiful code I had ever written" in the same essay.
Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal.
In 2009, Vladimir Yaroslavskiy proposed a new Quicksort implementation using two pivots instead of one.
In the Java core library mailing lists, he initiated a discussion claiming his new algorithm to be superior to the runtime library's sorting method, which was at that time based on the widely used and carefully tuned variant of classic Quicksort by Bentley and McIlroy.
Yaroslavskiy's Quicksort has been chosen as the new default sorting algorithm in Oracle's Java 7 runtime library after extensive empirical performance tests.

==
Algorithm ==
Quicksort is a type of divide and conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms.
Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range.
After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location.
Due to its recursive nature, quicksort (like  the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array.
The steps for in-place quicksort are:

If the range has less than two elements, return immediately as there is nothing to do.
Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped.

Otherwise pick a value, called a pivot, that occurs in the range
(the precise manner of choosing depends on the partition routine, and can involve randomness).

Partition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way.
Since at least one instance of the pivot is present,  most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).

Recursively apply the quicksort to the sub-range up to the point of division and to the sub-range after it, possibly excluding form both ranges the element equal to the pivot at the point of division.
(
If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.)The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays.
In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first.
Here we mention two specific partition methods.

===
Lomuto partition scheme ===
This scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls and Cormen et al.
in their book Introduction to Algorithms.
This scheme chooses a pivot that is typically the last element in the array.
The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot.
As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal.
This scheme degrades to O(n2) when the array is already in order.
There have been various variants proposed to boost performance including various ways to select pivot, deal with equal elements, use other sorting algorithms such as Insertion sort for small arrays and so on.
In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as:
algorithm
quicksort(A, lo, hi) is
if lo < hi then
p :=
partition(A, lo, hi)
quicksort(A, lo,
p - 1)
        quicksort(A,
p + 1
, hi)
algorithm partition(A, lo, hi) is
    pivot :=
A[hi]
i :=
lo
    for j :
= lo
to hi do
if A[j] < pivot then
            swap A[i] with A[j]
i :=
i + 1
    swap A[i] with A[hi]
return
i
Sorting the entire array is accomplished by quicksort(A, 0, length(A) - 1).

===
Hoare partition scheme ===
The original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the bound (Hoare's terms for the pivot value) at the first pointer, and one less than the bound at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged.
After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed).
With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing.
Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus termination of quicksort is ensured.

With respect to this original description, implementations often make minor but important variations.
Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion
(so "greater than or equal" and "less than or equal" tests are used instead of "greater than" respectively "less than"; since the formulation uses do...while rather than repeat...until this is actually reflected by the use of strict comparison operators).
While there is no reason to exchange elements equal to the bound, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range.
Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off.
(
The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion.
However using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.)
The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare.
Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place).
The division returned is after the final position of the second pointer, so the case to avoid is where the pivot is the final element of the range and all others are smaller than it.
Therefore the pivot choice must avoid the final element (in Hoare's description it could be any element in the range); this is done here by rounding down the middle position, using the floor function.
This illustrates that the argument for correctness of an implementation of the Hoare partition scheme can be subtle, and it is easy to get it wrong.

In pseudocode,
algorithm
quicksort(A, lo, hi) is
if lo < hi then
p :=
partition(A, lo, hi)
quicksort(A, lo, p)
quicksort(A,
p + 1
, hi)
algorithm partition(A, lo, hi) is
    pivot :=
A[ floor((hi + lo) / 2) ]
i :=
lo - 1
    j :=
hi + 1
    loop forever
do
i :=
i + 1
while A[i] <
pivot
do
j :=
j - 1
while A[j]
> pivot
if i
≥ j
then
            return j
        swap A[i] with A[j]
The entire array is sorted by quicksort(A, 0, length(A) - 1).

Hoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average.
Also, as mentioned, the implementation given creates a balanced partition even when all values are equal.
,
which Lomuto's scheme does not.
Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element.
With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)).
Like others, Hoare's partitioning doesn't produce a stable sort.
In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion.
The next two segments that the main algorithm recurs on are (lo..p) (elements ≤ pivot) and (p+1..hi) (elements ≥ pivot) as opposed to (lo..p-1)
and (p+1..hi) as in Lomuto's scheme.
===
Implementation issues ===


====
Choice of pivot ====
In the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element.
Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case.
The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick).
This "median-of-three" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.

Median-of-three code snippet for Lomuto partition:
mid :=
⌊(lo
+ hi)
/ 2⌋
if A[mid] < A[lo]
    swap A[lo]
with A[mid]
if A[hi
] < A[lo]
    swap A[lo] with A[hi]
if A[mid] < A[hi]
swap
A[mid]
with A[hi]
pivot := A[hi]
It puts a median into A[hi]
first, then that new value of A[hi] is used for a pivot, as in a basic algorithm presented above.

Specifically, the expected number of comparisons needed to sort n elements (see § Analysis of randomized quicksort) with random pivot selection is 1.386 n
log n. Median-of-three pivoting brings this down to Cn, 2 ≈ 1.188 n log n, at the expense of a three-percent increase in the expected number of swaps.
An even stronger pivoting rule, for larger arrays, is to pick the ninther, a recursive median-of-three (Mo3), defined as
ninther(a) =
median(Mo3(first ⅓ of a), Mo3(middle ⅓ of a)
, Mo3(final ⅓ of a))Selecting
a pivot element is also complicated by the existence of integer overflow.
If the boundary indices of the subarray being sorted are sufficiently large, the naïve expression for the middle index, (lo + hi)/2, will cause overflow and provide an invalid pivot index.
This can be overcome by using, for example, lo + (hi−lo)/2 to index the middle element, at the cost of more complex arithmetic.
Similar issues arise in some other methods of selecting the pivot element.

====
Repeated elements ====
With a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements.
The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed).
Consequently, the Lomuto partition scheme takes quadratic time to sort an array of equal values.
However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead).
In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above.

To solve the Lomuto partition scheme problem (sometimes called the Dutch national flag problem), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot.
(
Bentley and McIlroy call this a "fat partition" and it was already implemented in the qsort of Version 7 Unix.)
The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted.
In pseudocode, the quicksort algorithm becomes

algorithm
quicksort(A, lo
, hi) is
if lo < hi then
p :
= pivot(A, lo, hi)
left, right :=
partition(A, p, lo, hi)  //
note:
multiple return values
quicksort(A, lo, left - 1)
quicksort(A, right
+
1, hi)
The partition algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition.
Every item of the partition is equal to p and is therefore sorted.
Consequently, the items of the partition need not be included in the recursive calls to quicksort.

The best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k ≪ n elements).
In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the partition subroutine takes no longer than linear time).

====
Optimizations ====
Two other important optimizations, also suggested by Sedgewick and widely used in practice, are:
To make sure at most O(log n) space is used, recur first into the smaller side of the partition, then use a tail call to recur into the other, or update the parameters to no longer include the now sorted smaller side, and iterate to sort the larger side.

When the number of elements is below some threshold (perhaps ten elements), switch to a non-recursive sorting algorithm such as insertion sort that performs fewer swaps, comparisons or other operations on such small arrays.
The ideal 'threshold' will vary based on the details of the specific implementation.

An older variant of the previous optimization: when the number of elements is less than the threshold k, simply stop; then after the whole array has been processed, perform insertion sort on it.
Stopping the recursion early leaves the array k-sorted, meaning that each element is at most k positions away from its final sorted position.
In this case, insertion sort takes O(kn) time to finish the sort, which is linear if k is a constant.
Compared to the "many small sorts" optimization, this version may execute fewer instructions, but it makes suboptimal use of the cache memories in modern computers.

==== Parallelization ====
Quicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism.
The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array.
Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n)
additional scratch space.
After the array has been partitioned, the two partitions can be sorted recursively in parallel.
Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log² n) time using O(n) additional space.

Quicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization.
The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot.
Additionally, it is difficult to parallelize the partitioning step efficiently in-place.
The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.

Other more sophisticated parallel sorting algorithms can achieve even better time bounds.
For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW (concurrent read and concurrent write) PRAM (parallel random-access machine) with n processors by performing partitioning implicitly.

==
Formal analysis ==


===
Worst-case analysis ===
The most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n − 1.
This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal.

If this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list.
Consequently, we can make n − 1 nested calls before we reach a list of size 1.
This means that the call tree is a linear chain of n − 1 nested calls.
The ith call does O(n
−
i) work to do the partition, and 
  
    
      
        
          
            ∑
i
              =
0
n
(
n
          −
i
          )
=
O
(
          
            n
            
              2
            
          
          )
        
      
    
    {\displaystyle \textstyle
\sum _{i=0}^{n}(n-i)=O(n^{2})}
  , so in that case quicksort takes O(n²) time.

=== Best-case analysis ===
In the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces.
This means each recursive call processes a list of half the size.
Consequently, we can make only log2 n nested calls before we reach a list of size 1.
This means that the depth of the call tree is log2 n.
But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n)
time all together
(each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor).
The result is that the algorithm uses only O(n log n) time.

===
Average-case analysis ===
To sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n!
permutations of n elements with equal probability.
We list here three common proofs to this claim providing different insights into quicksort's workings.
====
Using percentiles ====
If each pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side.
If we could consistently choose such pivots, we would only have to split the list at most
log
          
            4
/
            
            3
⁡
n
      
    
    {\displaystyle \log _
{4/3}n}
   times before reaching lists of size 1, yielding an O(n log n) algorithm.

When the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent.
However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time.
That is good enough.
Imagine that a coin is flipped: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't.
Now imagine that the coin is flipped over and over until it gets k heads.
Although this could take a long time, on average only 2k flips are required, and the chance that the coin won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds).
By the same argument, Quicksort's recursion will terminate on average at a call depth of only 
  
    
      
        2
log
          
            4
            
              /
3
⁡
n
      
    
    {
\displaystyle 2\log _{4/3}n}
  .
But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n).
The algorithm does not have to verify that the pivot is in the middle half—
if we hit it any constant fraction of the times, that is enough for the desired complexity.

====
Using recurrences ====
An alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n.
In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n−1, so the recurrence relation is

  
    
      
        T
(
        n
        )
        =
O
(
        n
        )
+
T
        (
        0
        )
+
T
(
        n
        −
1
        )
        =
O
(
        n
        )
+
T
(
        n
        −
1
        )
        .

{\displaystyle T(n)=O(n)+T(0)+T(n-1)=O(n)+T(n-1).}

This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) =
O(n²).

In the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is

  
    
      
        T
(
        n
        )
        =
O
(
        n
        )
+
        2
        T
(
          
            
              n
2
            
          
          )
        
        .
{
\displaystyle T(n)=O(n)+2T\left({\frac {n}{2}}\right).}

The master theorem for divide-and-conquer recurrences tells us that T(n) =
O(n log n).

The outline of a formal proof of the O(n log n) expected time complexity follows.
Assume that there are no duplicates as duplicates could be handled with linear time pre-
and post-processing, or considered cases easier than the analyzed.
When the input is a random permutation, the rank of the pivot is uniform random from 0 to n −
1.
Then the resulting parts of the partition have sizes i and n −
i − 1,
and i is uniform random from 0 to n −
1.
So, averaging over all possible splits and noting that the number of comparisons for the partition is n − 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:
C
(
        n
        )
        =
n
−
1
+
        
          
            1
n
          
        
        
          ∑
i
            =
0
          
          
            n
−
1
(
        C
(
        i
        )
+
        C
(
        n
        −
i
        −
1
        )
        )
        =
n
−
1
+
        
          
            2
n
          
        
        
          ∑
i
            =
0
          
          
            n
−
            1
C
(
i
        )
{\displaystyle C(n)=n-1+{\frac {1}{n}}\sum _{i=0}^{n-1}(C(i)+C(n
-i-1))=n-1+{\frac {2}{n}}\sum _{i=0}^{n-1}C(i)}
  

  
    
      
        n
C
(
        n
        )
        =
n
(
        n
        −
1
        )
+
        2
∑
i
            =
0
          
          
            n
−
            1
C
(
i
        )
{\displaystyle nC(n)=n(n-1)+2\sum _{i=0}^{n-1}C(i)}
n
        C
(
        n
        )
        −
(
        n
        −
1
        )
C
(
        n
        −
1
        )
        =
n
(
        n
        −
1
        )
        −
(
        n
        −
1
        )
(
        n
        −
2
        )
+
        2
        C
(
        n
        −
1
        )
      
    
    {\displaystyle nC(n)-(n-1)C(n-1)=n(n-1)-(n-1)(n-2)+2C(n-1)}
n
        C
(
        n
        )
=
(
        n
+
        1
        )
        C
(
        n
        −
1
        )
+
        2
n
−
2
      
    
    {\displaystyle nC(n)=(n+1)C(n-1)+2n-2}
  
  
    
      
        
          
            
              
                
                  
                    
                      C
(
                      n
                      )
                    
                    
                      n
+
                      1
=
C
(
                      n
                      −
1
                      )
n
+
                
                  
                    2
                    
                      n
+
                      1
                    
                  
                
                −
                
                  
                    2
n
(
                      n
+
                      1
                      )
≤
                
                  
                    
                      C
(
                      n
                      −
1
                      )
n
+
                
                  
                    2
                    
                      n
+
                      1
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      C
(
                      n
                      −
                      2
                      )
n
−
1
+
                
                  
                    2
n
−
                
                  
                    2
(
                      n
                      −
                      1
                      )
n
+
                
                  
                    2
                    
                      n
+
                      1
≤
                
                  
                    
                      C
(
                      n
                      −
                      2
                      )
n
−
1
+
                
                  
                    2
n
+
                
                  
                    2
                    
                      n
+
                      1
                    
                  
                
              
            
            
              
              
                 
                 
                ⋮
              
            
            
              
              
                
                =
                
                  
                    
                      C
(
                      1
)
2
+
                
                  ∑
i
                    =
2
n
                  
                
                
                  
                    2
i
+
                      1
≤
                2
∑
i
                    =
1
                  
                  
                    n
−
1
                  
                
                
                  
                    1
i
≈
                2
∫
                  
                    1
                  
                  
                    n
                  
                
                
                  
                    1
x
                  
                
                
                  d
                
                x
                =
                2
                ln
                ⁡
n
              
            
          
        
      
    
    {\displaystyle {
\begin{aligned}{\frac {C(n)}{n+1}}&={\frac {C(n-1)}{n}}+{\frac {2}{n+1}}-{\frac {2}{n(n+1)}}\leq {\frac {C(n-1)}{n}}+{\frac {2}{n+1}}\\&={\frac {C(n-2)}{n-1}}+{\frac {2}{n}}-{\frac {2}{(n-1)n}}+{\frac
{2}{n+1}}\leq {\frac {C(n-2)}{n-1}}+{\frac {2}{n}}+{\frac {2}{n+1}}\\&\
\
\vdots
\\&={\frac {C(1)}{2}}+\sum _{i=2}^{n}{\frac {
2}{i+1}}\leq 2\sum
_
{i=1}^{n-1}{\frac {
1}{i}}\approx 2\int
_{1}^{n}{\frac {1}{x}}\mathrm {d} x=2\ln n\end{aligned}}}
Solving the recurrence gives C(n) =
2n ln n ≈ 1.39n log₂ n.
This means that, on average, quicksort performs only about 39% worse than in its best case.
In this sense, it is closer to the best case than the worst case.
A comparison sort cannot use less than log₂(n!)
comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log₂(n!)
≈
n(log₂ n − log₂ e)
, so quicksort is not much worse than an ideal comparison sort.
This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.

====
Using a binary search tree ====
The following binary search tree (BST) corresponds to each execution of quicksort: the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on.
The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions.
So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted 
  
    
      
        (
        
          x
          
            1
          
        
        ,
x
          
            2
          
        
        ,
…
        ,
        
          x
          
            n
          
        
        )
      
    
    {
\displaystyle (x_{1},x_{2},\ldots ,x_{n})}
   form a random permutation.

Consider a BST created by insertion of a sequence
(
        
          x
          
            1
          
        
        ,
x
          
            2
          
        
        ,
…
        ,
        
          x
          
            n
          
        
        )
      
    
    {
\displaystyle
(x_{1},x_{2},\ldots ,x_{n})}
   of values forming a random permutation.
Let C denote the cost of creation of the BST.
We have 
  
    
      
        C
        =
        
          ∑
i
          
        
        
          ∑
          
            j
            <
i
          
        
        
          c
i
            ,
j
          
        
      
    
    {\displaystyle C=\sum _{i}\sum _{j<i}c_{i,j}}
  , where 
  
    
      
        
          c
i
            ,
j
          
        
      
    
    {\displaystyle c_{i,j}}
   is a binary random variable expressing whether during the insertion of 
  
    
      
        
          x
i
          
        
      
    
    {\displaystyle x_{i}}
   there was a comparison to
x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
  .

By linearity of expectation, the expected value 
  
    
      
        E
⁡
[
        C
]
      
    
    {\displaystyle \operatorname {E} [C]}
   of C is 
  
    
      
        E
⁡
[
        C
        ]
        =
        
          ∑
i
          
        
        
          ∑
          
            j
            <
i
Pr
(
        
          c
i
            ,
j
          
        
        )
      
    
    {\displaystyle \operatorname {E}
[C]=\sum _{i}\sum _{j<i}\Pr(c_{i,j})}
  .

Fix
i and j<i.
The values
x
            
              1
            
          
          ,
          
            x
            
              2
            
          
          ,
          …
          ,
          
            x
            
              j
            
          
        
      
    
    {\displaystyle {x_{1},x_{2},\ldots ,x_{j}}}
  , once sorted, define j+1 intervals.
The core structural observation is that 
  
    
      
        
          x
i
          
        
      
    
    {\displaystyle x_{i}}
   is compared to 
  
    
      
        
          x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   in the algorithm if and only if 
  
    
      
        
          x
i
          
        
      
    
    {\displaystyle x_{i}}
   falls inside one of the two intervals adjacent to
x
j
          
        
      
    
    {\displaystyle x_{j}}
  .

Observe that since
(
        
          x
          
            1
          
        
        ,
x
          
            2
          
        
        ,
…
        ,
        
          x
          
            n
          
        
        )
      
    
    {
\displaystyle (x_{1},x_{2},\ldots ,x_{n})}
   is a random permutation, 
  
    
      
        (
        
          x
          
            1
          
        
        ,
x
          
            2
          
        
        ,
…
        ,
        
          x
          
            j
          
        
        ,
x
          
            i
          
        
        )
      
    
    {\displaystyle (x_{1},x_{2},\ldots ,x_{j},x_{i})}
   is also a random permutation, so the probability that 
  
    
      
        
          x
i
          
        
      
    
    {\displaystyle x_{i}}
   is adjacent to
x
          
            j
          
        
      
    
    {\displaystyle x_{j}}
   is exactly 
  
    
      
        
          
            2
j
+
1
            
          
        
      
    
    {\displaystyle {\frac {2}{j+1}}}
  .

We end with a short calculation:
E
⁡
[
        C
        ]
        =
        
          ∑
i
          
        
        
          ∑
          
            j
            <
i
          
        
        
          
            2
j
+
              1
            
          
        
        =
        O
(
          
            
              ∑
i
              
            
            log
⁡
i
          
          )
        
        =
O
(
n
        log
⁡
n
        )
        .
{\displaystyle \operatorname {E}
[C]=\sum _{i}\sum _
{j<i}{\frac {
2}{j+1}}=O\left(\sum _{i}\log i\right)=O(n\log n).}

===
Space complexity ===
The space used by quicksort depends on the version used.

The in-place version of quicksort has a space complexity of O(log n), even in the worst case, when it is carefully implemented using the following strategies.

In-place partitioning is used.
This unstable partition requires O(1) space.

After partitioning, the partition with the fewest elements is (recursively) sorted first, requiring at most O(log n) space.
Then the other partition is sorted using tail recursion or iteration, which doesn't add to the call stack.
This idea, as discussed above, was described by R. Sedgewick, and keeps the stack depth bounded by O(log
n).Quicksort with
in-place and unstable partitioning uses only constant additional space before making any recursive call.
Quicksort must store a constant amount of information for each nested recursive call.
Since the best case makes at most O(log n) nested recursive calls, it uses O(log n) space.
However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make O(n) nested recursive calls and need O(n) auxiliary space.

From a bit complexity viewpoint, variables such as lo and hi do not use constant space; it takes O(log n) bits to index into a list of n items.
Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires O((log n)²) bits of space.
This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least O(n log n) bits of space.

Another, less common, not-in-place, version of quicksort uses O(n) space for working storage and can implement a stable sort.
The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls.
Sedgewick's optimization is still appropriate.

==
Relation to other algorithms ==
Quicksort is a space-optimized version of the binary tree sort.
Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls.
The algorithms make exactly the same comparisons, but in a different order.
An often desirable property of a sorting algorithm is stability – that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way.
This property is hard to maintain for in situ (or in place) quicksort (that uses only constant additional space for pointers and buffers, and O(log n) additional space for the management of explicit or implicit recursion).
For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability.
The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.

The most direct competitor of quicksort is heapsort.
Heapsort's running time is O(n log n), but heapsort's average running time is usually considered slower than in-place quicksort.
This result is debatable; some publications indicate the opposite.
Introsort is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time.

Quicksort also competes with merge sort, another O(n log n) sorting algorithm.
Mergesort is a stable sort, unlike standard in-place quicksort and heapsort, and has excellent worst-case performance.

The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require O(n) auxiliary space, whereas the variant of quicksort with in-place partitioning and tail recursion uses only O(log n) space.

Mergesort works very well on linked lists, requiring only a small, constant amount of auxiliary storage.

Although quicksort can be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access.

Mergesort is also the algorithm of choice for external sorting of very large data sets stored on slow-to-access media such as disk storage or network-attached storage.

Bucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.

===
Selection-based pivoting ===
A selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting.
One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as quickselect.
The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element.
This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but the sorting algorithm is still O(n2).

A variant of quickselect, the median of medians algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time – O(n).
This same pivot strategy can be used to construct a variant of quicksort (median of medians quicksort) with O(n log n) time.
However, the overhead of choosing the pivot is significant, so this is generally not used in practice.

More abstractly, given an O(n) selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort and thus produce a sorting algorithm with O(n log n) running time.
Practical implementations of this variant are considerably slower on average, but they are of theoretical interest because they show an optimal selection algorithm can yield an optimal sorting algorithm.

===
Variants ===


====
Multi-pivot quicksort ====
Instead of partitioning into two subarrays using a single pivot, multi-pivot quicksort (also multiquicksort) partitions its input into some s number of subarrays using s − 1 pivots.
While the dual-pivot case (s = 3) was considered by Sedgewick and others already in the mid-1970s, the resulting algorithms were not faster in practice than the "classical" quicksort.
A 1999 assessment of a multiquicksort with a variable number of pivots, tuned to make efficient use of processor caches, found it to increase the instruction count by some 20%, but simulation results suggested that it would be more efficient on very large inputs.
A version of dual-pivot quicksort developed by Yaroslavskiy in 2009 turned out to be fast enough to warrant implementation in Java 7, as the standard algorithm to sort arrays of primitives (sorting arrays of objects is done using Timsort).
The performance benefit of this algorithm was subsequently found to be mostly related to cache performance, and experimental results indicate that the three-pivot variant may perform even better on modern machines.

====
External quicksort ====
For disk files, an external sort based on partitioning similar to quicksort is possible.
It is slower than external merge sort, but doesn't require extra disk space.
4 buffers are used, 2 for input, 2 for output.
Let N = number of records in the file, B =
the number of records per buffer, and M = N/B =
the number of buffer segments in the file.
Data is read (and written) from both ends of the file inwards.
Let X represent the segments that start at the beginning of the file and Y represent segments that start at the end of the file.
Data is read into the X and Y read buffers.
A pivot record is chosen and the records in the X and Y buffers other than
the pivot record are copied to the X write buffer in ascending order and Y write buffer in descending order based comparison with the pivot record.
Once either X or Y buffer is filled, it is written to the file and the next X or Y buffer is read from the file.
The process continues until all segments are read and one write buffer remains.
If that buffer is an X write buffer, the pivot record is appended to it and the X buffer written.
If that buffer is a Y write buffer, the pivot record is prepended to the Y buffer and the Y buffer written.
This constitutes one partition step of the file, and the file is now composed of two subfiles.
The start and end positions of each subfile are pushed/popped to a stand-alone stack or the main stack via recursion.
To limit stack space to O(log2(n)), the smaller subfile is processed first.
For a stand-alone stack, push the larger subfile parameters onto the stack, iterate on the smaller subfile.
For recursion, recurse on the smaller subfile first, then iterate to handle the larger subfile.
Once a sub-file is less than or equal to 4 B records, the subfile is sorted in place via quicksort and written.
That subfile is now sorted and in place in the file.
The process is continued until all sub-files are sorted and in place.
The average number of passes on the file is approximately 1 + ln(N+1)/(4 B), but worst case pattern is N passes (equivalent to O(n^2) for worst case internal sort).

====
Three-way radix quicksort ====
This algorithm is a combination of radix sort and quicksort.
Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey).
Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character.
Recursively sort the "less than" and "greater than" partitions on the same character.
Recursively sort the "equal to" partition by the next character (key).
Given we sort using bytes or words of length W bits, the best case is O(KN) and the worst case O(2KN) or at least O(N2) as for standard quicksort, given for unique keys N<2K, and K is a hidden constant in all standard comparison sort algorithms including quicksort.
This is a kind of three-way quicksort in which the middle partition represents a (trivially) sorted subarray of elements that are exactly equal to the pivot.

====
Quick radix sort ====
Also developed by Powers as an O(K)
parallel PRAM algorithm.
This is again a combination of radix sort and quicksort but the quicksort left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys.
All comparison sort algorithms impliclty assume the transdichotomous model with K in Θ(log N),
as if K is smaller we can sort in O(N) time using a hash table or integer sorting.

If K ≫ log N but elements are unique within O(log N) bits, the remaining bits will not be looked at by either quicksort or quick radix sort.

Failing that, all comparison sorting algorithms will also have the same overhead of looking through O(K)
relatively useless bits but quick radix sort will avoid the worst case O(N2) behaviours of standard quicksort and radix quicksort, and will be faster even in the best case of those comparison algorithms under these conditions of uniqueprefix(K) ≫ log N. See Powers for further discussion of the hidden overheads in comparison, radix and parallel sorting.

====
BlockQuicksort ====
In any comparison-based sorting algorithm, minimizing the number of comparisons requires maximizing the amount of information gained from each comparison, meaning that the comparison results are unpredictable.

This causes frequent branch mispredictions, limiting performance.
BlockQuicksort rearranges the computations of quicksort to convert unpredictable branches to data dependencies.

When partitioning, the input is divided into moderate-sized blocks (which fit easily into the data cache), and two arrays are filled with the positions of elements to swap.

(To avoid conditional branches, the position is unconditionally stored at the end of the array, and the index of the end is incremented if a swap is needed.)
A second pass exchanges the elements at the positions indicated in the arrays.

Both loops have only one conditional branch, a test for termination, which is usually taken.

====
Partial and incremental quicksort ====
Several variants of quicksort exist that separate the k smallest or largest elements from the rest of the input.

===
Generalization ===
Richard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most
n
        log
⁡
        n
+
        
          O
(
n
        )
      
    
    {\displaystyle n\log n+{O}(n)}
comparisons (close to the information theoretic lower bound) and 
  
    
      
        
          Θ
(
        n
        log
⁡
n
        )
      
    
    {\displaystyle {\Theta }(n\log n)}
   operations; at worst they perform 
  
    
      
        
          Θ
(
        n
        
          log
          
            2
⁡
n
        )
      
    
    {\displaystyle {\Theta }(n\log ^{2}n)}
   comparisons (and also operations); these are in-place, requiring only additional 
  
    
      
        
          O
(
        log
        ⁡
n
        )
{\displaystyle {O}(\log n)}
   space.
Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley-McIlroy).

==
See also ==
Introsort – Hybrid sorting algorithm


== Notes ==


==
References ==
Sedgewick, R. (1978).
"
Implementing Quicksort programs".
Comm.
ACM.
21 (10)
: 847–857.
doi:10.1145/359619.359631.
S2CID 10020756.

Dean, B. C. (2006).
"
A simple expected running time analysis for randomized 'divide and conquer' algorithms".
Discrete Applied Mathematics.
154: 1–5.
doi:10.1016/j.dam.2005.07.005.

Hoare, C. A. R. (1961).
"
Algorithm 63
: Partition".
Comm.
ACM.
4 (7): 321.
doi:10.1145/366622.366642.
S2CID 52800011.

Hoare, C. A. R. (1961).
"
Algorithm 65
: Find".
Comm.
ACM.
4 (7): 321–322.
doi:10.1145/366622.366647.

Hoare, C. A. R. (1962).
"
Quicksort".
Comput.
J. 5 (1):
10–16.
doi:10.1093
/comjnl/5.1.10.
(
Reprinted in Hoare and Jones:
Essays in computing science, 1989.)

Musser, David R. (1997). "
Introspective Sorting and Selection Algorithms".
Software: Practice and Experience.
27
(8): 983–993.
doi:10.1002/(SICI)1097
-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-#.

Donald Knuth.
The Art of Computer Programming, Volume 3:
Sorting and Searching, Third Edition.
Addison-Wesley, 1997.
ISBN 0-201-89685-0.
Pages 113–122 of section 5.2.2: Sorting by Exchanging.

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms, Second Edition.
MIT Press and McGraw-Hill, 2001.
ISBN 0-262-03293-7.
Chapter 7: Quicksort, pp.
145–164.

Faron Moller.
Analysis of Quicksort.
CS 332:
Designing Algorithms.
Department of Computer Science, Swansea University.

Martínez, C.;
Roura, S. (2001). "
Optimal Sampling Strategies in Quicksort and Quickselect".
SIAM J. Comput.
31 (3): 683–705.
CiteSeerX 10.1.1.17.4954.
doi:10.1137/S0097539700382108.

Bentley, J. L.; McIlroy, M. D. (1993).
"
Engineering a sort function".
Software: Practice and Experience.
23 (11):
1249–1265.
CiteSeerX 10.1.1.14.8162.
doi:10.1002
/spe.4380231105.
==
External links ==
"
Animated Sorting Algorithms:
Quick Sort".
Archived from the original on 2 March 2015.
Retrieved 25 November 2008.
–
graphical demonstration
"Animated Sorting Algorithms: Quick Sort (3-way partition)".
Archived from the original on 6 March 2015.
Retrieved 25 November 2008.

Open Data Structures – Section 11.1.2 – Quicksort, Pat Morin
Interactive illustration of Quicksort, with code walkthrough
In the analysis of algorithms, the master theorem for divide-and-conquer recurrences provides an asymptotic analysis (using Big O notation) for recurrence relations of types that occur in the analysis of many divide and conquer algorithms.
The approach was first presented by Jon Bentley, Dorothea Haken, and James B. Saxe in 1980, where it was described as a "unifying method" for solving such recurrences.
The name "master theorem" was popularized by the widely used algorithms textbook Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein.

Not all recurrence relations can be solved with the use of this theorem; its generalizations include the Akra–Bazzi method.

==
Introduction ==
Consider a problem that can be solved using a recursive algorithm such as the following:
procedure p(input x of size n):
if n < some constant
k:
Solve
x directly without recursion
    else
:
        Create a subproblems of x,
each having size
n/
b
        Call procedure p recursively on each subproblem
Combine the results from the subproblems
The above algorithm divides the problem into a number of subproblems recursively, each subproblem being of size n/b.
Its solution tree has a node for each recursive call, with the children of that node being the other calls made from that call.
The leaves of the tree are the base cases of the recursion, the subproblems (of size less than k) that do not recurse.
The above example would have a child nodes at each non-leaf node.
Each node does an amount of work that corresponds to the size of the sub problem
n passed to that instance of the recursive call and given by
f
(
        n
        )
      
    
    {\displaystyle f(n)}
  .
The total amount of work done by the entire algorithm is the sum of the work performed by all the nodes in the tree.

The runtime of an algorithm such as the 'p' above on an input of size 'n', usually denoted 
  
    
      
        T
        (
        n
        )
      
    
    {\displaystyle T(n)}
  , can be expressed by the recurrence relation

  
    
      
        T
(
        n
        )
        =
a
        
        T
(
          
            
              n
b
            
          
          )
        
        +
        f
(
        n
        )
        ,
      
    
    {\displaystyle T(n)=a\;T\left({\frac {n}{b}}\right)+f(n),}
  where 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)
}
   is the time to create the subproblems and combine their results in the above procedure.
This equation can be successively substituted into itself and expanded to obtain an expression for the total amount of work done.
The master theorem allows many recurrence relations of this form to be converted to Θ-notation directly, without doing an expansion of the recursive relation.

==
Generic form ==
The master theorem always yields asymptotically tight bounds to recurrences from divide and conquer algorithms that partition an input into smaller subproblems of equal sizes, solve the subproblems recursively, and then combine the subproblem solutions to give a solution to the original problem.
The time for such an algorithm can be expressed by adding the work that they perform at the top level of their recursion (to divide the problems into subproblems and then combine the subproblem solutions) together with the time made in the recursive calls of the algorithm.
If 
  
    
      
        T
        (
        n
        )
      
    
    {\displaystyle T(n)}
   denotes the total time for the algorithm on an input of size
n
      
    
    {\displaystyle n}
  , and 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
denotes the amount of time taken at the top level of the recurrence then the time can be expressed by a recurrence relation that takes the form:
T
(
        n
        )
        =
a
        
        T
(
          
            
              n
b
            
          
          )
        
        +
        f
(
n
        )
      
    
    {\displaystyle T(n)=a\;T\!\left({\frac {n}{b}}\right)+f(n)}
Here 
  
    
      
        n
      
    
    {\displaystyle n}
   is the size of an input problem,
a
      
    
    {\displaystyle a}
   is the number of subproblems in the recursion, and 
  
    
      
        b
      
    
    {\displaystyle b}
   is the factor by which the subproblem size is reduced in each recursive call.
The theorem below also assumes that, as a base case for the recurrence, 
  
    
      
        T
(
        n
        )
        =
Θ
(
        1
)
{\displaystyle T(n)=\Theta (1)}
   when 
  
    
      
        n
      
    
    {\displaystyle n}
   is less than some bound
κ
>
0
      
    
    {\displaystyle \kappa >0}
  , the smallest input size that will lead to a recursive call.

Recurrences of this form often satisfy one of the three following regimes, based on how the work to split/recombine the problem 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
   relates to the critical exponent 
  
    
      
        
          c
crit
          
        
        =
        
          log
          
            b
⁡
a
      
    
    {\displaystyle c_{\operatorname {crit} }=
\log
_{b}a}
  .
(
The table below uses standard big O notation.)

c
crit
          
        
        =
        
          log
          
            b
⁡
a
        =
log
⁡
(
        #
        
          subproblems
        
        )
        
          /
        
        log
⁡
(
        
          relative subproblem size
        
        )
      
    
    {\displaystyle c_{\operatorname {crit} }=
\log _{b}a=\log(\#{\text{subproblems}})/\log({\text{relative subproblem size}})}
A useful extension of Case 2 handles all values of
k
      
    
    {\displaystyle k}
  :
=== Examples ===


====
Case 1 example ====
T
(
n
        )
        =
        8
        T
(
          
            
              n
2
            
          
          )
+
1000
        
          n
          
            2
          
        
      
    
    {\displaystyle T(n)=8T\left({\frac {n}{2}}\right)+1000n^{2}}
As one can see from the formula above:

  
    
      
        a
=
        8
        ,
        
        b
=
2
        ,
        
        f
(
        n
        )
        =
1000
n
          
            2
          
        
      
    
    {\displaystyle a=8,\,b=2,\,f(n)=1000n^{2}}
, so

  
    
      
        f
        (
        n
        )
        =
        O
(
          
            n
            
              c
            
          
          )
        
      
    
    {\displaystyle f(n)=O\left(n^{c}\right)}
  , where 
  
    
      
        c
        =
2
      
    
    {\displaystyle c=2}
Next, we see if we satisfy the case 1 condition:

  
    
      
        
          log
          
            b
⁡
a
=
        
          log
          
            2
⁡
8
=
3
>
c
      
    
    {\displaystyle \log _{b}a=\log _{2}8=3>
c}
.It
follows from the first case of the master theorem that

  
    
      
        T
(
        n
        )
        =
Θ
(
          
            n
            
              
                log
b
⁡
a
            
          
          )
        
        =
Θ
(
          
            n
            
              3
            
          
          )
{\displaystyle T(n)=\Theta \left(n^{\log
_{b}a}\right)=\Theta \left(n^{3}\right)}
(This result is confirmed by the exact solution of the recurrence relation, which is 
  
    
      
        T
(
        n
        )
=
1001
n
          
            3
          
        
        −
1000
n
          
            2
          
        
      
    
    {\displaystyle T(n)=1001n^{3}-1000n^{2}}
  ,
assuming 
  
    
      
        T
(
        1
        )
=
1
      
    
    {\displaystyle T(1)=1}
  ).
====
Case 2 example ====
T
(
n
        )
        =
2
T
(
          
            
              n
2
            
          
          )
+
        10
n
      
    
    {\displaystyle
T(n)=2T\left({\frac {n}{2}}\right)+10n}
As we can see in the formula above the variables get the following values:

  
    
      
        a
=
        2
        ,
        
        b
=
2
        ,
        
        c
=
1
        ,
        
        f
(
        n
        )
=
10
n
      
    
    {\displaystyle a=2,\,b=2,\,c=1,\,f(n)=10n}
f
(
        n
        )
        =
Θ
(
          
            
              n
              
                c
              
            
            
              log
              
                k
              
            
            ⁡
n
          
          )
{\displaystyle f(n)=\Theta \left(n^{c}\log ^{k}n\right)}
   where 
  
    
      
        c
        =
1
        ,
k
        =
0
      
    
    {\displaystyle c=1,k=0}
Next, we see if we satisfy the case 2 condition:

  
    
      
        
          log
          
            b
⁡
a
=
        
          log
          
            2
⁡
2
=
1
      
    
    {\displaystyle
\log _{b}a=\log _{2}2=1}
  , and therefore, 
  
    
      
        c
=
        
          log
          
            b
⁡
a
      
    
    {\displaystyle c=\log _{b}a}
So it follows from the second case of the master theorem:
T
(
        n
        )
        =
Θ
(
          
            
              n
              
                
                  log
                  
                    b
⁡
a
              
            
            
              log
              
                k
+
                1
⁡
n
          
          )
        
        =
Θ
(
          
            
              n
              
                1
              
            
            
              log
              
                1
⁡
n
          
          )
        
        =
Θ
(
          
            n
            log
⁡
n
          
          )
        
      
    
    {\displaystyle T(n)=\Theta \left(n^{\log
_{b}a}\log ^{k+1}n\right)=\Theta \left(n^{1}\log
^{1}n\right)=\Theta \left(n\log n\right)}
Thus the given recurrence relation T(n) was in Θ(n log n).

(This result is confirmed by the exact solution of the recurrence relation, which is 
  
    
      
        T
(
        n
        )
        =
n
+
        10
n
log
          
            2
⁡
n
      
    
    {\displaystyle T(n)=n+10n\log _{2}n}
  ,
assuming 
  
    
      
        T
(
        1
        )
=
1
      
    
    {\displaystyle T(1)=1}
  .)
====
Case 3 example ====
T
(
n
        )
        =
2
T
(
          
            
              n
2
            
          
          )
+
n
          
            2
          
        
      
    
    {\displaystyle
T(n)=2T\left({\frac {n}{2}}\right)+n^{2}}
As we can see in the formula above the variables get the following values:

  
    
      
        a
=
        2
        ,
        
        b
=
2
        ,
        
        f
(
        n
        )
        =
        
          n
          
            2
          
        
      
    
    {\displaystyle a=2,\,b=2,\,f(n)=n^{2}}
  

  
    
      
        f
(
n
        )
        =
        Ω
        
          (
          
            n
            
              c
            
          
          )
        
      
    
    {\displaystyle f(n)=\Omega \left(n^{c}\right)}
  , where 
  
    
      
        c
        =
2
      
    
    {\displaystyle c=2}
Next, we see if we satisfy the case 3 condition:

  
    
      
        
          log
          
            b
⁡
a
=
        
          log
          
            2
⁡
2
=
1
      
    
    {\displaystyle
\log _{b}a=\log _{2}2=1}
  , and therefore, yes, 
  
    
      
        c
>
log
          
            b
⁡
a
      
    
    {\displaystyle c>\log _{b}a}
  The regularity condition also holds:

  
    
      
        2
(
          
            
              
                n
                
                  2
                
              
              4
            
          
          )
        
        ≤
k
n
          
            2
          
        
      
    
    {\displaystyle 2\left({\frac {n^{2}}{4}}\right)\leq kn^{2}}
  ,
choosing 
  
    
      
        k
        =
1
        
          /
2
      
    
    {\displaystyle k=1/2}
So it follows from the third case of the master theorem:
T
(
          n
          )
        
        =
Θ
        
          (
f
(
            n
            )
          
          )
        
        =
Θ
        
          (
          
            n
2
            
          
          )
        
        .

{\displaystyle T\left(n\right)=\Theta \left(f(n)\right)=\Theta \left(n^{2}\right).}
Thus the given recurrence relation
T
        (
        n
        )
      
    
    {\displaystyle T(n)}
   was in 
  
    
      
        Θ
(
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Theta (n^{2})}
  , that complies with the 
  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
   of the original formula.

(This result is confirmed by the exact solution of the recurrence relation, which is 
  
    
      
        T
(
n
        )
        =
2
        
          n
          
            2
          
        
        −
n
      
    
    {\displaystyle T(n)=2n^{2}-n}
  ,
assuming 
  
    
      
        T
(
        1
        )
=
1
      
    
    {\displaystyle T(1)=1}
  .)
==
Inadmissible equations ==
The following equations cannot be solved using the master theorem:
T
(
        n
        )
        =
        
          2
          
            n
          
        
        T
(
          
            
              n
2
            
          
          )
+
        
          n
n
          
        
      
    
    {
\displaystyle T(n)=2^{n}T\left({\frac {n}{2}}\right)+n^{n}}
a is not a constant; the number of subproblems should be fixed

  
    
      
        T
(
        n
        )
=
2
T
(
          
            
              n
2
            
          
          )
+
        
          
            n
log
⁡
n
            
          
        
      
    
    {\displaystyle T(n)=2T\left({\frac {n}{2}}\right)+{\frac {n}{\log n}}}
  
non-polynomial difference between 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
and
n
log
              
                b
⁡
a
          
        
      
    
    {\displaystyle n^{\log _{b}a}}
   (see below; extended version applies)
T
(
        n
        )
=
        0.5
        T
(
          
            
              n
2
            
          
          )
+
n
      
    
    {
\displaystyle T(n)=0.5T\left({\frac {n}{2}}\right)+n}
  

  
    
      
        a
        <
        1
      
    
    {\displaystyle a<1}
   cannot have less than one sub problem
T
(
n
        )
        =
64
        T
(
          
            
              n
8
            
          
          )
        
        −
n
          
            2
log
⁡
n
      
    
    {
\displaystyle T(n)=64T\left({\frac {n}{8}}\right)-n^{2}\log n}
  

  
    
      
        f
(
        n
        )
      
    
    {\displaystyle f(n)}
  , which is the combination time, is not positive
T
(
        n
        )
        =
T
(
          
            
              n
2
            
          
          )
+
        n
(
        2
        −
cos
⁡
n
        )
{\displaystyle T(n)=T\left({\frac {n}{2}}\right)+n(2-\cos n)}
case 3 but regularity violation.
In the second inadmissible example above, the difference between 
  
    
      
        f
(
n
        )
      
    
    {\displaystyle f(n)}
and
n
log
              
                b
⁡
a
          
        
      
    
    {\displaystyle n^{\log _{b}a}}
   can be expressed with the ratio 
  
    
      
        
          
            
              f
(
n
              )
n
              
                
                  log
                  
                    b
⁡
a
              
            
          
        
        =
n
              
                /
              
              log
⁡
n
            
            
              n
log
                  
                    2
⁡
2
              
            
          
        
        =
n
            
              n
log
⁡
n
=
        
          
            1
log
⁡
n
            
          
        
      
    
    {\displaystyle {\frac {f(n)}{n^{\log _{b}a}}}={\frac
{n/\log n}{n^{\log _{
2}2}}}={\frac {n}{n\log n}}={\frac {1}{\log n}}}
  .

It is clear that 
  
    
      
        
          
            1
log
⁡
n
            
          
        
        <
n
ϵ
          
        
      
    
    {\displaystyle {\frac {1}{\log n}}<n^{\epsilon }}
for any constant 
  
    
      
        ϵ
>
0
      
    
    {\displaystyle \epsilon >0}
  .

Therefore, the difference is not polynomial and the basic form of the Master Theorem does not apply.
The extended form (case 2b) does apply, giving the solution
T
(
        n
        )
        =
Θ
(
        n
        log
⁡
log
⁡
n
        )
      
    
    {\displaystyle T(n)=\Theta (n\log \log n)}
  .
==
Application to common algorithms ==


==
See also ==
Akra
–Bazzi method
Asymptotic complexity


== Notes ==


==
References ==
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.
Introduction to Algorithms, Second Edition.
MIT Press and McGraw–Hill, 2001.
ISBN 0-262-03293-7.
Sections 4.3 (The master method) and 4.4 (Proof of the master theorem), pp.
73–90.

Michael T. Goodrich and Roberto Tamassia.
Algorithm Design: Foundation, Analysis, and Internet Examples.
Wiley, 2002.
ISBN 0-471-38365-1.
The master theorem (including the version of Case 2 included here, which is stronger than the one from CLRS) is on pp.
268–270.

==
External links ==
Teorema Mestre e Exemplos Resolvidos (in Portuguese)
Ivan Edward Sutherland (born May 16, 1938) is an American computer scientist and Internet pioneer, widely regarded as a pioneer of computer graphics.
His early work in computer graphics as well as his teaching with David C. Evans in that subject at the University of Utah in the 1970s was pioneering in the field.

Sutherland, Evans, and their students from that era developed several foundations of modern computer graphics.

He received the Turing Award from the Association for Computing Machinery in 1988 for the invention of Sketchpad, an early predecessor to the sort of graphical user interface that has become ubiquitous in personal computers.
He is a member of the National Academy of Engineering, as well as the National Academy of Sciences among many other major awards.
In 2012 he was awarded the Kyoto Prize in Advanced Technology for "pioneering achievements in the development of computer graphics and interactive interfaces".
==
Biography ==
Sutherland's father was from New Zealand; his mother was from Scotland.
The family moved to Wilmette, Illinois, then Scarsdale, New York, for his father's career.
Bert Sutherland was his elder brother.
Ivan Sutherland earned his bachelor's degree in electrical engineering from the Carnegie Institute of Technology, his master's degree from Caltech, and his Ph.D. from MIT in EECS in 1963.Sutherland invented Sketchpad in 1962 while at MIT.
Claude Shannon signed on to supervise Sutherland's computer drawing thesis.
Among others on his thesis committee were Marvin Minsky and Steven Coons.
Sketchpad was an innovative program that influenced alternative forms of interaction with computers.
Sketchpad could accept constraints and specified relationships among segments and arcs, including the diameter of arcs.
It could draw both horizontal and vertical lines and combine them into figures and shapes.
Figures could be copied, moved, rotated, or resized, retaining their basic properties.
Sketchpad also had the first window-drawing program and clipping algorithm, which allowed zooming.
Sketchpad ran on the Lincoln TX-2 computer and influenced Douglas Engelbart's oN-Line System.
Sketchpad, in turn, was influenced by the conceptual Memex as envisioned by Vannevar Bush in his influential paper "
As We May Think".
Sutherland replaced J. C. R. Licklider as the head of the US Defense Department Advanced Research Project Agency's  Information Processing Techniques Office (IPTO),
when Licklider returned to MIT in 1964.From 1965 to 1968, Sutherland was an associate professor of electrical engineering at Harvard University.
Work with student Danny Cohen in 1967 led to the development of the Cohen–
Sutherland computer graphics line clipping algorithm.
In 1968, with his students Bob Sproull, Quintin Foster, Danny Cohen, and others he created the first head-mounted display that rendered images for the viewer's changing pose, as sensed by The Sword of Damocles, thus making the first virtual reality system.
A prior system, Sensorama, used a head-mounted display to play back static video and other sensory stimuli.
The optical see-through head-mounted display used in Sutherland's VR system was a stock item used by U.S. military helicopter pilots to view video from cameras mounted on the helicopter's belly.

From 1968 to 1974, Sutherland was a professor at the University of Utah.

Among his students there were Alan Kay, inventor of the Smalltalk language, Gordon W. Romney (computer and cybersecurity scientist), who rendered the first 3D images at U of U, Henri Gouraud, who devised the Gouraud shading technique, Frank Crow, who went on to develop antialiasing methods, Jim Clark, founder of Silicon Graphics, Henry Fuchs, and Edwin Catmull, co-founder of Pixar and now president of Walt Disney and Pixar Animation Studios.

In 1968 he co-founded Evans & Sutherland with his friend and colleague David C. Evans.

The company did pioneering work in the field of real-time hardware, accelerated 3D computer graphics, and printer languages.

Former employees of Evans & Sutherland included the future founders of Adobe (John Warnock) and Silicon Graphics (Jim Clark).

From 1974 to 1978 he was the Fletcher Jones Professor of Computer Science at California Institute of Technology, where he was the founding head of that school's computer science department.

He then founded a consulting firm, Sutherland, Sproull and Associates, which was purchased by Sun Microsystems to form the seed of its research division, Sun Labs.
Sutherland was a fellow and vice president at Sun Microsystems.

Sutherland was a visiting scholar in the computer science division at University of California, Berkeley (fall 2005 – spring 2008).

On May 28, 2006,  Ivan Sutherland married Marly Roncken.

Sutherland and Marly Roncken are leading the research in Asynchronous Systems at Portland State University.
He has two children.
His elder brother, Bert Sutherland, was also a computer science researcher.

==
Awards ==
Computer History Museum Fellow "for the Sketchpad computer-aided design system and for lifelong contributions to computer graphics and education," 2005
R&D 100 Award, 2004 (team)
IEEE John von Neumann Medal, 1998
Association for Computing Machinery Fellow, 1994
Electronic Frontier Foundation EFF Pioneer Award, 1994
ACM Software System Award, 1993
Turing Award, 1988
Computerworld Honors Program, Leadership Award, 1987
IEEE Emanuel R. Piore Award –  1986 "For pioneering work in the development of interactive computer graphics systems and contributions to computer science education."

Member, United States National Academy of Sciences, 1978
National Academy of Engineering member, 1973
Kyoto Prize 2012, in the category of advanced technology.

National Inventors Hall of Fame Inductee, 2016.

Washington Award, 2018
BBVA Fronteras del conocimiento 2019.

==
Quotes ==
"A display connected to a digital computer gives us a chance to gain familiarity with concepts not realizable in the physical world.

It is a looking glass into a mathematical wonderland.
"

"The ultimate display would, of course, be a room within which the computer can control the existence of matter.
A chair displayed in such a room would be good enough to sit in.
Handcuffs displayed in such a room would be confining, and a bullet displayed in such a room would be fatal."

When asked, "How could you possibly have done the first interactive graphics program, the first non-procedural programming language, the first object oriented software system, all in one year?"
Ivan replied: "Well, I didn't know it was hard."

"It’s not an idea until you write it down.
"

"Without the fun, none of us would go on!"

==
Patents ==
Sutherland has more than 60 patents, including:

US Patent 7,636,361 (2009) Apparatus and method for high-throughput asynchronous communication with flow control
US Patent 7,417,993
(2008) Apparatus and method for high-throughput asynchronous communication
US Patent 7,384,804 (2008
) Method and apparatus for electronically aligning capacitively coupled mini-bars
US patent 3,889,107 (1975)  System of polygon sorting by dissection
US patent 3,816,726 (1974)
Computer Graphics Clipping System for Polygons
US patent 3,732,557 (1973)
Incremental Position-Indicating System
US patent 3,684,876 (1972) Vector Computing System as for use in a Matrix Computer
US patent 3,639,736 (1972)
Display Windowing by Clipping


==
Publications ==
SketchPad, 2004 from "CAD software – history of CAD CAM" by CADAZZ
Sutherland's 1963 Ph.D. Thesis from Massachusetts Institute of Technology republished in 2003 by University of Cambridge as Technical Report Number 574, Sketchpad, A Man-Machine Graphical Communication System.

His thesis supervisor was Claude Shannon, father of information theory.

Duchess Chips for Process-Specific Wire Capacitance Characterization,
The, by Jon Lexau, Jonathan Gainsley, Ann Coulthard and Ivan E. Sutherland,
Sun Microsystems Laboratories Report Number TR-2001-100, October 2001
Technology And Courage by Ivan Sutherland,
Sun Microsystems Laboratories Perspectives Essay Series, Perspectives-96-1 (April 1996)
Biography, "Ivan Sutherland" circa 1996, hosted by the Georgia Institute of Technology College of Computing at Archive.today (archived July 18, 2011)
Counterflow Pipeline Processor Architecture, by Ivan E. Sutherland, Charles E. Molnar (Charles Molnar), and Robert F. Sproull (Bob Sproull),
Sun Microsystems Laboratories Report Number TR-94-25, April 1994
Oral history interview with Ivan Sutherland at Charles Babbage Institute, University of Minnesota, Minneapolis.

Sutherland describes his tenure as head of the Information Processing Techniques Office (IPTO) from 1963 to 1965.
He discusses the existing programs as established by J. C. R. Licklider and the new initiatives started while he was there: projects in graphics and networking, the ILLIAC IV, and the Macromodule program.

== See also ==
List of pioneers in computer science


==
References ==


==
External links ==
Ivan Sutherland at IMDb
Frequency illusion, also known as the Baader–Meinhof phenomenon, is a cognitive bias in which, after noticing something for the first time, there is a tendency to notice it more often, leading someone to believe that it has a high frequency (a form of selection bias).
It occurs when increased awareness of something creates the illusion that it is appearing more often.
Put plainly, the frequency illusion is when "a concept or thing you just found out about suddenly seems to crop up everywhere.
"The
name "Baader–Meinhof phenomenon" was derived from a particular instance of frequency illusion in which the Baader–Meinhof Group was mentioned.
In this instance, it was noticed by a man named Terry Mullen, who in 1994 wrote a letter to a newspaper column in which he mentioned that he had first heard of the Baader–Meinhof Group, and shortly thereafter coincidentally came across the term from another source.
After the story was published, various readers submitted letters detailing their own experiences of similar events, and the name "Baader–Meinhof phenomenon" was coined as a result.
The term "frequency illusion" was coined in 2006 by Arnold Zwicky, a professor of linguistics at Stanford University and The Ohio State University.
Arnold Zwicky considered this illusion a process involving two cognitive biases: selective attention bias (noticing things that are salient to us and disregarding the rest) followed by confirmation bias (looking for things that support our hypotheses while disregarding potential counter-evidence).
It is considered mostly harmless, but can cause worsening symptoms in patients with schizophrenia.
The frequency illusion may also have legal implications, as eye witness accounts and memory can be influenced by this illusion.

== See also ==
Chronostasis
Confirmation bias
List of cognitive biases
Recency illusion
Synchronicity
Availability cascade


=
= References ==
Advanced Placement Computer Science A (also called AP Comp Sci, AP Comp Sci A, APCS, APCSA, or AP Java) is an AP Computer Science course and examination offered by the College Board to high school students as an opportunity to earn college credit for a college-level computer science course.
AP Computer Science A is meant to be the equivalent of a first-semester course in computer science.
The AP exam currently tests students on their knowledge of Java.

AP Computer Science AB, which was equivalent to a full year, was discontinued following the May 2009 exam administration.

==
Course content ==
AP Computer Science emphasizes object-oriented programming methodology with an emphasis on problem solving and algorithm development.
It also includes the study of data structures and abstraction, but these topics were not covered to the extent that they were covered in AP Computer Science AB.
The Microsoft-sponsored program Technology Education and Literacy in Schools (TEALS) aims to increase the number of students taking AP Computer Science classes.
The units of the exam are as follows:


==
Case studies and labs ==
Historically, the AP exam used several programs in its free-response section to test students' knowledge of object-oriented programs without requiring them to develop an entire environment.
These programs were called Case Studies.

This practice was discontinued as of the 2014–15 school year and replaced with optional labs that teach concepts.

===
Case studies (discontinued) ===
Case studies were used in AP Computer Science curriculum starting in 1994.

====
Big Integer case study (1994-1999) ====
The Big Integer case study was in use prior to 2000.
It was replaced by the Marine Biology case study.

====
Marine Biology case study (2000-2007) ====
The Marine Biology Case Study (MBCS) was a program written in C++ until 2003, then in Java, for use with the A and AB examinations.
It served as an example of object-oriented programming (OOP) embedded in a more complicated design project than most students had worked with before.

The case study was designed to allow the College Board to quickly test a student's knowledge of object oriented programming ideas such as inheritance and encapsulation while requiring students to understand how objects such as "the environment", "the fish", and the simulation's control module interact with each other without having to develop the entire environment independently, which would be quite time-consuming.
The case study also gives all students taking the AP Computer Science exams with a common experience from which to draw additional test questions.

On each of the exams, at least one free-response question was derived from the case study.
There were also five multiple-choice questions that are derived from the case study.

This case study was discontinued from 2007, and was replaced by GridWorld.
====
GridWorld case study (2008-2014) ====
GridWorld is a computer program case study written in Java that was used with the AP Computer Science program from 2008 to 2014.
It serves as an example of object-oriented programming (OOP).
GridWorld succeeded the Marine Biology Simulation Case Study, which was used from 2000–2007.
The GridWorld framework was designed and implemented by Cay Horstmann, based on the Marine Biology Simulation Case Study.
The narrative was produced by Chris Nevison and Barbara Cloud Wells, Colgate University.

The GridWorld Case Study was used as a substitute for writing a single large program as a culminating project.
Due to obvious time restraints during the exam, the GridWorld Case Study was provided by the College Board  to students prior to the exam.
Students were expected to be familiar with the classes and interfaces (and how they interact) before taking the exam.
The case study was divided into five sections, the last of which was only tested on the AB exam.
Roughly five multiple-choice questions in Section I were devoted to the GridWorld Case Study, and it was the topic of one free response question in Section II.

GridWorld has been discontinued and replaced with a set of labs for the 2014–2015 school year.

Actors
The GridWorld Case Study employs an Actor class to construct objects in the grid.

The Actor class manages the object's color, direction, location, what the object does in the simulation, and how the object interacts with other objects.

Actors are broken down into the classes "Flower", "Rock", "Bug", and "Critter", which inherit the Actor class and often override certain methods (most notably the Act method).

Flowers can't move, and when forced to Act, they become darker.

Flowers are dropped by Bugs and eaten by Critters.

Rocks are also immobile and aren't dropped or eaten.

Bugs move directly ahead of themselves, unless blocked by a rock or another bug, in which case the Bug will make a 45 degree turn and try again.

They drop flowers in every space they occupy, eat flowers that are directly on their space of grid, and are consumed by Critters.

Critters move in a random direction to a space that isn't occupied by a Rock or other Critter and consume Flowers and Bugs.

Extensions
The Case Study also includes several extensions of the above classes.
"
BoxBug" extends "Bug" and moves in a box shape if its route is not blocked. "
ChameleonCritter" extends "Critter" and does not eat other Actors, instead changing its color to match the color one of its neighbors. "
Crab Critter" moves left or right and only eats Actors in front of it, but otherwise extends the "Critter" class.

Students often create their own extensions of the Actor class.
Some common examples of student created extensions are Warden organisms and SimCity-like structures, in which objects of certain types create objects of other types based on their neighbors (much like Conway's Game of Life).
Students have even created versions of the games Pac-Man, Fire Emblem, and Tetris.

Known issues
The version that is available at the College Board website, GridWorld 1.00, contains a bug (not to be confused with the Actor subclass Bug) that causes a SecurityException to be thrown when it is deployed as an applet.
This was fixed in the "unofficial code" release on the GridWorld website.
Also, after setting the environment to an invalid BoundedGrid, it will cause a NullPointerException.

===
Labs ===
Instead of the discontinued case studies, the College Board created three new labs that instructors are invited to use, but they are optional and are not tested on the exam.
There are no question on the specific content of the labs on the AP exam, but there are questions that test the concepts developed in the labs.
The three labs are:
The Magpie Lab
The Elevens Lab
The Picture Lab


== AP Exam ==


===
History ===
The AP exam in Computer Science was first offered in 1984.

Before 1999, the AP exam tested students on their knowledge of Pascal.
From 1999 to 2003, the exam tested students on their knowledge of C++ instead.
Since 2003, the AP Computer Science exam has tested students on their knowledge of computer science through Java.

=== Format ===
The exam is composed of two sections, formerly consisting of the following times:
Section I: Multiple Choice [1 hour and 15 minutes for 40 multiple-choice questions]
Section II: Free-Response [1 hour and 45 minutes for 4 problems involving extended reasoning]As of 2015,
however, the Multiple Choice section was extended by 15 minutes while the Free-Response section was reduced by 15 minutes for the following:
Section I: Multiple Choice [1 hour and 30 minutes for 40 multiple-choice questions]
Section II: Free-Response [1 hour and 30 minutes for 4 problems involving extended reasoning]


===
Grade distributions for AP Computer Science A ===
In the 2014 administration, 39,278 students took the exam.
The mean score was a 2.96 with a standard deviation of 1.55.
The grade distributions since 2003 were:


==
AP Computer Science AB ==


===
Course content ===
The discontinued AP Computer Science AB course included all the topics of AP Computer Science A, as well as a more formal and a more in-depth study of algorithms, data structures, and data abstraction.
For example, binary trees were studied in AP Computer Science AB but not in AP Computer Science A.
The use of recursive data structures and dynamically allocated structures were fundamental to AP Computer Science AB.
Due to low numbers of students taking the AP Computer Science AB exam, it was discontinued after the 2008–2009 year.

===
Grade distributions for AP Computer Science AB ===
The AP Computer Science AB Examination was discontinued as of May 2009.
The grade distributions from 2003 to 2009 are shown below:


==
See also ==
Computer science
Glossary of computer science


==
References ==


==
External links ==
College Board:
AP Computer Science A
The representativeness heuristic is used when making judgments about the probability of an event under uncertainty.
It is one of a group of heuristics (simple rules governing judgment or decision-making) proposed by psychologists Amos Tversky and Daniel Kahneman in the early 1970s as "the degree to which [an event] (i) is similar in essential characteristics to its parent population, and (ii) reflects the salient features of the process by which it is generated".

Heuristics are described as "judgmental shortcuts that generally get us where we need to go – and quickly – but at the cost of occasionally sending us off course.
"
Heuristics are useful because they use effort-reduction and simplification in decision-making.
When people rely on representativeness to make judgments, they are likely to judge wrongly because the fact that something is more representative does not actually make it more likely.
The representativeness heuristic is simply described as assessing similarity of objects and organizing them based around the category prototype (e.g., like goes with like, and causes and effects should resemble each other).

This heuristic is used because it is an easy computation.
The problem is that people overestimate its ability to accurately predict the likelihood of an event.
Thus, it can result in neglect of relevant base rates and other cognitive biases.

==
Determinants of representativeness ==
The representativeness heuristic is more likely to be used when the judgement or decision to be made has certain factors.

===
Similarity ===
When judging the representativeness of a new stimulus/event, people usually pay attention to the degree of similarity between the stimulus/event and a standard/process.
It is also important that those features be salient.
Nilsson, Juslin, and Olsson (2008) found this to be influenced by the exemplar account of memory (concrete examples of a category are stored in memory)
so that new instances were classified as representative if highly similar to a category as well as if frequently encountered.

Several examples of similarity have been described in the representativeness heuristic literature.
This research has focused on medical beliefs.

People often believe that medical symptoms should resemble their causes or treatments.
For example, people have long believed that ulcers were caused by stress, due to the representativeness heuristic, when in fact bacteria cause ulcers.

In a similar line of thinking, in some alternative medicine beliefs patients have been encouraged to eat organ meat that corresponds to their medical disorder.
Use of the representativeness heuristic can be seen in even simpler beliefs, such as the belief that eating fatty foods makes one fat.
Even physicians may be swayed by the representativeness heuristic when judging similarity, in diagnoses, for example.
The researcher found that clinicians use the representativeness heuristic in making diagnoses by judging how similar patients are to the stereotypical or prototypical patient with that disorder.

=== Randomness ===
Irregularity and local representativeness affect judgments of randomness.
Things that do not appear to have any logical sequence are regarded as representative of randomness and thus more likely to occur.
For example, THTHTH as a series of coin tosses would not be considered representative of randomly generated coin tosses as it is too well ordered.

Local representativeness is an assumption wherein people rely on the law of small numbers, whereby small samples are perceived to represent their population to the same extent as large samples (Tversky & Kahneman 1971).

A small sample which appears randomly distributed would reinforce the belief, under the assumption of local representativeness, that the population is randomly distributed.
Conversely, a small sample with a skewed distribution would weaken this belief.

If a coin toss is repeated several times and the majority of the results consists of "heads", the assumption of local representativeness will cause the observer to believe the coin is biased toward "heads".

==
Tversky and Kahneman's classic studies ==


===
Tom W. ===
In a study done in 1973, Kahneman and Tversky divided their participants into three groups:

"Base-rate group", who were given the instructions: "Consider all the first-year graduate students in the U.S. today.
Please write down your best guesses about the percentage of students who are now enrolled in the following nine fields of specialization."

The nine fields given were business administration, computer science, engineering, humanities and education, law, library science, medicine, physical and life sciences, and social science and social work.

"Similarity group", who were given a personality sketch. "
Tom W. is of high intelligence, although lacking in true creativity.
He has a need for order and clarity, and for neat and tidy systems in which every detail finds its appropriate place.
His writing is rather dull and mechanical, occasionally enlivened by somewhat corny puns and by flashes of imagination of the sci-fi type.
He has a strong drive for competence.
He seems to feel little sympathy for other people and does not enjoy interacting with others.
Self-centered, he nonetheless has a deep moral sense."

The participants in this group were asked to rank the nine areas listed in part 1 in terms of how similar Tom W. is to the prototypical graduate student of each area.

"Prediction group", who were given the personality sketch described in 2, but were also given the information
"The preceding personality sketch of Tom W. was written during Tom's senior year in high school by a psychologist, on the basis of projective tests.
Tom W. is currently a graduate student.
Please rank the following nine fields of graduate specialization in order of the likelihood that Tom W. is now a graduate student in each of these fields.
"
The judgments of likelihood were much closer for the judgments of similarity than for the estimated base rates.
The findings supported the authors' predictions that people make predictions based on how representative something is (similar), rather than based on relative base rate information.

For example, more than 95% of the participants said that Tom would be more likely to study computer science than education or humanities, when there were much higher base rate estimates for education and humanities than computer science.

===
The taxicab problem ===
In another study done by Tversky and Kahneman, subjects were given the following problem:
A cab was involved in a hit and run accident at night.
Two cab companies, the Green and the Blue, operate in the city.
85% of the cabs in the city are Green and 15% are Blue.
A witness identified the cab as Blue.
The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colours 80% of the time and failed 20% of the time.
What is the probability that the cab involved in the accident was Blue rather than Green knowing that this witness identified it as Blue?

Most subjects gave probabilities over 50%, and some gave answers over 80%.
The correct answer, found using Bayes' theorem, is lower than these estimates :
There is a 12% chance (15% times 80%) of the witness correctly identifying a blue cab.

There is a 17% chance (85% times 20%) of the witness incorrectly identifying a green cab as blue.

There is therefore a 29% chance (12% plus 17%)
the witness will identify the cab as blue.

This results in a 41% chance (12% divided by 29%)
that the cab identified as blue is actually blue.

Representativeness is cited in the similar effect of the gambler's fallacy, the regression fallacy and the conjunction fallacy.

==
Biases attributed to the representativeness heuristic ==


===
Base rate neglect and base rate fallacy ===
The use of the representativeness heuristic will likely lead to violations of Bayes' Theorem.

Bayes' Theorem states :

  
    
      
        P
(
        H
        
          |
        
        D
        )
        =
P
(
D
              
                |
H
              )
P
(
              H
              )
P
(
              D
              )
            
          
        
        .
{\displaystyle P(H|D)={\frac {P(D|H)\,P(H)}{P(D)}}.}

However, judgments by representativeness only look at the resemblance between the hypothesis and the data, thus inverse probabilities are equated :

  
    
      
        P
(
        H
        
          |
        
        D
        )
        =
P
(
D
|
H
        )
      
    
    {\displaystyle P(H|D)=P(D|H)}
As can be seen, the base rate P(H) is ignored in this equation, leading to the base rate fallacy.
A base rate is a phenomenon’s basic rate of incidence.
The base rate fallacy describes how people do not take the base rate of an event into account when solving probability problems.
This was explicitly tested by Dawes, Mirels, Gold and Donahue (1993) who had people judge both the base rate of people who had a particular personality trait and the probability that a person who had a given personality trait had another one.

For example, participants were asked how many people out of 100 answered true to the question "I am a conscientious person" and also, given that a person answered true to this question, how many would answer true to a different personality question.
They found that participants equated inverse probabilities (e.g., 
  
    
      
        P
(
        c
        o
n
        s
c
i
e
        n
t
i
o
u
s
        
          |
        
        n
e
u
r
        o
t
i
c
        )
=
P
(
        n
        e
u
r
        o
t
i
        c
        
          |
c
        o
n
        s
c
i
e
        n
t
i
o
u
s
)
      
    
    {\displaystyle P(conscientious|neurotic)=P(neurotic|conscientious)}
)
even when it was obvious that they were not the same (the two questions were answered immediately after each other).
A medical example is described by Axelsson.
Say a doctor performs a test that is 99% accurate, and you test positive for the disease.
However, the incidence of the disease is 1/10,000.
Your actual chance of having the disease is 1%, because the population of healthy people is so much larger than the disease.

This statistic often surprises people, due to the base rate fallacy, as many people do not take the basic incidence into account when judging probability.

Research by Maya Bar-Hillel (1980) suggests that perceived relevancy of information is vital to base-rate neglect: base rates are only included in judgments if they seem equally relevant to the other information.
Some research has explored base rate neglect in children, as there was a lack of understanding about how these judgment heuristics develop.
The authors of one such study wanted to understand the development of the heuristic, if it differs between social judgments and other judgments, and whether children use base rates when they are not using the representativeness heuristic.

The authors found that the use of the representativeness heuristic as a strategy begins early on and is consistent.
The authors also found that children use idiosyncratic strategies to make social judgments initially, and use base rates more as they get older, but the use of the representativeness heuristic in the social arena also increase as they get older  .
The authors found that, among the children surveyed, base rates were more readily used in judgments about objects than in social judgments.
After that research was conducted, Davidson (1995) was interested in exploring how the representativeness heuristic and conjunction fallacy in children related to children’s stereotyping.
Consistent with previous research, children based their responses to problems off of base rates when the problems contained nonstereotypic information or when the children were older.
There was also evidence that children commit the conjunction fallacy.
Finally, as students get older, they used the representativeness heuristic on stereotyped problems, and so made judgments consistent with stereotypes.
There is evidence that even children use the representativeness heuristic, commit the conjunction fallacy, and disregard base rates.
Research suggests that use or neglect of base rates can be influenced by how the problem is presented, which reminds us that the representativeness heuristic is not a "general, all purpose heuristic", but may have many contributing factors.
Base rates may be neglected more often when the information presented is not causal.
Base rates are used less if there is relevant individuating information.
Groups have been found to neglect base rate more than individuals do.
Use of base rates differs based on context.
Research on use of base rates has been inconsistent, with some authors suggesting a new model is necessary.
===
Conjunction fallacy ===
A group of undergraduates were provided with a description of Linda, modelled to be representative of an active feminist.
Then participants were then asked to evaluate the probability of her being a feminist, the probability of her being a bank teller, or the probability of being both a bank teller and feminist.

Probability theory dictates that the probability of being both a bank teller and feminist
(the conjunction of two sets) must be less than or equal to the probability of being either a feminist or a bank teller. .

A conjunction cannot be more probable than one of its constituents.
However, participants judged the conjunction (bank teller and feminist) as being more probable than being a bank teller alone.
Some research suggests that the conjunction error may partially be due to subtle linguistic factors, such as inexplicit wording or semantic interpretation of "probability".
The authors argue that both logic and language use may relate to the error, and it should be more fully investigated.

===
Disjunction fallacy ===
From probability theory the disjunction of two events is at least as likely as either of the events individually.

For example, the probability of being either a physics or biology major is at least as likely as being a physics major, if not more likely.
However, when a personality description (data) seems to be very representative of a physics major (e.g., pocket protector) over a biology major, people judge that it is more likely for this person to be a physics major than a natural sciences major (which is a superset of physics).
Evidence that the representativeness heuristic may cause the disjunction fallacy comes from Bar-Hillel and Neter (1993).
They found that people judge a person who is highly representative of being a statistics major (e.g., highly intelligent, does math competitions) as being more likely to be a statistics major than a social sciences major (superset of statistics), but they do not think that he is more likely to be a Hebrew language major than a humanities major (superset of Hebrew language).

Thus, only when the person seems highly representative of a category is that category judged as more probable than its superordinate category.
These incorrect appraisals remained even in the face of losing real money in bets on probabilities.

===
Insensitivity to sample size ===
Representativeness heuristic is also employed when subjects estimate the probability of a specific parameter of a sample.
If the parameter highly represents the population, the parameter is often given a high probability.
This estimation process usually ignores the impact of the sample size.

A concept proposed by Tversky and Kahneman provides an example of this bias; The example is of two hospitals of differing size.
Approximately 45 babies are born in the large hospital while 15 babies are born in the small hospital.
Half (50%) of all babies born in general are boys.
However, the percentage changes from 1 day to another.
For a 1-year period, each hospital recorded the days on which >60% of the babies born were boys.
The question posed is: Which hospital do you think recorded more such days?

The larger hospital (21)
The smaller hospital (21)
About the same (that is, within 5% of each other)
(53)The values shown in parentheses are the number of students choosing each answer.
The results show that more than half the respondents selected the wrong answer (third option).
This is due to the respondents ignoring the effect of sample size.
The respondents selected the third option most likely because the same statistic represents both the large and small hospitals.

According to statistical theory, a small sample size allows the statistical parameter to deviate considerably compared to a large sample.

Therefore, the large hospital would have a higher probability to stay close to the nominal value of 50%.

See more about this bias in the article below.
===
Misconceptions of chance and gambler's fallacy ===
===
Regression fallacy ===


==
See also ==
Affect heuristic
Attribute
substitution
Availability heuristic
List of biases in judgment and decision-making
Extension neglect


==
References ==


===
Works by Kahneman and Tversky ===
Tversky, Amos; Kahneman, Daniel (1971).
"
Belief in the law of small numbers".
Psychological Bulletin.
76 (2): 105–110.
CiteSeerX 10.1.1.592.3838.
doi:10.1037/h0031322.

Kahneman, Daniel; Tversky, Amos (1972).
"
Subjective probability: A judgment of representativeness" (PDF).
Cognitive Psychology.
3 (3)
: 430–454.
doi:10.1016/0010-0285(72)90016-3.

Kahneman, Daniel; Tversky, Amos (1973).
"
On the psychology of prediction".
Psychological Review.
80 (4): 237–251.
doi:10.1037/h0034747.

Tversky, Amos; Kahneman, Daniel (1974).
"
Judgment under Uncertainty: Heuristics and Biases" (PDF).
Science.
185 (4157):
1124–1131.
doi:10.1126/science.185.4157.1124.
PMID 17835457.
S2CID 143452957.

Tversky, Amos; Kahneman, Daniel (1982). "
Evidential Impact of Base Rates".

In Kahneman, Daniel; Slovic, Paul; Tversky, Amos (eds.).
Judgment Under Uncertainty: Heuristics and Biases.
Science.
185.
Cambridge University Press.
pp.
1124–31.
doi:10.1126
/science.185.4157.1124.
ISBN 978-0-521-28414-1.
PMID 17835457.
S2CID 143452957.
CS1 maint: discouraged parameter (link)
Tversky, Amos; Kahneman, Daniel (1983).
"
Extensional versus intuitive reasoning: The conjunction fallacy in probability judgment".
Psychological Review.
90 (4)
: 293–315.
doi:10.1037/0033-295X.90.4.293.

===
General references ===
Baron, Jonathan (2000).
Thinking and Deciding (3rd ed.).
Cambridge University Press.
ISBN 978-0-521-65972-7.

Plous, Scott (1993).
The Psychology of Judgment and Decision Making.
McGraw-Hill Education.
ISBN 978-0-07-050477-6.
==
External links ==
Powerpoint presentation on the representativeness heuristic (with further links to presentations of classical experiments)
Social heuristics are simple decision making strategies that guide behavior and decisions in the social environment when time, information, or cognitive resources are scarce.
Social environments tend to be characterised by complexity and uncertainty, and agents may use heuristics to simplify the decision making process through ignoring some information or relying on simple rules of thumb to make decisions.
The class of phenomena described by social heuristics overlap with those typically investigated by social psychology and game theory.
At the intersection of these fields, social heuristics have been applied to explain cooperation in economic games used in experimental research, based on the argument that cooperation is typically advantageous in daily life, and therefore people develop a cooperation heuristic that gets applied even to one-shot anonymous interactions (the so-called "social heuristics hypothesis" of human cooperation).

==
Overview ==
=== Bounded Rationality ===
In the decision-making process, optimisation is almost always intractable in any implementation, whether machine or neural..
Because of this, defined parameters or boundaries must be implemented in the process in order to achieve an acceptable outcome.
This method is known as applying bounded rationality, where an individual makes a collective and rational choice that considers “the limits of human capability to calculate, the severe deficiencies of human knowledge about the consequences of choice, and the limits of human ability to adjudicate among multiple goals”.
They are essentially incorporating a series of criteria, referred to as alternatives for choice.
These alternatives are often not initially given to the decision maker, so a theory of search is also incorporated.

=== Heuristics ===
Heuristics are a common alternative, which can be defined as simple strategies for decision making where the actor only pays attention to key pieces of information, allowing the decision to be made quickly and with less cognitive effort.

Daniel Kahneman and Shane Frederick have advanced the view that heuristics are decision making processes that employ attribute substitution, where the decision maker substitutes the "target attribute" of the thing he is trying to judge with a "heuristic attribute" that more easily comes to mind.
Shah and Daniel M. Oppenheimer have framed heuristics in terms of effort reduction, where the decision maker makes use of techniques that make decisions less effortful, such as only paying attention to some cues or only considering a subset of the available alternatives.
Another view of heuristics comes from Gerd Gigerenzer and colleagues, who conceptualize heuristics as "fast and frugal" techniques for decision making that simplify complex calculations and make up part of the "adaptive toolbox" of human capacities for reasoning and inference.
Under this framework, heuristics are ecologically rational, meaning a heuristic may be successful if the way it works matches the demands of the environment it is being used in.
Researchers in this vein also argue that heuristics may be just as or even more accurate when compared to more complex strategies such as multiple regression.

===
Social heuristics ===
Social heuristics can include heuristics that use social information, operate in social contexts, or both.
Examples of social information include information about the behavior of a social entity or the properties of a social system, while nonsocial information is information about something physical.
Contexts in which an organism may use social heuristics can include "games against nature" and "social games".
In games against nature, the organism strives to predict natural occurrences (such as the weather) or competes against other natural forces to accomplish something.
In social games, the organism is making decisions in a situation that involves other social beings.
Importantly, in social games, the most adaptive course of action also depends on the decisions and behavior of the other actors.
For instance, the follow-the-majority heuristic uses social information as inputs but is not necessarily applied in a social context, while the equity-heuristic uses non-social information but can be applied in a social context such as the allocation of parental resources amongst offspring.
Within social psychology, some researchers have viewed heuristics as closely linked to cognitive biases.
Others have argued that these biases result from the application of social heuristics depending on the structure of the environment that they operate in.
Researchers in the latter approach treat the study of social heuristics as closely linked to social rationality, a field of research that applies the ideas of bounded rationality and heuristics to the realm of social environments.
Under this view, social heuristics are seen as ecologically rational.
In the context of evolution, research utilizing evolutionary simulation models has found support for the evolution of social heuristics and cooperation when the outcomes of social interactions are uncertain.

== Examples ==
Examples of social heuristics include:
Imitate-the-majority heuristic, also referred to follow-the-majority heuristic.
An agent using the heuristic would imitate the behavior of the majority of agents in his reference group.
For instance, in deciding which restaurant to choose, people tend to choose the one with the longer waiting queue.

Imitate-the-successful heuristic, also referred to follow-the-best heuristic.
An agent using the heuristic would imitate the behavior of the most successful person in her reference group.

Equity heuristic, also referred to 1/N heuristic.
Using the heuristic means equally distributing resources among the available options.
The heuristic was found to be successful in the stock market and also been found to describe parental resource allocation decisions: parents typically allocate their time and effort equally amongst their children.

Social-circle heuristic.
The heuristic is used to infer which of two alternatives has the higher value.
An agent using the heuristic would search through her social circles in order of their proximity to the self (self, family, friends, and acquaintances), stopping the search as soon as the number of instances of one alternative within a circle exceeds that of the other, choosing the alternative with the higher tally.
For example, a person might decide which of two sports is more popular by thinking through how many members of each circle play each sport.

Tit-for-Tat heuristic.
In deciding whether to cooperate or defect, an agent using the heuristic would cooperate in the first round and in subsequent rounds, reciprocate his partner's action of cooperation or defection in the previous round.
The heuristic is typically investigated using a prisoner's dilemma in game theory, where there is substantial evidence that people use such a heuristic, leading to intuitive reciprocation.

Regret matching heuristic.
An agent using this heuristic will persist with a course of action in a cooperative game as long as she is not experiencing regret.
Once she experiences regret, this heuristic predicts a probability that the actor will switch her behavior that is proportional to the amount of regret she feels about missing out on a past payout.

Group recognition heuristic, which extends principles related to the recognition heuristic into a group decision making setting.
In individual decision making, the recognition heuristic is used when an individual asked which of two options has a higher value on a given criterion judges that the option he recognizes has a higher value than the option he does not recognize.
This is applied in group decision making settings when a group's choice of which of two options has a higher value is influenced by use of the recognition heuristic by some members of the group.

Majority heuristic (rule).
This is a decision rule used in group decision making by both humans and animals, where each member of the group votes for an alternative and a decision is reached based on the option with the most votes.
Researchers investigating majority rule (where the option with more than half of the votes is chosen) and plurality rule (where the option with the most votes in chosen) strategies for group decisions found such strategies to be both high-performing and computationally efficient for situations where there is a correct answer.

Base-Rate heuristic.
The process that involves using common mental shortcuts that help a decision to be made based on known probabilities.
For example, if an animal is heard howling in a large city, it is usually assumed to be a dog because the probability that a wolf is in a large city is very low.

Peak-and-end heuristic.
When past experiences are practically exclusively judged on how the agent was affected at the peak (both unpleasant and pleasant) and the end of event, creating a natural bias in the decision-making process as the whole experience is not analysed.

Familiarity heuristic.
The agent’s approach to solve a social decision in which they have experienced a similar event before involves them reflecting on comparable past situations, and often acting the same way they acted in the past.

==
Relation to other concepts ==


===
Dual-process approach ===
A dual-process approach to human cognition specifies two types of thought processes: one that is fast and happens unconsciously or automatically, and another that is slower and involves more conscious deliberation.
In the dominant dual-systems approach in social psychology, heuristics are believed to be automatically and unconsciously applied.
The study of social heuristics as a tool of bounded rationality asserts that heuristics may be used consciously or unconsciously.

===
Social heuristics hypothesis ===
The social heuristics hypothesis is a theory put forth by Rand and colleagues that explains the link between intuition and cooperation.
Under this theory, cooperating in everyday social situations tends to be successful, and as a result, cooperation is an internalized heuristic that is applied in unfamiliar social contexts, even those in which such behavior may not lead to the most personally advantageous result for the actor (such as a lab experiment).

Methods used by researchers to study cooperative behavior in the laboratory include economic games such as:
Prisoner's dilemma game: two players each decide whether to cooperate or defect; a player who defects when the other cooperates maximizes his payout, if both cooperate the payout is higher than if both defect.

Public goods game :
multiple players each choose how much money to put towards a public project; the amount in the public pot is increased by a given factor and distributed equally to those who contributed.

Trust game: one player transfers money to another player and the money is increased by a given factor; the other then decides whether and how much to transfer back.

Ultimatum game
: one player makes an offer for how to split a resource with the other player; the other player can accept the offer
(so that both players get the amount proposed by the split) or reject the offer (so that neither player gets anything).These economic games
all share the condition that, when played in a single round, an individual's payout is maximized if he acts selfishly and chooses not to cooperate.
However, over the course of repeated rounds, cooperation can be payout maximizing and thus be a self-interested strategy.
Following a dual-process framework, the social heuristics hypothesis contends that cooperation, which is automatic and intuitive, may be overridden by reflection.
The theory is supported by evidence from laboratory and online experiments suggesting that time pressure increases cooperation, though some evidence suggests this may be only among individuals who are not as familiar with the types of economic games typically used in this field of research.
Meta-analytic evidence based on 67 studies that looked at cooperation in the types of economic games described above suggests that cognitive-processing manipulations that encourage intuitive decision-making (such as time pressure or increased cognitive load) increase pure cooperation, where a one-shot action has no future consequences for the actor to consider and not cooperating is the most advantageous option.
However, such manipulations do not have an effect on strategic cooperation in situations in which cooperation may be the pay-off maximizing option because of a possibility of future interactions where the actor may be rewarded for cooperation.

== See also ==
Heuristics in judgment and decision-making
Bounded rationality


=
= References ==
A heuristic technique, or a heuristic (; Ancient Greek: εὑρίσκω, heurískō, 'I find, discover')
, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation.
Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.
Heuristics can be mental shortcuts that ease the cognitive load of making a decision.
Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.

==
Overview ==
Heuristics are the strategies derived from previous experiences with similar problems.
These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues.
When an individual applies heuristics in practice, generally performs as expected however
it can alternatively it could create systematic errors.

The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems.
In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification.
Here are a few commonly used heuristics from George Pólya's 1945 book, How to Solve It:
If you are having difficulty understanding a problem, try drawing a picture.

If you can't find a solution, try assuming that you have a solution and seeing what you can derive from that ("working backward").

If the problem is abstract, try examining a concrete example.

Try solving a more general problem first (the "inventor's paradox": the more ambitious plan may have more chances of success).

In psychology, heuristics are simple, efficient rules, learned or inculcated by evolutionary processes, that have been proposed to explain how people make decisions, come to judgments, and solve problems typically when facing complex problems or incomplete information.
Researchers test if people use those rules with various methods.
These rules work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.

==
History ==
The study of heuristics in human decision-making was developed in the 1970s and the 1980s by the psychologists Amos Tversky and Daniel Kahneman although the concept had been originally introduced by the Nobel laureate Herbert A. Simon, whose original, primary object of research was problem solving that showed that we operate within what he calls bounded rationality.
He coined the term satisficing, which denotes a situation in which people seek solutions, or accept choices or judgments, that are "good enough" for their purposes although they could be optimized.
Rudolf Groner analyzed the history of heuristics from its roots in ancient Greece up to contemporary work in cognitive psychology and artificial intelligence, proposing a cognitive style "heuristic versus algorithmic thinking," which can be assessed by means of a validated questionnaire.

===
Adaptive toolbox ===
Gerd Gigerenzer and his research group argued that models of heuristics need to be formal to allow for predictions of behavior that can be tested.
They study the fast and frugal heuristics in the "adaptive toolbox" of individuals or institutions, and the ecological rationality of these heuristics; that is, the conditions under which a given heuristic is likely to be successful.
The descriptive study of the "adaptive toolbox" is done by observation and experiment, the prescriptive study of the ecological rationality requires mathematical analysis and computer simulation.
Heuristics – such as the recognition heuristic, the take-the-best heuristic, and fast-and-frugal trees – have been shown to be effective in predictions, particularly in situations of uncertainty.
It is often said that heuristics trade accuracy for effort
but this is only the case in situations of risk.
Risk refers to situations where all possible actions, their outcomes and probabilities are known.
In the absence of this information, that is under uncertainty, heuristics can achieve higher accuracy with lower effort.
This finding, known as a less-is-more effect, would not have been found without formal models.
The valuable insight of this program is that heuristics are effective not despite of their simplicity — but because of it.
Furthermore, Gigerenzer and Wolfgang Gaissmaier found that both individuals and organizations rely on heuristics in an adaptive way.

=== Cognitive-experiential self-theory ===
Heuristics, through greater refinement and research, have begun to be applied to other theories, or be explained by them.
For example, the cognitive-experiential self-theory (CEST) also is an adaptive view of heuristic processing.
CEST breaks down two systems that process information.
At some times, roughly speaking, individuals consider issues rationally, systematically, logically, deliberately, effortfully, and verbally.
On other occasions, individuals consider issues intuitively, effortlessly, globally, and emotionally.
From this perspective, heuristics are part of a larger experiential processing system that is often adaptive, but vulnerable to error in situations that require logical analysis.

===
Attribute substitution ===
In 2002, Daniel Kahneman and Shane Frederick proposed that cognitive heuristics work by a process called attribute substitution, which happens without conscious awareness.
According to this theory, when somebody makes a judgment (of a "target attribute") that is computationally complex, a more easily calculated "heuristic attribute" is substituted.
In effect, a cognitively difficult problem is dealt with by answering a rather simpler problem, without being aware of this happening.
This theory explains cases where judgments fail to show regression toward the mean.
Heuristics can be considered to reduce the complexity of clinical judgments in health care.

==
Psychology ==


===
Informal models of heuristics ===
Affect heuristic —
Mental shortcut which uses emotion to influence the decision.
Emotion is the effect that plays the lead role that makes the decision or solves the problem quickly or efficiently.
Is used while judging the risks and benefits of something, depending on the positive or negative feelings that people associate with a stimulus.
Can also be considered the gut decision since if the gut feeling is right, then the benefits are high and the risks are low.
Anchoring and adjustment — Describes the common human tendency to rely more heavily on the first piece of information offered (the "anchor") when making decisions.

For example, in a study done with children, the children were told to estimate the number of jellybeans in a jar.

Groups of children were given either a high or low "base" number (anchor).

Children estimated the number of jellybeans to be closer to the anchor number that they were given.

Availability heuristic — A mental shortcut that occurs when people make judgments about the probability of events by the ease with which examples come to mind.

For example, in a 1973 Tversky & Kahneman experiment, the majority of participants reported that there were more words in the English language that start with the letter K than for which K was the third letter.

There are actually twice as many words in the English Language that have K as the third letter as those that start with K, but words that start with K are much easier to recall and bring to mind.

Balance Heuristic —
Applies to when an individual balances the negative and positive effects from a decision which makes the choice obvious.

Base Rate Heuristic —
When a decision involves probability this is a mental shortcut that uses relevant data to determine the probability of an outcome occurring.
When using this Heuristic there is a common issue where individuals misjudge the likelihood of a situation.
For example, if there is a test for a disease which has an accuracy of 90%, people may think it’s a 90%
they have the disease even though the disease only affects 1 in 500 people.

Common Sense Heuristic --- Used frequently by individuals when the potential outcomes of a decision appear obvious.
For example, when your television remote goes flat, you would change the batteries.
Contagion heuristic — follows the Law of Contagion or Similarity.
This leads people to avoid others that are viewed as “contaminated” to the observer.
This happens due to the fact of the observer viewing something that is seen as bad or to seek objects that have been associated with what seems good.
Somethings one can view as harmful can tend not to really be.
This sometimes leads to irrational thinking on behalf of the observer.

Default Heuristic —
In real world models it is common for consumers to apply this heuristic when selecting the default option regardless of whether the option was their preference.

Educated Guess Heuristic —
When an individual responds to a decision using relevant information they have stored relating to the problem.

Effort heuristic —
the worth of an object is determined by the amount of effort put into the production of the object.
Objects that took longer to produce are more valuable while the objects that took less time are deemed not as valuable.
Also applies to how much effort is put into achieving the product.
This can be seen as the difference of working and earning the object versus finding the object on the side of the street.
It can be the same object but the one found will not be deemed as valuable as the one that we earned.

Escalation of commitment — Describes the phenomenon where people justify increased investment in a decision, based on the cumulative prior investment, despite new evidence suggesting that the cost, starting today, of continuing the decision outweighs the expected benefit.
This is related to the sunk cost fallacy.

Fairness Heuristic —
Applies to the reaction of an individual to a decision from an authoritative figure.
If the decision is enacted in a fair manner the likelihood of the individual to comply voluntarily is higher than if it is unfair.

Familiarity heuristic —
A mental shortcut applied to various situations in which individuals assume that the circumstances underlying the past behavior still hold true for the present situation and that the past behavior thus can be correctly applied to the new situation.
Especially prevalent when the individual experiences a high cognitive load.

Naïve diversification —
When asked to make several choices at once, people tend to diversify more than when making the same type of decision sequentially.

Peak–end rule — experience of an event is judged by the feelings of the peak of the event and nothing more.
Usually not every event is seen as complete but what was felt at the climax whether the event was pleasant or unpleasant to the observer.

All other feelings is not lost but is not used.
This can also include how long the event happened.

Representativeness heuristic —
A mental shortcut used when making judgments about the probability of an event under uncertainty.
Or, judging a situation based on how similar the prospects are to the prototypes the person holds in his or her mind.
For example, in a 1982 Tversky and Kahneman experiment, participants were given a description of a woman named Linda.
Based on the description, it was likely that Linda was a feminist.

Eighty to ninety percent of participants, choosing from two options, chose that it was more likely for Linda to be a feminist and a bank teller than only a bank teller.

The likelihood of two events cannot be greater than that of either of the two events individually.
For this reason, the representativeness heuristic is exemplary of the conjunction fallacy.

Scarcity heuristic — works as the same as the economy.
The scarcer an object or event is, the more value that thing holds.
The abundance is the indicator of the value and is a mental shortcut that places a value on an item based on how easily it might be lost, especially to competitors.
The scarcity heuristic stems from the idea that the more difficult it is to acquire an item the more value that item has.
In many situations we use an item’s availability, its perceived abundance, to quickly estimate quality and/or utility.
This can lead to systemic errors or cognitive bias.

Simulation heuristic — simplified mental strategy in which people determine the likelihood of an event happening based on how easy it is to mentally picture the event happening.
People regret the events that are easier to image over the ones that would be harder to.
It is also thought that people will use this heuristic to predict the likelihood of another's behavior happening.
This shows that people are constantly simulating everything around them in order to be able to predict the likelihood of events around them.
It is believe that people do this by mentally undoing events that they have experienced and then running mental simulations of the events with the corresponding input values of the altered model.

Social proof — also known as the informational social influence which was given its name by Robert Cialdini in his book called Influence written in 1984.
It is where people copy the actions of others in order to attempt to undertake the behavior in a given situation.
It is more prominent in situations were people are unable to determine the appropriate mode of behavior and are driven to the assumption that the surrounding people have more knowledge about the current situation.
This can be see more dominantly in ambiguous social situations.

Working Backward Heuristic — When an individual assumes, they have already solved a problem they work backwards in order to find how to achieve the solution they originally figured out.

===
Formal models of heuristics ===
Elimination by Aspects heuristic
Fast-and-frugal trees
Fluency heuristic
Gaze heuristic
Recognition heuristic
Satisficing
Similarity heuristic
Take-the-best
heuristic


===
Cognitive maps ===
Heuristics were also found to be used in the manipulation and creation of cognitive maps.
Cognitive maps are internal representations of our physical environment, particularly associated with spatial relationships.
These internal representations are used by our memory as a guide in our external environment.
It was found that when questioned about maps imaging, distancing, etc.,
people commonly made distortions to images.
These distortions took shape in the regularization of images (i.e., images are represented as more like pure abstract geometric images, though they are irregular in shape).

There are several ways that humans form and use cognitive maps, with visual intake being an especially key part of mapping: the first is by using landmarks, whereby a person uses a mental image to estimate a relationship, usually distance, between two objects.
The second is route-road knowledge, and is generally developed after a person has performed a task and is relaying the information of that task to another person.
The third is a survey, whereby a person estimates a distance based on a mental image that, to them, might appear like an actual map.
This image is generally created when a person's brain begins making image corrections.
These are presented in five ways:
Right-angle bias:
when a person straightens out an image, like mapping an intersection, and begins to give everything 90-degree angles, when in reality it may not be that way.

Symmetry heuristic: when people tend to think of shapes, or buildings, as being more symmetrical than they really are.

Rotation heuristic: when a person takes a naturally (realistically) distorted image and straightens it out for their mental image.

Alignment heuristic:
similar to the previous, where people align objects mentally to make them straighter than they really are.

Relative-position heuristic: people do not accurately distance landmarks in their mental image based on how well they remember that particular item.
Another method of creating cognitive maps is by means of auditory intake based on verbal descriptions.
Using the mapping based from a person's visual intake, another person can create a mental image, such as directions to a certain location.

==
Philosophy ==
A heuristic device is used when an entity X exists to enable understanding of, or knowledge concerning, some other entity Y.
A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models.
Stories, metaphors, etc.,
can also be termed heuristic in this sense.
A classic example is the notion of utopia as described in Plato's best-known work, The Republic.
This means that the "ideal city" as depicted in The Republic is not given as something to be pursued, or to present an orientation-point for development.
Rather, it shows how things would have to be connected, and how one thing would lead to another (often with highly problematic results), if one opted for certain principles and carried them through rigorously.

Heuristic is also often used as a noun to describe a rule-of-thumb, procedure, or method.
Philosophers of science have emphasized the importance of heuristics in creative thought and the construction of scientific theories.
(
See The Logic of Scientific Discovery by Karl Popper; and philosophers such as Imre Lakatos, Lindley Darden, William C. Wimsatt, and others.)
== Law ==
In legal theory, especially in the theory of law and economics, heuristics are used in the law when case-by-case analysis would be impractical, insofar as "practicality" is defined by the interests of a governing body.
The present securities regulation regime largely assumes that all investors act as perfectly rational persons.

In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.
For instance, in all states in the United States the legal drinking age for unsupervised persons is 21 years, because it is argued that people need to be mature enough to make decisions involving the risks of alcohol consumption.
However, assuming people mature at different rates, the specific age of 21 would be too late for some and too early for others.
In this case, the somewhat arbitrary deadline is used because it is impossible or impractical to tell whether an individual is sufficiently mature for society to trust them with that kind of responsibility.
Some proposed changes, however, have included the completion of an alcohol education course rather than the attainment of 21 years of age as the criterion for legal alcohol possession.
This would put youth alcohol policy more on a case-by-case basis and less on a heuristic one, since the completion of such a course would presumably be voluntary and not uniform across the population.

The same reasoning applies to patent law.
Patents are justified on the grounds that inventors must be protected so they have incentive to invent.
It is therefore argued that it is in society's best interest that inventors receive a temporary government-granted monopoly on their idea, so that they can recoup investment costs and make economic profit for a limited period.
In the United States, the length of this temporary monopoly is 20 years from the date the patent application was filed, though the monopoly does not actually begin until the application has matured into a patent.
However, like the drinking-age problem above, the specific length of time would need to be different for every product to be efficient.
A 20-year term is used because it is difficult to tell what the number should be for any individual patent.
More recently, some, including University of North Dakota law professor Eric E. Johnson, have argued that patents in different kinds of industries – such as software patents – should be protected for different lengths of time.

==
Stereotyping ==
Stereotyping is a type of heuristic that people use to form opinions or make judgments about things they have never seen or experienced.
They work as a mental shortcut to assess everything from the social status of a person (based on their actions), to whether a plant is a tree based on the assumption that it is tall, has a trunk, and has leaves (even though the person making the evaluation might never have seen that particular type of tree before).

Stereotypes, as first described by journalist Walter Lippmann in his book Public Opinion (1922), are the pictures we have in our heads that are built around experiences as well as what we are told about the world.

==
Artificial intelligence ==
A heuristic can be used in artificial intelligence systems while searching a solution space.
The heuristic is derived by using some function that is put into the system by the designer, or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.

==
Critiques and controversies ==
The concept of heuristics has critiques and controversies.
The popular "We Cannot Be That Dumb" critique argues that people would be doomed if it weren't for their ability to make sound and effective judgments.

== See also ==
Algorithm
Behavioral economics
Erudition
Failure mode and effects analysis
Heuristics in judgment and decision-making
List of biases in judgment and decision making
Neuroheuristics
Priority heuristic
Social heuristics


==
References ==


==
Further reading ==
How To Solve It:
Modern Heuristics, Zbigniew Michalewicz and David B. Fogel, Springer Verlag, 2000.
ISBN 3-540-66061-5
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence:
A Modern Approach
(2nd ed.),
Upper Saddle River, New Jersey:
Prentice Hall, ISBN 0-13-790395-2
The Problem of Thinking Too Much, 2002-12-11, Persi Diaconis
In behavioural sciences, social rationality is a type of decision strategy used in social contexts, in which a set of simple rules is applied in complex and uncertain situations.

==
Definition ==
Social rationality is a form of bounded rationality applied to social contexts, where individuals make choices and predictions under uncertainty.
While game theory deals with well-defined situations, social rationality explicitly deals with situations in which not all alternatives, consequences, and  event probabilities can be foreseen.
The idea is that, similar to non-social environments, individuals rely, and should rely, on fast and frugal heuristics in order to deal with complex and  genuinely uncertain social environments.
This emphasis on simple rules in an uncertain world contrasts with the view that the complexity of social situations requires highly sophisticated mental strategies, as has been assumed in primate research and neuroscience, among others.

==
A descriptive and normative program ==
Social rationality is both a descriptive program and a normative program.
The descriptive program studies the repertoire of heuristics an individual or organization uses, that is, their adaptive toolbox.
The normative program studies the environmental conditions to which a heuristic is adapted
, that is, where it performs better than other decision strategies.
This approach is called the study of the ecological rationality of social heuristics.
It assumes that social heuristics are domain- and problem-specific.

==
Applications ==
Heuristics can be applied to social and non-social decision tasks (also called social games and games against nature), judgments, or categorizations.
They can use social or non-social input.
Social rationality is thus about three of the four possible combinations, excluding the case of heuristics using non-social input for non-social tasks. '
Games against nature' comprise situations where individuals face environmental uncertainty, and need to predict or outwit nature, e.g., harvest food or master hard-to-predict or unpredictable hazards. '
Social games' include situations, where the decision outcome depends on the choices of others, e.g., in cooperation, competition, mate search and even in morally significant situations.
Social rationality has been studied in a number of other fields than human decision-making, e.g. in evolutionary social learning, and social learning in animals.

===
Examples ===


====
Imitate-the-majority heuristic ====
An example for a heuristic that is not necessarily social but that requires social input is the imitate-the-majority heuristic, where in a situation of uncertainty, individuals follow the actions or choices of the majority of their peers regardless of their social status.
The domain of pro-environmental behavior provides numerous illustrations for this strategy, such as littering behavior in public places, the reuse of towels in hotel rooms, and changes in private energy consumption in response to information about the consumption of the majority of neighbors.

====
1/N (Equality heuristic) ====
Following the equality heuristic (sometimes called 1/N rule)
people divide and invest their resources equally in a number of N different options.
These options can be both social (e.g., time spent with children) and nonsocial entities (e.g., financial investments or natural resources).
For example, many parents invest their limited resources, such as affection, time, and money (e.g., for education) equally into their offspring.
In highly uncertain environments with large numbers of assets and only few possibilities to learn, the equality heuristic can outperform optimizing strategies and yield better performance on various measures of success than optimal asset allocation strategies.

==
Social heuristics ==
Adapted from Hertwig & Herzog, 2009.

Imitate-the-majority
heuristic
Social circle heuristic
Averaging heuristic
Tit-for-tat
Generous tit-for-tat (or tit-for-two-tat)
Status tree
Regret matching heuristic
Mirror
heuristic
1/N (Equality heuristic)
Group recognition heuristic
White coat heuristic/ Trust your doctor heuristic
Imitate-the-successful heuristic
Plurality
vote-based lexicographic heuristic


== See also ==
Social heuristics
Ecological rationality
Optimization
Risk
Uncertainty
Max Planck Institute for Human Development


==
Notes ==


==
References ==
Cialdini, R. B., Reno, R. R., & Kallgren, C. A. (1990).
A focus theory of normative conduct: Recycling the concept of norms to reduce littering in public places.
Journal of Personality and Social Psychology, 58(6), 1015–1026.

DeMiguel, V., Garlappi, L., & Uppal, R. (2009).
Optimal versus naive diversification: How inefficient ist the 1/N portfolio strategy?
The Review of Financial Studies, 22(5), 1915-1953.

Gigerenzer, G. (2010).
Moral satisficing: Rethinking moral behavior as bounded rationality.
Topics in Cognitive Science, 2(3), 528–554.
doi:10.1111/j.1756-8765.2010.01094.x
Gigerenzer, G., Todd, P., & the ABC Research Group (1999).
Simple heuristics that make us smart.
New York:
Oxford University Press.

Hertwig, R., & Herzog, S. M. (2009).
Fast and frugal heuristics: tools of social rationality.
Social Cognition, 27(5), 661–698.
Retrieved from http://guilfordjournals.com/doi/abs/10.1521/soco.2009.27.5.661
Hertwig, R. Hoffrage, U. & the ABC Research Group (2012).
Simple heuristics in a social world.
New York:
Oxford University Press.

Hertwig, R. & Hoffrage, U. (2012). "
Simple heuristics:
The foundations of adaptive social behavior".

In R. Hertwig, U. Hoffrage, & the ABC Research Group (ed.).
Simple heuristics in a social world (PDF).
New York:
Oxford University Press.
pp.
3–33.
doi:10.1093
/acprof:oso/9780195388435.001.0001.CS1
maint:
multiple names: authors list (link)
Morgan, T. J. H.; Rendell, L. E.; Ehn, M.; Hoppitt, W.; Laland, K. N. (2011-07-27).
"
The evolutionary basis of human social learning".
Proceedings of the Royal Society B: Biological Sciences.
The Royal Society.
279 (1729)
: 653–662.
doi:10.1098/rspb.2011.1172.
ISSN 0962-8452.
PMC 3248730.
PMID 21795267.

Rieucau, G., & Giraldeau, L.-A. (2011).
Exploring the costs and benefits of social information use: An appraisal of current experimental evidence.
Philosophical Transactions of the Royal Society B, 366(1567), 949–957.
doi:10.1098/
rstb.2010.0325
Seymour, B., & Dolan, R. (2008).
Emotion, decision making, and the amygdala.
Neuron, 58, 662–671.

Schultz, P. W., Nolan, J. M., Cialdini, R. B., Goldstein, N. J., & Griskevicius, V. (2007).
The constructive, destructive, and reconstructive power of social norms.
Psychological Science, 18(5), 429–434.

Simon, Herbert A. (1956).
Rational choice and the structure of the environment.
Psychological Review, 63(2), 129–138.
Heuristics are simple strategies or mental processes that humans, animals, organizations and machines use to quickly form judgments, make decisions, and find solutions to complex problems.
This happens when an individual focuses on the most relevant aspects of a problem or situation to formulate a solution.
Some heuristics are more applicable and useful than others depending on the situation.
Heuristic processes are used to find the answers and solutions that are most likely to work or be correct.
However, heuristics are not always right or the most accurate.
While they can differ from answers given by logic and probability, judgments and decisions based on a heuristic can be good enough to satisfy a need.
They are meant to serve as quick mental references for everyday experiences and decisions.
In situations of uncertainty, where information is incomplete, heuristics allow for the less-is-more effect, in which less information leads to greater accuracy.

==
History ==
Herbert A. Simon formulated one of the first models of heuristics, known as satisficing.
His more general research program posed the question of how humans make decisions when the conditions for rational choice theory are not met, that is how people decide under uncertainty.
Simon is also known as the father of bounded rationality, which he understood as the study of the match (or mismatch) between heuristics and decision environments.
This program was later extended into the study of ecological rationality.

In the early 1970s, psychologists Amos Tversky and Daniel Kahneman took a different approach, linking heuristics to cognitive biases.
Their typical experimental setup consisted of a rule of logic or probability, embedded in a verbal description of a judgement problem, and demonstrated that people's intuitive judgement deviated from the rule.
The "Linda problem" below gives an example.
The deviation is then explained by a heuristic.
This research, called the heuristics-and-biases program, challenged the idea that human beings are rational actors and first gained worldwide attention in 1974 with the Science paper "Judgment Under Uncertainty:
Heuristics and Biases"  and although the originally proposed heuristics have been refined over time, this research program has changed the field by permanently setting the research questions.
The original ideas by Herbert Simon were taken up in the 1990s by Gerd Gigerenzer and others.
According to their perspective, the study of heuristics requires formal models that allow predictions of behavior to be made ex ante.
Their program has three aspects:
What are the heuristics humans use?
(
the descriptive study of the "adaptive toolbox")
Under what conditions should humans rely on a given heuristic?
(
the prescriptive study of ecological rationality)
How to design heuristic decision aids that are easy to understand and execute?
(
the engineering study of intuitive design)Among others, this program has shown that heuristics can lead to fast, frugal, and accurate decisions in many real-world situations that are characterized by uncertainty.
These two different research programs have led to two kinds of models of heuristics, formal models and informal ones.
Formal models describe the decision process in terms of an algorithm, which allows for mathematical proofs and computer simulations.
In contrast, informal models are verbal descriptions.

==
Formal models of heuristics ==


===
Simon's satisficing strategy ===
Herbert Simon's satisficing heuristic can be used to choose one alternative from a set of alternatives in situations of uncertainty.
Here, uncertainty means that the total set of alternatives and their consequences is not known or knowable.
For instance, professional real-estate entrepreneurs rely on satisficing to decide in which location to invest to develop new commercial areas: "If I believe I can get at least x return within y years, then I take the option.
"
In general, satisficing is defined as:

Step 1:
Set an aspiration level α
Step 2
: Choose the first alternative that satisfies αIf no alternative is found, then the aspiration level can be adapted.

Step 3:
If after time β no alternative has satisfied α, then decrease α by some amount δ and return to step 1.Satisficing has been reported across many domains, for instance as a heuristic car dealers use to price used BMWs.

===
Elimination by aspects ===
Unlike satisficing, Amos Tversky's elimination-by-aspect heuristic can be used when all alternatives are simultaneously available.
The decision-maker gradually reduces the number of alternatives by eliminating alternatives that do not meet the aspiration level of a specific attribute (or aspect).

===
Recognition heuristic ===
The recognition heuristic exploits the basic psychological capacity for recognition in order to make inferences about unknown quantities in the world.
For two alternatives, the heuristic is:If one of two alternatives is recognized and the other not, then infer that the recognized alternative has the higher value with respect to the criterion.
For example, in the 2003 Wimbledon tennis tournament, Andy Roddick played Tommy Robredo.
If one has heard of Roddick but not of Robredo, the recognition heuristic leads to the prediction that Roddick will win.
The recognition heuristic exploits partial ignorance, if one has heard of both or no player, a different strategy is needed.
Studies of Wimbledon 2003 and 2005 have shown that the recognition heuristic applied by semi-ignorant amateur players predicted the outcomes of all gentlemen single games as well and better than the seedings of the Wimbledon experts (who had heard of all players), as well as the ATP rankings.
The recognition heuristic is ecologically rational (that is, it predicts well) when the recognition validity is substantially above chance.
In the present case, recognition of players' names is highly correlated with their chances of winning.

===
Take-the-best ===
The take-the-best heuristic exploits the basic psychological capacity for retrieving cues from memory in the order of their validity.

Based on the cue values, it infers which of two alternatives has a higher value on a criterion.
Unlike the recognition heuristic, it requires that all alternatives are recognized, and it thus can be applied when the recognition heuristic cannot.
For binary cues (where 1 indicates the higher criterion value), the heuristic is defined as:
Search rule: Search cues in the order of their validity v.  
Stopping rule: Stop search on finding the first cue that discriminates between the two alternatives (i.e., one cue values are 0 and 1).

Decision rule:
Infer that the alternative with the positive cue value (1) has the higher criterion value).

The validity vi of a cue i is defined as the proportion of correct decisions
ci:
vi =
ci / tiwhere ti is the number of cases the values of the two alternatives differ on cue i.
The validity of each cue can be estimated from samples of observation.

Take-the-best has remarkable properties.
In comparison with complex machine learning models, it has been shown that it can often predict better than regression models, classification-and-regression trees, neural networks, and support vector machines.
[
Brighton & Gigerenzer, 2015]
Similarly, psychological studies have shown that in situations where take-the-best is ecologically rational, a large proportion of people tend to rely on it.
This includes decision making by airport custom officers, professional burglars and police officers  and student populations.
The conditions under which take-the-best is ecologically rational are mostly known.
Take-the-best shows that the previous view that ignoring part of the information would be generally irrational is incorrect.
Less can be more.

===
Fast-and-frugal trees ===
A fast-and-frugal tree is a heuristic that allows to make a classifications, such as whether a patient with severe chest pain is likely to have a heart attack or not, or whether a car approaching a checkpoint is likely to be a terrorist or a civilian.
It is called “fast and frugal” because, just like take-the-best, it allows for quick decisions with only few cues or attributes.
It is called a “tree” because it can be represented like a decision tree in which one asks a sequence of questions.
Unlike a full decision tree, however, it is an incomplete tree – to save time and reduce the danger of overfitting.

Figure 1 shows a fast-and-frugal tree used for screening for HIV (human immunodeficiency virus).
Just like take-the-best, the tree has a search rule, stopping rule, and decision rule:
Search rule:
Search through cues in a specified order.

Stopping rule: Stop search if an exit is reached.

Decision rule:
Classify the person according to the exit (here:
No HIV or HIV).

In the HIV tree, an ELISA (enzyme-linked immunosorbent assay) test is conducted first.
If the outcome is negative, then the testing procedure stops and the client is informed of the good news, that is, “no HIV.”
If, however, the result is positive, a second ELISA test is performed, preferably from a different manufacturer.
If the second ELISA is negative, then the procedure stops and the client is informed of having “no HIV.”
However, if the result is positive, a final test, the Western blot, is conducted.

In general, for n binary cues, a fast-and-frugal tree has exactly n + 1 exits – one for each cue and two for the final cue.
A full decision tree, in contrast, requires 2n exits.
The order of cues (tests) in a fast-and-frugal tree is determined by the sensitivity and specificity of the cues, or by other considerations such as the costs of the tests.
In the case of the HIV tree, the ELISA is ranked first because it produces fewer misses than the Western blot test, and also is less expensive.
The Western blot test, in contrast, produces fewer false alarms.
In a full tree, in contrast, order does not matter for the accuracy of the classifications.

Fast-and-frugal trees are descriptive or prescriptive models of decision making under uncertainty.
For instance, an analysis or court decisions reported that the best model of how London magistrates make bail decisions is a fast and frugal tree.
The HIV tree is both prescriptive– physicians are taught the procedure – and a descriptive model, that is, most physicians actually follow the procedure.

==
Informal models of heuristics ==
In their initial research, Tversky and Kahneman proposed three heuristics—availability, representativeness, and anchoring and adjustment.
Subsequent work has identified many more.
Heuristics that underlie judgment are called "judgment heuristics".
Another type, called "evaluation heuristics", are used to judge the desirability of possible choices.

===
Availability ===
In psychology, availability is the ease with which a particular idea can be brought to mind.
When people estimate how likely or how frequent an event is on the basis of its availability, they are using the availability heuristic.
When an infrequent event can be brought easily and vividly to mind, this heuristic overestimates its likelihood.
For example, people overestimate their likelihood of dying in a dramatic event such as a tornado or terrorism.
Dramatic, violent deaths are usually more highly publicised and therefore have a higher availability.
On the other hand, common but mundane events are hard to bring to mind, so their likelihoods tend to be underestimated.
These include deaths from suicides, strokes, and diabetes.
This heuristic is one of the reasons why people are more easily swayed by a single, vivid story than by a large body of statistical evidence.
It may also play a role in the appeal of lotteries: to someone buying a ticket, the well-publicised, jubilant winners are more available than the millions of people who have won nothing.
When people judge whether more English words begin with T or with K ,  the availability heuristic gives a quick way to answer the question.
Words that begin with T come more readily to mind, and so subjects give a correct answer without counting out large numbers of words.
However, this heuristic can also produce errors.
When people are asked whether there are more English words with K in the first position or with K in the third position, they use the same process.
It is easy to think of words that begin with K, such as kangaroo, kitchen, or kept.
It is harder to think of words with K as the third letter, such as lake, or acknowledge, although objectively these are three times more common.
This leads people to the incorrect conclusion that K is more common at the start of words.
In another experiment, subjects heard the names of many celebrities, roughly equal numbers of whom were male and female.
The subjects were then asked whether the list of names included more men or more women.
When the men in the list were more famous, a great majority of subjects incorrectly thought there were more of them, and vice versa for women.
Tversky and Kahneman's interpretation of these results is that judgments of proportion are based on availability, which is higher for the names of better-known people.
In one experiment that occurred before the 1976 U.S. Presidential election, some participants were asked to imagine Gerald Ford winning, while others did the same for a Jimmy Carter victory.
Each group subsequently viewed their allocated candidate as significantly more likely to win.
The researchers found a similar effect when students imagined a good or a bad season for a college football team.
The effect of imagination on subjective likelihood has been replicated by several other researchers.
A concept's availability can be affected by how recently and how frequently it has been brought to mind.
In one study, subjects were given partial sentences to complete.
The words were selected to activate the concept either of hostility or of kindness: a process known as priming.
They then had to interpret the behavior of a man described in a short, ambiguous story.
Their interpretation was biased towards the emotion they had been primed with: the more priming,
the greater the effect.
A greater interval between the initial task and the judgment decreased the effect.
Tversky and Kahneman offered the availability heuristic as an explanation for illusory correlations in which people wrongly judge two events to be associated with each other.
They explained that people judge correlation on the basis of the ease of imagining or recalling the two events together.

===
Representativeness ===
The representativeness heuristic is seen when people use categories, for example when deciding whether or not a person is a criminal.
An individual thing has a high representativeness for a category if it is very similar to a prototype of that category.
When people categorise things on the basis of representativeness, they are using the representativeness heuristic.
"
Representative" is here meant in two different senses: the prototype used for comparison is representative of its category, and representativeness is also a relation between that prototype and the thing being categorised.
While it is effective for some problems, this heuristic involves attending to the particular characteristics of the individual, ignoring how common those categories are in the population (called the base rates).
Thus, people can overestimate the likelihood that something has a very rare property, or underestimate the likelihood of a very common property.
This is called the base rate fallacy.
Representativeness explains this and several other ways in which human judgments break the laws of probability.
The representativeness heuristic is also an explanation of how people judge cause and effect: when they make these judgements on the basis of similarity, they are also said to be using the representativeness heuristic.
This can lead to a bias, incorrectly finding causal relationships between things that resemble one another and missing them when the cause and effect are very different.
Examples of this include both the belief that "emotionally relevant events ought to have emotionally relevant causes", and magical associative thinking.

====
Representativeness of base rates ====
A 1973 experiment used a psychological profile of Tom W., a fictional graduate student.
One group of subjects had to rate Tom's similarity to a typical student in each of nine academic areas (including Law, Engineering and Library Science).
Another group had to rate how likely it is that Tom specialised in each area.
If these ratings of likelihood are governed by probability, then they should resemble the base rates, i.e. the proportion of students in each of the nine areas (which had been separately estimated by a third group).
If people based their judgments on probability, they would say that Tom is more likely to study Humanities than Library Science, because there are many more Humanities students, and the additional information in the profile is vague and unreliable.
Instead, the ratings of likelihood matched the ratings of similarity almost perfectly, both in this study and a similar one where subjects judged the likelihood of a fictional woman taking different careers.
This suggests that rather than estimating probability using base rates, subjects had substituted the more accessible attribute of similarity.

====
Conjunction fallacy ====
When people rely on representativeness, they can fall into an error which breaks a fundamental law of probability.
Tversky and Kahneman gave subjects a short character sketch of a woman called Linda, describing her as, "31 years old, single, outspoken, and very bright.
She majored in philosophy.
As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations".
People reading this description then ranked the likelihood of different statements about Linda.
Amongst others, these included "Linda is a bank teller", and, "Linda is a bank teller and is active in the feminist movement".
People showed a strong tendency to rate the latter, more specific statement as more likely, even though a conjunction of the form "Linda is both X and Y" can never be more probable than the more general statement
"Linda is X".
The explanation in terms of heuristics is that the judgment was distorted because, for the readers, the character sketch was representative of the sort of person who might be an active feminist but not of someone who works in a bank.
A similar exercise concerned Bill, described as "intelligent but unimaginative".
A great majority of people reading this character sketch rated "Bill is an accountant who plays jazz for a hobby", as more likely than "Bill plays jazz for a hobby".
Without success, Tversky and Kahneman used what they described as "a series of increasingly desperate manipulations" to get their subjects to recognise the logical error.
In one variation, subjects had to choose between a logical explanation of why "Linda is a bank teller" is more likely, and a deliberately illogical argument which said that "Linda is a feminist bank teller" is more likely "because she resembles an active feminist more than she resembles a bank teller".
Sixty-five percent of subjects found the illogical argument more convincing.

Other researchers also carried out variations of this study, exploring the possibility that people had misunderstood the question.
They did not eliminate the error.
It has been shown that individuals with high CRT scores are significantly less likely to be subject to the conjunction fallacy.
The error disappears when the question is posed in terms of frequencies.
Everyone in these versions of the study recognised that out of 100 people fitting an outline description, the conjunction statement ("She is X and Y") cannot apply to more people than the general statement ("She is X").

====
Ignorance of sample size ====
Tversky and Kahneman asked subjects to consider a problem about random variation.
Imagining for simplicity that exactly half of the babies born in a hospital are male, the ratio will not be exactly half in every time period.
On some days, more girls will be born and on others, more boys.
The question was, does the likelihood of deviating from exactly half depend on whether there are many or few births per day?
It is a well-established consequence of sampling theory that proportions will vary much more day-to-day when the typical number of births per day is small.
However, people's answers to the problem do not reflect this fact.
They typically reply that the number of births in the hospital makes no difference to the likelihood of more than 60% male babies in one day.
The explanation in terms of the heuristic is that people consider only how representative the figure of 60% is of the previously given average of 50%.

====
Dilution effect ====
Richard E. Nisbett and colleagues suggest that representativeness explains the dilution effect, in which irrelevant information weakens the effect of a stereotype.
Subjects in one study were asked whether "Paul" or "Susan" was more likely to be assertive, given no other information than their first names.
They rated Paul as more assertive, apparently basing their judgment on a gender stereotype.
Another group, told that Paul's and Susan's mothers each commute to work in a bank, did not show this stereotype effect; they rated Paul and Susan as equally assertive.
The explanation is that the additional information about Paul and Susan made them less representative of men or women in general, and so the subjects' expectations about men and women had a weaker effect.
This means unrelated and non-diagnostic information about certain issue can make relative information less powerful to the issue when people understand the phenomenon.

====
Misperception of randomness ===
=
Representativeness explains systematic errors that people make when judging the probability of random events.
For example, in a sequence of coin tosses, each of which comes up heads (H) or tails (T), people reliably tend to judge a clearly patterned sequence such as HHHTTT as less likely than a less patterned sequence such as HTHTTH.
These sequences have exactly the same probability, but people tend to see the more clearly patterned sequences as less representative of randomness, and so less likely to result from a random process.
Tversky and Kahneman argued that this effect underlies the gambler's fallacy; a tendency to expect outcomes to even out over the short run, like expecting a roulette wheel to come up black because the last several throws came up red.
They emphasised that even experts in statistics were susceptible to this illusion: in a 1971 survey of professional psychologists, they found that respondents expected samples to be overly representative of the population they were drawn from.
As a result, the psychologists systematically overestimated the statistical power of their tests, and underestimated the sample size needed for a meaningful test of their hypotheses.

===
Anchoring and adjustment ===
Anchoring and adjustment is a heuristic used in many situations where people estimate a number.
According to Tversky and Kahneman's original description, it involves starting from a readily available number—the "anchor"—and shifting either up or down to reach an answer that seems plausible.
In Tversky and Kahneman's experiments, people did not shift far enough away from the anchor.
Hence the anchor contaminates the estimate, even if it is clearly irrelevant.
In one experiment, subjects watched a number being selected from a spinning "wheel of fortune".
They had to say whether a given quantity was larger or smaller than that number.
For instance, they might be asked, "Is the percentage of African countries which are members of the United Nations larger or smaller than 65%?"
They then tried to guess the true percentage.
Their answers correlated well with the arbitrary number they had been given.

Insufficient adjustment from an anchor is not the only explanation for this effect.
An alternative theory is that people form their estimates on evidence which is selectively brought to mind by the anchor.

The anchoring effect has been demonstrated by a wide variety of experiments both in laboratories and in the real world.
It remains when the subjects are offered money as an incentive to be accurate, or when they are explicitly told not to base their judgment on the anchor.
The effect is stronger when people have to make their judgments quickly.
Subjects in these experiments lack introspective awareness of the heuristic, denying that the anchor affected their estimates.
Even when the anchor value is obviously random or extreme, it can still contaminate estimates.
One experiment asked subjects to estimate the year of Albert Einstein's first visit to the United States.
Anchors of 1215 and 1992 contaminated the answers just as much as more sensible anchor years.
Other experiments asked subjects if the average temperature in San Francisco is more or less than 558 degrees, or whether there had been more or fewer than 100,025 top ten albums by The Beatles.
These deliberately absurd anchors still affected estimates of the true numbers.
Anchoring results in a particularly strong bias when estimates are stated in the form of a confidence interval.
An example is where people predict the value of a stock market index on a particular day by defining an upper and lower bound so that they are 98% confident the true value will fall in that range.
A reliable finding is that people anchor their upper and lower bounds too close to their best estimate.
This leads to an overconfidence effect.
One much-replicated finding is that when people are 98% certain that a number is in a particular range, they are wrong about thirty to forty percent of the time.
Anchoring also causes particular difficulty when many numbers are combined into a composite judgment.
Tversky and Kahneman demonstrated this by asking a group of people to rapidly estimate the product 8 x 7 x 6 x 5 x 4 x 3 x 2 x 1.
Another group had to estimate the same product in reverse order; 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8.
Both groups underestimated the answer by a wide margin, but the latter group's average estimate was significantly smaller.
The explanation in terms of anchoring is that people multiply the first few terms of each product and anchor on that figure.
A less abstract task is to estimate the probability that an aircraft will crash, given that there are numerous possible faults each with a likelihood of one in a million.
A common finding from studies of these tasks is that people anchor on the small component probabilities and so underestimate the total.
A corresponding effect happens when people estimate the probability of multiple events happening in sequence, such as an accumulator bet in horse racing.
For this kind of judgment, anchoring on the individual probabilities results in an overestimation of the combined probability.

====
Examples ====
People's valuation of goods, and the quantities they buy, respond to anchoring effects.
In one experiment, people wrote down the last two digits of their social security numbers.
They were then asked to consider whether they would pay this number of dollars for items whose value they did not know, such as wine, chocolate, and computer equipment.
They then entered an auction to bid for these items.
Those with the highest two-digit numbers submitted bids that were many times higher than those with the lowest numbers.
When a stack of soup cans in a supermarket was labelled, "Limit 12 per customer", the label influenced customers to buy more cans.
In another experiment, real estate agents appraised the value of houses on the basis of a tour and extensive documentation.
Different agents were shown different listing prices, and these affected their valuations.
For one house, the appraised value ranged from US$114,204 to $128,754.Anchoring and adjustment has also been shown to affect grades given to students.
In one experiment, 48 teachers were given bundles of student essays, each of which had to be graded and returned.
They were also given a fictional list of the students' previous grades.
The mean of these grades affected the grades that teachers awarded for the essay.
One study showed that anchoring affected the sentences in a fictional rape trial.
The subjects were trial judges with, on average, more than fifteen years of experience.
They read documents including witness testimony, expert statements, the relevant penal code, and the final pleas from the prosecution and defence.
The two conditions of this experiment differed in just one respect: the prosecutor demanded a 34-month sentence in one condition and 12 months in the other; there was an eight-month difference between the average sentences handed out in these two conditions.
In a similar mock trial, the subjects took the role of jurors in a civil case.
They were either asked to award damages "in the range from $15 million to $50 million" or "in the range from $50 million to $150 million".
Although the facts of the case were the same each time, jurors given the higher range decided on an award that was about three times higher.
This happened even though the subjects were explicitly warned not to treat the requests as evidence.
Assessments can also be influenced by the stimuli provided.

In one review, researchers found that if a stimulus is perceived to be important or carry "weight" to a situation, that people were more likely to attribute that stimulus as heavier physically.
===
Affect heuristic ===
"Affect", in this context, is a feeling such as fear, pleasure or surprise.
It is shorter in duration than a mood, occurring rapidly and involuntarily in response to a stimulus.
While reading the words "lung cancer" might generate an affect of dread, the words "mother's love" can create an affect of affection and comfort.
When people use affect ("gut responses") to judge benefits or risks, they are using the affect heuristic.
The affect heuristic has been used to explain why messages framed to activate emotions are more persuasive than those framed in a purely factual way.

===
Others ===


==
Theories ==
There are competing theories of human judgment, which differ on whether the use of heuristics is irrational.
A cognitive laziness approach argues that heuristics are inevitable shortcuts given the limitations of the human brain.
According to the natural assessments approach, some complex calculations are already done rapidly and automatically by the brain, and other judgments make use of these processes rather than calculating from scratch.
This has led to a theory called "attribute substitution", which says that people often handle a complicated question by answering a different, related question, without being aware that this is what they are doing.
A third approach argues that heuristics perform just as well as more complicated decision-making procedures, but more quickly and with less information.
This perspective emphasises the "fast and frugal" nature of heuristics.

===
Cognitive laziness ===
An effort-reduction framework proposed by Anuj K. Shah and Daniel M. Oppenheimer states that people use a variety of techniques to reduce the effort of making decisions.

===
Attribute substitution ===
In 2002 Daniel Kahneman and Shane Frederick proposed a process called attribute substitution which happens without conscious awareness.
According to this theory, when somebody makes a judgment (of a target attribute) which is computationally complex, a rather more easily calculated heuristic attribute is substituted.
In effect, a difficult problem is dealt with by answering a rather simpler problem, without the person being aware this is happening.

This explains why individuals can be unaware of their own biases, and why biases persist even when the subject is made aware of them.
It also explains why human judgments often fail to show regression toward the mean.
This substitution is thought of as taking place in the automatic intuitive judgment system, rather than the more self-aware reflective system.
Hence, when someone tries to answer a difficult question, they may actually answer a related but different question, without realizing that a substitution has taken place.
In 1975, psychologist Stanley Smith Stevens proposed that the strength of a stimulus (e.g. the brightness of a light, the severity of a crime) is encoded by brain cells in a way that is independent of modality.
Kahneman and Frederick built on this idea, arguing that the target attribute and heuristic attribute could be very different in nature.

Kahneman and Frederick propose three conditions for attribute substitution:
The target attribute is relatively inaccessible.
Substitution is not expected to take place in answering factual questions that can be retrieved directly from memory ("What is your birthday?")
or about current experience ("Do you feel thirsty now?).

An associated attribute is highly accessible.
This might be because it is evaluated automatically in normal perception or because it has been primed.
For example, someone who has been thinking about their love life and is then asked how happy they are might substitute how happy they are with their love life rather than other areas.

The substitution is not detected and corrected by the reflective system.
For example, when asked "A bat and a ball together cost $1.10.
The bat costs $1 more than the ball.
How much does the ball cost?
"
many subjects incorrectly answer $0.10.
An explanation in terms of attribute substitution is that, rather than work out the sum, subjects parse the sum of $1.10 into a large amount and a small amount, which is easy to do.
Whether they feel that is the right answer will depend on whether they check the calculation with their reflective system.
Kahneman gives an example where some Americans were offered insurance against their own death in a terrorist attack while on a trip to Europe, while another group were offered insurance that would cover death of any kind on the trip.
Even though "death of any kind" includes "death in a terrorist attack", the former group were willing to pay more than the latter.
Kahneman suggests that the attribute of fear is being substituted for a calculation of the total risks of travel.
Fear of terrorism for these subjects was stronger than a general fear of dying on a foreign trip.

===
Fast and frugal ===
Gerd Gigerenzer and colleagues have argued that heuristics can be used to make judgments that are accurate rather than biased.
According to them, heuristics are "fast and frugal" alternatives to more complicated procedures, giving answers that are just as good.

==
Consequences ==


===
Efficient decision heuristics ===
Warren Thorngate, a social psychologist, implemented ten simple decision rules or heuristics in a computer program.
He determined how often each heuristic selected alternatives with highest-through-lowest expected value in a series of randomly-generated decision situations.
He found that most of the simulated heuristics selected alternatives with highest expected value and almost never selected alternatives with lowest expected value.

=== "Beautiful-is-familiar" effect ===
Psychologist Benoît Monin reports a series of experiments in which subjects, looking at photographs of faces, have to judge whether they have seen those faces before.
It is repeatedly found that attractive faces are more likely to be mistakenly labeled as familiar.
Monin interprets this result in terms of attribute substitution.
The heuristic attribute in this case is a "warm glow"; a positive feeling towards someone that might either be due to their being familiar or being attractive.
This interpretation has been criticised, because not all the variance in familiarity is accounted for by the attractiveness of the photograph.
===
Judgments of morality and fairness ===
Legal scholar Cass Sunstein has argued that attribute substitution is pervasive when people reason about moral, political or legal matters.
Given a difficult, novel problem in these areas, people search for a more familiar, related problem (a "prototypical case") and apply its solution as the solution to the harder problem.
According to Sunstein, the opinions of trusted political or religious authorities can serve as heuristic attributes when people are asked their own opinions on a matter.
Another source of heuristic attributes is emotion: people's moral opinions on sensitive subjects like sexuality and human cloning may be driven by reactions such as disgust, rather than by reasoned principles.
Sunstein has been challenged as not providing enough evidence that attribute substitution, rather than other processes, is at work in these cases.

===
Persuasion ===
An example of how persuasion plays a role in heuristic processing can be explained through the heuristic-systematic model.
This explains how there are often two ways we are able to process information from persuasive messages, one being heuristically and the other systematically.
A heuristic is when we make a quick short judgement into our decision making.
On the other hand, systematic processing involves more analytical and inquisitive cognitive thinking.
Individuals looks further than their own prior knowledge for the answers.
An example of this model could be used when watching an advertisement about a specific medication.
One without prior knowledge would see the person in the proper pharmaceutical attire and assume that they know what they are talking about.
Therefore, that person automatically has more credibility and is more likely to trust the content of the messages than they deliver.
While another who is also in that field of work or already has prior knowledge of the medication will not be persuaded by the ad because of their systematic way of thinking.
This was also formally demonstrated in an experiment conducted my Chaiken and Maheswaran (1994).
In addition to these examples, the fluency heuristic ties in perfectly with the topic of persuasion.
It is described as how we all easily make "the most of an automatic by-product of retrieval from memory".
An example would be a friend asking about good books to read.
Many could come to mind, but you name the first book recalled from your memory.
Since it was the first thought, therefore you value it as better than any other book one could suggest.
The effort heuristic is almost identical to fluency.
The one distinction would be that objects that take longer to produce are seen with more value.
One may conclude that a glass vase is more valuable than a drawing, merely because it may take longer for the vase.
These two varieties of heuristics confirms how we may be influenced easily our mental shortcuts, or what may come quickest to our mind.

== See also ==


==
Citations ==


==
References ==
Baron, Jonathan (2000), Thinking and deciding (
3rd ed.)
,
New York:
Cambridge University Press, ISBN 978-0521650304, OCLC
316403966
Gilovich, Thomas; Griffin, Dale W. (2002), "Introduction – Heuristics and Biases:
Then and Now",  in Gilovich, Thomas; Griffin, Dale W.; Kahneman, Daniel (eds.),
Heuristics and biases: the psychology of intuitive judgement, Cambridge University Press, pp.
1–18, ISBN 9780521796798
Hardman, David (2009), Judgment and decision making:
psychological perspectives, Wiley-Blackwell, ISBN
9781405123983
Hastie, Reid; Dawes, Robyn M. (29 September 2009), Rational Choice in an Uncertain World:
The Psychology of Judgment and Decision Making, SAGE, ISBN
9781412959032
Koehler, Derek J.; Harvey, Nigel (2004), Blackwell handbook of judgment and decision making, Wiley-Blackwell, ISBN 9781405107464
Kunda, Ziva (1999), Social Cognition:
Making Sense of People, MIT Press, ISBN 978-0-262-61143-5, OCLC 40618974
Mussweiler, Thomas; Englich, Birte; Strack, Fritz (2004), "Anchoring effect",  in Pohl, Rüdiger F. (ed.)
,
Cognitive Illusions:
A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
183–200, ISBN 9781841693514, OCLC 55124398
Plous, Scott (1993), The Psychology of Judgment and Decision Making, McGraw-Hill, ISBN 9780070504776, OCLC 26931106
Poundstone, William (2010), Priceless:
the myth of fair value (and how to take advantage of it), Hill and Wang, ISBN 9780809094691
Reber, Rolf (2004), "Availability",  in Pohl, Rüdiger F. (ed.),
Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
147–163, ISBN 9781841693514, OCLC 55124398
Sutherland, Stuart (2007), Irrationality (2nd ed.),
London: Pinter and Martin, ISBN 9781905177073, OCLC 72151566
Teigen, Karl Halvor (2004), "Judgements by representativeness",  in Pohl, Rüdiger F. (ed.),
Cognitive Illusions:
A Handbook on Fallacies and Biases in Thinking, Judgement and Memory, Hove, UK:
Psychology Press, pp.
165–182, ISBN 9781841693514, OCLC 55124398
Tversky, Amos; Kahneman, Daniel (1974), "Judgments Under Uncertainty: Heuristics and Biases" (PDF), Science, 185 (4157): 1124–1131,
Bibcode:1974Sci...
185.1124T, doi:10.1126/science.185.4157.1124, PMID 17835457, S2CID 143452957 reprinted in Daniel Kahneman; Paul Slovic; Amos Tversky, eds.
(
1982).
Judgment Under Uncertainty: Heuristics and Biases.
Cambridge:
Cambridge University Press.
pp.
3–20.
ISBN 9780521284141.

Yudkowsky, Eliezer (2011).
"
Cognitive biases potentially affecting judgment of global risks".

In Bostrom, Nick; Cirkovic, Milan M. (eds.).
Global Catastrophic Risks.
OUP Oxford.
pp.
91–119.
ISBN 978-0-19-960650-4.
==
Further reading ==
Slovic, Paul; Melissa Finucane; Ellen Peters; Donald G. MacGregor (2002).
"
The Affect Heuristic".

In Thomas Gilovich; Dale Griffin; Daniel Kahneman (eds.).
Heuristics and Biases: The Psychology of Intuitive Judgment.
Cambridge University Press.
pp.
397–420.
ISBN 9780521796798.

Gigerenzer, Gerd; Selten, Reinhard (2001).
Bounded rationality : the adaptive toolbox.
Cambridge, MA: MIT Press.
ISBN 0585388288.
OCLC 49569412.

Korteling, Johan E.; Brouwer, Anne-Marie; Toet, Alexander (3 September 2018). "
A Neural Network Framework for Cognitive Bias".
Frontiers in Psychology.
9:
1561.
doi:10.3389/fpsyg.2018.01561.
PMC 6129743.
PMID 30233451.

Chow, Sheldon (20 April 2011).
"
Heuristics, Concepts, and Cognitive Architecture:
Toward Understanding
How The Mind Works".
Electronic Thesis and Dissertation Repository.

Todd, P.M. (2001).
"
Heuristics for Decision and Choice".
International Encyclopedia of the Social & Behavioral Sciences.
pp.
6676–6679.
doi:10.1016
/B0-08-043076-7/00629-X. ISBN 978-0-08-043076-8.
==
External links ==
Test Yourself:
Decision Making and the Availability Heuristic
Computers are social actors (CASA) is a paradigm which states that humans mindlessly apply the same social heuristics used for human interactions to computers because they call to mind similar social attributes as humans.

==
History and context ==
Clifford Nass and Youngme Moon's article, "Machines and Mindlessness:
Social Responses to Computers", published in 2000, is the origin for CASA.
It states that CASA is the concept that people mindlessly apply social rules and expectations to computers, even though they know that these machines do not have feelings, intentions or human motivations.

In their 2000 article, Nass and Moon attribute their observation of anthropocentric reactions to computers and previous research on mindlessness as factors that lead them to study the phenomenon of computers as social actors.
Specifically, they observed consistent anthropocentric treatment of computers by individuals in natural and lab settings, even though these individuals agreed that computers are not human and shouldn't be treated as such.

Additionally, Nass and Moon found a similarity between this behavior and research by Harvard psychology professor Ellen Langer on mindlessness.
Langer states that mindlessness is when a specific context triggers an individual to rely on categories, associations, and habits of thought from the past with little to no conscious awareness.
When these contexts are triggered, the individual becomes oblivious to novel or alternative aspects of the situation.
In this respect, mindlessness is similar to habits and routines, but different in that with only one exposure to information, a person will create a cognitive commitment to the information and freeze its potential meaning.
With mindlessness, alternative meanings or uses of the information become unavailable for active cognitive use.
Social attributes that computers have which are similar to humans include:

Words for output
Interactivity (the computer 'responds' when a button is touched)
Ability to perform traditional human tasksAccording to CASA, the above attributes trigger scripts for human-human interaction, which leads an individual to ignore cues revealing the asocial nature of a computer.
Although individuals using computers exhibit a mindless social response to the computer, individuals who are sensitive to the situation can observe the inappropriateness of the cued social behaviors.
CASA has been extended to include robots and AI.
However, recently, there have been challenges to the CASA paradigm.

To account for the advances in technology, MASA has been forwarded as a significant extension of CASA.

==
Attributes ==
Cued social behaviors observed in research settings include some of the following:
Gender stereotyping:
When voice outputs are used on computers, this triggers gender stereotype scripts, expectations, and attributions from individuals.
For example, a 1997 study revealed that female-voiced tutor computers were rated as more informative about love and relationships than male-voiced computers, whereas male-voiced computers were more proficient in technical subjects than female-voiced computers.

Reciprocity: When a computer provides help, favours, or benefits, this triggers the mindless response of the participant feeling obliged to 'help' the computer.
For example, an experiment in 1997 found that when a specific computer 'helped' a person, that person was more likely to do more 'work' for that computer.

Specialist versus generalist: When a technology is labeled as 'specialist', this triggers a mindless response by influencing people's perceptions of the content the labeled technology presents.
For example, a 2000 study revealed when people watched a television labeled 'News Television', they thought the news segments on that TV were higher in quality, had more information, and were more interesting than people who saw the identical information on a TV labeled 'News and Entertainment Television'.

Personality: When a computer user mindlessly creates a personality for a computer based on verbal or paraverbal cues in the interface.
For example, research from 1996 and 2001 found people with dominant personalities preferred computers that also had a 'dominant personality'; that is, the computer used strong, assertive language during tasks.

==
Academic research ==
Three research articles have represented some of the advances in the field of CASA.
Specifically, researchers in this field are looking at how novel variables, manipulations, and new computer software influence mindlessness.

A 2010 article, "Cognitive load on social response to computers" by E.J. Lee discussed research on how human likeness of a computer interface, individuals' rationality, and cognitive load moderate the extent to which people apply social attributes to computers.
The research revealed that participants were more socially attracted to a computer that flattered them than a generic-comment computer, but they became more suspicious about the validity of the flattery computer's claims and more likely to dismiss its answer.
These negative effects disappeared when participants simultaneously engaged in a secondary task.

A 2011 study, "Computer emotion – impacts on trust" by Dimitrios Antos, Celso De Melo, Jonathan Gratch, and Barbara Grosz investigated whether computer agents can use the expression of emotion to influence human perceptions of trustworthiness in the context of a negotiation activity followed by a trust activity.
They found that computer agents displaying emotions congruent with their actions were preferred as partners in the trust game over computer agents whose emotion expressions and actions did not match.
They also found that when emotion did not carry useful new information, it did not strongly influence human decision-making behavior in a negotiation setting.

A 2011 study "Cloud computing – reexamination of CASA" by Hong and Sundar found that when people are in a cloud computing environment, they shift their source orientation—that is, users evaluate the system by focusing on service providers over the internet, instead of the machines in front of them.
Hong and Sundar concluded their study by stating, "if individuals no longer respond socially to computers in clouds, there will need to be a fundamental re-examination of the mindless social response of humans to computers.
"One
example of how CASA research can impact consumer behaviour and attitude is Moon's experiment, which tested the application of the principle of reciprocity and disclosure in a consumer context.

He tested this principle with intimate self-disclosure of high-risk information (when disclosure makes the person feel vulnerable) to a computer, and observed how that disclosure affects future attitudes and behaviors.
Participants interacted with a computer which questioned them using reciprocal wording and gradual revealing of intimate information, then participants did a puzzle on paper, and finally half the group went back to the same computer and the other half went to a different computer.

Both groups were shown 20 products and asked if they would purchase them.
Participants who used the same computer throughout the experiment had a higher purchase likelihood score and a higher attraction score toward the computer in the product presentation than participants who did not use the same computer throughout the experiment.

==
References ==
Scarcity, in the area of social psychology, works much like scarcity in the area of economics.

Simply put, humans place a higher value on an object that is scarce, and a lower value on those that are in abundance.
For example diamonds are more valuable than rocks because diamonds are not as abundant.
The scarcity heuristic is a mental shortcut that places a value on an item based on how easily it might be lost, especially to competitors.
The scarcity heuristic stems from the idea that the more difficult it is to acquire an item the more value that item has.
In many situations we use an item’s availability, its perceived abundance, to quickly estimate quality and/or utility.
This can lead to systemic errors or cognitive bias.
There are two social psychology principles that work with scarcity that increase its powerful force.

One is social proof.

This is a contributing factor to the effectiveness of scarcity, because if a product is sold out, or inventory is extremely low, humans interpret that to mean the product must be good since everyone else appears to be buying it.

The second contributing principle to scarcity is commitment and consistency.

If someone has already committed themselves to something, then find out they cannot have it, it makes the person want the item more.

== Examples ==
This idea is deeply embedded in the intensely popular “Black Friday” shopping extravaganza that U.S. consumers participate in every year on the day after Thanksgiving.

More than getting a bargain on a hot gift idea, shoppers thrive on the competition itself, in obtaining the scarce product.

== Heuristics ==
Heuristics are strategies that use readily accessible (though loosely applicable) information for problem solving.

We use heuristics to speed up our decision-making process when an exhaustive, deliberative process is perceived to be impractical or unnecessary.

Thus heuristics are simple, efficient rules, which have developed through either evolutionary proclivities or past learning.
While these “rules” work well in most circumstances, there are certain situations where they can lead to systemic errors or cognitive bias.
The scarcity heuristic is only one example of how mental “rules” can result in unintended bias in decision-making.

Other heuristics and biases include the availability heuristic, survivorship bias, confirmation bias, and the self-attribution bias.
Like the scarcity heuristic, all of these phenomena result from either evolutionary or past behavior patterns and can consistently lead to faulty decision-making in specific circumstances.

Scarcity appears to have created a number of heuristics such as when price is used as a cue to the quality of products, as cue to the healthfulness of medical conditions, and as a cue to the sexual content of books when age restrictions are put in place.
These heuristic judgments should increase the desirability of a stimulus to those who value the inferred attributes.
The scarcity heuristic does not only apply to a shortage in absolute resources.
According to Robert Cialdini, the scarcity heuristic leads to us to make biased decisions on a daily basis.
It is particularly common to be biased by the scarcity heuristic when assessing four parameters: quantity, rarity, time, and censorship.

===
Quantity ===
The simplest manifestation of the scarcity heuristic is the fear of losing access to some resource resulting from the possession of a small or diminishing quantity of the asset.
For example, your favorite shirt becomes more valuable when you know you cannot replace it.
If you had ten shirts of the same style and color, losing one would likely be less distressful because you have several others to take its place.

Cialdini theorizes that it is in our nature to fight against losing freedom, pointing out that we value possessions in low quantities partly because as resources become less available they are more likely not to be available at all at some point in the future.
If the option to use that resource disappears entirely, then options decrease and so does our freedom.

Cialdini draws his conclusion from psychological reactance theory, which states that whenever free choice is limited or threatened, the need to retain freedom makes us desire the object under threat more than if it was not in danger of being lost.
In the context of the scarcity heuristic, this implies that when something threatens our prior access to a resource, we will react against that interference by trying to possess the resource with more vigor than before.
===
Rarity ===
Objects can increase in value if we feel that they have unique properties, or are exceptionally difficult to replicate.
Collectors of rare baseball cards or stamps are simple examples of the principle of rarity.

===
Time ===
When time is scarce and information complex, people are prone to use heuristics in general.
When time is perceived to be short, politicians can exploit the scarcity heuristic.

The Bush administration used a variation of this theme in justifying the rush to war in Iraq: "time is running out for Saddam and unless we stop him now he will use his WMD against us".
The Scarcity Rule is the sales tool that is most obvious to us when we see advertising terms including, “
Sale ends June 30th”;
“The First Hundred People Receive…”; “Limited Time Only”; “
Offer Expires”.
===
Restriction and censorship ===
According to Worchel, Arnold & Baker (1975), our reaction to censorship is to want the censored information more than before it was restricted as well perceive the censored message more favorably than before the ban.

This research indicates that people not only want censored information more but have an increased susceptibility to the message of the censored material.
Worchel, Arnold, and Baker came to this by testing students’ attitudes toward co-ed dormitories at the University of North Carolina.
They found that when students were told that speech against the idea of co-ed dorms was banned, students saw co-ed dorms as less favorable than if the discourse about the dorms had remained open.
Thus, even without having heard any argument against co-ed dormitories, students were more prone to being persuaded to be opposed simply as a reaction to the ban.

Another experiment (Zellinger et al.
1975) divided students into two groups and gave them the same book.

In one group the book was clearly labeled as “mature content” and was restricted for readers 21 and older while the other group's book had no such warning.
When asked to indicate their feelings toward the literature the group with the warning demonstrated a higher desire to read the book and a stronger conviction that they would like the book than those without the warning.

==
Studies ==
Numerous studies have been conducted on the topic of scarcity in social psychology:
Scarcity rhetoric in a job advertisement for restaurant server positions has been investigated.
Subjects were presented with two help-wanted ads, one of which suggested numerous job vacancies, while the other suggested that very few were available.
The study found that subjects who were presented with the advertisement that suggested limited positions available viewed the company as being a better one to work for than the one that implied many job positions were available.
Subjects also felt that the advertisement that suggested limited vacancies translated to higher wages.
In short, subjects placed a positive, higher value on the company that suggested that there were scarce job vacancies available.
Another study examined how the scarcity of men may lead women to seek high-paying careers and to delay starting a family.
This effect was driven by how the sex ratio altered the mating market, not just the job market.
Sex ratios involving a scarcity of men led women to seek lucrative careers because of the difficulty women have in finding an investing, long-term mate under such circumstances.

==
Conditional variations ==
Although the scarcity heuristic can always affect judgment and perception, certain situations exacerbate the effect.
New scarcity and competition are common cases.

===
New scarcity ===
New scarcity occurs when our irrational desire for limited resources increases when we move from a state of abundance to a state of scarcity.
This is in line with psychological reactance theory, which states that a person will react strongly when they perceive that their options are likely to be lessened in the future.

Worchel, Lee & Adewole (1975) demonstrated this principle with a simple experiment.
They divided people into two groups, giving one group a jar of ten cookies and another a jar with only two cookies.
When asked to rate the quality of the cookie the group with two, in line with the scarcity heuristic, found the cookies more desirable.
The researchers then added a new element.

Some participants were first given a jar of ten cookies, but before participants could sample the cookie, experimenters removed 8 cookies so that there were again only two.
The group first having ten
but then were reduced to two, rated the cookies more desirable than both of the other groups.

===
Quantifying value in scarce and competitive situations ===
Mittone & Savadori (2009) created an experiment where the same good was abundant in one condition but scarce in another.
The scarcity condition involved a partner/competitor to create scarcity, while the abundant condition did not.
Results showed that more participants chose a good when it was scarce than when it was abundant, for two out of four sets of items (ballpoints, snacks, pencils, and key rings).

The experiment then created a WTA (willingness to accept) elicitation procedure that created subjective values for goods.
Results showed the scarce good receiving a higher WTA price by participants choosing it, than by those who did not, compared to the WTA of the abundant good, despite the fact that both types of participants assigned a lower market price to the scarce good, as compared to the abundant one.

====
Other applications ====
This idea could easily by applied to other fields.
In 1969, James C. Davis postulated that revolutions are most likely to occur during periods of improving economic and social conditions that are immediately followed by a short and sharp reversal in that trend.
Therefore, it is not the consistently downtrodden, those in a state of constant scarcity, who revolt but rather those who experience new scarcity that are most likely to feel a desire of sufficient intensity to incite action.

===
Competition ===
In situations when others are directly vying for scarce resources, the value we assign to objects is further inflated.
Advertisers commonly take advantage of scarcity heuristics by marketing products as “hot items” or by telling customers that certain goods will sell out quickly.

Worchel, Lee & Adewole (1975) also examined the competition bias in their cookie experiment, taking the group that had experienced new scarcity, going from ten to two cookies, and telling half of them that the reason they were losing cookies is because there was high demand for cookies from other participants taking the test.
They then told the other half that it was just because a mistake had been made.
It was found that the half we were told that they were having their cookie stock reduced due to social demand rated the cookies higher than those who were told it was only due to an error.

In 1983, Coleco Industries marketed a soft-sculpted doll that had exaggerated neonatal features and came with "adoption papers".
Demand for these dolls exceeded expectations, and spot shortages began to occur shortly after their introduction to the market.
This scarcity fueled demand even more and created what became known as the Cabbage Patch panic (Langway, Hughey, McAlevey, Wang, & Conant, 1983).
Customers scratched, choked, pushed, and fought one another in an attempt to get the dolls.

Several stores were wrecked during these riots, so many stores began requiring people to wait in line (for as long as 14 hours) in order to obtain one of the dolls.
A secondary market quickly developed where sellers were receiving up to $150 per doll.
Even at these prices, the dolls were so difficult to obtain that one Kansas City postman flew to London to get one for his daughter (Adler et al.,
1983).

== See also ==
Artificial scarcity
Principle
of least interest


==
References ==


==
Bibliography ==
Cialdini, Robert B. (2001)
[1984].
Influence: Science and Practice
(4th ed.).
Boston:
Allyn and Bacon.
ISBN 9780321011473.

Gigerenzer, Gerd (1991).
"
How to Make Cognitive Illusions Disappear: Beyond "Heuristics and Biases"" (PDF).
European Review of Social Psychology.
2
: 83–115.
CiteSeerX 10.1.1.336.9826.
doi:10.1080/14792779143000033.

Lynn, Michael (1989).
"
Scarcity effects on desirability: Mediated by assumed expensiveness?".
Journal of Economic Psychology.
10 (2): 257–274.
doi:10.1016/0167-4870(89)90023-8.
hdl:1813/72078.

Lynn, Michael (1992).
"
The Psychology of Unavailability:
Explaining Scarcity and Cost Effects on Value".
Basic and Applied Social Psychology.
13 (1): 3–7.
doi:10.1207
/s15324834basp1301_2.
hdl:1813/71653.

Mittone, Luigi; Savadori, Lucia (2009).
"
The Scarcity Bias".
Applied Psychology.
58 (3): 453–468.
doi:10.1111/j.1464-0597.2009.00401.x.

Pearl, Judea (1985).
Heuristics:
Intelligent search strategies for computer problem solving (Repr.
with corr.
ed.)
.
Reading, Mass.:
Addison-Wesley Pub.
Co. p. vii.
ISBN 978-0-201-05594-8.

Worchel, Stephen; Arnold, Susan; Baker, Michael (1975).
"
The Effects of Censorship on Attitude Change:
The Influence of Censor and Communication Characteristics" (PDF).
Journal of Applied Social Psychology.
5 (3): 227–239.
doi:10.1111/j.1559-1816.1975.tb00678.x.
Archived from the original on 2015-02-23.CS1 maint: bot: original URL status unknown (link)
Worchel, Stephen; Lee, Jerry; Adewole, Akanbi (1975).
"
Effects of supply and demand on ratings of object value".
Journal of Personality and Social Psychology.
32 (5): 906–914.
doi:10.1037/0022-3514.32.5.906.

Zellinger, David A.; Fromkin, Howard L.; Speller, Donald E.; Kohn, Carol A. (1975). "
A commodity theory analysis of the effects of age restrictions upon pornographic materials".
Journal of Applied Psychology.
60 (1):
94–99.
doi:10.1037/h0076350.
==
Further reading ==
Tauer, John M. (2007). "
Scarcity Principle".

In Baumeister, Roy; Vohs, Kathleen (eds.).
Encyclopedia of Social Psychology.
doi:10.4135/9781412956253.n466.
ISBN 9781412916707.
Bounded rationality is the idea that rationality is limited when individuals make decisions.
In other words, humans "...preferences are determined by changes in outcomes relative to a certain reference level..."
as stated by Esther-Mirjam Sent (2018) Limitations include the difficulty of the problem requiring a decision, the cognitive capability of the mind, and the time available to make the decision.
Decision-makers, in this view, act as satisficers, seeking a satisfactory solution, rather than an optimal solution.
Therefore, humans do not undertake a full cost-benefit analysis to determine the optimal decision, but rather, choose an option that fulfils their adequacy criteria.
Herbert A. Simon proposed bounded rationality as an alternative basis for the mathematical and neoclassical economic modelling of decision-making, as used in economics, political science, and related disciplines.
The concept of bounded rationality complements "rationality as optimization", which views decision-making as a fully rational process of finding an optimal choice given the information available.
Therefore, bounded rationality can be said to address the discrepancy between the assumed perfect rationality of human behaviour (which is utilised by other economics theories such as the Neoclassical approach), and the reality of human cognition.
Simon used the analogy of a pair of scissors, where one blade represents "cognitive limitations" of actual humans and the other the "structures of the environment", illustrating how minds compensate for limited resources by exploiting known structural regularity in the environment.
Many economics models assume that agents are on average rational, and can in large quantities be approximated to act according to their preferences in order to maximise utility.
With bounded rationality, Simon's goal was "to replace the global rationality of economic man with a kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist.
"
In short, the concept of bounded rationality revises notions of "perfect" rationality to account for the fact that perfectly rational decisions are often not feasible in practice because of the intractability of natural decision problems and the finite computational resources available for making them.

The concept of bounded rationality continues to influence (and be debated in) different disciplines, including economics, psychology, law, political science, and cognitive science.
Some models of human behavior in the social sciences assume that humans can be reasonably approximated or described as "rational" entities, as in rational choice theory or Downs' Political Agency Model.

==
Origins ==
Bounded rationality was coined by Herbert A. Simon.
In Models of Man, Simon argues that most people are only partly rational, and are irrational in the remaining part of their actions.
In another work, he states "boundedly rational agents experience limits in formulating and solving complex problems and in processing (receiving, storing, retrieving, transmitting) information".
Simon describes a number of dimensions along which "classical" models of rationality can be made somewhat more realistic, while remaining within the vein of fairly rigorous formalization.
These include:

limiting the types of utility functions
recognizing the costs of gathering and processing information
the possibility of having a "vector" or "multi-valued" utility functionSimon suggests that economic agents use heuristics to make decisions rather than a strict rigid rule of optimization.
They do this because of the complexity of the situation.
An example of behaviour inhibited by heuristics can be seen when comparing the cognitive strategies utilised in simple situations (e.g Tic-tac-toe), in comparison to strategies utilised in difficult situations (e.g Chess).
Both games, as defined by game theory economics, are finite games with perfect information, and therefore equivalent.
However, within Chess, mental capacities and abilities are a binding constraint, therefore optimal choices are not a possibility.
Thus, in order to test the mental limits of agents, complex problems, such as those within Chess, should be studied to test how individuals work around their cognitive limits, and what behaviours or heuristics are used to form solutions 


== Model extensions ==
As decision-makers have to make decisions about how and when to decide, Ariel Rubinstein proposed to model bounded rationality by explicitly specifying decision-making procedures.
This puts the study of decision procedures on the research agenda.

Gerd Gigerenzer opines that decision theorists, to some extent, have not adhered to Simon's original ideas.
Rather, they have considered how decisions may be crippled by limitations to rationality, or have modeled how people might cope with their inability to optimize.
Gigerenzer proposes and shows that simple heuristics often lead to better decisions than theoretically optimal procedures.
Moreover, Gigerenzer states, agents react relative to their environment and use their cognitive processes to adapt accordingly.
Huw Dixon later argues that it may not be necessary to analyze in detail the process of reasoning underlying bounded rationality.

If we believe that agents will choose an action that gets them "close" to the optimum, then we can use the notion of epsilon-optimization, which means we choose our actions so that the payoff is within epsilon of the optimum.
If we define the optimum (best possible) payoff as
U
          
            ∗
          
        
      
    
    {\displaystyle U^{*}}
  , then the set of epsilon-optimizing options S(ε) can be defined as all those options s such that:

  
    
      
        U
(
        s
        )
        ≥
U
          
            ∗
          
        
        −
ϵ
      
    
    {\displaystyle U(s)\geq U^{*}-\epsilon }
  .

The notion of strict rationality is then a special case (ε=0).
The advantage of this approach is that it avoids having to specify in detail the process of reasoning, but rather simply assumes that whatever the process is, it is good enough to get near to the optimum.

From a computational point of view, decision procedures can be encoded in algorithms and heuristics.
Edward Tsang argues that the effective rationality of an agent is determined by its computational intelligence.
Everything else being equal, an agent that has better algorithms and heuristics could make "more rational" (closer to optimal) decisions than one that has poorer heuristics and algorithms.
Tshilidzi Marwala and Evan Hurwitz in their study on bounded rationality observed that advances in technology (e.g. computer processing power because of Moore's law, artificial intelligence, and big data analytics) expand the bounds that define the feasible rationality space.
Because of this expansion of the bounds of rationality, machine automated decision making makes markets more efficient.

It is also important to consider that the model of bounded rationality also extends tobounded self-interest in which humans are sometimes willing to forsake their own self-interests for the benefits of others, something that has not been considered in earlier economic models.

==
Relationship to Behavioral Economics ==
Bounded rationality implies the idea that humans take reasoning shortcuts that may lead to sub-optimal decision-making.
Behavioural economists engage in mapping the decision shortcuts that agents use in order to help increase the effectiveness of human decision-making.
One treatment of this idea comes from Cass Sunstein and Richard Thaler's Nudge.
Sunstein and Thaler recommend that choice architectures are modified in light of human agents' bounded rationality.
A widely cited proposal from Sunstein and Thaler urges that healthier food be placed at sight level in order to increase the likelihood that a person will opt for that choice instead of a less healthy option.
Some critics of Nudge have lodged attacks that modifying choice architectures will lead to people becoming worse decision-makers.
Furthermore, bounded rationality attempts to address assumption points discussed within Neoclassical Economics theory during the 1950s.
This theory assumes that the complex problem, the way in which the problem is presented, all alternative choices, and a utility function, are all provided to decision-makers in advance, where this may not be realistic.
This was widely used and accepted for a number of decades, however economists realised some disadvantages exist in utilising this theory.
This theory did not consider how problems are initially discovered by decision-makers, which could have an impact on the overall decision.
Additionally, personal values, the way in which alternatives are discovered and created, and the environment surrounding the decision-making process are also not considered when using this theory .
Alternatively, bounded rationality focuses on the cognitive ability of the decision-maker and the factors which may inhibit optimal decision-making Additionally, placing a focus on organisations rather than focusing on markets as Neoclassical Economics theory does, bounded rationality is also the basis for many other economics theories (e.g. Organisational theory) as it emphasises that the "...performance and success of an organisation is governed primarily by the psychological limitations of its members..." as stated by John D.W. Morecroft (1981) .

==
Relationship to Psychology ==
The collaborative works of Daniel Kahneman and Amos Tversky expand upon Herbert A. Simon's ideas in the attempt to create a map of bounded rationality.
The research attempted to explore the choices made by what was assumed as rational agents compared to the choices made by individuals optimal beliefs and their satisficing behaviour.
Kahneman cites that the research contributes mainly to the school of psychology due to imprecision of psychological research to fit the formal economic models, however, the theories are useful to economic theory as a way to expand simple and precise models and cover diverse psychological phenomena.
Three major topics covered by the works of Daniel Kahneman and Amos Tversky include Heuristics of judgement, risky choice, and framing effect, which were a culmination of research that fit under what was defined by Herbert A. Simon as the Psychology of Bounded Rationality.
In contrast to the work of Simon; Kahneman and Tversky aimed to focus on the effects bounded rationality had on simple tasks which therefore placed more emphasis on errors in cognitive mechanisms irrespective of the situation.

==
Influence on social network structure ==
Recent research has shown that bounded rationality of individuals may influence the topology of the social networks that evolve among them.
In particular, Kasthurirathna and Piraveenan have shown that in socio-ecological systems, the drive towards improved rationality on average might be an evolutionary reason for the emergence of scale-free properties.
They did this by simulating a number of strategic games on an initially random network with distributed bounded rationality, then re-wiring the network so that the network on average converged towards Nash equilibria, despite the bounded rationality of nodes.
They observed that this re-wiring process results in scale-free networks.
Since scale-free networks are ubiquitous in social systems, the link between bounded rationality distributions and social structure is an important one in explaining social phenomena.

==
Conclusion ==
To conclude, bounded rationality challenges the rationality assumptions widely accepted between the 1950s and 1970s which were initially used when considering [utility] maximisation, [probability] judgements, and other market-focused economic calculations .
Not only does the concept focus on the ways in which humans subconsciously use [[1]] in order to make decisions, but also emphasises that humans infer to a great extent, given the limited information they access prior to decision-making for complex problems.
Although this concept realistically delves into decision-making and human cognition, challenging earlier theories which assumed perfect rational cognition and behaviour, bounded rationality can mean something different to everyone, and the way each person satisfices can vary dependant on their environment and the information they have access to .

== See also ==


==
Reference List ==


==
Further reading ==
Bayer, R. C., Renner, E., & Sausgruber, R. (2009).
Confusion and reinforcement learning in experimental public goods games.
NRN working papers 2009–22,
The Austrian Center for Labor Economics and the Analysis of the Welfare State, Johannes Kepler University Linz, Austria.

Elster, Jon (1983).
Sour Grapes: Studies in the Subversion of Rationality.
Cambridge, UK:
Cambridge University Press.
ISBN 978-0-521-25230-0.

Felin, T., Koenderink, J., & Krueger, J. (2017).
"
Rationality, perception and the all-seeing eye."
Psychonomic Bulletin and Review, 25: 1040-1059.
DOI
10.3758/s13423-016-1198-z
Gershman, S.J., Horvitz, E.J., & Tenenbaum, J.B. (2015).
Computational rationality: A converging paradigm for intelligence in brains, minds, and machines.
Science, 49: 273-278.
DOI:
10.1126/
science.aac6076
Gigerenzer, Gerd & Selten, Reinhard (2002).
Bounded Rationality.
Cambridge:
MIT Press.
ISBN 978-0-262-57164-7.

Hayek, F.A (1948)
Individualism and Economic order
Kahneman, Daniel (2003).
"
Maps of bounded rationality: psychology for behavioral economics" (PDF).
The American Economic Review.
93 (5):
1449–75.
CiteSeerX 10.1.1.194.6554.
doi:10.1257/000282803322655392.
Archived from the original (PDF) on 2018-02-19.
Retrieved 2017-11-01.

March, James G. (1994).
A Primer on Decision Making: How Decisions Happen.
New York:
The Free Press.
ISBN 978-0-02-920035-3.

Simon, Herbert (1957). "
A Behavioral Model of Rational Choice", in Models of Man, Social and Rational:
Mathematical Essays on Rational Human Behavior in a Social Setting.
New York: Wiley.

March, James G. & Simon, Herbert (1958).
Organizations.
John Wiley and Sons.
ISBN 978-0-471-56793-6.

Simon, Herbert (1990).
"
A mechanism for social selection and successful altruism".
Science.
250 (4988)
: 1665–8.
Bibcode:1990Sci...
250.1665S.
doi:10.1126/science.2270480.
PMID 2270480.

Simon, Herbert (1991). "
Bounded Rationality and Organizational Learning".
Organization Science.
2 (1): 125–134.
doi:10.1287/orsc.2.1.125.

Tisdell, Clem (1996).
Bounded Rationality and Economic Evolution:
A Contribution to Decision Making, Economics, and Management.
Cheltenham, UK: Brookfield.
ISBN 978-1-85898-352-3.

Wheeler, Gregory (2018). "
Bounded Rationality".

In Edward Zalta (ed.).
Stanford Encyclopedia of Philosophy.
Stanford, CA.

Williamson, Oliver E. (1981).
"
The economics of organization: the transaction cost approach".
American Journal of Sociology.
87 (3): 548–577 (press +).
doi:10.1086/227496.
S2CID 154070008.

==
External links ==
Bounded Rationality in Stanford Encyclopedia of Philosophy
Mapping Bounded Rationality by Daniel Kahneman
Artificial Intelligence and Economic Theory chapter 7 of Surfing Economics by Huw Dixon.

"Resource Bounded Agents".
Internet Encyclopedia of Philosophy.
Social psychology is the scientific study of how the thoughts, feelings, and behaviors of individuals are influenced by the actual, imagined, and implied presence of others, 'imagined' and 'implied presences' referring to the internalized social norms that humans are influenced by even when alone.
Social psychologists typically explain human behavior as being a result of the relationship between mental state and social situation, studying the conditions under which thoughts, feelings, and behaviors occur and how these variables influence social interactions.

Social psychology has bridged the gap between psychology and sociology to an extent, but a divide still exists between the two fields.
Nevertheless, sociological approaches to psychology remain an important counterpart to conventional psychological research.
In addition to the split between psychology and sociology, there is difference in emphasis between American and European social psychologists, as the former traditionally have focused more on the individual, whereas the latter have generally paid more attention to group-level phenomena.

==
History ==
Although issues in social psychology already had been discussed in philosophy for much of human history—such as the writings of the Islamic philosopher Al-Farabi, which dealt with similar issues—the modern, scientific discipline began in the United States at the end of the 19th century.

===
19th century ===
In the 19th century, social psychologist was an emerging field from the larger field of psychology.
At the time, many psychologists were concerned with developing concrete explanations for the different aspects of human nature.
They attempted to discover concrete cause-and-effect relationships that explained social interactions.
In order to do so, they applied the scientific method to human behavior.
The first published study in the field was Norman Triplett's 1898 experiment on the phenomenon of social facilitation.
These psychological experiments later went on to form the foundation of much of 20th century social psychological findings.

===
Early 20th century ===
During the 1930s, many Gestalt psychologists, most notably Kurt Lewin, fled to the United States from Nazi Germany.
They were instrumental in developing the field as an area separate from the dominant behavioral and psychoanalytic schools of that time.
Attitudes and small group phenomena were the topics most commonly studied in this era.
During World War II, social psychologists were primarily engaged with studies of persuasion and propaganda for the U.S. military (see also psychological warfare).
Following the war, researchers became interested in a variety of social problems, including issues of gender and racial prejudice.
Most notable and contentious of these were the Milgram experiments.
During the years immediately following World War II, there were frequent collaborations between psychologists and sociologists.
The two disciplines, however, have become increasingly specialized and isolated from each other in recent years, with sociologists generally focusing on macro features whereas psychologists generally focusing on more micro features.

===
Late 20th century and modernity ==
=
In the 1960s, there was growing interest in topics such as cognitive dissonance, bystander intervention, and aggression.
By the 1970s, however, social psychology in America had reached a crisis, as heated debates emerged over issues such as ethical concerns about laboratory experimentation, whether attitude could actually predict behavior, and how much science could be done in a cultural context.
This was also a time when situationism came to challenge the relevance of self and personality in psychology.
Throughout the 1980s and 1990s, social psychology reached a more mature level, especially in regard to theory and methodology.
Now, careful ethical standards regulate research, and pluralistic and multicultural perspectives have emerged.
Modern researchers are interested in many phenomena, though attribution, social cognition, and the self-concept are perhaps the areas of greatest growth in recent years.
Social psychologists have also maintained their applied interests with contributions in the social psychology of health, education, law, and the workplace.

==
Intrapersonal phenomena ==


===
Attitudes ===
In social psychology, attitude is defined as learned, global evaluations (e.g. of people or issues) that influence thought and action.
Attitudes are basic expressions of approval and disapproval, or as Bem (1970) suggests, likes and dislikes
(e.g. enjoying chocolate ice cream, or endorsing the values of a particular political party).
Because people are influenced by other factors in any given situation, general attitudes are not always good predictors of specific behavior.
For example, a person may value the environment but may not recycle a plastic bottle on a particular day.

Research on attitudes has examined the distinction between traditional, self-reported attitudes and implicit, unconscious attitudes.
Experiments using the implicit-association test, for instance, have found that people often demonstrate implicit bias against other races, even when their explicit responses profess equal mindedness.
Likewise, one study found that in interracial interactions, explicit attitudes correlate with verbal behavior while implicit attitudes correlate with nonverbal behavior.
One hypothesis on how attitudes are formed, first proposed in 1983 by Abraham Tesser, is that strong likes and dislikes are ingrained in our genetic make-up.
Tesser speculated that individuals are disposed to hold certain strong attitudes as a result of inborn personality traits and physical, sensory, and cognitive skills.
Attitudes are also formed as a result of exposure to different experiences, environments, and through the learning process.
Numerous studies have shown that people can form strong attitudes toward neutral objects that are in some way linked to emotionally charged stimuli.
Attitudes are also involved in several other areas of the discipline, such as conformity, interpersonal attraction, social perception, and prejudice.

===
Persuasion ===
Persuasion is an active method of influencing that attempts to guide people toward the adoption of an attitude, idea, or behavior by rational or emotive means.
Persuasion relies on appeals rather than strong pressure or coercion.
The process of persuasion has been found to be influenced by numerous variables that generally fall into one of five major categories:
Communicator: includes credibility, expertise, trustworthiness, and attractiveness.

Message: includes varying degrees of reason, emotion (e.g. fear), one-sided or two sided arguments, and other types of informational content.

Audience: includes a variety of demographics, personality traits, and preferences.

Channel/medium:
includes printed word, radio, television, the internet, or face-to-face interactions.

Context: includes environment, group dynamics, and preliminary information to that of Message (category #2).Dual-process theories of persuasion (such as the elaboration likelihood model) maintain that persuasion is mediated by two separate routes: central and peripheral.
The central route of persuasion is more fact-based and results in longer-lasting change, but requires motivation to process.
The peripheral route is more superficial and results in shorter-lasting change, but does not require as much motivation to process.
An example of peripheral persuasion is a politician using a flag lapel pin, smiling, and wearing a crisp, clean shirt.
This does not require motivation to be persuasive, but should not last as long as central persuasion.
If that politician were to outline what they believe and their previous voting record, he would be centrally persuasive, resulting in longer-lasting change at the expense of greater motivation required for processing.

===
Social cognition ===
Social cognition studies how people perceive, think about, and remember information about others.
Much research rests on the assertion that people think about other people differently from non-social targets.
This assertion is supported by the social-cognitive deficits exhibited by people with Williams syndrome and autism.
Person perception is the study of how people form impressions of others.
The study of how people form beliefs about each other while interacting is interpersonal perception.

A major research topic in social cognition is attribution.
Attributions are how we explain people's behavior, either our own behavior or the behavior of others.
One element of attribution ascribes the cause of a behavior to internal and external factors.
An internal, or dispositional, attribution reasons that behavior is caused by inner traits such as personality, disposition, character, and ability.
An external, or situational, attribution reasons that behaviour is caused by situational elements such as the weather.
A second element of attribution ascribes the cause of behavior to stable and unstable factors (i.e. whether the behavior will be repeated or changed under similar circumstances).
Individuals also attribute causes of behavior to controllable and uncontrollable factors (i.e. how much control one has over the situation at hand).

Numerous biases in the attribution process have been discovered.
For instance, the fundamental attribution error is the tendency to make dispositional attributions for behavior, overestimating the influence of personality and underestimating the influence of the situational.
The actor-observer bias is a refinement of this; it is the tendency to make dispositional attributions for other people's behavior and situational attributions for our own.
The self-serving bias is the tendency to attribute dispositional causes for successes, and situational causes for failure, particularly when self-esteem is threatened.
This leads to assuming one's successes are from innate traits, and one's failures are due to situations.
Other ways people protect their self-esteem are by believing in a just world, blaming victims for their suffering, and making defensive attributions that explain our behavior in ways that defend us from feelings of vulnerability and mortality.
Researchers have found that mildly depressed individuals often lack this bias and actually have more realistic perceptions of reality as measured by the opinions of others.

==== Heuristics ====
Heuristics are cognitive shortcuts.
Instead of weighing all the evidence when making a decision, people rely on heuristics to save time and energy.
The availability heuristic occurs when people estimate the probability of an outcome based on how easy that outcome is to imagine.
As such, vivid or highly memorable possibilities will be perceived as more likely than those that are harder to picture or difficult to understand, resulting in a corresponding cognitive bias.
The representativeness heuristic is a shortcut people use to categorize something based on how similar it is to a prototype they know of.
Numerous other biases have been found by social cognition researchers.
The hindsight bias is a false memory of having predicted events, or an exaggeration of actual predictions, after becoming aware of the outcome.
The confirmation bias is a type of bias leading to the tendency to search for or interpret information in a way that confirms one's preconceptions.

====
Schemas ====
Another key concept in social cognition is the assumption that reality is too complex to easily discern.
As a result, we tend to see the world according to simplified schemas or images of reality.
Schemas are generalized mental representations that organize knowledge and guide information processing.
Schemas often operate automatically and unintentionally, and can lead to biases in perception and memory.
Schemas may induce expectations that lead us to see something that is not there.
One experiment found that people are more likely to misperceive a weapon in the hands of a black man than a white man.
This type of schema is a stereotype, a generalized set of beliefs about a particular group of people (when incorrect, an ultimate attribution error).
Stereotypes are often related to negative or preferential attitudes (prejudice) and behavior (discrimination).
Schemas for behaviors (e.g., going to a restaurant, doing laundry) are known as scripts.

===
Self-concept ===
Self-concept is the whole sum of beliefs that people have about themselves.
The self-concept is made up of cognitive aspects called self-schemas—
beliefs that people have about themselves and that guide the processing of self-referential information.
For example, an athlete at a university would have multiple selves that would process different information pertinent to each self: the student would be oneself, who would process information pertinent to a student (taking notes in class, completing a homework assignment, etc.);
the athlete would be the self who processes information about things related to being an athlete (recognizing an incoming pass, aiming a shot, etc.).
These selves are part of one's identity and the self-referential information is that which relies on the appropriate self to process and react to it.
If a self is not part of one's identity, then it is much more difficult for one to react.
For example, a civilian may not know how to handle a hostile threat as well as a trained Marine would.
The Marine contains a self that would enable him/her to process the information about the hostile threat and react accordingly, whereas a civilian may not contain that self, lessening the civilian's ability to properly assess the threat and act accordingly.

The self-concept comprises multiple self-schemas.
For example, people whose body image is a significant self-concept aspect are considered schematics with respect to weight.
In contrast, people who do not regard their weight as an important part of their lives are aschematic with respect to that attribute.
For individuals, a range of otherwise mundane events—grocery shopping, new clothes, eating out, or going to the beach—can trigger thoughts about the self.
The self is a special object of our attention.
Whether one is mentally focused on a memory, a conversation, a foul smell,
the song that is stuck in one's head, or this sentence
, consciousness is like a spotlight.
This spotlight can shine on only one object at a time, but it can switch rapidly from one object to another.
In this spotlight the self is front and center: things relating to the self have the spotlight more often.
The ABCs of self are:
Affect (i.e. emotion): How do people evaluate themselves, enhance their self-image, and maintain a secure sense of identity?

Behavior: How do people regulate their own actions and present themselves to others according to interpersonal demands?

Cognition: How do individuals become themselves, build a self-concept, and uphold a stable sense of identity?Affective forecasting
is the process of predicting how one would feel in response to future emotional events.
Studies done in 2003 by Timothy Wilson and Daniel Gilbert
have shown that people overestimate the strength of their reactions to anticipated positive and negative life events, more than they actually feel when the event does occur.
There are many theories on the perception of our own behavior.
Leon Festinger's 1954 social comparison theory is that people evaluate their own abilities and opinions by comparing themselves to others when they are uncertain of their own ability or opinions.
Daryl Bem's 1972 self-perception theory claims that when internal cues are difficult to interpret, people gain self-insight by observing their own behavior.
There is also the facial feedback hypothesis: changes in facial expression can lead to corresponding changes in emotion.
The self-concept is often divided into a cognitive component, known as the self-schema, and an evaluative component, the self-esteem.
The need to maintain a healthy self-esteem is recognized as a central human motivation.
Self-efficacy beliefs are associated with the self-schema.
These are expectations that performance of some task will be effective and successful.
Social psychologists also study such self-related processes as self-control and self-presentation.
People develop their self-concepts by various means, including introspection, feedback from others, self-perception, and social comparison.
By comparing themselves to others, people gain information about themselves, and they make inferences that are relevant to self-esteem.
Social comparisons can be either upward or downward, that is, comparisons to people who are either higher or lower in status or ability.
Downward comparisons are often made in order to elevate self-esteem.
Self-perception is a specialized form of attribution that involves making inferences about oneself after observing one's own behavior.
Psychologists have found that too many extrinsic rewards (e.g. money) tend to reduce intrinsic motivation through the self-perception process, a phenomenon known as overjustification.
People's attention is directed to the reward, and they lose interest in the task when the reward is no longer offered.
This is an important exception to reinforcement theory.

==
Interpersonal phenomena ==


===
Social influence ===
Social influence is an overarching term that denotes the persuasive effects people have on each other.
It is seen as a fundamental value in social psychology.
The study of it overlaps considerably with research into attitudes and persuasion.
The three main areas of social influence include: conformity, compliance, and obedience.
Social influence is also closely related to the study of group dynamics, as most effects of influence are strongest when they take place in social groups.

The first major area of social influence is conformity.
Conformity is defined as the tendency to act or think like other members of a group.
The identity of members within a group (i.e. status), similarity, expertise, as well as cohesion, prior commitment, and accountability to the group help to determine the level of conformity of an individual.
Individual variations among group members plays a key role in the dynamic of how willing people will be to conform.
Conformity is usually viewed as a negative tendency in American culture, but a certain amount of conformity is adaptive in some situations, as is nonconformity in other situations.

The second major area of social influence research is compliance, which refers to any change in behavior that is due to a request or suggestion from another person.
The foot-in-the-door technique is a compliance method in which the persuader requests a small favor and then follows up with requesting a larger favor, e.g., asking for the time and then asking for ten dollars.
A related trick is the bait and switch.
The third major form of social influence is obedience; this is a change in behavior that is the result of a direct order or command from another person.
Obedience as a form of compliance was dramatically highlighted by the Milgram study, wherein people were ready to administer shocks to a person in distress on a researcher's command.
An unusual kind of social influence is the self-fulfilling prophecy.
This is a prediction that, in being made, causes itself to become true.
For example, in the stock market, if it is widely believed that a crash is imminent, investors may lose confidence, sell most of their stock, and thus cause a crash.
Similarly, people may expect hostility in others and induce this hostility by their own behavior.
Psychologists have spent decades studying the power of social influence, and the way in which it manipulates people's opinions and behavior.
Specifically, social influence refers to the way in which individuals change their ideas and actions to meet the demands of a social group, received authority, social role, or a minority within a group wielding influence over the majority.

===
Group dynamics ===
A group can be defined as two or more individuals who are connected to each another by social relationships.
Groups tend to interact, influence each other, and share a common identity.
They have a number of emergent qualities that distinguish them from coincidental, temporary gatherings, which are termed social aggregates:
Norms:
Implicit rules and expectations for group members to follow (e.g. saying thank you, shaking hands).

Roles:
Implicit rules and expectations for specific members within the group (e.g. the oldest sibling, who may have additional responsibilities in the family).

Relations:
Patterns of liking within the group, and also differences in prestige or status (e.g. leaders, popular people).Temporary groups and aggregates share few or none of these features and do not qualify as true social groups.
People waiting in line to get on a bus, for example, do not constitute a group.
Groups are important not only because they offer social support, resources, and a feeling of belonging, but because they supplement an individual's self-concept.
To a large extent, humans define themselves by the group memberships which form their social identity.
The shared social identity of individuals within a group influences intergroup behavior, which denotes the way in which groups behave towards and perceive each other.
These perceptions and behaviors in turn define the social identity of individuals within the interacting groups.
The tendency to define oneself by membership in a group may lead to intergroup discrimination, which involves favorable perceptions and behaviors directed towards the in-group, but negative perceptions and behaviors directed towards the out-group.
On the other hand, such discrimination and segregation may sometimes exist partly to facilitate a diversity that strengthens society.
Intergroup discrimination leads to prejudicial stereotyping, while the processes of social facilitation and group polarization encourage extreme behaviors towards the out-group.

Groups often moderate and improve decision making, and are frequently relied upon for these benefits, such as in committees and juries.
A number of group biases, however, can interfere with effective decision making.
For example, group polarization, formerly known as the "risky shift", occurs when people polarize their views in a more extreme direction after group discussion.
More problematic is the phenomenon of groupthink, which is a collective thinking defect that is characterized by a premature consensus or an incorrect assumption of consensus, caused by members of a group failing to promote views that are not consistent with the views of other members.
Groupthink occurs in a variety of situations, including isolation of a group and the presence of a highly directive leader.
Janis offered the 1961 Bay of Pigs Invasion as a historical case of groupthink.
Groups also affect performance and productivity.
Social facilitation, for example, is a tendency to work harder and faster in the presence of others.
Social facilitation increases the dominant response's likelihood, which tends to improve performance on simple tasks and reduce it on complex tasks.
In contrast, social loafing is the tendency of individuals to slack off when working in a group.
Social loafing is common when the task is considered unimportant and individual contributions are not easy to see.
Social psychologists study group-related (collective) phenomena such as the behavior of crowds.
An important concept in this area is deindividuation, a reduced state of self-awareness that can be caused by feelings of anonymity.
Deindividuation is associated with uninhibited and sometimes dangerous behavior.
It is common in crowds and mobs, but it can also be caused by a disguise, a uniform, alcohol, dark environments, or online anonymity.
===
Interpersonal attraction ===
A major area of study of people's relations to each other is interpersonal attraction, which refers to all forces that lead people to like each other, establish relationships, and (in some cases) fall in love.
Several general principles of attraction have been discovered by social psychologists.
One of the most important factors in interpersonal attraction is how similar two particular people are.
The more similar two people are in general attitudes, backgrounds, environments, worldviews, and other traits, the more likely they will be attracted to each other.
Physical attractiveness is an important element of romantic relationships, particularly in the early stages characterized by high levels of passion.
Later on, similarity and other compatibility factors become more important, and the type of love people experience shifts from passionate to companionate.
In 1986, Robert Sternberg suggested that there are actually three components of love: intimacy, passion, and commitment.
When two (or more) people experience all three, they are said to be in a state of consummate love.

According to social exchange theory, relationships are based on rational choice and cost-benefit analysis.
A person may leave a relationship if their partner's "costs" begin to outweigh their benefits, especially if there are good alternatives available.
This theory is similar to the minimax principle proposed by mathematicians and economists (despite the fact that human relationships are not zero-sum games).
With time, long-term relationships tend to become communal rather than simply based on exchange.

==
Research ==


===
Methods ===
Social psychology is an empirical science that attempts to answer questions about human behavior by testing hypotheses, both in the laboratory and in the field.
Careful attention to research design, sampling, and statistical analysis is important; results are published in peer-reviewed journals such as the Journal of Experimental Social Psychology, Personality and Social Psychology Bulletin and the Journal of Personality and Social Psychology.
Social psychology studies also appear in general science journals such as Psychological Science and Science.

Experimental methods involve the researcher altering a variable in the environment and measuring the effect on another variable.
An example would be allowing two groups of children to play violent or nonviolent videogames and then observing their subsequent level of aggression during the free-play period.
A valid experiment is controlled and uses random assignment.

Correlational methods examine the statistical association between two naturally occurring variables.
For example, one could correlate the number of violent television shows children watch at home with the number of violent incidents the children participate in at school.
Note that this study would not prove that violent TV causes aggression in children: it is quite possible that aggressive children choose to watch more violent TV.

Observational methods are purely descriptive and include naturalistic observation, contrived observation, participant observation, and archival analysis.
These are less common in social psychology but are sometimes used when first investigating a phenomenon.
An example would be to unobtrusively observe children on a playground (with a videocamera, perhaps) and record the number and types of aggressive actions displayed.

Whenever possible, social psychologists rely on controlled experimentation, which requires the manipulation of one or more independent variables in order to examine the effect on a dependent variable.
Experiments are useful in social psychology because they are high in internal validity, meaning that they are free from the influence of confounding or extraneous variables, and so are more likely to accurately indicate a causal relationship.
However, the small samples used in controlled experiments are typically low in external validity, or the degree to which the results can be generalized to the larger population.
There is usually a trade-off between experimental control (internal validity) and being able to generalize to the population (external validity).

Because it is usually impossible to test everyone, research tends to be conducted on a sample of persons from the wider population.
Social psychologists frequently use survey research when they are interested in results that are high in external validity.
Surveys use various forms of random sampling to obtain a sample of respondents that is representative of a population.
This type of research is usually descriptive or correlational because there is no experimental control over variables.
Some psychologists have raised concerns for social psychological research relying too heavily on studies conducted on university undergraduates in academic settings, or participants from crowdsourcing labor markets such as Amazon Mechanical Turk.
In a 1986 study by David O. Sears, over 70% of experiments used North American undergraduates as subjects, a subset of the population that is unrepresentative of the population as a whole.
Regardless of which method has been chosen, the significance of the results is reviewed before accepting them in evaluating an underlying hypothesis.
There are two different types of tests that social psychologists use to review their results.
Statistics and probability testing define what constitutes a significant finding, which can be as low as 5% or less, that is unlikely due to chance.
Replications testing is important in ensuring that the results are valid and not due to chance.
False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are a hazard in the field.

===
Famous experiments ===


====
Asch conformity experiments ====
The Asch conformity experiments demonstrated the power of the impulse to conform within small groups, by the use of a line-length estimation task that was designed to be easy to assess but where deliberately wrong answers were given by at least some, oftentimes most, of the other participants.
In well over a third of the trials, participants conformed to the majority, even though the majority judgment was clearly wrong.
Seventy-five percent of the participants conformed at least once during the experiment.
Additional manipulations of the experiment showed that participant conformity decreased when at least one other individual failed to conform but increased when the individual began conforming or withdrew from the experiment.
Also, participant conformity increased substantially as the number of "incorrect" individuals increased from one to three, and remained high as the incorrect majority grew.
Participants with three other, incorrect participants made mistakes 31.8% of the time, while those with one or two incorrect participants made mistakes only 3.6% and 13.6% of the time, respectively.

==== Festinger (cognitive dissonance) ====
In Leon Festinger's cognitive dissonance experiment, after being divided into two groups participants were asked to perform a boring task and later asked to dishonestly give their opinion of the task, afterwards being rewarded according to two different pay scales.
At the study's end, some participants were paid $1 to say that they enjoyed the task and another group of participants was paid $20 to tell the same lie.
The first group ($1) later reported liking the task better than the second group ($20).
Festinger's explanation was that for people in the first group being paid only $1 is not sufficient incentive for lying and those who were paid $1 experienced dissonance.
They could only overcome that dissonance by justifying their lies by changing their previously unfavorable attitudes about the task.
Being paid $20 provides a reason for doing the boring task resulting in no dissonance.

====
Milgram experiment ====
The Milgram experiment was designed to study how far people would go in obeying an authority figure.
Following the events of The Holocaust in World War II, the experiment showed that normal American citizens were capable of following orders even when they believed they were causing an innocent person to suffer.

====
Stanford prison experiment ====
Philip Zimbardo's Stanford prison study, a simulated exercise involving students playing at being prison guards and inmates, ostensibly showed how far people would go in such role playing.
In just a few days, the guards became brutal and cruel, and the prisoners became miserable and compliant.
This was initially argued to be an important demonstration of the power of the immediate social situation and its capacity to overwhelm normal personality traits.
Subsequent research has contested the initial conclusions of the study.
For example, it has been pointed out that participant self-selection may have affected the participants' behavior, and that the participants' personalities influenced their reactions in a variety of ways, including how long they chose to remain in the study.
The 2002 BBC prison study, designed to replicate the conditions in the Stanford study, produced conclusions that were drastically different from the initial findings.

====
Others ====
Muzafer Sherif's robbers' cave study divided boys into two competing groups to explore how much hostility and aggression would emerge.
Sherif's explanation of the results became known as realistic group conflict theory, because the intergroup conflict was induced through competition for resources.
Inducing cooperation and superordinate goals later reversed this effect.

Albert Bandura's Bobo doll experiment demonstrated how aggression is learned by imitation.
This set of studies fueled debates regarding media violence, a topic that continues to be debated among scholars.

===
Ethics ===
The goal of social psychology is to understand cognition and behavior as they naturally occur in a social context, but the very act of observing people can influence and alter their behavior.
For this reason, many social psychology experiments utilize deception to conceal or distort certain aspects of the study.
Deception may include false cover stories, false participants (known as confederates or stooges),
false feedback given to the participants, and so on.
The practice of deception has been challenged by psychologists who maintain that deception under any circumstances is unethical and that other research strategies (e.g., role-playing) should be used instead.
Unfortunately, research has shown that role-playing studies do not produce the same results as deception studies, and this has cast doubt on their validity.
In addition to deception, experimenters have at times put people into potentially uncomfortable or embarrassing situations
(e.g., the Milgram experiment and Stanford prison experiment), and this has also been criticized for ethical reasons.

To protect the rights and well-being of research participants, and at the same time discover meaningful results and insights into human behavior, virtually all social psychology research must pass an ethical review.
At most colleges and universities, this is conducted by an ethics committee or Institutional Review Board, which examines the proposed research to make sure that no harm is likely to come to the participants, and that the study's benefits outweigh any possible risks or discomforts to people taking part.

Furthermore, a process of informed consent is often used to make sure that volunteers know what will asked of them in the experiment and understand that they are allowed to quit the experiment at any time.
A debriefing is typically done at the experiment's conclusion in order to reveal any deceptions used and generally make sure that the participants are unharmed by the procedures.
Today, most research in social psychology involves no more risk of harm than can be expected from routine psychological testing or normal daily activities.

===
Adolescents ===
Social psychology studies what plays key roles in a child's development.
During this time, teens are faced with many issues and decisions that can impact their social development.
They are faced with self-esteem issues, peer pressure, drugs, alcohol, tobacco, sex, and social media.
Psychologists today are not fully aware of the effect of social media.
Social media is worldwide, so one can be influenced by something they will never encounter in real life.
In 2019, social media became the single most important activity in adolescents' and even some older adults' lives.

===
Replication crisis ===
Many social psychological research findings have proven difficult to replicate, leading some to argue that social psychology is undergoing a replication crisis.
Replication failures are not unique to social psychology and are found in all fields of science.
Some factors have been identified in social psychological research that has led the field to undergo its current crisis.

Firstly, questionable research practices have been identified as common.
Such practices, while not necessarily intentionally fraudulent, involve converting undesired statistical outcomes into desired outcomes via the manipulation of statistical analyses, sample sizes, or data management systems, typically to convert non-significant findings into significant ones.
Some studies have suggested that at least mild versions of these practices are prevalent.
One of the criticisms of Daryl Bem in the feeling the future controversy is that the evidence for precognition in the study could be attributed to questionable practices.

Secondly, some social psychologists have published fraudulent research that has entered into mainstream academia, most notably the admitted data fabrication by Diederik Stapel as well as allegations against others.
Fraudulent research is not the main contributor to the replication crisis.
Several effects in social psychology have been found to be difficult to replicate even before the current replication crisis.
For example, the scientific journal Judgment and Decision Making has published several studies over the years that fail to provide support for the unconscious thought theory.
Replications appear particularly difficult when research trials are pre-registered and conducted by research groups not highly invested in the theory under questioning.

These three elements together have resulted in renewed attention to replication supported by Daniel Kahneman.
Scrutiny of many effects have shown that several core beliefs are hard to replicate.
A 2014 special edition of Social Psychology focused on replication studies, and a number of previously held beliefs were found to be difficult to replicate.
Likewise, a 2012 special edition of Perspectives on Psychological Science focused on issues ranging from publication bias to null-aversion that contribute to the replication crisis in psychology.
It is important to note that this replication crisis does not mean that social psychology is unscientific.
Rather, this reexamination is
a healthy if sometimes acrimonious part of the scientific process in which old ideas or those that cannot withstand careful scrutiny are pruned.
The consequence is that some areas of social psychology once considered solid, such as social priming, have come under increased scrutiny due to failure to replicate findings.

===
Academic journals ===


==
See also ==
== Notes ==


==
References ==


==
External links ==
Social Psychology Network
Introduction to Social Psychology
Social Psychology — basics
Social psychology  on PLOS —
subject area page
Social psychology on All About Psychology — information and resources
page
What is Social Psychology?
on YouTube
Cognitive biases are systematic patterns of deviation from norm and/or rationality in judgment.
They are often studied in psychology and behavioral economics.
Although the reality of most of these biases is confirmed by reproducible research, there are often controversies about how to classify these biases or how to explain them.
Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing).
Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.
Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments.
Biases have a variety of forms and appear as cognitive ("cold") bias, such as mental noise, or motivational ("hot") bias, such as when beliefs are distorted by wishful thinking.
Both effects can be present at the same time.
There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior.
For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person.
However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.
Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well.
For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.

==
Belief, decision-making and behavioral ==
These biases affect belief formation, reasoning processes, business and economic decisions, and human behavior in general.

==
Social ==


== Memory ==
In psychology and cognitive science, a memory bias is a cognitive bias that either enhances or impairs the recall of a memory (either the chances that the memory will be recalled at all, or the amount of time it takes for it to be recalled, or both), or that alters the content of a reported memory.
There are many types of memory bias, including:


==
See also ==


==
Footnotes ==


==
References ==
Gerd Gigerenzer (born September 3, 1947, Wallersdorf, Germany) is a German psychologist who has studied the use of bounded rationality and heuristics in decision making.
Gigerenzer is director emeritus of the Center for Adaptive Behavior and Cognition (ABC) at the Max Planck Institute for Human Development and director of the Harding Center for Risk Literacy, both in Berlin, Germany.

Gigerenzer investigates how humans make inferences about their world with limited time and knowledge.

He proposes that, in an uncertain world, probability theory is not sufficient; people also use smart heuristics, that is, rules of thumb.

He conceptualizes rational decisions in terms of the adaptive toolbox (the repertoire of heuristics an individual or institution has) and the ability to choose a good heuristics for the task at hand.

A heuristic is called ecologically rational to the degree that it is adapted to the structure of an environment.

Gigerenzer argues that heuristics are not irrational or always second-best to optimization, as the accuracy-effort trade-off view assumes, in which heuristics are seen as short-cuts that trade less effort for less accuracy.

In contrast, his and associated researchers' studies have identified situations in which "less is more", that is, where heuristics make more accurate decisions with less effort.

This contradicts the traditional view that more information is always better or at least can never hurt if it is free.
Less-is-more effects have been shown experimentally, analytically, and by computer simulations.

==
Biography ==


===
Academic career ===
Gigerenzer received his PhD from the University of Munich in 1977 and became a professor of psychology there the same year.
In 1984 he moved to the University of Konstanz and in 1990 to the University of Salzburg.
From 1992 to 1995 he was Professor of Psychology at the University of Chicago and has been the John M. Olin
Distinguished Visiting Professor, School of Law at the University of Virginia.
In 1995 he became director of the Max Planck Institute for Psychological Research in Munich.
Since 2009 he has been director of the Harding Center for Risk Literacy in Berlin.

Gigerenzer was awarded honorary doctorates from the University of Basel and the Open University of the Netherlands.
He is also Batten Fellow at the Darden Business School, University of Virginia, Fellow of the Berlin-Brandenburg Academy of Sciences and the German Academy of Sciences Leopoldina, and Honorary Member of the American Academy of Arts and Sciences and the American Philosophical Society.

=== Heuristics ===
With Daniel Goldstein he first theorized the recognition heuristic and the take-the-best heuristic.
They proved analytically conditions under which semi-ignorance (lack of recognition) can lead to better inferences than with more knowledge.
These results were experimentally confirmed in many experiments, e.g., by showing that semi-ignorant people who rely on recognition are as good as or better than the Association of Tennis Professionals (ATP) Rankings and experts at predicting the outcomes of the Wimbledon tennis tournaments.
Similarly, decisions by experienced experts (e.g., police, professional burglars, airport security) were found to follow the take-the-best heuristic rather than weight and add all information, while inexperienced students tend to do the latter.
A third class of heuristics, Fast-And-Frugal trees, are designed for categorization and are used for instance in emergency units to predict heart attacks, and model bail decisions made by magistrates in London courts.
In such cases, the risks are not knowable and professionals hence face uncertainty.
To better understand the logic of Fast-And-Frugal trees and other heuristics, Gigerenzer and his colleagues use the strategy of mapping its concepts onto those of well-understood optimization theories, such as signal-detection theory.

A critic of the work of Daniel Kahneman and Amos Tversky, Gigerenzer argues that heuristics should not lead us to conceive of human thinking as riddled with irrational cognitive biases, but rather to conceive rationality as an adaptive tool that is not identical to the rules of formal logic or the probability calculus.
He and his collaborators have theoretically and experimentally shown that many cognitive fallacies are better understood as adaptive responses to a world of uncertainty—such as the conjunction fallacy, the base rate fallacy, and overconfidence.

===
The adaptive toolbox ===
The basic idea of the adaptive toolbox is that different domains of thought require different specialized cognitive mechanisms instead of one universal strategy.
The analysis of the adaptive toolbox and its evolution is descriptive research with the goal of specifying the core cognitive capacities (such as recognition memory) and the heuristics that exploit these (such as the recognition heuristic).

===
Risk communication ===
Alongside his research on heuristics, Gigerenzer investigates risk communication in situations where risks can actually be calculated or precisely estimated.
He has developed an ecological approach to risk communication where the key is the match between cognition and the presentation of the information in the environment.
For instance, lay people as well as professionals often have problems making Bayesian inferences, typically committing what has been called the base-rate fallacy in the cognitive illusions literature.
Gigerenzer and Ulrich Hoffrage were the first to develop and test a representation called natural frequencies, which helps people make Bayesian inferences correctly without any outside help.
Later it was shown that with this method, even 4th graders were able to make correct inferences.
Once again, the problem is not simply in the human mind, but in the representation of the information.
Gigerenzer has taught risk literacy to some 1,000 doctors in their CMU and some 50 US federal judges, and natural frequencies has now entered the vocabulary of evidence-based medicine.
In recent years, medical schools around the world have begun to teach tools such as natural frequencies to help young doctors understand test results.

==
Intellectual background ==
Intellectually, Gigerenzer's work is rooted in Herbert Simon's work on satisficing (as opposed to maximizing) and on ecological and evolutionary views of cognition, where adaptive function and success is central, as opposed to logical structure and consistency, although the latter can be means towards function.

Gigerenzer and colleagues write of the mid-17th century "probabilistic revolution", "the demise of the dream of certainty and the rise of a calculus of uncertainty – probability theory".
Gigerenzer calls for a second revolution, "replacing the image of an omniscient mind computing intricate probabilities and utilities with that of a bounded mind reaching into an adaptive toolbox filled with fast and frugal heuristics".
These heuristics would equip humans to deal more specifically with the many situations they face in which not all alternatives and probabilities are known, and surprises can happen.

==
Personal ==
Gigerenzer is a jazz and Dixieland musician.
He was part of The Munich Beefeaters Dixieland Band which performed in a TV ad for the VW Golf around the time it came out in 1974.
The ad can be viewed on YouTube, with Gigerenzer at the steering wheel and on the banjo.

He is married to Lorraine Daston, director at the Max Planck Institute for the History of Science and has one daughter, Thalia Gigerenzer.

==
Awards ==
Gigerenzer is recipient of the AAAS Prize for Behavioral Science Research for the best article in the behavioral sciences, the Association of American Publishers Prize for the best book in the social and behavioral sciences, the German Psychology Prize, and the Communicator Award of the German Research Association (DFG), among others.
(
See the German Wikipedia entry, Gerd Gigerenzer, for an extensive list of honors and awards.)
==
Publications ==
===
Books ===
Cognition as intuitive statistics (1987) with David Murray
The Empire of Chance:
How Probability Changed Science and Everyday Life (1989)
Simple Heuristics That Make Us Smart (1999)
Bounded Rationality:
The Adaptive Toolbox (2001) with Reinhard Selten
Reckoning with Risk: Learning to Live with Uncertainty (2002, published in the U.S. as Calculated Risks: How to Know When Numbers Deceive You)
Gut Feelings:
The Intelligence of the Unconscious (2007)
Rationality for Mortals (2008)
Heuristics:
The Foundation of Adaptive Behavior (2011) with Ralph Hertwig & Torsten Pachur
Risk Savvy:
How to Make Good Decisions (2014)
Simply Rational:
Decision Making in the Real World
(2015)


== Video ==
Video on Gerd Gigerenzer's research (Latest Thinking)


==
See also ==


==
References ==


==
External links ==
Resume
Books
Edge.org
bio
Article:
Simple tools for understanding risks: from innumeracy to insight
Harding Center for Risk Literacy
Gerd Gigerenzer in the German National Library catalogue
